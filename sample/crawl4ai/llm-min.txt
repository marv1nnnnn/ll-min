#LIB:llm-min#VER:1.0#DATE:2024-07-26T12:00:00Z
#SCHEMA_DEF_BEGIN
AIU_FIELDS:id;typ;name;purp;in;out;use;rel;src
IN_FIELDS:p;t;d;def;ex
OUT_FIELDS:f;t;d
REL_FIELDS:id;typ
#SCHEMA_DEF_END
#AIU_LIST_BEGIN
#AIU#id:c4ai_0001;typ:Feat;name:Core Crawling;purp:"Perform basic web page fetching.";in:[{p:url;t:str;d:The URL to crawl.;ex:https://example.com},{p:config;t:AIU_c4ai_0003;d:Crawler run configuration.;ex:AIU_c4ai_0003}];out:[{f:result;t:AIU_c4ai_0005;d:The result of the crawl.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url='https://example.com')\n        print(result.markdown[:300])\nasyncio.run(main())";rel:[{id:c4ai_0002;typ:U},{id:c4ai_0005;typ:R},{id:c4ai_0003;typ:A}];src:"Quick Start;Simple Crawling"#END_AIU
#AIU#id:c4ai_0002;typ:Cls;name:AsyncWebCrawler;purp:"The main class for asynchronous web crawling, managing browser lifecycle and crawl tasks.";in:[{p:config;t:AIU_c4ai_0004;d:Browser configuration.;ex:AIU_c4ai_0004}];out:[{f:instance;t:AIU_c4ai_0002;d:An AsyncWebCrawler instance.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync with AsyncWebCrawler() as crawler:\n    # Use crawler\n    pass";rel:[{id:c4ai_0001;typ:HM},{id:c4ai_0010;typ:HM},{id:c4ai_0004;typ:A}];src:"Quick Start;AsyncWebCrawler;Simple Crawling"#END_AIU
#AIU#id:c4ai_0003;typ:CfgObj;name:CrawlerRunConfig;purp:"Configures settings specific to a single crawl operation.";in:[{p:cache_mode;t:AIU_c4ai_0011;d:Caching behavior.;ex:AIU_c4ai_0011},{p:check_robots_txt;t:T/F;d:Respect robots.txt rules.;ex:T},{p:word_count_threshold;t:int;d:Min word count for content blocks.;ex:10},{p:css_selector;t:str;d:CSS selector to focus content on.;ex:main.content},{p:js_code;t:list_str;d:JS code to run after page load.;ex:["window.scrollTo(0, document.body.scrollHeight);"]},{p:wait_for;t:str;d:Condition to wait for before extracting.;ex:css:.loaded},{p:screenshot;t:T/F;d:Capture a screenshot.;ex:T},{p:pdf;t:T/F;d:Generate a PDF.;ex:T},{p:extraction_strategy;t:AIU_cai_0026;d:Strategy for structured data extraction.;ex:AIU_cai_0026},{p:session_id;t:str;d:ID to reuse browser session.;ex:my_session}];out:[];use:"from crawl4ai.async_configs import CrawlerRunConfig\nconfig = CrawlerRunConfig(cache_mode='BYPASS', verbose=True)";rel:[{id:c4ai_0001;typ:P},{id:c4ai_0010;typ:P},{id:c4ai_0004;typ:A},{id:c4ai_0011;typ:P},{id:cai_0026;typ:P}];src:"Basic Configuration (Light Introduction);arun() Parameter Guide;Browser, Crawler & LLM Configuration;Simple Crawling;Content Selection"#END_AIU
#AIU#id:c4ai_0004;typ:CfgObj;name:BrowserConfig;purp:"Configures settings for the browser environment.";in:[{p:browser_type;t:str;d:Browser engine.;ex:chromium},{p:headless;t:T/F;d:Run browser without UI.;ex:T},{p:proxy_config;t:dict;d:Proxy server configuration.;ex:{\"server\": \"http://proxy.example.com:8080\"}},{p:user_data_dir;t:str;d:Directory for persistent profile data.;ex:/path/to/profile},{p:storage_state;t:dict;d:Initial session state (cookies, local storage).;ex:{\"cookies\": [...]}}];out:[];use:"from crawl4ai.async_configs import BrowserConfig\nconfig = BrowserConfig(headless=True, browser_type='chromium')";rel:[{id:c4ai_0002;typ:P},{id:c4ai_0003;typ:A},{id:c4ai_0013;typ:P},{id:c4ai_0014;typ:P},{id:c4ai_0016;typ:P},{id:c4ai_0017;typ:P},{id:c4ai_0033;typ:P},{id:c4ai_0034;typ:P}];src:"Basic Configuration (Light Introduction);Browser, Crawler & LLM Configuration;Simple Crawling;Proxy;Session Persistence & Local Storage;Identity Based Crawling"#END_AIU
#AIU#id:c4ai_0005;typ:DataObj;name:CrawlResult;purp:"Contains all output data and metadata from a single crawl.";in:[];out:[{f:url;t:str;d:Final URL after redirects.},{f:html;t:str;d:Original page HTML.},{f:cleaned_html;t:str;d:Sanitized HTML.},{f:markdown;t:AIU_c4ai_0022;d:Markdown output object.},{f:extracted_content;t:str;d:Structured data (JSON) if extraction used.},{f:media;t:dict;d:Info about images, videos, tables.},{f:links;t:dict;d:Internal and external links.},{f:success;t:T/F;d:True if crawl succeeded.},{f:error_message;t:str;d:Details if success is False.},{f:status_code;t:int;d:HTTP status code.}];use:"if result.success:\n    print(result.url)\n    print(result.markdown[:100])\n    print(result.extracted_content)\nelse:\n    print(result.error_message)";rel:[{id:c4ai_0001;typ:R},{id:c4ai_0010;typ:R},{id:c4ai_0007;typ:P},{id:c4ai_0008;typ:P},{id:c4ai_0009;typ:P},{id:c4ai_0015;typ:P},{id:c4ai_0018;typ:P},{id:c4ai_0019;typ:P},{id:c4ai_0020;typ:P},{id:c4ai_0021;typ:P},{id:c4ai_0022;typ:P},{id:c4ai_0023;typ:P},{id:c4ai_0024;typ:P},{id:c4ai_0025;typ:P},{id:c4ai_0030;typ:P},{id:c4ai_0031;typ:P},{id:c4ai_0032;typ:P}];src:"Crawler Result;CrawlResult Reference;Simple Crawling"#END_AIU
#AIU#id:c4ai_0006;typ:HowTo;name:Perform Basic Crawl;purp:"Fetch a web page and get its content.";in:[{p:url;t:str;d:The URL to crawl.;ex:https://example.com}];out:[{f:markdown;t:str;d:The page content in markdown.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url='https://example.com')\n        print(result.markdown[:300])\nasyncio.run(main())";rel:[{id:c4ai_0001;typ:U},{id:c4ai_0002;typ:U}];src:"Quick Start;Simple Crawling"#END_AIU
#AIU#id:c4ai_0007;typ:DataObj;name:CrawlResult.url;purp:"The final URL of the crawled page after redirects.";in:[];out:[{f:url;t:str;d:The URL.}];use:"print(result.url)";rel:[{id:c4ai_0005;typ:P}];src:"CrawlResult Reference"#END_AIU
#AIU#id:c4ai_0008;typ:DataObj;name:CrawlResult.success;purp:"Indicates if the crawl completed without major errors.";in:[];out:[{f:success;t:T/F;d:True for success, False otherwise.}];use:"if result.success:\n    print('Crawl successful')";rel:[{id:c4ai_0005;typ:P}];src:"CrawlResult Reference"#END_AIU
#AIU#id:c4ai_0009;typ:DataObj;name:CrawlResult.error_message;purp:"Provides details if the crawl failed.";in:[];out:[{f:message;t:str;d:The error description.}];use:"if not result.success:\n    print(result.error_message)";rel:[{id:c4ai_0005;typ:P}];src:"CrawlResult Reference"#END_AIU
#AIU#id:c4ai_0010;typ:Func;name:arun_many;purp:"Crawl multiple URLs concurrently or in batches.";in:[{p:urls;t:list_str;d:List of URLs to crawl.;ex:[\"url1\", \"url2\"]},{p:config;t:AIU_c4ai_0003;d:Crawler run configuration.;ex:AIU_c4ai_0003},{p:dispatcher;t:AIU_c4ai_0040;d:Concurrency controller.;ex:AIU_c4ai_0040}];out:[{f:results;t:list_AIU_c4ai_0005;d:List of crawl results (batch mode).},{f:result_generator;t:AIU_c4ai_0005;d:Async generator of crawl results (streaming mode).}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nasync def main():\n    urls = [\"https://example.com/1\", \"https://example.com/2\"]\n    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=True)\n    async with AsyncWebCrawler() as crawler:\n        async for result in crawler.arun_many(urls, config=config):\n            print(f\"Crawled {result.url}: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0002;typ:HM},{id:c4ai_0003;typ:A},{id:c4ai_0005;typ:R},{id:c4ai_0040;typ:A},{id:c4ai_0038;typ:U},{id:c4ai_0039;typ:U}];src:"arun_many(...) Reference;Multi-URL Crawling"#END_AIU
#AIU#id:c4ai_0011;typ:CfgObj;name:CacheMode;purp:"Enum to control caching behavior for a crawl.";in:[];out:[{f:ENABLED;t:AIU_c4ai_0011;d:Read from cache if available, write if not.},{f:DISABLED;t:AIU_c4ai_0011;d:Do not use cache.},{f:READ_ONLY;t:AIU_c4ai_0011;d:Only read from cache, never write.},{f:WRITE_ONLY;t:AIU_c4ai_0011;d:Only write to cache, never read.},{f:BYPASS;t:AIU_c4ai_0011;d:Skip cache read for this operation.}];use:"from crawl4ai import CacheMode\ncache_setting = CacheMode.BYPASS";rel:[{id:c4ai_0003;typ:P}];src:"Cache Modes;Crawl4AI Cache System and Migration Guide;arun() Parameter Guide;Browser, Crawler & LLM Configuration;Simple Crawling"#END_AIU
#AIU#id:c4ai_0012;typ:HowTo;name:Use CacheMode;purp:"Control how the crawler uses the local cache.";in:[{p:cache_mode;t:AIU_c4ai_0011;d:Desired cache behavior.;ex:CacheMode.BYPASS}];out:[];use:"from crawl4ai import CacheMode\nfrom crawl4ai.async_configs import CrawlerRunConfig\nconfig = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n# Pass config to crawler.arun()";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0011;typ:A}];src:"Crawl4AI Cache System and Migration Guide"#END_AIU
#AIU#id:c4ai_0013;typ:ParamSet;name:BrowserConfig.headless;purp:"Run the browser without a visible user interface.";in:[{p:headless;t:T/F;d:Set to True for no UI, False for visible UI.;ex:T}];out:[];use:"from crawl4ai.async_configs import BrowserConfig\nconfig = BrowserConfig(headless=False)";rel:[{id:c4ai_0004;typ:P}];src:"Browser, Crawler & LLM Configuration;BrowserConfig Reference;Simple Crawling"#END_AIU
#AIU#id:c4ai_0014;typ:ParamSet;name:BrowserConfig.proxy_config;purp:"Configure traffic to be routed through a proxy server.";in:[{p:proxy_config;t:dict;d:Proxy details like server, username, password.;ex:{\"server\": \"http://proxy.example.com:8080\", \"username\": \"user\", \"password\": \"pass\"}}];out:[];use:"from crawl4ai.async_configs import BrowserConfig\nconfig = BrowserConfig(proxy_config={\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"myuser\",\n    \"password\": \"mypass\",\n})";rel:[{id:c4ai_0004;typ:P}];src:"Proxy Usage;BrowserConfig Reference;Proxy"#END_AIU
#AIU#id:c4ai_0015;typ:DataObj;name:CrawlResult.pdf;purp:"Raw bytes of the page rendered as a PDF.";in:[];out:[{f:pdf_bytes;t:bytes;d:The PDF data.}];use:"if result.pdf:\n    with open(\"page.pdf\", \"wb\") as f:\n        f.write(result.pdf)";rel:[{id:c4ai_0005;typ:P}];src:"Crawler Result;CrawlResult Reference;Capturing PDFs & Screenshots"#END_AIU
#AIU#id:c4ai_0016;typ:ParamSet;name:BrowserConfig.storage_state;purp:"Load browser session state (cookies, local storage) from a dictionary or file.";in:[{p:storage_state;t:dict;d:Session state dictionary;ex:{\"cookies\": [...]}},{p:storage_state;t:str;d:Path to a session state file;ex:my_storage.json}];out:[];use:"from crawl4ai import AsyncWebCrawler, BrowserConfig\nstorage_dict = {\"cookies\": [...], \"origins\": [...]}\nconfig = BrowserConfig(storage_state=storage_dict)";rel:[{id:c4ai_0004;typ:P}];src:"Session Persistence & Local Storage;BrowserConfig Reference"#END_AIU
#AIU#id:c4ai_0017;typ:ParamSet;name:BrowserConfig.user_data_dir;purp:"Directory for storing a persistent browser profile (cookies, local storage, etc.).";in:[{p:user_data_dir;t:str;d:Path to the user data directory.;ex:/path/to/my-profile}];out:[];use:"from crawl4ai.async_configs import BrowserConfig\nconfig = BrowserConfig(user_data_dir=\"/path/to/my-chrome-profile\", use_managed_browser=True)";rel:[{id:c4ai_0004;typ:P}];src:"Identity Based Crawling;BrowserConfig Reference"#END_AIU
#AIU#id:c4ai_0018;typ:DataObj;name:CrawlResult.screenshot;purp:"Base64 encoded string of the page screenshot.";in:[];out:[{f:screenshot_base64;t:str;d:The screenshot data as base64.}];use:"import base64\nif result.screenshot:\n    with open(\"page.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))";rel:[{id:c4ai_0005;typ:P}];src:"Crawler Result;CrawlResult Reference;Capturing PDFs & Screenshots"#END_AIU
#AIU#id:c4ai_0019;typ:DataObj;name:CrawlResult.ssl_certificate;purp:"Object containing details about the site's SSL certificate.";in:[];out:[{f:certificate;t:AIU_c4ai_0035;d:The SSLCertificate object.}];use:"if result.ssl_certificate:\n    print(\"Issuer CN:\", result.ssl_certificate.issuer.get(\"CN\", \"\"))";rel:[{id:c4ai_0005;typ:P},{id:c4ai_0035;typ:R}];src:"Crawler Result;CrawlResult Reference;Handling SSL Certificates"#END_AIU
#AIU#id:c4ai_0020;typ:DataObj;name:CrawlResult.network_requests;purp:"List of captured network requests and responses.";in:[];out:[{f:network_events;t:list_dict;d:List of network events with details.}];use:"if result.network_requests:\n    print(f\"Captured {len(result.network_requests)} network events\")";rel:[{id:c4ai_0005;typ:P}];src:"Network Requests & Console Message Capturing;CrawlResult Reference"#END_AIU
#AIU#id:c4ai_0021;typ:DataObj;name:CrawlResult.console_messages;purp:"List of captured browser console messages.";in:[];out:[{f:console_messages;t:list_dict;d:List of console messages with type, text, etc.}];use:"if result.console_messages:\n    print(f\"Captured {len(result.console_messages)} console messages\")";rel:[{id:c4ai_0005;typ:P}];src:"Network Requests & Console Message Capturing;CrawlResult Reference"#END_AIU
#AIU#id:c4ai_0022;typ:DataObj;name:CrawlResult.markdown;purp:"Object containing various forms of markdown output (raw, filtered, etc.).";in:[];out:[{f:markdown_result;t:AIU_c4ai_0023;d:The MarkdownGenerationResult object.}];use:"if result.markdown:\n    print(result.markdown.raw_markdown)";rel:[{id:c4ai_0005;typ:P},{id:c4ai_0023;typ:R}];src:"Crawler Result;CrawlResult and Output;CrawlResult Reference;Markdown Generation Basics"#END_AIU
#AIU#id:c4ai_0023;typ:DataObj;name:MarkdownGenerationResult;purp:"Contains different versions of markdown output (raw, cited, filtered) and corresponding HTML.";in:[];out:[{f:raw_markdown;t:str;d:Basic HTML-to-Markdown conversion.},{f:markdown_with_citations;t:str;d:Markdown with links converted to citations.},{f:references_markdown;t:str;d:List of references/citations.},{f:fit_markdown;t:str;d:Filtered markdown (if content filter used).},{f:fit_html;t:str;d:HTML corresponding to fit_markdown.}];use:"md_obj = result.markdown\nprint(md_obj.raw_markdown)\nprint(md_obj.fit_markdown)";rel:[{id:c4ai_0022;typ:P}];src:"Crawl Result and Output;CrawlResult Reference;Markdown Generation Basics"#END_AIU
#AIU#id:c4ai_0024;typ:DataObj;name:CrawlResult.media;purp:"Dictionary containing extracted media information (images, videos, audio, tables).";in:[];out:[{f:media_data;t:dict;d:Media info keyed by type (e.g., 'images', 'tables').}];use:"images = result.media.get(\"images\", [])\nfor img in images:\n    print(img['src'])";rel:[{id:c4ai_0005;typ:P},{id:c4ai_0044;typ:P}];src:"Crawler Result;CrawlResult Reference;Link & Media"#END_AIU
#AIU#id:c4ai_0025;typ:DataObj;name:CrawlResult.links;purp:"Dictionary containing extracted link information (internal, external).";in:[];out:[{f:link_data;t:dict;d:Link info keyed by type ('internal', 'external').}];use:"internal_links = result.links.get(\"internal\", [])\nfor link in internal_links:\n    print(link['href'])";rel:[{id:c4ai_0005;typ:P}];src:"Crawler Result;CrawlResult Reference;Link & Media"#END_AIU
#AIU#id:cai_0026;typ:Cls;name:ExtractionStrategy;purp:"Base class for structured data extraction strategies.";in:[];out:[];use:"# Inherit from this class to create custom strategies";rel:[{id:c4ai_0003;typ:A},{id:cai_0027;typ:I},{id:cai_0029;typ:I}];src:"Extraction & Chunking Strategies API"#END_AIU
#AIU#id:cai_0027;typ:Cls;name:JsonCssExtractionStrategy;purp:"Extract structured JSON data using CSS selectors.";in:[{p:schema;t:dict;d:Defines the structure and CSS selectors for extraction.;ex:{\"baseSelector\": \".item\", \"fields\": [{\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}]}}];out:[];use:"from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nschema = {\"baseSelector\": \".item\", \"fields\": [{\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}]}\nstrategy = JsonCssExtractionStrategy(schema)\n# Use strategy in CrawlerRunConfig";rel:[{id:cai_0026;typ:I}];src:"Extraction & Chunking Strategies API;Extracting JSON (No LLM)"#END_AIU
#AIU#id:cai_0028;typ:HowTo;name:Extract JSON with CSS Selectors;purp:"Define a schema with CSS selectors to extract structured data into JSON.";in:[{p:schema;t:dict;d:Extraction schema.;ex:{\"baseSelector\": \".item\", \"fields\": [...]}},{p:url;t:str;d:URL or raw HTML.;ex:https://example.com}];out:[{f:extracted_content;t:str;d:JSON string with extracted data.}];use:"import asyncio, json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nschema = {\"baseSelector\": \".item\", \"fields\": [{\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}]}\nasync def main():\n    config = CrawlerRunConfig(extraction_strategy=JsonCssExtractionStrategy(schema))\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url='raw://<html><div class=\"item\"><h2>Title</h2></div></html>', config=config)\n        data = json.loads(result.extracted_content)\n        print(data)\nasyncio.run(main())";rel:[{id:cai_0027;typ:U},{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U},{id:c4ai_0005;typ:R}];src:"Extracting JSON (No LLM);Content Selection"#END_AIU
#AIU#id:cai_0029;typ:Cls;name:LLMExtractionStrategy;purp:"Extract structured JSON data using a Language Model.";in:[{p:llm_config;t:AIU_c4ai_0037;d:LLM provider configuration.;ex:AIU_c4ai_0037},{p:schema;t:dict;d:Pydantic schema for structured output.;ex:{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}}}},{p:instruction;t:str;d:Prompt text for the LLM.;ex:Extract product names and prices.}];out:[];use:"from crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom crawl4ai import LLMConfig\nllm_cfg = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=\"...\")\nstrategy = LLMExtractionStrategy(llm_config=llm_cfg, instruction=\"Extract data.\")\n# Use strategy in CrawlerRunConfig";rel:[{id:cai_0026;typ:I},{id:c4ai_0037;typ:A}];src:"Extraction & Chunking Strategies API;Extracting JSON (LLM)"#END_AIU
#AIU#id:c4ai_0030;typ:DataObj;name:CrawlResult.extracted_content;purp:"Structured data output (typically JSON string) from an extraction strategy.";in:[];out:[{f:content;t:str;d:The extracted data string.}];use:"import json\nif result.extracted_content:\n    data = json.loads(result.extracted_content)\n    print(data)";rel:[{id:c4ai_0005;typ:P}];src:"Crawler Result;CrawlResult Reference;Extracting JSON (No LLM)"#END_AIU
#AIU#id:c4ai_0031;typ:DataObj;name:CrawlResult.downloaded_files;purp:"List of local file paths for files downloaded during the crawl.";in:[];out:[{f:file_paths;t:list_str;d:List of paths.}];use:"if result.downloaded_files:\n    for f in result.downloaded_files:\n        print(f)";rel:[{id:c4ai_0005;typ:P}];src:"Crawler Result;CrawlResult Reference;File Downloading"#END_AIU
#AIU#id:c4ai_0032;typ:DataObj;name:CrawlResult.mhtml;purp:"MHTML snapshot of the page, including resources, as a string.";in:[];out:[{f:mhtml_string;t:str;d:The MHTML data.}];use:"if result.mhtml:\n    with open(\"page.mhtml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.mhtml)";rel:[{id:c4ai_0005;typ:P}];src:"CrawlResult Reference;Link & Media"#END_AIU
#AIU#id:c4ai_0033;typ:ParamSet;name:BrowserConfig.accept_downloads;purp:"Enable file downloads during the crawl.";in:[{p:accept;t:T/F;d:Set to True to accept downloads.;ex:T}];out:[];use:"from crawl4ai.async_configs import BrowserConfig\nconfig = BrowserConfig(accept_downloads=True)";rel:[{id:c4ai_0004;typ:P}];src:"File Downloading;BrowserConfig Reference"#END_AIU
#AIU#id:c4ai_0034;typ:ParamSet;name:BrowserConfig.downloads_path;purp:"Specify the directory to save downloaded files.";in:[{p:path;t:str;d:Local directory path.;ex:/path/to/downloads}];out:[];use:"import os\nfrom crawl4ai.async_configs import BrowserConfig\ndownload_dir = os.path.join(os.getcwd(), \"my_downloads\")\nos.makedirs(download_dir, exist_ok=True)\nconfig = BrowserConfig(downloads_path=download_dir)";rel:[{id:c4ai_0004;typ:P}];src:"File Downloading;BrowserConfig Reference"#END_AIU
#AIU#id:c4ai_0035;typ:Cls;name:SSLCertificate;purp:"Represents a site's SSL certificate with properties and export methods.";in:[];out:[{f:issuer;t:dict;d:Certificate issuer details.},{f:subject;t:dict;d:Certificate subject details.},{f:valid_until;t:str;d:Certificate expiration date.},{f:fingerprint;t:str;d:Certificate fingerprint.}];use:"if result.ssl_certificate:\n    cert = result.ssl_certificate\n    print(cert.fingerprint)";rel:[{id:c4ai_0019;typ:R},{id:c4ai_0036;typ:HM},{id:c4ai_0045;typ:HM},{id:c4ai_0046;typ:HM}];src:"SSLCertificate Reference;Handling SSL Certificates"#END_AIU
#AIU#id:c4ai_0036;typ:ClsMth;name:SSLCertificate.to_json;purp:"Export the SSL certificate data as a JSON string or file.";in:[{p:filepath;t:str;d:Optional path to save the JSON.;ex:cert.json}];out:[{f:json_data;t:str;d:JSON string if filepath is None.}];use:"json_data = cert.to_json() # Get string\ncert.to_json(\"cert.json\") # Save to file";rel:[{id:c4ai_0035;typ:HM}];src:"SSLCertificate Reference"#END_AIU
#AIU#id:c4ai_0037;typ:CfgObj;name:LLMConfig;purp:"Configures settings for LLM providers used in extraction or filtering.";in:[{p:provider;t:str;d:LLM provider/model identifier.;ex:openai/gpt-4o-mini},{p:api_token;t:str;d:API token for the provider.;ex:env:OPENAI_API_KEY},{p:base_url;t:str;d:Custom API endpoint URL.;ex:http://localhost:11434}];out:[];use:"from crawl4ai import LLMConfig\nconfig = LLMConfig(provider=\"ollama/llama2\", api_token=None)";rel:[{id:cai_0029;typ:A},{id:cai_0047;typ:A}];src:"LLMConfig Essentials;Browser, Crawler & LLM Configuration;LLMConfig Reference"#END_AIU
#AIU#id:c4ai_0038;typ:HowTo;name:Crawl Multiple URLs in Batch;purp:"Crawl a list of URLs and get all results together as a list.";in:[{p:urls;t:list_str;d:List of URLs.;ex:[\"url1\", \"url2\"]},{p:config;t:AIU_c4ai_0003;d:Crawler run config (stream=False).;ex:CrawlerRunConfig(stream=F)}];out:[{f:results;t:list_AIU_c4ai_0005;d:List of CrawlResult objects.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    urls = [\"https://example.com/1\", \"https://example.com/2\"]\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun_many(urls, config=CrawlerRunConfig(stream=False))\n        for result in results:\n            print(f\"Crawled {result.url}: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0010;typ:U}];src:"arun_many(...) Reference;Multi-URL Crawling"#END_AIU
#AIU#id:c4ai_0039;typ:HowTo;name:Crawl Multiple URLs Streaming;purp:"Crawl a list of URLs and process results as they become available.";in:[{p:urls;t:list_str;d:List of URLs.;ex:[\"url1\", \"url2\"]},{p:config;t:AIU_c4ai_0003;d:Crawler run config (stream=True).;ex:CrawlerRunConfig(stream=T)}];out:[{f:result_generator;t:AIU_c4ai_0005;d:Async generator yielding CrawlResult objects.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    urls = [\"https://example.com/1\", \"https://example.com/2\"]\n    async with AsyncWebCrawler() as crawler:\n        async for result in crawler.arun_many(urls, config=CrawlerRunConfig(stream=True)):\n            print(f\"Streamed {result.url}: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0010;typ:U}];src:"arun_many(...) Reference;Multi-URL Crawling"#END_AIU
#AIU#id:c4ai_0040;typ:Cls;name:BaseDispatcher;purp:"Base class for managing concurrency in multi-URL crawls.";in:[];out:[];use:"# Inherit from this class to create custom dispatchers";rel:[{id:c4ai_0010;typ:A},{id:c4ai_0041;typ:I},{id:c4ai_0042;typ:I}];src:"Multi-URL Crawling;arun_many(...) Reference"#END_AIU
#AIU#id:c4ai_0041;typ:Cls;name:MemoryAdaptiveDispatcher;purp:"Manages concurrent crawls based on system memory usage.";in:[{p:memory_threshold_percent;t:float;d:Pause if memory exceeds this percentage.;ex:90.0},{p:max_session_permit;t:int;d:Maximum concurrent tasks.;ex:10}];out:[];use:"from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher\ndispatcher = MemoryAdaptiveDispatcher(memory_threshold_percent=70.0, max_session_permit=10)\n# Use dispatcher in crawler.arun_many()";rel:[{id:c4ai_0040;typ:I}];src:"Multi-URL Crawling;Crawl Dispatcher;arun_many(...) Reference"#END_AIU
#AIU#id:c4ai_0042;typ:Cls;name:SemaphoreDispatcher;purp:"Manages concurrent crawls using a fixed semaphore limit.";in:[{p:max_session_permit;t:int;d:Maximum concurrent tasks.;ex:20}];out:[];use:"from crawl4ai.async_dispatcher import SemaphoreDispatcher\ndispatcher = SemaphoreDispatcher(max_session_permit=5)\n# Use dispatcher in crawler.arun_many()";rel:[{id:c4ai_0040;typ:I}];src:"Multi-URL Crawling;Crawl Dispatcher;arun_many(...) Reference"#END_AIU
#AIU#id:c4ai_0043;typ:HowTo;name:Handle Downloads;purp:"Configure the crawler to accept and save file downloads.";in:[{p:url;t:str;d:URL of the page with download links.;ex:https://example.com/downloads},{p:downloads_path;t:str;d:Directory to save files.;ex:/path/to/downloads},{p:js_code;t:str;d:JS to trigger download click.;ex:document.querySelector('a[download]').click();}];out:[{f:downloaded_files;t:list_str;d:List of paths to saved files.}];use:"import asyncio\nimport os\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\nasync def main():\n    download_dir = os.path.join(os.getcwd(), \"my_downloads\")\n    os.makedirs(download_dir, exist_ok=True)\n    browser_cfg = BrowserConfig(accept_downloads=True, downloads_path=download_dir)\n    crawler_cfg = CrawlerRunConfig(js_code=\"document.querySelector('a[download]')?.click();\", wait_for=5)\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(url=\"https://example.com/downloads\", config=crawler_cfg)\n        if result.downloaded_files:\n            print(f\"Downloaded: {result.downloaded_files[0]}\")\nasyncio.run(main())";rel:[{id:c4ai_0033;typ:U},{id:c4ai_0034;typ:U},{id:c4ai_0031;typ:R},{id:c4ai_0004;typ:U},{id:c4ai_0003;typ:U}];src:"File Downloading"#END_AIU
#AIU#id:c4ai_0044;typ:DataObj;name:CrawlResult.media.tables;purp:"List of extracted data tables from the page.";in:[];out:[{f:tables;t:list_dict;d:List of tables, each with headers and rows.}];use:"if result.media.get(\"tables\"):\n    for table in result.media[\"tables\"]:\n        print(table['headers'])";rel:[{id:c4ai_0024;typ:P}];src:"Link & Media"#END_AIU
#AIU#id:c4ai_0045;typ:ClsMth;name:SSLCertificate.to_pem;purp:"Export the SSL certificate as a PEM encoded string or file.";in:[{p:filepath;t:str;d:Optional path to save the PEM.;ex:cert.pem}];out:[{f:pem_data;t:str;d:PEM string if filepath is None.}];use:"pem_str = cert.to_pem()";rel:[{id:c4ai_0035;typ:HM}];src:"SSLCertificate Reference"#END_AIU
#AIU#id:c4ai_0046;typ:ClsMth;name:SSLCertificate.to_der;purp:"Export the SSL certificate as DER binary bytes or file.";in:[{p:filepath;t:str;d:Optional path to save the DER.;ex:cert.der}];out:[{f:der_data;t:bytes;d:DER bytes if filepath is None.}];use:"der_bytes = cert.to_der()";rel:[{id:c4ai_0035;typ:HM}];src:"SSLCertificate Reference"#END_AIU
#AIU#id:cai_0047;typ:Cls;name:LLMContentFilter;purp:"Uses an LLM to filter content and generate focused markdown.";in:[{p:llm_config;t:AIU_c4ai_0037;d:LLM provider configuration.;ex:AIU_c4ai_0037},{p:instruction;t:str;d:Instructions for the LLM on how to filter/format.;ex:Extract core content.}];out:[];use:"from crawl4ai.content_filter_strategy import LLMContentFilter\nfrom crawl4ai import LLMConfig\nllm_cfg = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=\"...\")\ncontent_filter = LLMContentFilter(llm_config=llm_cfg, instruction=\"Extract key points.\")\n# Use content_filter in DefaultMarkdownGenerator";rel:[{id:c4ai_0037;typ:A},{id:c4ai_0048;typ:A}];src:"Markdown Generation Basics"#END_AIU
#AIU#id:c4ai_0048;typ:Cls;name:DefaultMarkdownGenerator;purp:"Generates markdown from HTML, optionally applying content filters.";in:[{p:content_filter;t:AIU_cai_0049;d:Filter to prune/rank content before markdown generation.;ex:AIU_cai_0049},{p:content_source;t:str;d:Which HTML source to use ('raw_html', 'cleaned_html', 'fit_html').;ex:cleaned_html},{p:options;t:dict;d:HTML-to-text conversion options.;ex:{\"ignore_links\": T}}];out:[];use:"from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nmd_generator = DefaultMarkdownGenerator(options={'ignore_links': True})\n# Use md_generator in CrawlerRunConfig";rel:[{id:c4ai_0003;typ:A},{id:cai_0049;typ:A}];src:"Markdown Generation Basics"#END_AIU
#AIU#id:cai_0049;typ:Cls;name:RelevantContentFilter;purp:"Base class for filtering content based on relevance or heuristics.";in:[];out:[];use:"# Inherit from this class to create custom filters";rel:[{id:c4ai_0048;typ:A},{id:cai_0050;typ:I},{id:cai_0051;typ:I},{id:cai_0047;typ:I}];src:"Fit Markdown with Pruning & BM25"#END_AIU
#AIU#id:cai_0050;typ:Cls;name:PruningContentFilter;purp:"Filters content by pruning less relevant nodes based on text/link density and structure.";in:[{p:threshold;t:float;d:Score boundary for pruning.;ex:0.45},{p:threshold_type;t:str;d:Type of threshold ('fixed' or 'dynamic').;ex:fixed},{p:min_word_threshold;t:int;d:Min words per block to avoid pruning.;ex:10}];out:[];use:"from crawl4ai.content_filter_strategy import PruningContentFilter\nfilter = PruningContentFilter(threshold=0.5, min_word_threshold=10)\n# Use filter in DefaultMarkdownGenerator";rel:[{id:cai_0049;typ:I}];src:"Fit Markdown with Pruning & BM25"#END_AIU
#AIU#id:cai_0051;typ:Cls;name:BM25ContentFilter;purp:"Filters content based on relevance to a user query using the BM25 algorithm.";in:[{p:user_query;t:str;d:Keywords/query to rank content by.;ex:startup fundraising},{p:bm25_threshold;t:float;d:Minimum BM25 score to keep a block.;ex:1.0}];out:[];use:"from crawl4ai.content_filter_strategy import BM25ContentFilter\nfilter = BM25ContentFilter(user_query=\"machine learning\", bm25_threshold=1.2)\n# Use filter in DefaultMarkdownGenerator";rel:[{id:cai_0049;typ:I}];src:"Fit Markdown with Pruning & BM25"#END_AIU
#AIU#id:c4ai_0052;typ:HowTo;name:Handle Lazy Loading;purp:"Ensure lazy-loaded images and content appear in the crawl result.";in:[{p:url;t:str;d:URL of the page.;ex:https://example.com/gallery},{p:wait_for_images;t:T/F;d:Wait for images to load.;ex:T},{p:scan_full_page;t:T/F;d:Automatically scroll the page.;ex:T},{p:scroll_delay;t:float;d:Delay between scroll steps.;ex:0.5}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result including loaded content/media.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    config = CrawlerRunConfig(\n        wait_for_images=True,\n        scan_full_page=True,\n        scroll_delay=0.5,\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/lazy-page\", config=config)\n        print(f\"Images found: {len(result.media.get('images', []))}\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U}];src:"Lazy Loading"#END_AIU
#AIU#id:c4ai_0053;typ:HowTo;name:Capture Network and Console Logs;purp:"Capture network requests and browser console messages during a crawl.";in:[{p:url;t:str;d:URL to crawl.;ex:https://example.com},{p:capture_network_requests;t:T/F;d:Enable network capture.;ex:T},{p:capture_console_messages;t:T/F;d:Enable console capture.;ex:T}];out:[{f:network_requests;t:list_dict;d:List of network events.},{f:console_messages;t:list_dict;d:List of console messages.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    config = CrawlerRunConfig(\n        capture_network_requests=True,\n        capture_console_messages=True\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n        print(f\"Captured {len(result.network_requests or [])} network requests\")\n        print(f\"Captured {len(result.console_messages or [])} console messages\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U},{id:c4ai_0020;typ:R},{id:c4ai_0021;typ:R}];src:"Network Requests & Console Message Capturing"#END_AIU
#AIU#id:c4ai_0054;typ:Cls;name:BrowserProfiler;purp:"Utility for creating, listing, and deleting persistent browser profiles.";in:[];out:[];use:"from crawl4ai import BrowserProfiler\nprofiler = BrowserProfiler()\nprofiles = profiler.list_profiles()\nprint(profiles)";rel:[{id:c4ai_0055;typ:HM},{id:c4ai_0056;typ:HM},{id:c4ai_0057;typ:HM}];src:"Identity Based Crawling"#END_AIU
#AIU#id:c4ai_0055;typ:ClsMth;name:BrowserProfiler.create_profile;purp:"Create a new persistent browser profile interactively.";in:[{p:profile_name;t:str;d:Optional name for the profile.;ex:my-login-profile}];out:[{f:profile_path;t:str;d:Path to the created profile directory.}];use:"import asyncio\nfrom crawl4ai import BrowserProfiler\nasync def main():\n    profiler = BrowserProfiler()\n    path = await profiler.create_profile(profile_name=\"my-new-profile\")\n    print(f\"Profile created at {path}\")\nasyncio.run(main())";rel:[{id:c4ai_0054;typ:HM}];src:"Identity Based Crawling"#END_AIU
#AIU#id:c4ai_0056;typ:ClsMth;name:BrowserProfiler.list_profiles;purp:"List existing persistent browser profiles.";in:[];out:[{f:profiles;t:list_dict;d:List of profile dictionaries (name, path, created, type).}];use:"from crawl4ai import BrowserProfiler\nprofiler = BrowserProfiler()\nprofiles = profiler.list_profiles()\nfor p in profiles:\n    print(p['name'], p['path'])";rel:[{id:c4ai_0054;typ:HM}];src:"Identity Based Crawling"#END_AIU
#AIU#id:c4ai_0057;typ:ClsMth;name:BrowserProfiler.delete_profile;purp:"Delete a persistent browser profile by name.";in:[{p:profile_name;t:str;d:Name of the profile to delete.;ex:old-profile}];out:[{f:success;t:T/F;d:True if deletion was successful.}];use:"from crawl4ai import BrowserProfiler\nprofiler = BrowserProfiler()\nsuccess = profiler.delete_profile(\"my-old-profile\")";rel:[{id:c4ai_0054;typ:HM}];src:"Identity Based Crawling"#END_AIU
#AIU#id:c4ai_0058;typ:HowTo;name:Use Persistent Browser Profile;purp:"Crawl using a saved browser profile to maintain cookies, storage, etc.";in:[{p:url;t:str;d:URL to crawl.;ex:https://example.com/private},{p:user_data_dir;t:str;d:Path to the persistent profile directory.;ex:/path/to/my-profile},{p:use_managed_browser;t:T/F;d:Enable managed browser.;ex:T}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\nasync def main():\n    profile_path = \"/path/to/my-profile\"\n    browser_config = BrowserConfig(use_managed_browser=True, user_data_dir=profile_path, headless=True)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com/private\")\n        print(f\"Accessed private page: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0004;typ:U},{id:c4ai_0017;typ:U},{id:c4ai_0001;typ:U},{id:c4ai_0054;typ:U}];src:"Identity Based Crawling"#END_AIU
#AIU#id:c4ai_0059;typ:HowTo;name:Set Browser Location;purp:"Configure the browser's locale, timezone, and geolocation.";in:[{p:url;t:str;d:URL to crawl.;ex:https://maps.google.com},{p:locale;t:str;d:Browser locale (e.g., 'en-US', 'fr-FR').;ex:fr-FR},{p:timezone_id;t:str;d:Browser timezone ID (e.g., 'America/New_York').;ex:Europe/Paris},{p:geolocation;t:dict;d:GPS coordinates ({latitude, longitude, accuracy}).;ex:{\"latitude\": 48.8566, \"longitude\": 2.3522}}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, GeolocationConfig\nasync def main():\n    config = CrawlerRunConfig(\n        locale=\"fr-FR\",\n        timezone_id=\"Europe/Paris\",\n        geolocation=GeolocationConfig(latitude=48.8566, longitude=2.3522)\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://browserleaks.com/geo\", config=config)\n        print(f\"Crawled with location config: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U}];src:"Identity Based Crawling"#END_AIU
#AIU#id:c4ai_0060;typ:HowTo;name:Use Basic Proxy;purp:"Configure a proxy server for crawling.";in:[{p:url;t:str;d:URL to crawl.;ex:https://example.com},{p:proxy_url;t:str;d:Proxy URL string (e.g., http://host:port).;ex:http://proxy.example.com:8080}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\nasync def main():\n    browser_config = BrowserConfig(proxy=\"http://proxy.example.com:8080\")\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://www.whatismyip.com/\")\n        print(f\"Crawled via proxy: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0004;typ:U},{id:c4ai_0001;typ:U},{id:c4ai_0061;typ:U}];src:"Proxy"#END_AIU
#AIU#id:c4ai_0061;typ:ParamSet;name:BrowserConfig.proxy;purp:"Simple proxy URL string (host:port or http://user:pass@host:port).";in:[{p:proxy_url;t:str;d:The proxy URL string.;ex:http://user:pass@proxy.com:8080}];out:[];use:"from crawl4ai.async_configs import BrowserConfig\nconfig = BrowserConfig(proxy=\"http://user:pass@proxy.com:8080\")";rel:[{id:c4ai_0004;typ:P}];src:"BrowserConfig Reference;Proxy"#END_AIU
#AIU#id:c4ai_0062;typ:HowTo;name:Respect Robots.txt;purp:"Configure the crawler to check and follow website robots.txt rules.";in:[{p:url;t:str;d:URL to crawl.;ex:https://example.com},{p:check_robots_txt;t:T/F;d:Enable robots.txt checking.;ex:T}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result (status_code 403 if blocked).}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nasync def main():\n    config = CrawlerRunConfig(fetch_ssl_certificate=True, cache_mode=CacheMode.BYPASS)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n        if not result.success and result.status_code == 403:\n            print(\"Access denied by robots.txt\")\n        else:\n            print(f\"Crawled: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U}];src:"Robots.txt Compliance"#END_AIU
#AIU#id:c4ai_0063;typ:HowTo;name:Use Session ID;purp:"Reuse the same browser session across multiple crawl operations.";in:[{p:urls;t:list_str;d:URLs to crawl sequentially.;ex:[\"url1\", \"url2\"]},{p:session_id;t:str;d:Unique ID for the session.;ex:my_session},{p:js_only;t:T/F;d:Only run JS, no new navigation (for subsequent calls).;ex:T}];out:[{f:results;t:list_AIU_c4ai_0005;d:List of crawl results.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    session_id = \"my_unique_session\"\n    async with AsyncWebCrawler() as crawler:\n        # First call navigates normally\n        result1 = await crawler.arun(url=\"https://example.com/page1\", config=CrawlerRunConfig(session_id=session_id))\n        # Subsequent calls reuse session, can apply JS without full navigation\n        result2 = await crawler.arun(url=\"https://example.com/page2\", config=CrawlerRunConfig(session_id=session_id, js_only=True))\n        print(f\"Crawled page1: {result1.success}, page2: {result2.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U}];src:"Session Management;Page Interaction"#END_AIU
#AIU#id:c4ai_0064;typ:Feat;name:Markdown Generation;purp:"Convert HTML content into clean, structured markdown.";in:[{p:markdown_generator;t:AIU_c4ai_0048;d:Configuration for markdown generation.;ex:AIU_c4ai_0048}];out:[{f:markdown;t:AIU_c4ai_0022;d:Markdown output.}];use:"from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.async_configs import CrawlerRunConfig\nconfig = CrawlerRunConfig(markdown_generator=DefaultMarkdownGenerator())\n# Use config in crawler.arun()";rel:[{id:c4ai_0003;typ:A},{id:c4ai_0048;typ:U}];src:"Markdown Generation Basics"#END_AIU
#AIU#id:c4ai_0065;typ:HowTo;name:Crawl Raw HTML;purp:"Process raw HTML content directly without making an HTTP request.";in:[{p:raw_html;t:str;d:The HTML content string.;ex:<html><body>...</body></html>}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n    raw_html = \"<html><body><h1>Hello!</h1></body></html>\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"raw:\" + raw_html)\n        print(result.markdown)\nasyncio.run(main())";rel:[{id:c4ai_0001;typ:U}];src:"Local Files & Raw HTML;Prefix-Based Input Handling"#END_AIU
#AIU#id:c4ai_0066;typ:Feat;name:Deep Crawling;purp:"Explore links on a website to crawl multiple pages based on strategy and filters.";in:[{p:deep_crawl_strategy;t:AIU_c4ai_0067;d:Strategy for traversal (BFS, DFS, Best-First).;ex:AIU_c4ai_0067},{p:url;t:str;d:Starting URL.;ex:https://example.com}];out:[{f:results;t:list_AIU_c4ai_0005;d:List of results (batch mode).},{f:result_generator;t:AIU_c4ai_0005;d:Async generator (streaming).}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nasync def main():\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1)\n    )\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun(\"https://example.com\", config=config)\n        print(f\"Crawled {len(results)} pages\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:A},{id:c4ai_0001;typ:U},{id:c4ai_0067;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0067;typ:Cls;name:DeepCrawlStrategy;purp:"Base class for deep crawling strategies.";in:[{p:max_depth;t:int;d:Maximum depth to crawl.;ex:2},{p:include_external;t:T/F;d:Include external links.;ex:F},{p:max_pages;t:int;d:Maximum number of pages to crawl.;ex:50}];out:[];use:"# Inherit from this class";rel:[{id:c4ai_0066;typ:A},{id:c4ai_0068;typ:I},{id:c4ai_0069;typ:I},{id:c4ai_0070;typ:I}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0068;typ:Cls;name:BFSDeepCrawlStrategy;purp:"Deep crawling strategy using Breadth-First Search (level by level).";in:[{p:max_depth;t:int;d:Maximum depth to crawl.;ex:2},{p:include_external;t:T/F;d:Include external links.;ex:F}];out:[];use:"from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nstrategy = BFSDeepCrawlStrategy(max_depth=1)";rel:[{id:c4ai_0067;typ:I}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0069;typ:Cls;name:DFSDeepCrawlStrategy;purp:"Deep crawling strategy using Depth-First Search (down each branch).";in:[{p:max_depth;t:int;d:Maximum depth to crawl.;ex:2},{p:include_external;t:T/F;d:Include external links.;ex:F}];out:[];use:"from crawl4ai.deep_crawling import DFSDeepCrawlStrategy\nstrategy = DFSDeepCrawlStrategy(max_depth=1)";rel:[{id:c4ai_0067;typ:I}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0070;typ:Cls;name:BestFirstCrawlingStrategy;purp:"Deep crawling strategy prioritizing URLs based on a score.";in:[{p:max_depth;t:int;d:Maximum depth to crawl.;ex:2},{p:include_external;t:T/F;d:Include external links.;ex:F},{p:url_scorer;t:AIU_c4ai_0071;d:Scorer to prioritize URLs.;ex:AIU_c4ai_0071}];out:[];use:"from crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\nscorer = KeywordRelevanceScorer(keywords=[\"important\"])\nstrategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer)";rel:[{id:c4ai_0067;typ:I},{id:c4ai_0071;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0071;typ:Cls;name:URLScorer;purp:"Base class for scoring URLs in deep crawls.";in:[];out:[];use:"# Inherit from this class to create custom scorers";rel:[{id:c4ai_0070;typ:A},{id:c4ai_0072;typ:I}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0072;typ:Cls;name:KeywordRelevanceScorer;purp:"Scores URLs based on the presence and weighting of keywords.";in:[{p:keywords;t:list_str;d:Keywords to score relevance.;ex:[\"api\", \"guide\"]},{p:weight;t:float;d:Importance weight of this scorer.;ex:0.7}];out:[];use:"from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\nscorer = KeywordRelevanceScorer(keywords=[\"crawl\", \"example\"], weight=0.8)\n# Use scorer in BestFirstCrawlingStrategy";rel:[{id:c4ai_0071;typ:I}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0073;typ:Cls;name:FilterChain;purp:"Combines multiple filters to determine which URLs to crawl in a deep crawl.";in:[{p:filters;t:list;d:List of filter objects.;ex:[AIU_c4ai_0074]}];out:[];use:"from crawl4ai.deep_crawling.filters import FilterChain, DomainFilter\nfilter_chain = FilterChain([DomainFilter(allowed_domains=[\"example.com\"])])\n# Use filter_chain in DeepCrawlStrategy";rel:[{id:c4ai_0067;typ:A},{id:c4ai_0074;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0074;typ:Cls;name:DomainFilter;purp:"Filters URLs based on allowed or blocked domains.";in:[{p:allowed_domains;t:list_str;d:List of domains allowed.;ex:[\"example.com\"]},{p:blocked_domains;t:list_str;d:List of domains blocked.;ex:[\"spam.com\"]}];out:[];use:"from crawl4ai.deep_crawling.filters import DomainFilter\nfilter = DomainFilter(allowed_domains=[\"docs.crawl4ai.com\"])";rel:[{id:c4ai_0073;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0075;typ:HowTo;name:Filter URLs in Deep Crawl;purp:"Use filters to limit which pages are included in a deep crawl.";in:[{p:deep_crawl_strategy;t:AIU_c4ai_0067;d:Deep crawl strategy.;ex:BFSDeepCrawlStrategy(max_depth=1)},{p:filter_chain;t:AIU_c4ai_0073;d:Configured filter chain.;ex:AIU_c4ai_0073}];out:[{f:results;t:list_AIU_c4ai_0005;d:Crawl results for filtered URLs.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nfrom crawl4ai.deep_crawling.filters import FilterChain, DomainFilter\nfilter_chain = FilterChain([DomainFilter(allowed_domains=[\"example.com\"])])\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1, filter_chain=filter_chain)\n)\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun(\"https://example.com\", config=config)\n        print(f\"Crawled {len(results)} filtered pages\")\nasyncio.run(main())";rel:[{id:c4ai_0066;typ:U},{id:c4ai_0073;typ:U},{id:c4ai_0067;typ:U}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0076;typ:Cls;name:URLPatternFilter;purp:"Filters URLs based on wildcard patterns.";in:[{p:patterns;t:list_str;d:List of wildcard patterns.;ex:[\"*blog*\", \"*docs*\"]}];out:[];use:"from crawl4ai.deep_crawling.filters import URLPatternFilter\nfilter = URLPatternFilter(patterns=[\"*guide*\"])";rel:[{id:c4ai_0073;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0077;typ:Cls;name:ContentTypeFilter;purp:"Filters URLs based on HTTP Content-Type header.";in:[{p:allowed_types;t:list_str;d:List of allowed content types (e.g., 'text/html').;ex:[\"text/html\"]}];out:[];use:"from crawl4ai.deep_crawling.filters import ContentTypeFilter\nfilter = ContentTypeFilter(allowed_types=[\"text/html\"])";rel:[{id:c4ai_0073;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0078;typ:Cls;name:ContentRelevanceFilter;purp:"Filters content based on semantic similarity to a query.";in:[{p:query;t:str;d:Query text for relevance.;ex:web crawling with python},{p:threshold;t:float;d:Minimum similarity score.;ex:0.7}];out:[];use:"from crawl4ai.deep_crawling.filters import ContentRelevanceFilter\nfilter = ContentRelevanceFilter(query=\"data extraction\", threshold=0.6)";rel:[{id:c4ai_0073;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0079;typ:Cls;name:SEOFilter;purp:"Filters content based on SEO characteristics (meta tags, headers).";in:[{p:threshold;t:float;d:Minimum SEO score.;ex:0.5},{p:keywords;t:list_str;d:Keywords to look for in SEO elements.;ex:[\"tutorial\"]}];out:[];use:"from crawl4ai.deep_crawling.filters import SEOFilter\nfilter = SEOFilter(threshold=0.7, keywords=[\"documentation\"])";rel:[{id:c4ai_0073;typ:A}];src:"Deep Crawling"#END_AIU
#AIU#id:c4ai_0080;typ:HowTo;name:Extract Tables;purp:"Extract structured data from HTML tables.";in:[{p:url;t:str;d:URL of the page.;ex:https://example.com},{p:table_score_threshold;t:int;d:Min score for tables to be extracted.;ex:7}];out:[{f:tables;t:list_dict;d:List of extracted tables.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    config = CrawlerRunConfig(table_score_threshold=7)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com/tables\", config=config)\n        if result.media.get('tables'):\n            print(f\"Found {len(result.media['tables'])} tables\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U},{id:c4ai_0044;typ:R}];src:"Link & Media"#END_AIU
#AIU#id:c4ai_0081;typ:HowTo;name:Crawl Local HTML File;purp:"Crawl HTML content from a file on the local filesystem.";in:[{p:file_path;t:str;d:Path to the local HTML file.;ex:/path/to/page.html}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n    file_path = \"/path/to/your/file.html\" # Replace with actual path\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"file://\" + file_path)\n        print(result.markdown)\nasyncio.run(main())";rel:[{id:c4ai_0001;typ:U}];src:"Local Files & Raw HTML;Prefix-Based Input Handling"#END_AIU
#AIU#id:c4ai_0082;typ:HowTo;name:Perform Dynamic Page Interaction;purp:"Execute JavaScript code and wait for dynamic content to appear.";in:[{p:url;t:str;d:URL of the page.;ex:https://example.com},{p:js_code;t:list_str;d:JS commands.;ex:[\"window.scroll(...)\", \"button.click()\"]},{p:wait_for;t:str;d:Wait condition (CSS or JS).;ex:css:.new-item}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result after interaction.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    config = CrawlerRunConfig(js_code=\"window.scrollTo(0, document.body.scrollHeight);\", wait_for=\"css:.new-item\")\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com/dynamic\", config=config)\n        print(f\"Crawled after interaction: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U}];src:"Page Interaction"#END_AIU
#AIU#id:c4ai_0083;typ:HowTo;name:Extract JSON with LLM;purp:"Use a language model and a schema/instruction to extract structured data.";in:[{p:url;t:str;d:URL to crawl.;ex:https://example.com},{p:extraction_strategy;t:AIU_cai_0029;d:Configured LLM extraction strategy.;ex:AIU_cai_0029}];out:[{f:extracted_content;t:str;d:JSON string with extracted data.}];use:"import asyncio, json, os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nasync def main():\n    llm_cfg = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv('OPENAI_API_KEY'))\n    llm_strat = LLMExtractionStrategy(llm_config=llm_cfg, instruction=\"Extract details.\", schema={})\n    config = CrawlerRunConfig(extraction_strategy=llm_strat)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n        if result.success:\n            print(json.loads(result.extracted_content))\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U},{id:cai_0029;typ:U},{id:c4ai_0030;typ:R}];src:"Extracting JSON (LLM)";editor_note:"example code uses schema={}, should be a valid dict"}]
#AIU#id:c4ai_0084;typ:HowTo;name:Use Magic Mode;purp:"Simplify automation with features simulating user behavior and handling overlays.";in:[{p:url;t:str;d:URL to crawl.;ex:https://example.com},{p:magic;t:T/F;d:Enable magic mode.;ex:T}];out:[{f:result;t:AIU_c4ai_0005;d:Crawl result.}];use:"import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n    config = CrawlerRunConfig(magic=True, remove_overlay_elements=True)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n        print(f\"Crawled with magic mode: {result.success}\")\nasyncio.run(main())";rel:[{id:c4ai_0003;typ:U},{id:c4ai_0001;typ:U}];src:"Identity Based Crawling"#END_AIU
#AIU_LIST_END