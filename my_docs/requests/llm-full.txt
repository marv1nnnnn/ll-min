# Requests: HTTP for Humans™¶
Release v2.32.3. (Installation)
![Requests Downloads Per Month Badge](https://static.pepy.tech/badge/requests/month) ![License Badge](https://img.shields.io/pypi/l/requests.svg) ![Wheel Support Badge](https://img.shields.io/pypi/wheel/requests.svg) ![Python Version Support Badge](https://img.shields.io/pypi/pyversions/requests.svg)
**Requests** is an elegant and simple HTTP library for Python, built for human beings.
**Behold, the power of Requests** :
```
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
'{"type":"User"...'
>>> r.json()
{'private_gists': 419, 'total_private_repos': 77, ...}

```

See similar code, sans Requests.
**Requests** allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, thanks to urllib3.
## Beloved Features¶
Requests is ready for today’s web.
  * Keep-Alive & Connection Pooling
  * International Domains and URLs
  * Sessions with Cookie Persistence
  * Browser-style SSL Verification
  * Automatic Content Decoding
  * Basic/Digest Authentication
  * Elegant Key/Value Cookies
  * Automatic Decompression
  * Unicode Response Bodies
  * HTTP(S) Proxy Support
  * Multipart File Uploads
  * Streaming Downloads
  * Connection Timeouts
  * Chunked Requests
  * `.netrc` Support


Requests officially supports Python 3.8+, and runs great on PyPy.
## The User Guide¶
This part of the documentation, which is mostly prose, begins with some background information about Requests, then focuses on step-by-step instructions for getting the most out of Requests.
  * Installation of Requests
    * $ python -m pip install requests
    * Get the Source Code
  * Quickstart
    * Make a Request
    * Passing Parameters In URLs
    * Response Content
    * Binary Response Content
    * JSON Response Content
    * Raw Response Content
    * Custom Headers
    * More complicated POST requests
    * POST a Multipart-Encoded File
    * Response Status Codes
    * Response Headers
    * Cookies
    * Redirection and History
    * Timeouts
    * Errors and Exceptions
  * Advanced Usage
    * Session Objects
    * Request and Response Objects
    * Prepared Requests
    * SSL Cert Verification
    * Client Side Certificates
    * CA Certificates
    * Body Content Workflow
    * Keep-Alive
    * Streaming Uploads
    * Chunk-Encoded Requests
    * POST Multiple Multipart-Encoded Files
    * Event Hooks
    * Custom Authentication
    * Streaming Requests
    * Proxies
    * Compliance
    * HTTP Verbs
    * Custom Verbs
    * Link Headers
    * Transport Adapters
    * Blocking Or Non-Blocking?
    * Header Ordering
    * Timeouts
  * Authentication
    * Basic Authentication
    * Digest Authentication
    * OAuth 1 Authentication
    * OAuth 2 and OpenID Connect Authentication
    * Other Authentication
    * New Forms of Authentication


## The Community Guide¶
This part of the documentation, which is mostly prose, details the Requests ecosystem and community.
  * Recommended Packages and Extensions
    * Certifi CA Bundle
    * CacheControl
    * Requests-Toolbelt
    * Requests-Threads
    * Requests-OAuthlib
    * Betamax
  * Frequently Asked Questions
    * Encoded Data?
    * Custom User-Agents?
    * Why not Httplib2?
    * Python 3 Support?
    * Python 2 Support?
    * What are “hostname doesn’t match” errors?
  * Integrations
  * Articles & Talks
  * Support
    * Stack Overflow
    * File an Issue
    * Send a Tweet
  * Vulnerability Disclosure
  * Release Process and Rules
    * Major Releases
    * Minor Releases
    * Hotfix Releases
    * Reasoning


  * Community Updates
  * Release History


## The API Documentation / Guide¶
If you are looking for information on a specific function, class, or method, this part of the documentation is for you.
  * Developer Interface
    * Main Interface
    * Exceptions
    * Request Sessions
    * Lower-Level Classes
    * Lower-Lower-Level Classes
    * Authentication
    * Encodings
    * Cookies
    * Status Code Lookup
    * Migrating to 1.x
    * Migrating to 2.x


## The Contributor Guide¶
If you want to contribute to the project, this part of the documentation is for you.
  * Contributor’s Guide
    * Be Cordial
    * Get Early Feedback
    * Contribution Suitability
    * Code Contributions
      * Steps for Submitting Code
      * Code Review
      * Code Style
      * New Contributors
    * Documentation Contributions
    * Bug Reports
    * Feature Requests
  * Authors
    * Keepers of the Crystals
    * Previous Keepers of Crystals
    * Patches and Suggestions


There are no more guides. You are now guideless. Good luck.
![Requests logo](https://requests.readthedocs.io/en/latest/_static/requests-sidebar.png)
Requests is an elegant and simple HTTP library for Python, built for human beings. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Requests: HTTP for Humans™¶
Release v2.32.3. (Installation)
![Requests Downloads Per Month Badge](https://static.pepy.tech/badge/requests/month) ![License Badge](https://img.shields.io/pypi/l/requests.svg) ![Wheel Support Badge](https://img.shields.io/pypi/wheel/requests.svg) ![Python Version Support Badge](https://img.shields.io/pypi/pyversions/requests.svg)
**Requests** is an elegant and simple HTTP library for Python, built for human beings.
**Behold, the power of Requests** :
```
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
'{"type":"User"...'
>>> r.json()
{'private_gists': 419, 'total_private_repos': 77, ...}

```

See similar code, sans Requests.
**Requests** allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, thanks to urllib3.
## Beloved Features¶
Requests is ready for today’s web.
  * Keep-Alive & Connection Pooling
  * International Domains and URLs
  * Sessions with Cookie Persistence
  * Browser-style SSL Verification
  * Automatic Content Decoding
  * Basic/Digest Authentication
  * Elegant Key/Value Cookies
  * Automatic Decompression
  * Unicode Response Bodies
  * HTTP(S) Proxy Support
  * Multipart File Uploads
  * Streaming Downloads
  * Connection Timeouts
  * Chunked Requests
  * `.netrc` Support


Requests officially supports Python 3.8+, and runs great on PyPy.
## The User Guide¶
This part of the documentation, which is mostly prose, begins with some background information about Requests, then focuses on step-by-step instructions for getting the most out of Requests.
  * Installation of Requests
    * $ python -m pip install requests
    * Get the Source Code
  * Quickstart
    * Make a Request
    * Passing Parameters In URLs
    * Response Content
    * Binary Response Content
    * JSON Response Content
    * Raw Response Content
    * Custom Headers
    * More complicated POST requests
    * POST a Multipart-Encoded File
    * Response Status Codes
    * Response Headers
    * Cookies
    * Redirection and History
    * Timeouts
    * Errors and Exceptions
  * Advanced Usage
    * Session Objects
    * Request and Response Objects
    * Prepared Requests
    * SSL Cert Verification
    * Client Side Certificates
    * CA Certificates
    * Body Content Workflow
    * Keep-Alive
    * Streaming Uploads
    * Chunk-Encoded Requests
    * POST Multiple Multipart-Encoded Files
    * Event Hooks
    * Custom Authentication
    * Streaming Requests
    * Proxies
    * Compliance
    * HTTP Verbs
    * Custom Verbs
    * Link Headers
    * Transport Adapters
    * Blocking Or Non-Blocking?
    * Header Ordering
    * Timeouts
  * Authentication
    * Basic Authentication
    * Digest Authentication
    * OAuth 1 Authentication
    * OAuth 2 and OpenID Connect Authentication
    * Other Authentication
    * New Forms of Authentication


## The Community Guide¶
This part of the documentation, which is mostly prose, details the Requests ecosystem and community.
  * Recommended Packages and Extensions
    * Certifi CA Bundle
    * CacheControl
    * Requests-Toolbelt
    * Requests-Threads
    * Requests-OAuthlib
    * Betamax
  * Frequently Asked Questions
    * Encoded Data?
    * Custom User-Agents?
    * Why not Httplib2?
    * Python 3 Support?
    * Python 2 Support?
    * What are “hostname doesn’t match” errors?
  * Integrations
  * Articles & Talks
  * Support
    * Stack Overflow
    * File an Issue
    * Send a Tweet
  * Vulnerability Disclosure
  * Release Process and Rules
    * Major Releases
    * Minor Releases
    * Hotfix Releases
    * Reasoning


  * Community Updates
  * Release History


## The API Documentation / Guide¶
If you are looking for information on a specific function, class, or method, this part of the documentation is for you.
  * Developer Interface
    * Main Interface
    * Exceptions
    * Request Sessions
    * Lower-Level Classes
    * Lower-Lower-Level Classes
    * Authentication
    * Encodings
    * Cookies
    * Status Code Lookup
    * Migrating to 1.x
    * Migrating to 2.x


## The Contributor Guide¶
If you want to contribute to the project, this part of the documentation is for you.
  * Contributor’s Guide
    * Be Cordial
    * Get Early Feedback
    * Contribution Suitability
    * Code Contributions
      * Steps for Submitting Code
      * Code Review
      * Code Style
      * New Contributors
    * Documentation Contributions
    * Bug Reports
    * Feature Requests
  * Authors
    * Keepers of the Crystals
    * Previous Keepers of Crystals
    * Patches and Suggestions


There are no more guides. You are now guideless. Good luck.
![Requests logo](https://requests.readthedocs.io/en/latest/_static/requests-sidebar.png)
Requests is an elegant and simple HTTP library for Python, built for human beings. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Developer Interface¶
This part of the documentation covers all the interfaces of Requests. For parts where Requests depends on external libraries, we document the most important right here and provide links to the canonical documentation.
## Main Interface¶
All of Requests’ functionality can be accessed by these 7 methods. They all return an instance of the `Response` object. 

requests.request(_method_ , _url_ , _** kwargs_)[source]¶
    
Constructs and sends a `Request`. 

Parameters:
    
  * **method** – method for the new `Request` object: `GET`, `OPTIONS`, `HEAD`, `POST`, `PUT`, `PATCH`, or `DELETE`.
  * **url** – URL for the new `Request` object.
  * **params** – (optional) Dictionary, list of tuples or bytes to send in the query string for the `Request`.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * **json** – (optional) A JSON serializable Python object to send in the body of the `Request`.
  * **headers** – (optional) Dictionary of HTTP Headers to send with the `Request`.
  * **cookies** – (optional) Dict or CookieJar object to send with the `Request`.
  * **files** – (optional) Dictionary of `'name': file-like-objects` (or `{'name': file-tuple}`) for multipart encoding upload. `file-tuple` can be a 2-tuple `('filename', fileobj)`, 3-tuple `('filename', fileobj, 'content_type')` or a 4-tuple `('filename', fileobj, 'content_type', custom_headers)`, where `'content_type'` is a string defining the content type of the given file and `custom_headers` a dict-like object containing additional headers to add for the file.
  * **auth** – (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
  * **timeout** (_float_ _or_ _tuple_) – (optional) How many seconds to wait for the server to send data before giving up, as a float, or a (connect timeout, read timeout) tuple.
  * **allow_redirects** (_bool_) – (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to `True`.
  * **proxies** – (optional) Dictionary mapping protocol to the URL of the proxy.
  * **verify** – (optional) Either a boolean, in which case it controls whether we verify the server’s TLS certificate, or a string, in which case it must be a path to a CA bundle to use. Defaults to `True`.
  * **stream** – (optional) if `False`, the response content will be immediately downloaded.
  * **cert** – (optional) if String, path to ssl client cert file (.pem). If Tuple, (‘cert’, ‘key’) pair.



Returns:
    
`Response` object 

Return type:
    
requests.Response
Usage:
```
>>> importrequests
>>> req = requests.request('GET', 'https://httpbin.org/get')
>>> req
<Response [200]>

```


requests.head(_url_ , _** kwargs_)[source]¶
    
Sends a HEAD request. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * ****kwargs** – Optional arguments that `request` takes. If allow_redirects is not provided, it will be set to False (as opposed to the default `request` behavior).



Returns:
    
`Response` object 

Return type:
    
requests.Response 

requests.get(_url_ , _params =None_, _** kwargs_)[source]¶
    
Sends a GET request. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * **params** – (optional) Dictionary, list of tuples or bytes to send in the query string for the `Request`.
  * ****kwargs** – Optional arguments that `request` takes.



Returns:
    
`Response` object 

Return type:
    
requests.Response 

requests.post(_url_ , _data =None_, _json =None_, _** kwargs_)[source]¶
    
Sends a POST request. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * **json** – (optional) A JSON serializable Python object to send in the body of the `Request`.
  * ****kwargs** – Optional arguments that `request` takes.



Returns:
    
`Response` object 

Return type:
    
requests.Response 

requests.put(_url_ , _data =None_, _** kwargs_)[source]¶
    
Sends a PUT request. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * **json** – (optional) A JSON serializable Python object to send in the body of the `Request`.
  * ****kwargs** – Optional arguments that `request` takes.



Returns:
    
`Response` object 

Return type:
    
requests.Response 

requests.patch(_url_ , _data =None_, _** kwargs_)[source]¶
    
Sends a PATCH request. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * **json** – (optional) A JSON serializable Python object to send in the body of the `Request`.
  * ****kwargs** – Optional arguments that `request` takes.



Returns:
    
`Response` object 

Return type:
    
requests.Response 

requests.delete(_url_ , _** kwargs_)[source]¶
    
Sends a DELETE request. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * ****kwargs** – Optional arguments that `request` takes.



Returns:
    
`Response` object 

Return type:
    
requests.Response
## Exceptions¶ 

_exception_ requests.RequestException(_* args_, _** kwargs_)[source]¶
    
There was an ambiguous exception that occurred while handling your request. 

_exception_ requests.ConnectionError(_* args_, _** kwargs_)[source]¶
    
A Connection error occurred. 

_exception_ requests.HTTPError(_* args_, _** kwargs_)[source]¶
    
An HTTP error occurred. 

_exception_ requests.TooManyRedirects(_* args_, _** kwargs_)[source]¶
    
Too many redirects. 

_exception_ requests.ConnectTimeout(_* args_, _** kwargs_)[source]¶
    
The request timed out while trying to connect to the remote server.
Requests that produced this error are safe to retry. 

_exception_ requests.ReadTimeout(_* args_, _** kwargs_)[source]¶
    
The server did not send any data in the allotted amount of time. 

_exception_ requests.Timeout(_* args_, _** kwargs_)[source]¶
    
The request timed out.
Catching this error will catch both `ConnectTimeout` and `ReadTimeout` errors. 

_exception_ requests.JSONDecodeError(_* args_, _** kwargs_)[source]¶
    
Couldn’t decode the text into json
## Request Sessions¶ 

_class_ requests.Session[source]¶
    
A Requests session.
Provides cookie persistence, connection-pooling, and configuration.
Basic Usage:
```
>>> importrequests
>>> s = requests.Session()
>>> s.get('https://httpbin.org/get')
<Response [200]>

```

Or as a context manager:
```
>>> with requests.Session() as s:
...   s.get('https://httpbin.org/get')
<Response [200]>

```


auth¶
    
Default Authentication tuple or object to attach to `Request`. 

cert¶
    
SSL client certificate default, if String, path to ssl client cert file (.pem). If Tuple, (‘cert’, ‘key’) pair. 

close()[source]¶
    
Closes all adapters and as such the session 

cookies¶
    
A CookieJar containing all currently outstanding cookies set on this session. By default it is a `RequestsCookieJar`, but may be any other `cookielib.CookieJar` compatible object. 

delete(_url_ , _** kwargs_)[source]¶
    
Sends a DELETE request. Returns `Response` object. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * ****kwargs** – Optional arguments that `request` takes.



Return type:
    
requests.Response 

get(_url_ , _** kwargs_)[source]¶
    
Sends a GET request. Returns `Response` object. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * ****kwargs** – Optional arguments that `request` takes.



Return type:
    
requests.Response 

get_adapter(_url_)[source]¶
    
Returns the appropriate connection adapter for the given URL. 

Return type:
    
requests.adapters.BaseAdapter 

get_redirect_target(_resp_)¶
    
Receives a Response. Returns a redirect URI or `None` 

head(_url_ , _** kwargs_)[source]¶
    
Sends a HEAD request. Returns `Response` object. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * ****kwargs** – Optional arguments that `request` takes.



Return type:
    
requests.Response 

headers¶
    
A case-insensitive dictionary of headers to be sent on each `Request` sent from this `Session`. 

hooks¶
    
Event-handling hooks. 

max_redirects¶
    
Maximum number of redirects allowed. If the request exceeds this limit, a `TooManyRedirects` exception is raised. This defaults to requests.models.DEFAULT_REDIRECT_LIMIT, which is 30. 

merge_environment_settings(_url_ , _proxies_ , _stream_ , _verify_ , _cert_)[source]¶
    
Check the environment and merge it with some settings. 

Return type:
    
dict 

mount(_prefix_ , _adapter_)[source]¶
    
Registers a connection adapter to a prefix.
Adapters are sorted in descending order by prefix length. 

options(_url_ , _** kwargs_)[source]¶
    
Sends a OPTIONS request. Returns `Response` object. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * ****kwargs** – Optional arguments that `request` takes.



Return type:
    
requests.Response 

params¶
    
Dictionary of querystring data to attach to each `Request`. The dictionary values may be lists for representing multivalued query parameters. 

patch(_url_ , _data =None_, _** kwargs_)[source]¶
    
Sends a PATCH request. Returns `Response` object. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * ****kwargs** – Optional arguments that `request` takes.



Return type:
    
requests.Response 

post(_url_ , _data =None_, _json =None_, _** kwargs_)[source]¶
    
Sends a POST request. Returns `Response` object. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * **json** – (optional) json to send in the body of the `Request`.
  * ****kwargs** – Optional arguments that `request` takes.



Return type:
    
requests.Response 

prepare_request(_request_)[source]¶
    
Constructs a `PreparedRequest` for transmission and returns it. The `PreparedRequest` has settings merged from the `Request` instance and those of the `Session`. 

Parameters:
    
**request** – `Request` instance to prepare with this session’s settings. 

Return type:
    
requests.PreparedRequest 

proxies¶
    
Dictionary mapping protocol or protocol and host to the URL of the proxy (e.g. {‘http’: ‘foo.bar:3128’, ‘http://host.name’: ‘foo.bar:4012’}) to be used on each `Request`. 

put(_url_ , _data =None_, _** kwargs_)[source]¶
    
Sends a PUT request. Returns `Response` object. 

Parameters:
    
  * **url** – URL for the new `Request` object.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * ****kwargs** – Optional arguments that `request` takes.



Return type:
    
requests.Response 

rebuild_auth(_prepared_request_ , _response_)¶
    
When being redirected we may want to strip authentication from the request to avoid leaking credentials. This method intelligently removes and reapplies authentication where possible to avoid credential loss. 

rebuild_method(_prepared_request_ , _response_)¶
    
When being redirected we may want to change the method of the request based on certain specs or browser behavior. 

rebuild_proxies(_prepared_request_ , _proxies_)¶
    
This method re-evaluates the proxy configuration by considering the environment variables. If we are redirected to a URL covered by NO_PROXY, we strip the proxy configuration. Otherwise, we set missing proxy keys for this URL (in case they were stripped by a previous redirect).
This method also replaces the Proxy-Authorization header where necessary. 

Return type:
    
dict 

request(_method_ , _url_ , _params =None_, _data =None_, _headers =None_, _cookies =None_, _files =None_, _auth =None_, _timeout =None_, _allow_redirects =True_, _proxies =None_, _hooks =None_, _stream =None_, _verify =None_, _cert =None_, _json =None_)[source]¶
    
Constructs a `Request`, prepares it and sends it. Returns `Response` object. 

Parameters:
    
  * **method** – method for the new `Request` object.
  * **url** – URL for the new `Request` object.
  * **params** – (optional) Dictionary or bytes to be sent in the query string for the `Request`.
  * **data** – (optional) Dictionary, list of tuples, bytes, or file-like object to send in the body of the `Request`.
  * **json** – (optional) json to send in the body of the `Request`.
  * **headers** – (optional) Dictionary of HTTP Headers to send with the `Request`.
  * **cookies** – (optional) Dict or CookieJar object to send with the `Request`.
  * **files** – (optional) Dictionary of `'filename': file-like-objects` for multipart encoding upload.
  * **auth** – (optional) Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.
  * **timeout** (_float_ _or_ _tuple_) – (optional) How long to wait for the server to send data before giving up, as a float, or a (connect timeout, read timeout) tuple.
  * **allow_redirects** (_bool_) – (optional) Set to True by default.
  * **proxies** – (optional) Dictionary mapping protocol or protocol and hostname to the URL of the proxy.
  * **hooks** – (optional) Dictionary mapping hook name to one event or list of events, event must be callable.
  * **stream** – (optional) whether to immediately download the response content. Defaults to `False`.
  * **verify** – (optional) Either a boolean, in which case it controls whether we verify the server’s TLS certificate, or a string, in which case it must be a path to a CA bundle to use. Defaults to `True`. When set to `False`, requests will accept any TLS certificate presented by the server, and will ignore hostname mismatches and/or expired certificates, which will make your application vulnerable to man-in-the-middle (MitM) attacks. Setting verify to `False` may be useful during local development or testing.
  * **cert** – (optional) if String, path to ssl client cert file (.pem). If Tuple, (‘cert’, ‘key’) pair.



Return type:
    
requests.Response 

resolve_redirects(_resp_ , _req_ , _stream =False_, _timeout =None_, _verify =True_, _cert =None_, _proxies =None_, _yield_requests =False_, _** adapter_kwargs_)¶
    
Receives a Response. Returns a generator of Responses or Requests. 

send(_request_ , _** kwargs_)[source]¶
    
Send a given PreparedRequest. 

Return type:
    
requests.Response 

should_strip_auth(_old_url_ , _new_url_)¶
    
Decide whether Authorization header should be removed when redirecting 

stream¶
    
Stream response content default. 

trust_env¶
    
Trust environment settings for proxy configuration, default authentication and similar. 

verify¶
    
SSL Verification default. Defaults to True, requiring requests to verify the TLS certificate at the remote end. If verify is set to False, requests will accept any TLS certificate presented by the server, and will ignore hostname mismatches and/or expired certificates, which will make your application vulnerable to man-in-the-middle (MitM) attacks. Only set this to False for testing.
## Lower-Level Classes¶ 

_class_ requests.Request(_method =None_, _url =None_, _headers =None_, _files =None_, _data =None_, _params =None_, _auth =None_, _cookies =None_, _hooks =None_, _json =None_)[source]¶
    
A user-created `Request` object.
Used to prepare a `PreparedRequest`, which is sent to the server. 

Parameters:
    
  * **method** – HTTP method to use.
  * **url** – URL to send.
  * **headers** – dictionary of headers to send.
  * **files** – dictionary of {filename: fileobject} files to multipart upload.
  * **data** – the body to attach to the request. If a dictionary or list of tuples `[(key, value)]` is provided, form-encoding will take place.
  * **json** – json for the body to attach to the request (if files or data is not specified).
  * **params** – URL parameters to append to the URL. If a dictionary or list of tuples `[(key, value)]` is provided, form-encoding will take place.
  * **auth** – Auth handler or (user, pass) tuple.
  * **cookies** – dictionary or CookieJar of cookies to attach to this request.
  * **hooks** – dictionary of callback hooks, for internal usage.


Usage:
```
>>> importrequests
>>> req = requests.Request('GET', 'https://httpbin.org/get')
>>> req.prepare()
<PreparedRequest [GET]>

```


deregister_hook(_event_ , _hook_)¶
    
Deregister a previously registered hook. Returns True if the hook existed, False if not. 

prepare()[source]¶
    
Constructs a `PreparedRequest` for transmission and returns it. 

register_hook(_event_ , _hook_)¶
    
Properly register a hook. 

_class_ requests.Response[source]¶
    
The `Response` object, which contains a server’s response to an HTTP request. 

_property_ apparent_encoding¶
    
The apparent encoding, provided by the charset_normalizer or chardet libraries. 

close()[source]¶
    
Releases the connection back to the pool. Once this method has been called the underlying `raw` object must not be accessed again.
_Note: Should not normally need to be called explicitly._ 

_property_ content¶
    
Content of the response, in bytes. 

cookies¶
    
A CookieJar of Cookies the server sent back. 

elapsed¶
    
The amount of time elapsed between sending the request and the arrival of the response (as a timedelta). This property specifically measures the time taken between sending the first byte of the request and finishing parsing the headers. It is therefore unaffected by consuming the response content or the value of the `stream` keyword argument. 

encoding¶
    
Encoding to decode with when accessing r.text. 

headers¶
    
Case-insensitive Dictionary of Response Headers. For example, `headers['content-encoding']` will return the value of a `'Content-Encoding'` response header. 

history¶
    
A list of `Response` objects from the history of the Request. Any redirect responses will end up here. The list is sorted from the oldest to the most recent request. 

_property_ is_permanent_redirect¶
    
True if this Response one of the permanent versions of redirect. 

_property_ is_redirect¶
    
True if this Response is a well-formed HTTP redirect that could have been processed automatically (by `Session.resolve_redirects`). 

iter_content(_chunk_size =1_, _decode_unicode =False_)[source]¶
    
Iterates over the response data. When stream=True is set on the request, this avoids reading the content at once into memory for large responses. The chunk size is the number of bytes it should read into memory. This is not necessarily the length of each item returned as decoding can take place.
chunk_size must be of type int or None. A value of None will function differently depending on the value of stream. stream=True will read data as it arrives in whatever size the chunks are received. If stream=False, data is returned as a single chunk.
If decode_unicode is True, content will be decoded using the best available encoding based on the response. 

iter_lines(_chunk_size =512_, _decode_unicode =False_, _delimiter =None_)[source]¶
    
Iterates over the response data, one line at a time. When stream=True is set on the request, this avoids reading the content at once into memory for large responses.
Note
This method is not reentrant safe. 

json(_** kwargs_)[source]¶
    
Decodes the JSON response body (if any) as a Python object.
This may return a dictionary, list, etc. depending on what is in the response. 

Parameters:
    
****kwargs** – Optional arguments that `json.loads` takes. 

Raises:
    
**requests.exceptions.JSONDecodeError** – If the response body does not contain valid json. 

_property_ links¶
    
Returns the parsed header links of the response, if any. 

_property_ next¶
    
Returns a PreparedRequest for the next request in a redirect chain, if there is one. 

_property_ ok¶
    
Returns True if `status_code` is less than 400, False if not.
This attribute checks if the status code of the response is between 400 and 600 to see if there was a client error or a server error. If the status code is between 200 and 400, this will return True. This is **not** a check to see if the response code is `200 OK`. 

raise_for_status()[source]¶
    
Raises `HTTPError`, if one occurred. 

raw¶
    
File-like object representation of response (for advanced usage). Use of `raw` requires that `stream=True` be set on the request. This requirement does not apply for use internally to Requests. 

reason¶
    
Textual reason of responded HTTP Status, e.g. “Not Found” or “OK”. 

request¶
    
The `PreparedRequest` object to which this is a response. 

status_code¶
    
Integer Code of responded HTTP Status, e.g. 404 or 200. 

_property_ text¶
    
Content of the response, in unicode.
If Response.encoding is None, encoding will be guessed using `charset_normalizer` or `chardet`.
The encoding of the response content is determined based solely on HTTP headers, following RFC 2616 to the letter. If you can take advantage of non-HTTP knowledge to make a better guess at the encoding, you should set `r.encoding` appropriately before accessing this property. 

url¶
    
Final URL location of Response.
## Lower-Lower-Level Classes¶ 

_class_ requests.PreparedRequest[source]¶
    
The fully mutable `PreparedRequest` object, containing the exact bytes that will be sent to the server.
Instances are generated from a `Request` object, and should not be instantiated manually; doing so may produce undesirable effects.
Usage:
```
>>> importrequests
>>> req = requests.Request('GET', 'https://httpbin.org/get')
>>> r = req.prepare()
>>> r
<PreparedRequest [GET]>
>>> s = requests.Session()
>>> s.send(r)
<Response [200]>

```


body¶
    
request body to send to the server. 

deregister_hook(_event_ , _hook_)¶
    
Deregister a previously registered hook. Returns True if the hook existed, False if not. 

headers¶
    
dictionary of HTTP headers. 

hooks¶
    
dictionary of callback hooks, for internal usage. 

method¶
    
HTTP verb to send to the server. 

_property_ path_url¶
    
Build the path URL to use. 

prepare(_method =None_, _url =None_, _headers =None_, _files =None_, _data =None_, _params =None_, _auth =None_, _cookies =None_, _hooks =None_, _json =None_)[source]¶
    
Prepares the entire request with the given parameters. 

prepare_auth(_auth_ , _url =''_)[source]¶
    
Prepares the given HTTP auth data. 

prepare_body(_data_ , _files_ , _json =None_)[source]¶
    
Prepares the given HTTP body data. 

prepare_content_length(_body_)[source]¶
    
Prepare Content-Length header based on request method and body 

prepare_cookies(_cookies_)[source]¶
    
Prepares the given HTTP cookie data.
This function eventually generates a `Cookie` header from the given cookies using cookielib. Due to cookielib’s design, the header will not be regenerated if it already exists, meaning this function can only be called once for the life of the `PreparedRequest` object. Any subsequent calls to `prepare_cookies` will have no actual effect, unless the “Cookie” header is removed beforehand. 

prepare_headers(_headers_)[source]¶
    
Prepares the given HTTP headers. 

prepare_hooks(_hooks_)[source]¶
    
Prepares the given hooks. 

prepare_method(_method_)[source]¶
    
Prepares the given HTTP method. 

prepare_url(_url_ , _params_)[source]¶
    
Prepares the given HTTP URL. 

register_hook(_event_ , _hook_)¶
    
Properly register a hook. 

url¶
    
HTTP URL to send the request to. 

_class_ requests.adapters.BaseAdapter[source]¶
    
The Base Transport Adapter 

close()[source]¶
    
Cleans up adapter specific items. 

send(_request_ , _stream =False_, _timeout =None_, _verify =True_, _cert =None_, _proxies =None_)[source]¶
    
Sends PreparedRequest object. Returns Response object. 

Parameters:
    
  * **request** – The `PreparedRequest` being sent.
  * **stream** – (optional) Whether to stream the request content.
  * **timeout** (_float_ _or_ _tuple_) – (optional) How long to wait for the server to send data before giving up, as a float, or a (connect timeout, read timeout) tuple.
  * **verify** – (optional) Either a boolean, in which case it controls whether we verify the server’s TLS certificate, or a string, in which case it must be a path to a CA bundle to use
  * **cert** – (optional) Any user-provided SSL certificate to be trusted.
  * **proxies** – (optional) The proxies dictionary to apply to the request.



_class_ requests.adapters.HTTPAdapter(_pool_connections =10_, _pool_maxsize =10_, _max_retries =0_, _pool_block =False_)[source]¶
    
The built-in HTTP Adapter for urllib3.
Provides a general-case interface for Requests sessions to contact HTTP and HTTPS urls by implementing the Transport Adapter interface. This class will usually be created by the `Session` class under the covers. 

Parameters:
    
  * **pool_connections** – The number of urllib3 connection pools to cache.
  * **pool_maxsize** – The maximum number of connections to save in the pool.
  * **max_retries** – The maximum number of retries each connection should attempt. Note, this applies only to failed DNS lookups, socket connections and connection timeouts, never to requests where data has made it to the server. By default, Requests does not retry failed connections. If you need granular control over the conditions under which we retry a request, import urllib3’s `Retry` class and pass that instead.
  * **pool_block** – Whether the connection pool should block for connections.


Usage:
```
>>> importrequests
>>> s = requests.Session()
>>> a = requests.adapters.HTTPAdapter(max_retries=3)
>>> s.mount('http://', a)

```


add_headers(_request_ , _** kwargs_)[source]¶
    
Add any headers needed by the connection. As of v2.0 this does nothing by default, but is left for overriding by users that subclass the `HTTPAdapter`.
This should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
  * **request** – The `PreparedRequest` to add headers to.
  * **kwargs** – The keyword arguments from the call to send().



build_connection_pool_key_attributes(_request_ , _verify_ , _cert =None_)[source]¶
    
Build the PoolKey attributes used by urllib3 to return a connection.
This looks at the PreparedRequest, the user-specified verify value, and the value of the cert parameter to determine what PoolKey values to use to select a connection from a given urllib3 Connection Pool.
The SSL related pool key arguments are not consistently set. As of this writing, use the following to determine what keys may be in that dictionary:
  * If `verify` is `True`, `"ssl_context"` will be set and will be the default Requests SSL Context
  * If `verify` is `False`, `"ssl_context"` will not be set but `"cert_reqs"` will be set
  * If `verify` is a string, (i.e., it is a user-specified trust bundle) `"ca_certs"` will be set if the string is not a directory recognized by `os.path.isdir`, otherwise `"ca_certs_dir"` will be set.
  * If `"cert"` is specified, `"cert_file"` will always be set. If `"cert"` is a tuple with a second item, `"key_file"` will also be present


To override these settings, one may subclass this class, call this method and use the above logic to change parameters as desired. For example, if one wishes to use a custom `ssl.SSLContext` one must both set `"ssl_context"` and based on what else they require, alter the other keys to ensure the desired behaviour. 

Parameters:
    
  * **request** (`PreparedRequest`) – The PreparedReqest being sent over the connection.
  * **verify** – Either a boolean, in which case it controls whether we verify the server’s TLS certificate, or a string, in which case it must be a path to a CA bundle to use.
  * **cert** – (optional) Any user-provided SSL certificate for client authentication (a.k.a., mTLS). This may be a string (i.e., just the path to a file which holds both certificate and key) or a tuple of length 2 with the certificate file path and key file path.



Returns:
    
A tuple of two dictionaries. The first is the “host parameters” portion of the Pool Key including scheme, hostname, and port. The second is a dictionary of SSLContext related parameters. 

build_response(_req_ , _resp_)[source]¶
    
Builds a `Response` object from a urllib3 response. This should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter` 

Parameters:
    
  * **req** – The `PreparedRequest` used to generate the response.
  * **resp** – The urllib3 response object.



Return type:
    
requests.Response 

cert_verify(_conn_ , _url_ , _verify_ , _cert_)[source]¶
    
Verify a SSL certificate. This method should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
  * **conn** – The urllib3 connection object associated with the cert.
  * **url** – The requested URL.
  * **verify** – Either a boolean, in which case it controls whether we verify the server’s TLS certificate, or a string, in which case it must be a path to a CA bundle to use
  * **cert** – The SSL certificate to verify.



close()[source]¶
    
Disposes of any internal state.
Currently, this closes the PoolManager and any active ProxyManager, which closes any pooled connections. 

get_connection(_url_ , _proxies =None_)[source]¶
    
DEPRECATED: Users should move to get_connection_with_tls_context for all subclasses of HTTPAdapter using Requests>=2.32.2.
Returns a urllib3 connection for the given URL. This should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
  * **url** – The URL to connect to.
  * **proxies** – (optional) A Requests-style dictionary of proxies used on this request.



Return type:
    
urllib3.ConnectionPool 

get_connection_with_tls_context(_request_ , _verify_ , _proxies =None_, _cert =None_)[source]¶
    
Returns a urllib3 connection for the given request and TLS settings. This should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
  * **request** – The `PreparedRequest` object to be sent over the connection.
  * **verify** – Either a boolean, in which case it controls whether we verify the server’s TLS certificate, or a string, in which case it must be a path to a CA bundle to use.
  * **proxies** – (optional) The proxies dictionary to apply to the request.
  * **cert** – (optional) Any user-provided SSL certificate to be used for client authentication (a.k.a., mTLS).



Return type:
    
urllib3.ConnectionPool 

init_poolmanager(_connections_ , _maxsize_ , _block =False_, _** pool_kwargs_)[source]¶
    
Initializes a urllib3 PoolManager.
This method should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
  * **connections** – The number of urllib3 connection pools to cache.
  * **maxsize** – The maximum number of connections to save in the pool.
  * **block** – Block when no free connections are available.
  * **pool_kwargs** – Extra keyword arguments used to initialize the Pool Manager.



proxy_headers(_proxy_)[source]¶
    
Returns a dictionary of the headers to add to any request sent through a proxy. This works with urllib3 magic to ensure that they are correctly sent to the proxy, rather than in a tunnelled request if CONNECT is being used.
This should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
**proxy** – The url of the proxy being used for this request. 

Return type:
    
dict 

proxy_manager_for(_proxy_ , _** proxy_kwargs_)[source]¶
    
Return urllib3 ProxyManager for the given proxy.
This method should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
  * **proxy** – The proxy to return a urllib3 ProxyManager for.
  * **proxy_kwargs** – Extra keyword arguments used to configure the Proxy Manager.



Returns:
    
ProxyManager 

Return type:
    
urllib3.ProxyManager 

request_url(_request_ , _proxies_)[source]¶
    
Obtain the url to use when making the final request.
If the message is being sent through a HTTP proxy, the full URL has to be used. Otherwise, we should only use the path portion of the URL.
This should not be called from user code, and is only exposed for use when subclassing the `HTTPAdapter`. 

Parameters:
    
  * **request** – The `PreparedRequest` being sent.
  * **proxies** – A dictionary of schemes or schemes and hosts to proxy URLs.



Return type:
    
str 

send(_request_ , _stream =False_, _timeout =None_, _verify =True_, _cert =None_, _proxies =None_)[source]¶
    
Sends PreparedRequest object. Returns Response object. 

Parameters:
    
  * **request** – The `PreparedRequest` being sent.
  * **stream** – (optional) Whether to stream the request content.
  * **timeout** (_float_ _or_ _tuple_ _or_ _urllib3 Timeout object_) – (optional) How long to wait for the server to send data before giving up, as a float, or a (connect timeout, read timeout) tuple.
  * **verify** – (optional) Either a boolean, in which case it controls whether we verify the server’s TLS certificate, or a string, in which case it must be a path to a CA bundle to use
  * **cert** – (optional) Any user-provided SSL certificate to be trusted.
  * **proxies** – (optional) The proxies dictionary to apply to the request.



Return type:
    
requests.Response
## Authentication¶ 

_class_ requests.auth.AuthBase[source]¶
    
Base class that all auth implementations derive from 

_class_ requests.auth.HTTPBasicAuth(_username_ , _password_)[source]¶
    
Attaches HTTP Basic Authentication to the given Request object. 

_class_ requests.auth.HTTPProxyAuth(_username_ , _password_)[source]¶
    
Attaches HTTP Proxy Authentication to a given Request object. 

_class_ requests.auth.HTTPDigestAuth(_username_ , _password_)[source]¶
    
Attaches HTTP Digest Authentication to the given Request object.
## Encodings¶ 

requests.utils.get_encodings_from_content(_content_)[source]¶
    
Returns encodings from given content string. 

Parameters:
    
**content** – bytestring to extract encodings from. 

requests.utils.get_encoding_from_headers(_headers_)[source]¶
    
Returns encodings from given HTTP Header Dict. 

Parameters:
    
**headers** – dictionary to extract encoding from. 

Return type:
    
str 

requests.utils.get_unicode_from_response(_r_)[source]¶
    
Returns the requested content back in unicode. 

Parameters:
    
**r** – Response object to get unicode content from.
Tried:
  1. charset from content-type
  2. fall back and replace all unicode characters



Return type:
    
str
## Cookies¶ 

requests.utils.dict_from_cookiejar(_cj_)[source]¶
    
Returns a key/value dictionary from a CookieJar. 

Parameters:
    
**cj** – CookieJar object to extract cookies from. 

Return type:
    
dict 

requests.utils.add_dict_to_cookiejar(_cj_ , _cookie_dict_)[source]¶
    
Returns a CookieJar from a key/value dictionary. 

Parameters:
    
  * **cj** – CookieJar to insert cookies into.
  * **cookie_dict** – Dict of key/values to insert into CookieJar.



Return type:
    
CookieJar 

requests.cookies.cookiejar_from_dict(_cookie_dict_ , _cookiejar =None_, _overwrite =True_)[source]¶
    
Returns a CookieJar from a key/value dictionary. 

Parameters:
    
  * **cookie_dict** – Dict of key/values to insert into CookieJar.
  * **cookiejar** – (optional) A cookiejar to add the cookies to.
  * **overwrite** – (optional) If False, will not replace cookies already in the jar with new ones.



Return type:
    
CookieJar 

_class_ requests.cookies.RequestsCookieJar(_policy =None_)[source]¶
    
Compatibility class; is a http.cookiejar.CookieJar, but exposes a dict interface.
This is the CookieJar we create by default for requests and sessions that don’t specify one, since some clients may expect response.cookies and session.cookies to support dict operations.
Requests does not use the dict interface internally; it’s just for compatibility with external client code. All requests code should work out of the box with externally provided instances of `CookieJar`, e.g. `LWPCookieJar` and `FileCookieJar`.
Unlike a regular CookieJar, this class is pickleable.
Warning
dictionary operations that are normally O(1) may be O(n). 

add_cookie_header(_request_)¶
    
Add correct Cookie: header to request (urllib.request.Request object).
The Cookie2 header is also added unless policy.hide_cookie2 is true. 

clear(_domain =None_, _path =None_, _name =None_)¶
    
Clear some cookies.
Invoking this method without arguments will clear all cookies. If given a single argument, only cookies belonging to that domain will be removed. If given two arguments, cookies belonging to the specified path within that domain are removed. If given three arguments, then the cookie with the specified name, path and domain is removed.
Raises KeyError if no matching cookie exists. 

clear_expired_cookies()¶
    
Discard all expired cookies.
You probably don’t need to call this method: expired cookies are never sent back to the server (provided you’re using DefaultCookiePolicy), this method is called by CookieJar itself every so often, and the .save() method won’t save expired cookies anyway (unless you ask otherwise by passing a true ignore_expires argument). 

clear_session_cookies()¶
    
Discard all session cookies.
Note that the .save() method won’t save session cookies anyway, unless you ask otherwise by passing a true ignore_discard argument. 

copy()[source]¶
    
Return a copy of this RequestsCookieJar. 

extract_cookies(_response_ , _request_)¶
    
Extract cookies from response, where allowable given the request. 

get(_name_ , _default =None_, _domain =None_, _path =None_)[source]¶
    
Dict-like get() that also supports optional domain and path args in order to resolve naming collisions from using one cookie jar over multiple domains.
Warning
operation is O(n), not O(1). 

get_dict(_domain =None_, _path =None_)[source]¶
    
Takes as an argument an optional domain and path and returns a plain old Python dict of name-value pairs of cookies that meet the requirements. 

Return type:
    
dict 

get_policy()[source]¶
    
Return the CookiePolicy instance used. 

items()[source]¶
    
Dict-like items() that returns a list of name-value tuples from the jar. Allows client-code to call `dict(RequestsCookieJar)` and get a vanilla python dict of key value pairs.
See also
keys() and values(). 

iteritems()[source]¶
    
Dict-like iteritems() that returns an iterator of name-value tuples from the jar.
See also
iterkeys() and itervalues(). 

iterkeys()[source]¶
    
Dict-like iterkeys() that returns an iterator of names of cookies from the jar.
See also
itervalues() and iteritems(). 

itervalues()[source]¶
    
Dict-like itervalues() that returns an iterator of values of cookies from the jar.
See also
iterkeys() and iteritems(). 

keys()[source]¶
    
Dict-like keys() that returns a list of names of cookies from the jar.
See also
values() and items(). 

list_domains()[source]¶
    
Utility method to list all the domains in the jar. 

list_paths()[source]¶
    
Utility method to list all the paths in the jar. 

make_cookies(_response_ , _request_)¶
    
Return sequence of Cookie objects extracted from response object. 

multiple_domains()[source]¶
    
Returns True if there are multiple domains in the jar. Returns False otherwise. 

Return type:
    
bool 

pop(_k_[, _d_]) → v, remove specified key and return the corresponding value.¶
    
If key is not found, d is returned if given, otherwise KeyError is raised. 

popitem() → (k, v), remove and return some (key, value) pair¶
    
as a 2-tuple; but raise KeyError if D is empty. 

set(_name_ , _value_ , _** kwargs_)[source]¶
    
Dict-like set() that also supports optional domain and path args in order to resolve naming collisions from using one cookie jar over multiple domains. 

set_cookie(_cookie_ , _* args_, _** kwargs_)[source]¶
    
Set a cookie, without checking whether or not it should be set. 

set_cookie_if_ok(_cookie_ , _request_)¶
    
Set a cookie if policy says it’s OK to do so. 

setdefault(_k_[, _d_]) → D.get(k,d), also set D[k]=d if k not in D¶


update(_other_)[source]¶
    
Updates this jar with cookies from another CookieJar or dict-like 

values()[source]¶
    
Dict-like values() that returns a list of values of cookies from the jar.
See also
keys() and items(). 

_class_ requests.cookies.CookieConflictError[source]¶
    
There are two cookies that meet the criteria specified in the cookie jar. Use .get and .set and include domain and path args in order to be more specific. 

add_note()¶
    
Exception.add_note(note) – add a note to the exception 

with_traceback()¶
    
Exception.with_traceback(tb) – set self.__traceback__ to tb and return self.
## Status Code Lookup¶ 

requests.codes¶
    
alias of {}
The `codes` object defines a mapping from common names for HTTP statuses to their numerical codes, accessible either as attributes or as dictionary items.
Example:
```
>>> importrequests
>>> requests.codes['temporary_redirect']
307
>>> requests.codes.teapot
418
>>> requests.codes['\o/']
200

```

Some codes have multiple names, and both upper- and lower-case versions of the names are allowed. For example, `codes.ok`, `codes.OK`, and `codes.okay` all correspond to the HTTP status code 200.
  * 100: `continue`
  * 101: `switching_protocols`
  * 102: `processing`, `early-hints`
  * 103: `checkpoint`
  * 122: `uri_too_long`, `request_uri_too_long`
  * 200: `ok`, `okay`, `all_ok`, `all_okay`, `all_good`, `\o/`, `✓`
  * 201: `created`
  * 202: `accepted`
  * 203: `non_authoritative_info`, `non_authoritative_information`
  * 204: `no_content`
  * 205: `reset_content`, `reset`
  * 206: `partial_content`, `partial`
  * 207: `multi_status`, `multiple_status`, `multi_stati`, `multiple_stati`
  * 208: `already_reported`
  * 226: `im_used`
  * 300: `multiple_choices`
  * 301: `moved_permanently`, `moved`, `\o-`
  * 302: `found`
  * 303: `see_other`, `other`
  * 304: `not_modified`
  * 305: `use_proxy`
  * 306: `switch_proxy`
  * 307: `temporary_redirect`, `temporary_moved`, `temporary`
  * 308: `permanent_redirect`, `resume_incomplete`, `resume`
  * 400: `bad_request`, `bad`
  * 401: `unauthorized`
  * 402: `payment_required`, `payment`
  * 403: `forbidden`
  * 404: `not_found`, `-o-`
  * 405: `method_not_allowed`, `not_allowed`
  * 406: `not_acceptable`
  * 407: `proxy_authentication_required`, `proxy_auth`, `proxy_authentication`
  * 408: `request_timeout`, `timeout`
  * 409: `conflict`
  * 410: `gone`
  * 411: `length_required`
  * 412: `precondition_failed`, `precondition`
  * 413: `request_entity_too_large`, `content_too_large`
  * 414: `request_uri_too_large`, `uri_too_long`
  * 415: `unsupported_media_type`, `unsupported_media`, `media_type`
  * 416: `requested_range_not_satisfiable`, `requested_range`, `range_not_satisfiable`
  * 417: `expectation_failed`
  * 418: `im_a_teapot`, `teapot`, `i_am_a_teapot`
  * 421: `misdirected_request`
  * 422: `unprocessable_entity`, `unprocessable`, `unprocessable_content`
  * 423: `locked`
  * 424: `failed_dependency`, `dependency`
  * 425: `unordered_collection`, `unordered`, `too_early`
  * 426: `upgrade_required`, `upgrade`
  * 428: `precondition_required`, `precondition`
  * 429: `too_many_requests`, `too_many`
  * 431: `header_fields_too_large`, `fields_too_large`
  * 444: `no_response`, `none`
  * 449: `retry_with`, `retry`
  * 450: `blocked_by_windows_parental_controls`, `parental_controls`
  * 451: `unavailable_for_legal_reasons`, `legal_reasons`
  * 499: `client_closed_request`
  * 500: `internal_server_error`, `server_error`, `/o\`, `✗`
  * 501: `not_implemented`
  * 502: `bad_gateway`
  * 503: `service_unavailable`, `unavailable`
  * 504: `gateway_timeout`
  * 505: `http_version_not_supported`, `http_version`
  * 506: `variant_also_negotiates`
  * 507: `insufficient_storage`
  * 509: `bandwidth_limit_exceeded`, `bandwidth`
  * 510: `not_extended`
  * 511: `network_authentication_required`, `network_auth`, `network_authentication`


## Migrating to 1.x¶
This section details the main differences between 0.x and 1.x and is meant to ease the pain of upgrading.
### API Changes¶
  * `Response.json` is now a callable and not a property of a response.
```
importrequests
r = requests.get('https://api.github.com/events')
r.json()  # This *call* raises an exception if JSON decoding fails

```

  * The `Session` API has changed. Sessions objects no longer take parameters. `Session` is also now capitalized, but it can still be instantiated with a lowercase `session` for backwards compatibility.
```
s = requests.Session()  # formerly, session took parameters
s.auth = auth
s.headers.update(headers)
r = s.get('https://httpbin.org/headers')

```

  * All request hooks have been removed except ‘response’.
  * Authentication helpers have been broken out into separate modules. See requests-oauthlib and requests-kerberos.


  * The parameter for streaming requests was changed from `prefetch` to `stream` and the logic was inverted. In addition, `stream` is now required for raw response reading.
```
# in 0.x, passing prefetch=False would accomplish the same thing
r = requests.get('https://api.github.com/events', stream=True)
for chunk in r.iter_content(8192):
  ...

```

  * The `config` parameter to the requests method has been removed. Some of these options are now configured on a `Session` such as keep-alive and maximum number of redirects. The verbosity option should be handled by configuring logging.
```
importrequests
importlogging
# Enabling debugging at http.client level (requests->urllib3->http.client)
# you will see the REQUEST, including HEADERS and DATA, and RESPONSE with HEADERS but without DATA.
# the only thing missing will be the response.body which is not logged.
try: # for Python 3
  fromhttp.clientimport HTTPConnection
except ImportError:
  fromhttplibimport HTTPConnection
HTTPConnection.debuglevel = 1
logging.basicConfig() # you need to initialize logging, otherwise you will not see anything from requests
logging.getLogger().setLevel(logging.DEBUG)
requests_log = logging.getLogger("urllib3")
requests_log.setLevel(logging.DEBUG)
requests_log.propagate = True
requests.get('https://httpbin.org/headers')

```



### Licensing¶
One key difference that has nothing to do with the API is a change in the license from the ISC license to the Apache 2.0 license. The Apache 2.0 license ensures that contributions to Requests are also covered by the Apache 2.0 license.
## Migrating to 2.x¶
Compared with the 1.0 release, there were relatively few backwards incompatible changes, but there are still a few issues to be aware of with this major release.
For more details on the changes in this release including new APIs, links to the relevant GitHub issues and some of the bug fixes, read Cory’s blog on the subject.
### API Changes¶
  * There were a couple changes to how Requests handles exceptions. `RequestException` is now a subclass of `IOError` rather than `RuntimeError` as that more accurately categorizes the type of error. In addition, an invalid URL escape sequence now raises a subclass of `RequestException` rather than a `ValueError`.
```
requests.get('http://%zz/')  # raises requests.exceptions.InvalidURL

```

Lastly, `httplib.IncompleteRead` exceptions caused by incorrect chunked encoding will now raise a Requests `ChunkedEncodingError` instead.
  * The proxy API has changed slightly. The scheme for a proxy URL is now required.
```
proxies = {
 "http": "10.10.1.10:3128",  # use http://10.10.1.10:3128 instead
}
# In requests 1.x, this was legal, in requests 2.x,
# this raises requests.exceptions.MissingSchema
requests.get("http://example.org", proxies=proxies)

```



### Behavioural Changes¶
  * Keys in the `headers` dictionary are now native strings on all Python versions, i.e. bytestrings on Python 2 and unicode on Python 3. If the keys are not native strings (unicode on Python 2 or bytestrings on Python 3) they will be converted to the native string type assuming UTF-8 encoding.
  * Values in the `headers` dictionary should always be strings. This has been the project’s position since before 1.0 but a recent change (since version 2.11.0) enforces this more strictly. It’s advised to avoid passing header values as unicode when possible.


Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Developer Interface
    * Main Interface
      * `request`
      * `head`
      * `get`
      * `post`
      * `put`
      * `patch`
      * `delete`
    * Exceptions
      * `RequestException`
      * `ConnectionError`
      * `HTTPError`
      * `TooManyRedirects`
      * `ConnectTimeout`
      * `ReadTimeout`
      * `Timeout`
      * `JSONDecodeError`
    * Request Sessions
      * `Session`
        * `Session.auth`
        * `Session.cert`
        * `Session.close`
        * `Session.cookies`
        * `Session.delete`
        * `Session.get`
        * `Session.get_adapter`
        * `Session.get_redirect_target`
        * `Session.head`
        * `Session.headers`
        * `Session.hooks`
        * `Session.max_redirects`
        * `Session.merge_environment_settings`
        * `Session.mount`
        * `Session.options`
        * `Session.params`
        * `Session.patch`
        * `Session.post`
        * `Session.prepare_request`
        * `Session.proxies`
        * `Session.put`
        * `Session.rebuild_auth`
        * `Session.rebuild_method`
        * `Session.rebuild_proxies`
        * `Session.request`
        * `Session.resolve_redirects`
        * `Session.send`
        * `Session.should_strip_auth`
        * `Session.stream`
        * `Session.trust_env`
        * `Session.verify`
    * Lower-Level Classes
      * `Request`
        * `Request.deregister_hook`
        * `Request.prepare`
        * `Request.register_hook`
      * `Response`
        * `Response.apparent_encoding`
        * `Response.close`
        * `Response.content`
        * `Response.cookies`
        * `Response.elapsed`
        * `Response.encoding`
        * `Response.headers`
        * `Response.history`
        * `Response.is_permanent_redirect`
        * `Response.is_redirect`
        * `Response.iter_content`
        * `Response.iter_lines`
        * `Response.json`
        * `Response.links`
        * `Response.next`
        * `Response.ok`
        * `Response.raise_for_status`
        * `Response.raw`
        * `Response.reason`
        * `Response.request`
        * `Response.status_code`
        * `Response.text`
        * `Response.url`
    * Lower-Lower-Level Classes
      * `PreparedRequest`
        * `PreparedRequest.body`
        * `PreparedRequest.deregister_hook`
        * `PreparedRequest.headers`
        * `PreparedRequest.hooks`
        * `PreparedRequest.method`
        * `PreparedRequest.path_url`
        * `PreparedRequest.prepare`
        * `PreparedRequest.prepare_auth`
        * `PreparedRequest.prepare_body`
        * `PreparedRequest.prepare_content_length`
        * `PreparedRequest.prepare_cookies`
        * `PreparedRequest.prepare_headers`
        * `PreparedRequest.prepare_hooks`
        * `PreparedRequest.prepare_method`
        * `PreparedRequest.prepare_url`
        * `PreparedRequest.register_hook`
        * `PreparedRequest.url`
      * `BaseAdapter`
        * `BaseAdapter.close`
        * `BaseAdapter.send`
      * `HTTPAdapter`
        * `HTTPAdapter.add_headers`
        * `HTTPAdapter.build_connection_pool_key_attributes`
        * `HTTPAdapter.build_response`
        * `HTTPAdapter.cert_verify`
        * `HTTPAdapter.close`
        * `HTTPAdapter.get_connection`
        * `HTTPAdapter.get_connection_with_tls_context`
        * `HTTPAdapter.init_poolmanager`
        * `HTTPAdapter.proxy_headers`
        * `HTTPAdapter.proxy_manager_for`
        * `HTTPAdapter.request_url`
        * `HTTPAdapter.send`
    * Authentication
      * `AuthBase`
      * `HTTPBasicAuth`
      * `HTTPProxyAuth`
      * `HTTPDigestAuth`
    * Encodings
      * `get_encodings_from_content`
      * `get_encoding_from_headers`
      * `get_unicode_from_response`
    * Cookies
      * `dict_from_cookiejar`
      * `add_dict_to_cookiejar`
      * `cookiejar_from_dict`
      * `RequestsCookieJar`
        * `RequestsCookieJar.add_cookie_header`
        * `RequestsCookieJar.clear`
        * `RequestsCookieJar.clear_expired_cookies`
        * `RequestsCookieJar.clear_session_cookies`
        * `RequestsCookieJar.copy`
        * `RequestsCookieJar.extract_cookies`
        * `RequestsCookieJar.get`
        * `RequestsCookieJar.get_dict`
        * `RequestsCookieJar.get_policy`
        * `RequestsCookieJar.items`
        * `RequestsCookieJar.iteritems`
        * `RequestsCookieJar.iterkeys`
        * `RequestsCookieJar.itervalues`
        * `RequestsCookieJar.keys`
        * `RequestsCookieJar.list_domains`
        * `RequestsCookieJar.list_paths`
        * `RequestsCookieJar.make_cookies`
        * `RequestsCookieJar.multiple_domains`
        * `RequestsCookieJar.pop`
        * `RequestsCookieJar.popitem`
        * `RequestsCookieJar.set`
        * `RequestsCookieJar.set_cookie`
        * `RequestsCookieJar.set_cookie_if_ok`
        * `RequestsCookieJar.setdefault`
        * `RequestsCookieJar.update`
        * `RequestsCookieJar.values`
      * `CookieConflictError`
        * `CookieConflictError.add_note`
        * `CookieConflictError.with_traceback`
    * Status Code Lookup
      * `codes`
    * Migrating to 1.x
      * API Changes
      * Licensing
    * Migrating to 2.x
      * API Changes
      * Behavioural Changes


### Related Topics
  * Documentation overview
    * Previous: Community Updates
    * Next: Contributor’s Guide


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Frequently Asked Questions¶
This part of the documentation answers common questions about Requests.
## Encoded Data?¶
Requests automatically decompresses gzip-encoded responses, and does its best to decode response content to unicode when possible.
When either the brotli or brotlicffi package is installed, requests also decodes Brotli-encoded responses.
You can get direct access to the raw response (and even the socket), if needed as well.
## Custom User-Agents?¶
Requests allows you to easily override User-Agent strings, along with any other HTTP Header. See documentation about headers.
## Why not Httplib2?¶
Chris Adams gave an excellent summary on Hacker News:
> httplib2 is part of why you should use requests: it’s far more respectable as a client but not as well documented and it still takes way too much code for basic operations. I appreciate what httplib2 is trying to do, that there’s a ton of hard low-level annoyances in building a modern HTTP client, but really, just use requests instead. Kenneth Reitz is very motivated and he gets the degree to which simple things should be simple whereas httplib2 feels more like an academic exercise than something people should use to build production systems[1].
> Disclosure: I’m listed in the requests AUTHORS file but can claim credit for, oh, about 0.0001% of the awesomeness.
> 1. http://code.google.com/p/httplib2/issues/detail?id=96 is a good example: an annoying bug which affect many people, there was a fix available for months, which worked great when I applied it in a fork and pounded a couple TB of data through it, but it took over a year to make it into trunk and even longer to make it onto PyPI where any other project which required ” httplib2” would get the working version.
## Python 3 Support?¶
Yes! Requests officially supports Python 3.8+ and PyPy.
## Python 2 Support?¶
No! As of Requests 2.28.0, Requests no longer supports Python 2.7. Users who have been unable to migrate should pin to requests<2.28. Full information can be found in psf/requests#6023.
It is _highly_ recommended users migrate to Python 3.8+ now since Python 2.7 is no longer receiving bug fixes or security updates as of January 1, 2020.
## What are “hostname doesn’t match” errors?¶
These errors occur when SSL certificate verification fails to match the certificate the server responds with to the hostname Requests thinks it’s contacting. If you’re certain the server’s SSL setup is correct (for example, because you can visit the site with your browser) and you’re using Python 2.7, a possible explanation is that you need Server-Name-Indication.
Server-Name-Indication, or SNI, is an official extension to SSL where the client tells the server what hostname it is contacting. This is important when servers are using Virtual Hosting. When such servers are hosting more than one SSL site they need to be able to return the appropriate certificate based on the hostname the client is connecting to.
Python 3 already includes native support for SNI in their SSL modules.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Frequently Asked Questions
    * Encoded Data?
    * Custom User-Agents?
    * Why not Httplib2?
    * Python 3 Support?
    * Python 2 Support?
    * What are “hostname doesn’t match” errors?


### Related Topics
  * Documentation overview
    * Previous: Recommended Packages and Extensions
    * Next: Integrations


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Integrations¶
# Articles & Talks¶
  * Daniel Greenfeld’s Review of Requests
  * Issac Kelly’s ‘Consuming Web APIs’ talk
  * Blog post about Requests via Yum
  * Russian blog post introducing Requests
  * Sending JSON in Requests


Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Integrations
  * Articles & Talks


### Related Topics
  * Documentation overview
    * Previous: Frequently Asked Questions
    * Next: Support


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Recommended Packages and Extensions¶
Requests has a great variety of powerful and useful third-party extensions. This page provides an overview of some of the best of them.
## Certifi CA Bundle¶
Certifi is a carefully curated collection of Root Certificates for validating the trustworthiness of SSL certificates while verifying the identity of TLS hosts. It has been extracted from the Requests project.
## CacheControl¶
CacheControl is an extension that adds a full HTTP cache to Requests. This makes your web requests substantially more efficient, and should be used whenever you’re making a lot of web requests.
## Requests-Toolbelt¶
Requests-Toolbelt is a collection of utilities that some users of Requests may desire, but do not belong in Requests proper. This library is actively maintained by members of the Requests core team, and reflects the functionality most requested by users within the community.
## Requests-Threads¶
Requests-Threads is a Requests session that returns the amazing Twisted’s awaitable Deferreds instead of Response objects. This allows the use of `async`/`await` keyword usage on Python 3, or Twisted’s style of programming, if desired.
## Requests-OAuthlib¶
requests-oauthlib makes it possible to do the OAuth dance from Requests automatically. This is useful for the large number of websites that use OAuth to provide authentication. It also provides a lot of tweaks that handle ways that specific OAuth providers differ from the standard specifications.
## Betamax¶
Betamax records your HTTP interactions so the NSA does not have to. A VCR imitation designed only for Python-Requests.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Recommended Packages and Extensions
    * Certifi CA Bundle
    * CacheControl
    * Requests-Toolbelt
    * Requests-Threads
    * Requests-OAuthlib
    * Betamax


### Related Topics
  * Documentation overview
    * Previous: Authentication
    * Next: Frequently Asked Questions


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Release Process and Rules¶
New in version v2.6.2.
Starting with the version to be released after `v2.6.2`, the following rules will govern and describe how the Requests core team produces a new release.
## Major Releases¶
A major release will include breaking changes. When it is versioned, it will be versioned as `vX.0.0`. For example, if the previous release was `v10.2.7` the next version will be `v11.0.0`.
Breaking changes are changes that break backwards compatibility with prior versions. If the project were to change the `text` attribute on a `Response` object to a method, that would only happen in a Major release.
Major releases may also include miscellaneous bug fixes. The core developers of Requests are committed to providing a good user experience. This means we’re also committed to preserving backwards compatibility as much as possible. Major releases will be infrequent and will need strong justifications before they are considered.
## Minor Releases¶
A minor release will not include breaking changes but may include miscellaneous bug fixes. If the previous version of Requests released was `v10.2.7` a minor release would be versioned as `v10.3.0`.
Minor releases will be backwards compatible with releases that have the same major version number. In other words, all versions that would start with `v10.` should be compatible with each other.
## Hotfix Releases¶
A hotfix release will only include bug fixes that were missed when the project released the previous version. If the previous version of Requests released `v10.2.7` the hotfix release would be versioned as `v10.2.8`.
Hotfixes will **not** include upgrades to vendored dependencies after `v2.6.2`
## Reasoning¶
In the 2.5 and 2.6 release series, the Requests core team upgraded vendored dependencies and caused a great deal of headaches for both users and the core team. To reduce this pain, we’re forming a concrete set of procedures so expectations will be properly set.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Release Process and Rules
    * Major Releases
    * Minor Releases
    * Hotfix Releases
    * Reasoning


### Related Topics
  * Documentation overview
    * Previous: Vulnerability Disclosure
    * Next: Community Updates


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Support¶
If you have questions or issues about Requests, there are several options:
## Stack Overflow¶
If your question does not contain sensitive (possibly proprietary) information or can be properly anonymized, please ask a question on Stack Overflow and use the tag `python-requests`.
## File an Issue¶
If you notice some unexpected behaviour in Requests, or want to see support for a new feature, file an issue on GitHub.
## Send a Tweet¶
If your question is less than 280 characters, feel free to send a tweet to @nateprewitt, @sethmlarson, or @sigmavirus24.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Support
    * Stack Overflow
    * File an Issue
    * Send a Tweet


### Related Topics
  * Documentation overview
    * Previous: Integrations
    * Next: Vulnerability Disclosure


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Authors¶
Requests was lovingly created by Kenneth Reitz.
## Keepers of the Crystals¶
  * Nate Prewitt @nateprewitt.
  * Seth M. Larson @sethmlarson.


## Previous Keepers of Crystals¶
  * Kenneth Reitz <me@kennethreitz.org> @kennethreitz, reluctant Keeper of the Master Crystal.
  * Cory Benfield <cory@lukasa.co.uk> @lukasa
  * Ian Cordasco <graffatcolmingov@gmail.com> @sigmavirus24.


## Patches and Suggestions¶
  * Various Pocoo Members
  * Chris Adams
  * Flavio Percoco Premoli
  * Dj Gilcrease
  * Justin Murphy
  * Rob Madole
  * Aram Dulyan
  * Johannes Gorset
  * 村山めがね (Megane Murayama)
  * James Rowe
  * Daniel Schauenberg
  * Zbigniew Siciarz
  * Daniele Tricoli ‘Eriol’
  * Richard Boulton
  * Miguel Olivares <miguel@moliware.com>
  * Alberto Paro
  * Jérémy Bethmont
  * 潘旭 (Xu Pan)
  * Tamás Gulácsi
  * Rubén Abad
  * Peter Manser
  * Jeremy Selier
  * Jens Diemer
  * Alex (@alopatin)
  * Tom Hogans <tomhsx@gmail.com>
  * Armin Ronacher
  * Shrikant Sharat Kandula
  * Mikko Ohtamaa
  * Den Shabalin
  * Daniel Miller <danielm@vs-networks.com>
  * Alejandro Giacometti
  * Rick Mak
  * Johan Bergström
  * Josselin Jacquard
  * Travis N. Vaught
  * Fredrik Möllerstrand
  * Daniel Hengeveld
  * Dan Head
  * Bruno Renié
  * David Fischer
  * Joseph McCullough
  * Juergen Brendel
  * Juan Riaza
  * Ryan Kelly
  * Rolando Espinoza La fuente
  * Robert Gieseke
  * Idan Gazit
  * Ed Summers
  * Chris Van Horne
  * Christopher Davis
  * Ori Livneh
  * Jason Emerick
  * Bryan Helmig
  * Jonas Obrist
  * Lucian Ursu
  * Tom Moertel
  * Frank Kumro Jr
  * Chase Sterling
  * Marty Alchin
  * takluyver
  * Ben Toews (@mastahyeti)
  * David Kemp
  * Brendon Crawford
  * Denis (@Telofy)
  * Matt Giuca
  * Adam Tauber
  * Honza Javorek
  * Brendan Maguire <maguire.brendan@gmail.com>
  * Chris Dary
  * Danver Braganza <danverbraganza@gmail.com>
  * Max Countryman
  * Nick Chadwick
  * Jonathan Drosdeck
  * Jiri Machalek
  * Steve Pulec
  * Michael Kelly
  * Michael Newman <newmaniese@gmail.com>
  * Jonty Wareing <jonty@jonty.co.uk>
  * Shivaram Lingamneni
  * Miguel Turner
  * Rohan Jain (@crodjer)
  * Justin Barber <barber.justin@gmail.com>
  * Roman Haritonov (@reclosedev)
  * Josh Imhoff <joshimhoff13@gmail.com>
  * Arup Malakar <amalakar@gmail.com>
  * Danilo Bargen (@dbrgn)
  * Torsten Landschoff
  * Michael Holler (@apotheos)
  * Timnit Gebru
  * Sarah Gonzalez
  * Victoria Mo
  * Leila Muhtasib
  * Matthias Rahlf <matthias@webding.de>
  * Jakub Roztocil <jakub@roztocil.name>
  * Rhys Elsmore
  * André Graf (@dergraf)
  * Stephen Zhuang (@everbird)
  * Martijn Pieters
  * Jonatan Heyman
  * David Bonner <dbonner@gmail.com> (@rascalking)
  * Vinod Chandru
  * Johnny Goodnow <j.goodnow29@gmail.com>
  * Denis Ryzhkov <denisr@denisr.com>
  * Wilfred Hughes <me@wilfred.me.uk>
  * Dmitry Medvinsky <me@dmedvinsky.name>
  * Bryce Boe <bbzbryce@gmail.com> (@bboe)
  * Colin Dunklau <colin.dunklau@gmail.com> (@cdunklau)
  * Bob Carroll <bob.carroll@alum.rit.edu> (@rcarz)
  * Hugo Osvaldo Barrera <hugo@barrera.io> (@hobarrera)
  * Łukasz Langa <lukasz@langa.pl>
  * Dave Shawley <daveshawley@gmail.com>
  * James Clarke (@jam)
  * Kevin Burke <kev@inburke.com>
  * Flavio Curella
  * David Pursehouse <david.pursehouse@gmail.com> (@dpursehouse)
  * Jon Parise (@jparise)
  * Alexander Karpinsky (@homm86)
  * Marc Schlaich (@schlamar)
  * Park Ilsu <daftonshady@gmail.com> (@daftshady)
  * Matt Spitz (@mattspitz)
  * Vikram Oberoi (@voberoi)
  * Can Ibanoglu <can.ibanoglu@gmail.com> (@canibanoglu)
  * Thomas Weißschuh <thomas@t-8ch.de> (@t-8ch)
  * Jayson Vantuyl <jayson@aggressive.ly>
  * Pengfei.X <pengphy@gmail.com>
  * Kamil Madac <kamil.madac@gmail.com>
  * Michael Becker <mike@beckerfuffle.com> (@beckerfuffle)
  * Erik Wickstrom <erik@erikwickstrom.com> (@erikwickstrom)
  * Константин Подшумок (@podshumok)
  * Ben Bass (@codedstructure)
  * Jonathan Wong <evolutionace@gmail.com> (@ContinuousFunction)
  * Martin Jul (@mjul)
  * Joe Alcorn (@buttscicles)
  * Syed Suhail Ahmed <ssuhail.ahmed93@gmail.com> (@syedsuhail)
  * Scott Sadler (@ssadler)
  * Arthur Darcet (@arthurdarcet)
  * Ulrich Petri (@ulope)
  * Muhammad Yasoob Ullah Khalid <yasoob.khld@gmail.com> (@yasoob)
  * Paul van der Linden (@pvanderlinden)
  * Colin Dickson (@colindickson)
  * Smiley Barry (@smiley)
  * Shagun Sodhani (@shagunsodhani)
  * Robin Linderborg (@vienno)
  * Brian Samek (@bsamek)
  * Dmitry Dygalo (@Stranger6667)
  * piotrjurkiewicz
  * Jesse Shapiro <jesse@jesseshapiro.net> (@haikuginger)
  * Nate Prewitt <nate.prewitt@gmail.com> (@nateprewitt)
  * Maik Himstedt
  * Michael Hunsinger
  * Brian Bamsch <bbamsch32@gmail.com> (@bbamsch)
  * Om Prakash Kumar <omprakash070@gmail.com> (@iamprakashom)
  * Philipp Konrad <gardiac2002@gmail.com> (@gardiac2002)
  * Hussain Tamboli <hussaintamboli18@gmail.com> (@hussaintamboli)
  * Casey Davidson (@davidsoncasey)
  * Andrii Soldatenko (@a_soldatenko)
  * Moinuddin Quadri <moin18@gmail.com> (@moin18)
  * Matt Kohl (@mattkohl)
  * Jonathan Vanasco (@jvanasco)
  * David Fontenot (@davidfontenot)
  * Shmuel Amar (@shmuelamar)
  * Gary Wu (@garywu)
  * Ryan Pineo (@ryanpineo)
  * Ed Morley (@edmorley)
  * Matt Liu <liumatt@gmail.com> (@mlcrazy)
  * Taylor Hoff <primdevs@protonmail.com> (@PrimordialHelios)
  * Arthur Vigil (@ahvigil)
  * Nehal J Wani (@nehaljwani)
  * Demetrios Bairaktaris (@DemetriosBairaktaris)
  * Darren Dormer (@ddormer)
  * Rajiv Mayani (@mayani)
  * Antti Kaihola (@akaihola)
  * “Dull Bananas” <dull.bananas0@gmail.com> (@dullbananas)
  * Alessio Izzo (@aless10)
  * Sylvain Marié (@smarie)
  * Hod Bin Noon (@hodbn)
  * Mike Fiedler (@miketheman)


Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Authors
    * Keepers of the Crystals
    * Previous Keepers of Crystals
    * Patches and Suggestions


### Related Topics
  * Documentation overview
    * Previous: Contributor’s Guide


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Vulnerability Disclosure¶
The latest vulnerability disclosure information can be found on GitHub in our Security Policy.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Previous: Support
    * Next: Release Process and Rules


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Community Updates¶
If you’d like to stay up to date on the community and development of Requests, there are several options:
## GitHub¶
The best way to track the development of Requests is through the GitHub repo.
# Release History¶
## dev¶
  * [Short description of non-trivial change.]


## 2.32.3 (2024-05-29)¶
**Bugfixes** - Fixed bug breaking the ability to specify custom SSLContexts in sub-classes of
> HTTPAdapter. (#6716)
  * Fixed issue where Requests started failing to run on Python versions compiled without the ssl module. (#6724)


## 2.32.2 (2024-05-21)¶
**Deprecations** - To provide a more stable migration for custom HTTPAdapters impacted
> by the CVE changes in 2.32.0, we’ve renamed _get_connection to a new public API, get_connection_with_tls_context. Existing custom HTTPAdapters will need to migrate their code to use this new API. get_connection is considered deprecated in all versions of Requests>=2.32.0.
> A minimal (2-line) example has been provided in the linked PR to ease migration, but we strongly urge users to evaluate if their custom adapter is subject to the same issue described in CVE-2024-35195. (#6710)
## 2.32.1 (2024-05-20)¶
**Bugfixes** - Add missing test certs to the sdist distributed on PyPI.
## 2.32.0 (2024-05-20)¶
**Security** - Fixed an issue where setting verify=False on the first request from a
> Session will cause subsequent requests to the _same origin_ to also ignore cert verification, regardless of the value of verify. (https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56)
**Improvements** - verify=True now reuses a global SSLContext which should improve
> request time variance between first and subsequent requests. It should also minimize certificate load time on Windows systems when using a Python version built with OpenSSL 3.x. (#6667)
  * Requests now supports optional use of character detection (chardet or charset_normalizer) when repackaged or vendored. This enables pip and other projects to minimize their vendoring surface area. The Response.text() and apparent_encoding APIs will default to utf-8 if neither library is present. (#6702)


**Bugfixes** - Fixed bug in length detection where emoji length was incorrectly
> calculated in the request content-length. (#6589)
  * Fixed deserialization bug in JSONDecodeError. (#6629)
  * Fixed bug where an extra leading / (path separator) could lead urllib3 to unnecessarily reparse the request URI. (#6644)


**Deprecations**
  * Requests has officially added support for CPython 3.12 (#6503)
  * Requests has officially added support for PyPy 3.9 and 3.10 (#6641)
  * Requests has officially dropped support for CPython 3.7 (#6642)
  * Requests has officially dropped support for PyPy 3.7 and 3.8 (#6641)


**Documentation** - Various typo fixes and doc improvements.
**Packaging** - Requests has started adopting some modern packaging practices.
> The source files for the projects (formerly requests) is now located in src/requests in the Requests sdist. (#6506)
  * Starting in Requests 2.33.0, Requests will migrate to a PEP 517 build system using hatchling. This should not impact the average user, but extremely old versions of packaging utilities may have issues with the new packaging format.


## 2.31.0 (2023-05-22)¶
**Security** - Versions of Requests between v2.3.0 and v2.30.0 are vulnerable to potential
> forwarding of Proxy-Authorization headers to destination servers when following HTTPS redirects.
> When proxies are defined with user info (https://user:pass@proxy:8080), Requests will construct a Proxy-Authorization header that is attached to the request to authenticate with the proxy.
> In cases where Requests receives a redirect response, it previously reattached the Proxy-Authorization header incorrectly, resulting in the value being sent through the tunneled connection to the destination server. Users who rely on defining their proxy credentials in the URL are _strongly_ encouraged to upgrade to Requests 2.31.0+ to prevent unintentional leakage and rotate their proxy credentials once the change has been fully deployed.
> Users who do not use a proxy or do not supply their proxy credentials through the user information portion of their proxy URL are not subject to this vulnerability.
> Full details can be read in our [Github Security Advisory](https://github.com/psf/requests/security/advisories/GHSA-j8r2-6x86-q33q) and [CVE-2023-32681](https://nvd.nist.gov/vuln/detail/CVE-2023-32681).
## 2.30.0 (2023-05-03)¶
**Dependencies** - ⚠️ Added support for urllib3 2.0. ⚠️
> This may contain minor breaking changes so we advise careful testing and reviewing https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html prior to upgrading.
> Users who wish to stay on urllib3 1.x can pin to urllib3<2.
## 2.29.0 (2023-04-26)¶
**Improvements**
  * Requests now defers chunked requests to the urllib3 implementation to improve standardization. (#6226)
  * Requests relaxes header component requirements to support bytes/str subclasses. (#6356)


## 2.28.2 (2023-01-12)¶
**Dependencies**
  * Requests now supports charset_normalizer 3.x. (#6261)


**Bugfixes**
  * Updated MissingSchema exception to suggest https scheme rather than http. (#6188)


## 2.28.1 (2022-06-29)¶
**Improvements**
  * Speed optimization in iter_content with transition to yield from. (#6170)


**Dependencies**
  * Added support for chardet 5.0.0 (#6179)
  * Added support for charset-normalizer 2.1.0 (#6169)


## 2.28.0 (2022-06-09)¶
**Deprecations**
  * ⚠️ Requests has officially dropped support for Python 2.7. ⚠️ (#6091)
  * Requests has officially dropped support for Python 3.6 (including pypy3.6). (#6091)


**Improvements**
  * Wrap JSON parsing issues in Request’s JSONDecodeError for payloads without an encoding to make json() API consistent. (#6097)
  * Parse header components consistently, raising an InvalidHeader error in all invalid cases. (#6154)
  * Added provisional 3.11 support with current beta build. (#6155)
  * Requests got a makeover and we decided to paint it black. (#6095)


**Bugfixes**
  * Fixed bug where setting CURL_CA_BUNDLE to an empty string would disable cert verification. All Requests 2.x versions before 2.28.0 are affected. (#6074)
  * Fixed urllib3 exception leak, wrapping urllib3.exceptions.SSLError with requests.exceptions.SSLError for content and iter_content. (#6057)
  * Fixed issue where invalid Windows registry entries caused proxy resolution to raise an exception rather than ignoring the entry. (#6149)
  * Fixed issue where entire payload could be included in the error message for JSONDecodeError. (#6036)


## 2.27.1 (2022-01-05)¶
**Bugfixes**
  * Fixed parsing issue that resulted in the auth component being dropped from proxy URLs. (#6028)


## 2.27.0 (2022-01-03)¶
**Improvements**
  * Officially added support for Python 3.10. (#5928)
  * Added a requests.exceptions.JSONDecodeError to unify JSON exceptions between Python 2 and 3. This gets raised in the response.json() method, and is backwards compatible as it inherits from previously thrown exceptions. Can be caught from requests.exceptions.RequestException as well. (#5856)
  * Improved error text for misnamed InvalidSchema and MissingSchema exceptions. This is a temporary fix until exceptions can be renamed (Schema->Scheme). (#6017)
  * Improved proxy parsing for proxy URLs missing a scheme. This will address recent changes to urlparse in Python 3.9+. (#5917)


**Bugfixes**
  * Fixed defect in extract_zipped_paths which could result in an infinite loop for some paths. (#5851)
  * Fixed handling for AttributeError when calculating length of files obtained by Tarfile.extractfile(). (#5239)
  * Fixed urllib3 exception leak, wrapping urllib3.exceptions.InvalidHeader with requests.exceptions.InvalidHeader. (#5914)
  * Fixed bug where two Host headers were sent for chunked requests. (#5391)
  * Fixed regression in Requests 2.26.0 where Proxy-Authorization was incorrectly stripped from all requests sent with Session.send. (#5924)
  * Fixed performance regression in 2.26.0 for hosts with a large number of proxies available in the environment. (#5924)
  * Fixed idna exception leak, wrapping UnicodeError with requests.exceptions.InvalidURL for URLs with a leading dot (.) in the domain. (#5414)


**Deprecations**
  * Requests support for Python 2.7 and 3.6 will be ending in 2022. While we don’t have exact dates, Requests 2.27.x is likely to be the last release series providing support.


## 2.26.0 (2021-07-13)¶
**Improvements**
  * Requests now supports Brotli compression, if either the brotli or brotlicffi package is installed. (#5783)
  * Session.send now correctly resolves proxy configurations from both the Session and Request. Behavior now matches Session.request. (#5681)


**Bugfixes**
  * Fixed a race condition in zip extraction when using Requests in parallel from zip archive. (#5707)


**Dependencies**
  * Instead of chardet, use the MIT-licensed charset_normalizer for Python3 to remove license ambiguity for projects bundling requests. If chardet is already installed on your machine it will be used instead of charset_normalizer to keep backwards compatibility. (#5797)
You can also install chardet while installing requests by specifying [use_chardet_on_py3] extra as follows:
> ``shell pip install "requests[use_chardet_on_py3]" ``
Python2 still depends upon the chardet module.
  * Requests now supports idna 3.x on Python 3. idna 2.x will continue to be used on Python 2 installations. (#5711)


**Deprecations**
  * The requests[security] extra has been converted to a no-op install. PyOpenSSL is no longer the recommended secure option for Requests. (#5867)
  * Requests has officially dropped support for Python 3.5. (#5867)


## 2.25.1 (2020-12-16)¶
**Bugfixes**
  * Requests now treats application/json as utf8 by default. Resolving inconsistencies between r.text and r.json output. (#5673)


**Dependencies**
  * Requests now supports chardet v4.x.


## 2.25.0 (2020-11-11)¶
**Improvements**
  * Added support for NETRC environment variable. (#5643)


**Dependencies**
  * Requests now supports urllib3 v1.26.


**Deprecations**
  * Requests v2.25.x will be the last release series with support for Python 3.5.
  * The requests[security] extra is officially deprecated and will be removed in Requests v2.26.0.


## 2.24.0 (2020-06-17)¶
**Improvements**
  * pyOpenSSL TLS implementation is now only used if Python either doesn’t have an ssl module or doesn’t support SNI. Previously pyOpenSSL was unconditionally used if available. This applies even if pyOpenSSL is installed via the requests[security] extra (#5443)
  * Redirect resolution should now only occur when allow_redirects is True. (#5492)
  * No longer perform unnecessary Content-Length calculation for requests that won’t use it. (#5496)


## 2.23.0 (2020-02-19)¶
**Improvements**
  * Remove defunct reference to prefetch in Session __attrs__ (#5110)


**Bugfixes**
  * Requests no longer outputs password in basic auth usage warning. (#5099)


**Dependencies**
  * Pinning for chardet and idna now uses major version instead of minor. This hopefully reduces the need for releases every time a dependency is updated.


## 2.22.0 (2019-05-15)¶
**Dependencies**
  * Requests now supports urllib3 v1.25.2. (note: 1.25.0 and 1.25.1 are incompatible)


**Deprecations**
  * Requests has officially stopped support for Python 3.4.


## 2.21.0 (2018-12-10)¶
**Dependencies**
  * Requests now supports idna v2.8.


## 2.20.1 (2018-11-08)¶
**Bugfixes**
  * Fixed bug with unintended Authorization header stripping for redirects using default ports (http/80, https/443).


## 2.20.0 (2018-10-18)¶
**Bugfixes**
  * Content-Type header parsing is now case-insensitive (e.g. charset=utf8 v Charset=utf8).
  * Fixed exception leak where certain redirect urls would raise uncaught urllib3 exceptions.
  * Requests removes Authorization header from requests redirected from https to http on the same hostname. (CVE-2018-18074)
  * should_bypass_proxies now handles URIs without hostnames (e.g. files).


**Dependencies**
  * Requests now supports urllib3 v1.24.


**Deprecations**
  * Requests has officially stopped support for Python 2.6.


## 2.19.1 (2018-06-14)¶
**Bugfixes**
  * Fixed issue where status_codes.py’s init function failed trying to append to a __doc__ value of None.


## 2.19.0 (2018-06-12)¶
**Improvements**
  * Warn user about possible slowdown when using cryptography version &lt; 1.3.4
  * Check for invalid host in proxy URL, before forwarding request to adapter.
  * Fragments are now properly maintained across redirects. (RFC7231 7.1.2)
  * Removed use of cgi module to expedite library load time.
  * Added support for SHA-256 and SHA-512 digest auth algorithms.
  * Minor performance improvement to Request.content.
  * Migrate to using collections.abc for 3.7 compatibility.


**Bugfixes**
  * Parsing empty Link headers with parse_header_links() no longer return one bogus entry.
  * Fixed issue where loading the default certificate bundle from a zip archive would raise an IOError.
  * Fixed issue with unexpected ImportError on windows system which do not support winreg module.
  * DNS resolution in proxy bypass no longer includes the username and password in the request. This also fixes the issue of DNS queries failing on macOS.
  * Properly normalize adapter prefixes for url comparison.
  * Passing None as a file pointer to the files param no longer raises an exception.
  * Calling copy on a RequestsCookieJar will now preserve the cookie policy correctly.


**Dependencies**
  * We now support idna v2.7.
  * We now support urllib3 v1.23.


## 2.18.4 (2017-08-15)¶
**Improvements**
  * Error messages for invalid headers now include the header name for easier debugging


**Dependencies**
  * We now support idna v2.6.


## 2.18.3 (2017-08-02)¶
**Improvements**
  * Running $ python -m requests.help now includes the installed version of idna.


**Bugfixes**
  * Fixed issue where Requests would raise ConnectionError instead of SSLError when encountering SSL problems when using urllib3 v1.22.


## 2.18.2 (2017-07-25)¶
**Bugfixes**
  * requests.help no longer fails on Python 2.6 due to the absence of ssl.OPENSSL_VERSION_NUMBER.


**Dependencies**
  * We now support urllib3 v1.22.


## 2.18.1 (2017-06-14)¶
**Bugfixes**
  * Fix an error in the packaging whereby the *.whl contained incorrect data that regressed the fix in v2.17.3.


## 2.18.0 (2017-06-14)¶
**Improvements**
  * Response is now a context manager, so can be used directly in a with statement without first having to be wrapped by contextlib.closing().


**Bugfixes**
  * Resolve installation failure if multiprocessing is not available
  * Resolve tests crash if multiprocessing is not able to determine the number of CPU cores
  * Resolve error swallowing in utils set_environ generator


## 2.17.3 (2017-05-29)¶
**Improvements**
  * Improved packages namespace identity support, for monkeypatching libraries.


## 2.17.2 (2017-05-29)¶
**Improvements**
  * Improved packages namespace identity support, for monkeypatching libraries.


## 2.17.1 (2017-05-29)¶
**Improvements**
  * Improved packages namespace identity support, for monkeypatching libraries.


## 2.17.0 (2017-05-29)¶
**Improvements**
  * Removal of the 301 redirect cache. This improves thread-safety.


## 2.16.5 (2017-05-28)¶
  * Improvements to $ python -m requests.help.


## 2.16.4 (2017-05-27)¶
  * Introduction of the $ python -m requests.help command, for debugging with maintainers!


## 2.16.3 (2017-05-27)¶
  * Further restored the requests.packages namespace for compatibility reasons.


## 2.16.2 (2017-05-27)¶
  * Further restored the requests.packages namespace for compatibility reasons.


No code modification (noted below) should be necessary any longer.
## 2.16.1 (2017-05-27)¶
  * Restored the requests.packages namespace for compatibility reasons.
  * Bugfix for urllib3 version parsing.


**Note** : code that was written to import against the requests.packages namespace previously will have to import code that rests at this module-level now.
For example:
> from requests.packages.urllib3.poolmanager import PoolManager
Will need to be re-written to be:
> from requests.packages import urllib3 urllib3.poolmanager.PoolManager
Or, even better:
> from urllib3.poolmanager import PoolManager
## 2.16.0 (2017-05-26)¶
  * Unvendor ALL the things!


## 2.15.1 (2017-05-26)¶
  * Everyone makes mistakes.


## 2.15.0 (2017-05-26)¶
**Improvements**
  * Introduction of the Response.next property, for getting the next PreparedResponse from a redirect chain (when allow_redirects=False).
  * Internal refactoring of __version__ module.


**Bugfixes**
  * Restored once-optional parameter for requests.utils.get_environ_proxies().


## 2.14.2 (2017-05-10)¶
**Bugfixes**
  * Changed a less-than to an equal-to and an or in the dependency markers to widen compatibility with older setuptools releases.


## 2.14.1 (2017-05-09)¶
**Bugfixes**
  * Changed the dependency markers to widen compatibility with older pip releases.


## 2.14.0 (2017-05-09)¶
**Improvements**
  * It is now possible to pass no_proxy as a key to the proxies dictionary to provide handling similar to the NO_PROXY environment variable.
  * When users provide invalid paths to certificate bundle files or directories Requests now raises IOError, rather than failing at the time of the HTTPS request with a fairly inscrutable certificate validation error.
  * The behavior of SessionRedirectMixin was slightly altered. resolve_redirects will now detect a redirect by calling get_redirect_target(response) instead of directly querying Response.is_redirect and Response.headers[‘location’]. Advanced users will be able to process malformed redirects more easily.
  * Changed the internal calculation of elapsed request time to have higher resolution on Windows.
  * Added win_inet_pton as conditional dependency for the [socks] extra on Windows with Python 2.7.
  * Changed the proxy bypass implementation on Windows: the proxy bypass check doesn’t use forward and reverse DNS requests anymore
  * URLs with schemes that begin with http but are not http or https no longer have their host parts forced to lowercase.


**Bugfixes**
  * Much improved handling of non-ASCII Location header values in redirects. Fewer UnicodeDecodeErrors are encountered on Python 2, and Python 3 now correctly understands that Latin-1 is unlikely to be the correct encoding.
  * If an attempt to seek file to find out its length fails, we now appropriately handle that by aborting our content-length calculations.
  * Restricted HTTPDigestAuth to only respond to auth challenges made on 4XX responses, rather than to all auth challenges.
  * Fixed some code that was firing DeprecationWarning on Python 3.6.
  * The dismayed person emoticon (/o\) no longer has a big head. I’m sure this is what you were all worrying about most.


**Miscellaneous**
  * Updated bundled urllib3 to v1.21.1.
  * Updated bundled chardet to v3.0.2.
  * Updated bundled idna to v2.5.
  * Updated bundled certifi to 2017.4.17.


## 2.13.0 (2017-01-24)¶
**Features**
  * Only load the idna library when we’ve determined we need it. This will save some memory for users.


**Miscellaneous**
  * Updated bundled urllib3 to 1.20.
  * Updated bundled idna to 2.2.


## 2.12.5 (2017-01-18)¶
**Bugfixes**
  * Fixed an issue with JSON encoding detection, specifically detecting big-endian UTF-32 with BOM.


## 2.12.4 (2016-12-14)¶
**Bugfixes**
  * Fixed regression from 2.12.2 where non-string types were rejected in the basic auth parameters. While support for this behaviour has been re-added, the behaviour is deprecated and will be removed in the future.


## 2.12.3 (2016-12-01)¶
**Bugfixes**
  * Fixed regression from v2.12.1 for URLs with schemes that begin with “http”. These URLs have historically been processed as though they were HTTP-schemed URLs, and so have had parameters added. This was removed in v2.12.2 in an overzealous attempt to resolve problems with IDNA-encoding those URLs. This change was reverted: the other fixes for IDNA-encoding have been judged to be sufficient to return to the behaviour Requests had before v2.12.0.


## 2.12.2 (2016-11-30)¶
**Bugfixes**
  * Fixed several issues with IDNA-encoding URLs that are technically invalid but which are widely accepted. Requests will now attempt to IDNA-encode a URL if it can but, if it fails, and the host contains only ASCII characters, it will be passed through optimistically. This will allow users to opt-in to using IDNA2003 themselves if they want to, and will also allow technically invalid but still common hostnames.
  * Fixed an issue where URLs with leading whitespace would raise InvalidSchema errors.
  * Fixed an issue where some URLs without the HTTP or HTTPS schemes would still have HTTP URL preparation applied to them.
  * Fixed an issue where Unicode strings could not be used in basic auth.
  * Fixed an issue encountered by some Requests plugins where constructing a Response object would cause Response.content to raise an AttributeError.


## 2.12.1 (2016-11-16)¶
**Bugfixes**
  * Updated setuptools ‘security’ extra for the new PyOpenSSL backend in urllib3.


**Miscellaneous**
  * Updated bundled urllib3 to 1.19.1.


## 2.12.0 (2016-11-15)¶
**Improvements**
  * Updated support for internationalized domain names from IDNA2003 to IDNA2008. This updated support is required for several forms of IDNs and is mandatory for .de domains.
  * Much improved heuristics for guessing content lengths: Requests will no longer read an entire StringIO into memory.
  * Much improved logic for recalculating Content-Length headers for PreparedRequest objects.
  * Improved tolerance for file-like objects that have no tell method but do have a seek method.
  * Anything that is a subclass of Mapping is now treated like a dictionary by the data= keyword argument.
  * Requests now tolerates empty passwords in proxy credentials, rather than stripping the credentials.
  * If a request is made with a file-like object as the body and that request is redirected with a 307 or 308 status code, Requests will now attempt to rewind the body object so it can be replayed.


**Bugfixes**
  * When calling response.close, the call to close will be propagated through to non-urllib3 backends.
  * Fixed issue where the ALL_PROXY environment variable would be preferred over scheme-specific variables like HTTP_PROXY.
  * Fixed issue where non-UTF8 reason phrases got severely mangled by falling back to decoding using ISO 8859-1 instead.
  * Fixed a bug where Requests would not correctly correlate cookies set when using custom Host headers if those Host headers did not use the native string type for the platform.


**Miscellaneous**
  * Updated bundled urllib3 to 1.19.
  * Updated bundled certifi certs to 2016.09.26.


## 2.11.1 (2016-08-17)¶
**Bugfixes**
  * Fixed a bug when using iter_content with decode_unicode=True for streamed bodies would raise AttributeError. This bug was introduced in 2.11.
  * Strip Content-Type and Transfer-Encoding headers from the header block when following a redirect that transforms the verb from POST/PUT to GET.


## 2.11.0 (2016-08-08)¶
**Improvements**
  * Added support for the ALL_PROXY environment variable.
  * Reject header values that contain leading whitespace or newline characters to reduce risk of header smuggling.


**Bugfixes**
  * Fixed occasional TypeError when attempting to decode a JSON response that occurred in an error case. Now correctly returns a ValueError.
  * Requests would incorrectly ignore a non-CIDR IP address in the NO_PROXY environment variables: Requests now treats it as a specific IP.
  * Fixed a bug when sending JSON data that could cause us to encounter obscure OpenSSL errors in certain network conditions (yes, really).
  * Added type checks to ensure that iter_content only accepts integers and None for chunk sizes.
  * Fixed issue where responses whose body had not been fully consumed would have the underlying connection closed but not returned to the connection pool, which could cause Requests to hang in situations where the HTTPAdapter had been configured to use a blocking connection pool.


**Miscellaneous**
  * Updated bundled urllib3 to 1.16.
  * Some previous releases accidentally accepted non-strings as acceptable header values. This release does not.


## 2.10.0 (2016-04-29)¶
**New Features**
  * SOCKS Proxy Support! (requires PySocks; $ pip install requests[socks])


**Miscellaneous**
  * Updated bundled urllib3 to 1.15.1.


## 2.9.2 (2016-04-29)¶
**Improvements**
  * Change built-in CaseInsensitiveDict (used for headers) to use OrderedDict as its underlying datastore.


**Bugfixes**
  * Don’t use redirect_cache if allow_redirects=False
  * When passed objects that throw exceptions from tell(), send them via chunked transfer encoding instead of failing.
  * Raise a ProxyError for proxy related connection issues.


## 2.9.1 (2015-12-21)¶
**Bugfixes**
  * Resolve regression introduced in 2.9.0 that made it impossible to send binary strings as bodies in Python 3.
  * Fixed errors when calculating cookie expiration dates in certain locales.


**Miscellaneous**
  * Updated bundled urllib3 to 1.13.1.


## 2.9.0 (2015-12-15)¶
**Minor Improvements** (Backwards compatible)
  * The verify keyword argument now supports being passed a path to a directory of CA certificates, not just a single-file bundle.
  * Warnings are now emitted when sending files opened in text mode.
  * Added the 511 Network Authentication Required status code to the status code registry.


**Bugfixes**
  * For file-like objects that are not sought to the very beginning, we now send the content length for the number of bytes we will actually read, rather than the total size of the file, allowing partial file uploads.
  * When uploading file-like objects, if they are empty or have no obvious content length we set Transfer-Encoding: chunked rather than Content-Length: 0.
  * We correctly receive the response in buffered mode when uploading chunked bodies.
  * We now handle being passed a query string as a bytestring on Python 3, by decoding it as UTF-8.
  * Sessions are now closed in all cases (exceptional and not) when using the functional API rather than leaking and waiting for the garbage collector to clean them up.
  * Correctly handle digest auth headers with a malformed qop directive that contains no token, by treating it the same as if no qop directive was provided at all.
  * Minor performance improvements when removing specific cookies by name.


**Miscellaneous**
  * Updated urllib3 to 1.13.


## 2.8.1 (2015-10-13)¶
**Bugfixes**
  * Update certificate bundle to match certifi 2015.9.6.2’s weak certificate bundle.
  * Fix a bug in 2.8.0 where requests would raise ConnectTimeout instead of ConnectionError
  * When using the PreparedRequest flow, requests will now correctly respect the json parameter. Broken in 2.8.0.
  * When using the PreparedRequest flow, requests will now correctly handle a Unicode-string method name on Python 2. Broken in 2.8.0.


## 2.8.0 (2015-10-05)¶
**Minor Improvements** (Backwards Compatible)
  * Requests now supports per-host proxies. This allows the proxies dictionary to have entries of the form {‘<scheme>://<hostname>’: ‘<proxy>’}. Host-specific proxies will be used in preference to the previously-supported scheme-specific ones, but the previous syntax will continue to work.
  * Response.raise_for_status now prints the URL that failed as part of the exception message.
  * requests.utils.get_netrc_auth now takes an raise_errors kwarg, defaulting to False. When True, errors parsing .netrc files cause exceptions to be thrown.
  * Change to bundled projects import logic to make it easier to unbundle requests downstream.
  * Changed the default User-Agent string to avoid leaking data on Linux: now contains only the requests version.


**Bugfixes**
  * The json parameter to post() and friends will now only be used if neither data nor files are present, consistent with the documentation.
  * We now ignore empty fields in the NO_PROXY environment variable.
  * Fixed problem where httplib.BadStatusLine would get raised if combining stream=True with contextlib.closing.
  * Prevented bugs where we would attempt to return the same connection back to the connection pool twice when sending a Chunked body.
  * Miscellaneous minor internal changes.
  * Digest Auth support is now thread safe.


**Updates**
  * Updated urllib3 to 1.12.


## 2.7.0 (2015-05-03)¶
This is the first release that follows our new release process. For more, see [our documentation](https://requests.readthedocs.io/en/latest/community/release-process/).
**Bugfixes**
  * Updated urllib3 to 1.10.4, resolving several bugs involving chunked transfer encoding and response framing.


## 2.6.2 (2015-04-23)¶
**Bugfixes**
  * Fix regression where compressed data that was sent as chunked data was not properly decompressed. (#2561)


## 2.6.1 (2015-04-22)¶
**Bugfixes**
  * Remove VendorAlias import machinery introduced in v2.5.2.
  * Simplify the PreparedRequest.prepare API: We no longer require the user to pass an empty list to the hooks keyword argument. (c.f. #2552)
  * Resolve redirects now receives and forwards all of the original arguments to the adapter. (#2503)
  * Handle UnicodeDecodeErrors when trying to deal with a unicode URL that cannot be encoded in ASCII. (#2540)
  * Populate the parsed path of the URI field when performing Digest Authentication. (#2426)
  * Copy a PreparedRequest’s CookieJar more reliably when it is not an instance of RequestsCookieJar. (#2527)


## 2.6.0 (2015-03-14)¶
**Bugfixes**
  * CVE-2015-2296: Fix handling of cookies on redirect. Previously a cookie without a host value set would use the hostname for the redirected URL exposing requests users to session fixation attacks and potentially cookie stealing. This was disclosed privately by Matthew Daley of [BugFuzz](https://bugfuzz.com). This affects all versions of requests from v2.1.0 to v2.5.3 (inclusive on both ends).
  * Fix error when requests is an install_requires dependency and python setup.py test is run. (#2462)
  * Fix error when urllib3 is unbundled and requests continues to use the vendored import location.
  * Include fixes to urllib3’s header handling.
  * Requests’ handling of unvendored dependencies is now more restrictive.


**Features and Improvements**
  * Support bytearrays when passed as parameters in the files argument. (#2468)
  * Avoid data duplication when creating a request with str, bytes, or bytearray input to the files argument.


## 2.5.3 (2015-02-24)¶
**Bugfixes**
  * Revert changes to our vendored certificate bundle. For more context see (#2455, #2456, and <https://bugs.python.org/issue23476>)


## 2.5.2 (2015-02-23)¶
**Features and Improvements**
  * Add sha256 fingerprint support. ([shazow/urllib3#540](https://github.com/shazow/urllib3/pull/540))
  * Improve the performance of headers. ([shazow/urllib3#544](https://github.com/shazow/urllib3/pull/544))


**Bugfixes**
  * Copy pip’s import machinery. When downstream redistributors remove requests.packages.urllib3 the import machinery will continue to let those same symbols work. Example usage in requests’ documentation and 3rd-party libraries relying on the vendored copies of urllib3 will work without having to fallback to the system urllib3.
  * Attempt to quote parts of the URL on redirect if unquoting and then quoting fails. (#2356)
  * Fix filename type check for multipart form-data uploads. (#2411)
  * Properly handle the case where a server issuing digest authentication challenges provides both auth and auth-int qop-values. (#2408)
  * Fix a socket leak. ([shazow/urllib3#549](https://github.com/shazow/urllib3/pull/549))
  * Fix multiple Set-Cookie headers properly. ([shazow/urllib3#534](https://github.com/shazow/urllib3/pull/534))
  * Disable the built-in hostname verification. ([shazow/urllib3#526](https://github.com/shazow/urllib3/pull/526))
  * Fix the behaviour of decoding an exhausted stream. ([shazow/urllib3#535](https://github.com/shazow/urllib3/pull/535))


**Security**
  * Pulled in an updated cacert.pem.
  * Drop RC4 from the default cipher list. ([shazow/urllib3#551](https://github.com/shazow/urllib3/pull/551))


## 2.5.1 (2014-12-23)¶
**Behavioural Changes**
  * Only catch HTTPErrors in raise_for_status (#2382)


**Bugfixes**
  * Handle LocationParseError from urllib3 (#2344)
  * Handle file-like object filenames that are not strings (#2379)
  * Unbreak HTTPDigestAuth handler. Allow new nonces to be negotiated (#2389)


## 2.5.0 (2014-12-01)¶
**Improvements**
  * Allow usage of urllib3’s Retry object with HTTPAdapters (#2216)
  * The iter_lines method on a response now accepts a delimiter with which to split the content (#2295)


**Behavioural Changes**
  * Add deprecation warnings to functions in requests.utils that will be removed in 3.0 (#2309)
  * Sessions used by the functional API are always closed (#2326)
  * Restrict requests to HTTP/1.1 and HTTP/1.0 (stop accepting HTTP/0.9) (#2323)


**Bugfixes**
  * Only parse the URL once (#2353)
  * Allow Content-Length header to always be overridden (#2332)
  * Properly handle files in HTTPDigestAuth (#2333)
  * Cap redirect_cache size to prevent memory abuse (#2299)
  * Fix HTTPDigestAuth handling of redirects after authenticating successfully (#2253)
  * Fix crash with custom method parameter to Session.request (#2317)
  * Fix how Link headers are parsed using the regular expression library (#2271)


**Documentation**
  * Add more references for interlinking (#2348)
  * Update CSS for theme (#2290)
  * Update width of buttons and sidebar (#2289)
  * Replace references of Gittip with Gratipay (#2282)
  * Add link to changelog in sidebar (#2273)


## 2.4.3 (2014-10-06)¶
**Bugfixes**
  * Unicode URL improvements for Python 2.
  * Re-order JSON param for backwards compat.
  * Automatically defrag authentication schemes from host/pass URIs. ([#2249](https://github.com/psf/requests/issues/2249))


## 2.4.2 (2014-10-05)¶
**Improvements**
  * FINALLY! Add json parameter for uploads! ([#2258](https://github.com/psf/requests/pull/2258))
  * Support for bytestring URLs on Python 3.x ([#2238](https://github.com/psf/requests/pull/2238))


**Bugfixes**
  * Avoid getting stuck in a loop ([#2244](https://github.com/psf/requests/pull/2244))
  * Multiple calls to iter* fail with unhelpful error. ([#2240](https://github.com/psf/requests/issues/2240), [#2241](https://github.com/psf/requests/issues/2241))


**Documentation**
  * Correct redirection introduction ([#2245](https://github.com/psf/requests/pull/2245/))
  * Added example of how to send multiple files in one request. ([#2227](https://github.com/psf/requests/pull/2227/))
  * Clarify how to pass a custom set of CAs ([#2248](https://github.com/psf/requests/pull/2248/))


## 2.4.1 (2014-09-09)¶
  * Now has a “security” package extras set, $ pip install requests[security]
  * Requests will now use Certifi if it is available.
  * Capture and re-raise urllib3 ProtocolError
  * Bugfix for responses that attempt to redirect to themselves forever (wtf?).


## 2.4.0 (2014-08-29)¶
**Behavioral Changes**
  * Connection: keep-alive header is now sent automatically.


**Improvements**
  * Support for connect timeouts! Timeout now accepts a tuple (connect, read) which is used to set individual connect and read timeouts.
  * Allow copying of PreparedRequests without headers/cookies.
  * Updated bundled urllib3 version.
  * Refactored settings loading from environment – new Session.merge_environment_settings.
  * Handle socket errors in iter_content.


## 2.3.0 (2014-05-16)¶
**API Changes**
  * New Response property is_redirect, which is true when the library could have processed this response as a redirection (whether or not it actually did).
  * The timeout parameter now affects requests with both stream=True and stream=False equally.
  * The change in v2.0.0 to mandate explicit proxy schemes has been reverted. Proxy schemes now default to http://.
  * The CaseInsensitiveDict used for HTTP headers now behaves like a normal dictionary when references as string or viewed in the interpreter.


**Bugfixes**
  * No longer expose Authorization or Proxy-Authorization headers on redirect. Fix CVE-2014-1829 and CVE-2014-1830 respectively.
  * Authorization is re-evaluated each redirect.
  * On redirect, pass url as native strings.
  * Fall-back to autodetected encoding for JSON when Unicode detection fails.
  * Headers set to None on the Session are now correctly not sent.
  * Correctly honor decode_unicode even if it wasn’t used earlier in the same response.
  * Stop advertising compress as a supported Content-Encoding.
  * The Response.history parameter is now always a list.
  * Many, many urllib3 bugfixes.


## 2.2.1 (2014-01-23)¶
**Bugfixes**
  * Fixes incorrect parsing of proxy credentials that contain a literal or encoded ‘#’ character.
  * Assorted urllib3 fixes.


## 2.2.0 (2014-01-09)¶
**API Changes**
  * New exception: ContentDecodingError. Raised instead of urllib3 DecodeError exceptions.


**Bugfixes**
  * Avoid many many exceptions from the buggy implementation of proxy_bypass on OS X in Python 2.6.
  * Avoid crashing when attempting to get authentication credentials from ~/.netrc when running as a user without a home directory.
  * Use the correct pool size for pools of connections to proxies.
  * Fix iteration of CookieJar objects.
  * Ensure that cookies are persisted over redirect.
  * Switch back to using chardet, since it has merged with charade.


## 2.1.0 (2013-12-05)¶
  * Updated CA Bundle, of course.
  * Cookies set on individual Requests through a Session (e.g. via Session.get()) are no longer persisted to the Session.
  * Clean up connections when we hit problems during chunked upload, rather than leaking them.
  * Return connections to the pool when a chunked upload is successful, rather than leaking it.
  * Match the HTTPbis recommendation for HTTP 301 redirects.
  * Prevent hanging when using streaming uploads and Digest Auth when a 401 is received.
  * Values of headers set by Requests are now always the native string type.
  * Fix previously broken SNI support.
  * Fix accessing HTTP proxies using proxy authentication.
  * Unencode HTTP Basic usernames and passwords extracted from URLs.
  * Support for IP address ranges for no_proxy environment variable
  * Parse headers correctly when users override the default Host: header.
  * Avoid munging the URL in case of case-sensitive servers.
  * Looser URL handling for non-HTTP/HTTPS urls.
  * Accept unicode methods in Python 2.6 and 2.7.
  * More resilient cookie handling.
  * Make Response objects pickleable.
  * Actually added MD5-sess to Digest Auth instead of pretending to like last time.
  * Updated internal urllib3.
  * Fixed @Lukasa’s lack of taste.


## 2.0.1 (2013-10-24)¶
  * Updated included CA Bundle with new mistrusts and automated process for the future
  * Added MD5-sess to Digest Auth
  * Accept per-file headers in multipart file POST messages.
  * Fixed: Don’t send the full URL on CONNECT messages.
  * Fixed: Correctly lowercase a redirect scheme.
  * Fixed: Cookies not persisted when set via functional API.
  * Fixed: Translate urllib3 ProxyError into a requests ProxyError derived from ConnectionError.
  * Updated internal urllib3 and chardet.


## 2.0.0 (2013-09-24)¶
**API Changes:**
  * Keys in the Headers dictionary are now native strings on all Python versions, i.e. bytestrings on Python 2, unicode on Python 3.
  * Proxy URLs now _must_ have an explicit scheme. A MissingSchema exception will be raised if they don’t.
  * Timeouts now apply to read time if Stream=False.
  * RequestException is now a subclass of IOError, not RuntimeError.
  * Added new method to PreparedRequest objects: PreparedRequest.copy().
  * Added new method to Session objects: Session.update_request(). This method updates a Request object with the data (e.g. cookies) stored on the Session.
  * Added new method to Session objects: Session.prepare_request(). This method updates and prepares a Request object, and returns the corresponding PreparedRequest object.
  * Added new method to HTTPAdapter objects: HTTPAdapter.proxy_headers(). This should not be called directly, but improves the subclass interface.
  * httplib.IncompleteRead exceptions caused by incorrect chunked encoding will now raise a Requests ChunkedEncodingError instead.
  * Invalid percent-escape sequences now cause a Requests InvalidURL exception to be raised.
  * HTTP 208 no longer uses reason phrase “im_used”. Correctly uses “already_reported”.
  * HTTP 226 reason added (“im_used”).


**Bugfixes:**
  * Vastly improved proxy support, including the CONNECT verb. Special thanks to the many contributors who worked towards this improvement.
  * Cookies are now properly managed when 401 authentication responses are received.
  * Chunked encoding fixes.
  * Support for mixed case schemes.
  * Better handling of streaming downloads.
  * Retrieve environment proxies from more locations.
  * Minor cookies fixes.
  * Improved redirect behaviour.
  * Improved streaming behaviour, particularly for compressed data.
  * Miscellaneous small Python 3 text encoding bugs.
  * .netrc no longer overrides explicit auth.
  * Cookies set by hooks are now correctly persisted on Sessions.
  * Fix problem with cookies that specify port numbers in their host field.
  * BytesIO can be used to perform streaming uploads.
  * More generous parsing of the no_proxy environment variable.
  * Non-string objects can be passed in data values alongside files.


## 1.2.3 (2013-05-25)¶
  * Simple packaging fix


## 1.2.2 (2013-05-23)¶
  * Simple packaging fix


## 1.2.1 (2013-05-20)¶
  * 301 and 302 redirects now change the verb to GET for all verbs, not just POST, improving browser compatibility.
  * Python 3.3.2 compatibility
  * Always percent-encode location headers
  * Fix connection adapter matching to be most-specific first
  * new argument to the default connection adapter for passing a block argument
  * prevent a KeyError when there’s no link headers


## 1.2.0 (2013-03-31)¶
  * Fixed cookies on sessions and on requests
  * Significantly change how hooks are dispatched - hooks now receive all the arguments specified by the user when making a request so hooks can make a secondary request with the same parameters. This is especially necessary for authentication handler authors
  * certifi support was removed
  * Fixed bug where using OAuth 1 with body signature_type sent no data
  * Major proxy work thanks to @Lukasa including parsing of proxy authentication from the proxy url
  * Fix DigestAuth handling too many 401s
  * Update vendored urllib3 to include SSL bug fixes
  * Allow keyword arguments to be passed to json.loads() via the Response.json() method
  * Don’t send Content-Length header by default on GET or HEAD requests
  * Add elapsed attribute to Response objects to time how long a request took.
  * Fix RequestsCookieJar
  * Sessions and Adapters are now picklable, i.e., can be used with the multiprocessing library
  * Update charade to version 1.0.3


The change in how hooks are dispatched will likely cause a great deal of issues.
## 1.1.0 (2013-01-10)¶
  * CHUNKED REQUESTS
  * Support for iterable response bodies
  * Assume servers persist redirect params
  * Allow explicit content types to be specified for file data
  * Make merge_kwargs case-insensitive when looking up keys


## 1.0.3 (2012-12-18)¶
  * Fix file upload encoding bug
  * Fix cookie behavior


## 1.0.2 (2012-12-17)¶
  * Proxy fix for HTTPAdapter.


## 1.0.1 (2012-12-17)¶
  * Cert verification exception bug.
  * Proxy fix for HTTPAdapter.


## 1.0.0 (2012-12-17)¶
  * Massive Refactor and Simplification
  * Switch to Apache 2.0 license
  * Swappable Connection Adapters
  * Mountable Connection Adapters
  * Mutable ProcessedRequest chain
  * /s/prefetch/stream
  * Removal of all configuration
  * Standard library logging
  * Make Response.json() callable, not property.
  * Usage of new charade project, which provides python 2 and 3 simultaneous chardet.
  * Removal of all hooks except ‘response’
  * Removal of all authentication helpers (OAuth, Kerberos)


This is not a backwards compatible change.
## 0.14.2 (2012-10-27)¶
  * Improved mime-compatible JSON handling
  * Proxy fixes
  * Path hack fixes
  * Case-Insensitive Content-Encoding headers
  * Support for CJK parameters in form posts


## 0.14.1 (2012-10-01)¶
  * Python 3.3 Compatibility
  * Simply default accept-encoding
  * Bugfixes


## 0.14.0 (2012-09-02)¶
  * No more iter_content errors if already downloaded.


## 0.13.9 (2012-08-25)¶
  * Fix for OAuth + POSTs
  * Remove exception eating from dispatch_hook
  * General bugfixes


## 0.13.8 (2012-08-21)¶
  * Incredible Link header support :)


## 0.13.7 (2012-08-19)¶
  * Support for (key, value) lists everywhere.
  * Digest Authentication improvements.
  * Ensure proxy exclusions work properly.
  * Clearer UnicodeError exceptions.
  * Automatic casting of URLs to strings (fURL and such)
  * Bugfixes.


## 0.13.6 (2012-08-06)¶
  * Long awaited fix for hanging connections!


## 0.13.5 (2012-07-27)¶
  * Packaging fix


## 0.13.4 (2012-07-27)¶
  * GSSAPI/Kerberos authentication!
  * App Engine 2.7 Fixes!
  * Fix leaking connections (from urllib3 update)
  * OAuthlib path hack fix
  * OAuthlib URL parameters fix.


## 0.13.3 (2012-07-12)¶
  * Use simplejson if available.
  * Do not hide SSLErrors behind Timeouts.
  * Fixed param handling with urls containing fragments.
  * Significantly improved information in User Agent.
  * client certificates are ignored when verify=False


## 0.13.2 (2012-06-28)¶
  * Zero dependencies (once again)!
  * New: Response.reason
  * Sign querystring parameters in OAuth 1.0
  * Client certificates no longer ignored when verify=False
  * Add openSUSE certificate support


## 0.13.1 (2012-06-07)¶
  * Allow passing a file or file-like object as data.
  * Allow hooks to return responses that indicate errors.
  * Fix Response.text and Response.json for body-less responses.


## 0.13.0 (2012-05-29)¶
  * Removal of Requests.async in favor of [grequests](https://github.com/kennethreitz/grequests)
  * Allow disabling of cookie persistence.
  * New implementation of safe_mode
  * cookies.get now supports default argument
  * Session cookies not saved when Session.request is called with return_response=False
  * Env: no_proxy support.
  * RequestsCookieJar improvements.
  * Various bug fixes.


## 0.12.1 (2012-05-08)¶
  * New Response.json property.
  * Ability to add string file uploads.
  * Fix out-of-range issue with iter_lines.
  * Fix iter_content default size.
  * Fix POST redirects containing files.


## 0.12.0 (2012-05-02)¶
  * EXPERIMENTAL OAUTH SUPPORT!
  * Proper CookieJar-backed cookies interface with awesome dict-like interface.
  * Speed fix for non-iterated content chunks.
  * Move pre_request to a more usable place.
  * New pre_send hook.
  * Lazily encode data, params, files.
  * Load system Certificate Bundle if certify isn’t available.
  * Cleanups, fixes.


## 0.11.2 (2012-04-22)¶
  * Attempt to use the OS’s certificate bundle if certifi isn’t available.
  * Infinite digest auth redirect fix.
  * Multi-part file upload improvements.
  * Fix decoding of invalid %encodings in URLs.
  * If there is no content in a response don’t throw an error the second time that content is attempted to be read.
  * Upload data on redirects.


## 0.11.1 (2012-03-30)¶
  * POST redirects now break RFC to do what browsers do: Follow up with a GET.
  * New strict_mode configuration to disable new redirect behavior.


## 0.11.0 (2012-03-14)¶
  * Private SSL Certificate support
  * Remove select.poll from Gevent monkeypatching
  * Remove redundant generator for chunked transfer encoding
  * Fix: Response.ok raises Timeout Exception in safe_mode


## 0.10.8 (2012-03-09)¶
  * Generate chunked ValueError fix
  * Proxy configuration by environment variables
  * Simplification of iter_lines.
  * New trust_env configuration for disabling system/environment hints.
  * Suppress cookie errors.


## 0.10.7 (2012-03-07)¶
  * encode_uri = False


## 0.10.6 (2012-02-25)¶
  * Allow ‘=’ in cookies.


## 0.10.5 (2012-02-25)¶
  * Response body with 0 content-length fix.
  * New async.imap.
  * Don’t fail on netrc.


## 0.10.4 (2012-02-20)¶
  * Honor netrc.


## 0.10.3 (2012-02-20)¶
  * HEAD requests don’t follow redirects anymore.
  * raise_for_status() doesn’t raise for 3xx anymore.
  * Make Session objects picklable.
  * ValueError for invalid schema URLs.


## 0.10.2 (2012-01-15)¶
  * Vastly improved URL quoting.
  * Additional allowed cookie key values.
  * Attempted fix for “Too many open files” Error
  * Replace unicode errors on first pass, no need for second pass.
  * Append ‘/’ to bare-domain urls before query insertion.
  * Exceptions now inherit from RuntimeError.
  * Binary uploads + auth fix.
  * Bugfixes.


## 0.10.1 (2012-01-23)¶
  * PYTHON 3 SUPPORT!
  * Dropped 2.5 Support. (_Backwards Incompatible_)


## 0.10.0 (2012-01-21)¶
  * Response.content is now bytes-only. (_Backwards Incompatible_)
  * New Response.text is unicode-only.
  * If no Response.encoding is specified and chardet is available, Response.text will guess an encoding.
  * Default to ISO-8859-1 (Western) encoding for “text” subtypes.
  * Removal of decode_unicode. (_Backwards Incompatible_)
  * New multiple-hooks system.
  * New Response.register_hook for registering hooks within the pipeline.
  * Response.url is now Unicode.


## 0.9.3 (2012-01-18)¶
  * SSL verify=False bugfix (apparent on windows machines).


## 0.9.2 (2012-01-18)¶
  * Asynchronous async.send method.
  * Support for proper chunk streams with boundaries.
  * session argument for Session classes.
  * Print entire hook tracebacks, not just exception instance.
  * Fix response.iter_lines from pending next line.
  * Fix but in HTTP-digest auth w/ URI having query strings.
  * Fix in Event Hooks section.
  * Urllib3 update.


## 0.9.1 (2012-01-06)¶
  * danger_mode for automatic Response.raise_for_status()
  * Response.iter_lines refactor


## 0.9.0 (2011-12-28)¶
  * verify ssl is default.


## 0.8.9 (2011-12-28)¶
  * Packaging fix.


## 0.8.8 (2011-12-28)¶
  * SSL CERT VERIFICATION!
  * Release of Cerifi: Mozilla’s cert list.
  * New ‘verify’ argument for SSL requests.
  * Urllib3 update.


## 0.8.7 (2011-12-24)¶
  * iter_lines last-line truncation fix
  * Force safe_mode for async requests
  * Handle safe_mode exceptions more consistently
  * Fix iteration on null responses in safe_mode


## 0.8.6 (2011-12-18)¶
  * Socket timeout fixes.
  * Proxy Authorization support.


## 0.8.5 (2011-12-14)¶
  * Response.iter_lines!


## 0.8.4 (2011-12-11)¶
  * Prefetch bugfix.
  * Added license to installed version.


## 0.8.3 (2011-11-27)¶
  * Converted auth system to use simpler callable objects.
  * New session parameter to API methods.
  * Display full URL while logging.


## 0.8.2 (2011-11-19)¶
  * New Unicode decoding system, based on over-ridable Response.encoding.
  * Proper URL slash-quote handling.
  * Cookies with [, ], and _ allowed.


## 0.8.1 (2011-11-15)¶
  * URL Request path fix
  * Proxy fix.
  * Timeouts fix.


## 0.8.0 (2011-11-13)¶
  * Keep-alive support!
  * Complete removal of Urllib2
  * Complete removal of Poster
  * Complete removal of CookieJars
  * New ConnectionError raising
  * Safe_mode for error catching
  * prefetch parameter for request methods
  * OPTION method
  * Async pool size throttling
  * File uploads send real names
  * Vendored in urllib3


## 0.7.6 (2011-11-07)¶
  * Digest authentication bugfix (attach query data to path)


## 0.7.5 (2011-11-04)¶
  * Response.content = None if there was an invalid response.
  * Redirection auth handling.


## 0.7.4 (2011-10-26)¶
  * Session Hooks fix.


## 0.7.3 (2011-10-23)¶
  * Digest Auth fix.


## 0.7.2 (2011-10-23)¶
  * PATCH Fix.


## 0.7.1 (2011-10-23)¶
  * Move away from urllib2 authentication handling.
  * Fully Remove AuthManager, AuthObject, &c.
  * New tuple-based auth system with handler callbacks.


## 0.7.0 (2011-10-22)¶
  * Sessions are now the primary interface.
  * Deprecated InvalidMethodException.
  * PATCH fix.
  * New config system (no more global settings).


## 0.6.6 (2011-10-19)¶
  * Session parameter bugfix (params merging).


## 0.6.5 (2011-10-18)¶
  * Offline (fast) test suite.
  * Session dictionary argument merging.


## 0.6.4 (2011-10-13)¶
  * Automatic decoding of unicode, based on HTTP Headers.
  * New decode_unicode setting.
  * Removal of r.read/close methods.
  * New r.faw interface for advanced response usage.*
  * Automatic expansion of parameterized headers.


## 0.6.3 (2011-10-13)¶
  * Beautiful requests.async module, for making async requests w/ gevent.


## 0.6.2 (2011-10-09)¶
  * GET/HEAD obeys allow_redirects=False.


## 0.6.1 (2011-08-20)¶
  * Enhanced status codes experience o/
  * Set a maximum number of redirects (settings.max_redirects)
  * Full Unicode URL support
  * Support for protocol-less redirects.
  * Allow for arbitrary request types.
  * Bugfixes


## 0.6.0 (2011-08-17)¶
  * New callback hook system
  * New persistent sessions object and context manager
  * Transparent Dict-cookie handling
  * Status code reference object
  * Removed Response.cached
  * Added Response.request
  * All args are kwargs
  * Relative redirect support
  * HTTPError handling improvements
  * Improved https testing
  * Bugfixes


## 0.5.1 (2011-07-23)¶
  * International Domain Name Support!
  * Access headers without fetching entire body (read())
  * Use lists as dicts for parameters
  * Add Forced Basic Authentication
  * Forced Basic is default authentication type
  * python-requests.org default User-Agent header
  * CaseInsensitiveDict lower-case caching
  * Response.history bugfix


## 0.5.0 (2011-06-21)¶
  * PATCH Support
  * Support for Proxies
  * HTTPBin Test Suite
  * Redirect Fixes
  * settings.verbose stream writing
  * Querystrings for all methods
  * URLErrors (Connection Refused, Timeout, Invalid URLs) are treated as explicitly raised r.requests.get(‘hwe://blah’); r.raise_for_status()


## 0.4.1 (2011-05-22)¶
  * Improved Redirection Handling
  * New ‘allow_redirects’ param for following non-GET/HEAD Redirects
  * Settings module refactoring


## 0.4.0 (2011-05-15)¶
  * Response.history: list of redirected responses
  * Case-Insensitive Header Dictionaries!
  * Unicode URLs


## 0.3.4 (2011-05-14)¶
  * Urllib2 HTTPAuthentication Recursion fix (Basic/Digest)
  * Internal Refactor
  * Bytes data upload Bugfix


## 0.3.3 (2011-05-12)¶
  * Request timeouts
  * Unicode url-encoded data
  * Settings context manager and module


## 0.3.2 (2011-04-15)¶
  * Automatic Decompression of GZip Encoded Content
  * AutoAuth Support for Tupled HTTP Auth


## 0.3.1 (2011-04-01)¶
  * Cookie Changes
  * Response.read()
  * Poster fix


## 0.3.0 (2011-02-25)¶
  * Automatic Authentication API Change
  * Smarter Query URL Parameterization
  * Allow file uploads and POST data together
  * New Authentication Manager System 

: - Simpler Basic HTTP System
    
    * Supports all built-in urllib2 Auths
    * Allows for custom Auth Handlers


## 0.2.4 (2011-02-19)¶
  * Python 2.5 Support
  * PyPy-c v1.4 Support
  * Auto-Authentication tests
  * Improved Request object constructor


## 0.2.3 (2011-02-15)¶
  * New HTTPHandling Methods 

: - Response.__nonzero__ (false if bad HTTP Status)
    
    * Response.ok (True if expected HTTP Status)
    * Response.error (Logged HTTPError if bad HTTP Status)
    * Response.raise_for_status() (Raises stored HTTPError)


## 0.2.2 (2011-02-14)¶
  * Still handles request in the event of an HTTPError. (Issue #2)
  * Eventlet and Gevent Monkeypatch support.
  * Cookie Support (Issue #1)


## 0.2.1 (2011-02-14)¶
  * Added file attribute to POST and PUT requests for multipart-encode file uploads.
  * Added Request.url attribute for context and redirects


## 0.2.0 (2011-02-14)¶
  * Birth!


## 0.0.1 (2011-02-13)¶
  * Frustration
  * Conception


Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Community Updates
    * GitHub
  * Release History
    * dev
    * 2.32.3 (2024-05-29)
    * 2.32.2 (2024-05-21)
    * 2.32.1 (2024-05-20)
    * 2.32.0 (2024-05-20)
    * 2.31.0 (2023-05-22)
    * 2.30.0 (2023-05-03)
    * 2.29.0 (2023-04-26)
    * 2.28.2 (2023-01-12)
    * 2.28.1 (2022-06-29)
    * 2.28.0 (2022-06-09)
    * 2.27.1 (2022-01-05)
    * 2.27.0 (2022-01-03)
    * 2.26.0 (2021-07-13)
    * 2.25.1 (2020-12-16)
    * 2.25.0 (2020-11-11)
    * 2.24.0 (2020-06-17)
    * 2.23.0 (2020-02-19)
    * 2.22.0 (2019-05-15)
    * 2.21.0 (2018-12-10)
    * 2.20.1 (2018-11-08)
    * 2.20.0 (2018-10-18)
    * 2.19.1 (2018-06-14)
    * 2.19.0 (2018-06-12)
    * 2.18.4 (2017-08-15)
    * 2.18.3 (2017-08-02)
    * 2.18.2 (2017-07-25)
    * 2.18.1 (2017-06-14)
    * 2.18.0 (2017-06-14)
    * 2.17.3 (2017-05-29)
    * 2.17.2 (2017-05-29)
    * 2.17.1 (2017-05-29)
    * 2.17.0 (2017-05-29)
    * 2.16.5 (2017-05-28)
    * 2.16.4 (2017-05-27)
    * 2.16.3 (2017-05-27)
    * 2.16.2 (2017-05-27)
    * 2.16.1 (2017-05-27)
    * 2.16.0 (2017-05-26)
    * 2.15.1 (2017-05-26)
    * 2.15.0 (2017-05-26)
    * 2.14.2 (2017-05-10)
    * 2.14.1 (2017-05-09)
    * 2.14.0 (2017-05-09)
    * 2.13.0 (2017-01-24)
    * 2.12.5 (2017-01-18)
    * 2.12.4 (2016-12-14)
    * 2.12.3 (2016-12-01)
    * 2.12.2 (2016-11-30)
    * 2.12.1 (2016-11-16)
    * 2.12.0 (2016-11-15)
    * 2.11.1 (2016-08-17)
    * 2.11.0 (2016-08-08)
    * 2.10.0 (2016-04-29)
    * 2.9.2 (2016-04-29)
    * 2.9.1 (2015-12-21)
    * 2.9.0 (2015-12-15)
    * 2.8.1 (2015-10-13)
    * 2.8.0 (2015-10-05)
    * 2.7.0 (2015-05-03)
    * 2.6.2 (2015-04-23)
    * 2.6.1 (2015-04-22)
    * 2.6.0 (2015-03-14)
    * 2.5.3 (2015-02-24)
    * 2.5.2 (2015-02-23)
    * 2.5.1 (2014-12-23)
    * 2.5.0 (2014-12-01)
    * 2.4.3 (2014-10-06)
    * 2.4.2 (2014-10-05)
    * 2.4.1 (2014-09-09)
    * 2.4.0 (2014-08-29)
    * 2.3.0 (2014-05-16)
    * 2.2.1 (2014-01-23)
    * 2.2.0 (2014-01-09)
    * 2.1.0 (2013-12-05)
    * 2.0.1 (2013-10-24)
    * 2.0.0 (2013-09-24)
    * 1.2.3 (2013-05-25)
    * 1.2.2 (2013-05-23)
    * 1.2.1 (2013-05-20)
    * 1.2.0 (2013-03-31)
    * 1.1.0 (2013-01-10)
    * 1.0.3 (2012-12-18)
    * 1.0.2 (2012-12-17)
    * 1.0.1 (2012-12-17)
    * 1.0.0 (2012-12-17)
    * 0.14.2 (2012-10-27)
    * 0.14.1 (2012-10-01)
    * 0.14.0 (2012-09-02)
    * 0.13.9 (2012-08-25)
    * 0.13.8 (2012-08-21)
    * 0.13.7 (2012-08-19)
    * 0.13.6 (2012-08-06)
    * 0.13.5 (2012-07-27)
    * 0.13.4 (2012-07-27)
    * 0.13.3 (2012-07-12)
    * 0.13.2 (2012-06-28)
    * 0.13.1 (2012-06-07)
    * 0.13.0 (2012-05-29)
    * 0.12.1 (2012-05-08)
    * 0.12.0 (2012-05-02)
    * 0.11.2 (2012-04-22)
    * 0.11.1 (2012-03-30)
    * 0.11.0 (2012-03-14)
    * 0.10.8 (2012-03-09)
    * 0.10.7 (2012-03-07)
    * 0.10.6 (2012-02-25)
    * 0.10.5 (2012-02-25)
    * 0.10.4 (2012-02-20)
    * 0.10.3 (2012-02-20)
    * 0.10.2 (2012-01-15)
    * 0.10.1 (2012-01-23)
    * 0.10.0 (2012-01-21)
    * 0.9.3 (2012-01-18)
    * 0.9.2 (2012-01-18)
    * 0.9.1 (2012-01-06)
    * 0.9.0 (2011-12-28)
    * 0.8.9 (2011-12-28)
    * 0.8.8 (2011-12-28)
    * 0.8.7 (2011-12-24)
    * 0.8.6 (2011-12-18)
    * 0.8.5 (2011-12-14)
    * 0.8.4 (2011-12-11)
    * 0.8.3 (2011-11-27)
    * 0.8.2 (2011-11-19)
    * 0.8.1 (2011-11-15)
    * 0.8.0 (2011-11-13)
    * 0.7.6 (2011-11-07)
    * 0.7.5 (2011-11-04)
    * 0.7.4 (2011-10-26)
    * 0.7.3 (2011-10-23)
    * 0.7.2 (2011-10-23)
    * 0.7.1 (2011-10-23)
    * 0.7.0 (2011-10-22)
    * 0.6.6 (2011-10-19)
    * 0.6.5 (2011-10-18)
    * 0.6.4 (2011-10-13)
    * 0.6.3 (2011-10-13)
    * 0.6.2 (2011-10-09)
    * 0.6.1 (2011-08-20)
    * 0.6.0 (2011-08-17)
    * 0.5.1 (2011-07-23)
    * 0.5.0 (2011-06-21)
    * 0.4.1 (2011-05-22)
    * 0.4.0 (2011-05-15)
    * 0.3.4 (2011-05-14)
    * 0.3.3 (2011-05-12)
    * 0.3.2 (2011-04-15)
    * 0.3.1 (2011-04-01)
    * 0.3.0 (2011-02-25)
    * 0.2.4 (2011-02-19)
    * 0.2.3 (2011-02-15)
    * 0.2.2 (2011-02-14)
    * 0.2.1 (2011-02-14)
    * 0.2.0 (2011-02-14)
    * 0.0.1 (2011-02-13)


### Related Topics
  * Documentation overview
    * Previous: Release Process and Rules
    * Next: Developer Interface


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Contributor’s Guide¶
If you’re reading this, you’re probably interested in contributing to Requests. Thank you very much! Open source projects live-and-die based on the support they receive from others, and the fact that you’re even considering contributing to the Requests project is _very_ generous of you.
This document lays out guidelines and advice for contributing to this project. If you’re thinking of contributing, please start by reading this document and getting a feel for how contributing to this project works. If you have any questions, feel free to reach out to either Nate Prewitt, Ian Cordasco, or Seth Michael Larson, the primary maintainers.
The guide is split into sections based on the type of contribution you’re thinking of making, with a section that covers general guidelines for all contributors.
## Be Cordial¶
> **Be cordial or be on your way**. _—Kenneth Reitz_
Requests has one very important rule governing all forms of contribution, including reporting bugs or requesting features. This golden rule is “be cordial or be on your way”.
**All contributions are welcome** , as long as everyone involved is treated with respect.
## Get Early Feedback¶
If you are contributing, do not feel the need to sit on your contribution until it is perfectly polished and complete. It helps everyone involved for you to seek feedback as early as you possibly can. Submitting an early, unfinished version of your contribution for feedback in no way prejudices your chances of getting that contribution accepted, and can save you from putting a lot of work into a contribution that is not suitable for the project.
## Contribution Suitability¶
Our project maintainers have the last word on whether or not a contribution is suitable for Requests. All contributions will be considered carefully, but from time to time, contributions will be rejected because they do not suit the current goals or needs of the project.
If your contribution is rejected, don’t despair! As long as you followed these guidelines, you will have a much better chance of getting your next contribution accepted.
## Code Contributions¶
### Steps for Submitting Code¶
When contributing code, you’ll want to follow this checklist:
  1. Fork the repository on GitHub.
  2. Run the tests to confirm they all pass on your system. If they don’t, you’ll need to investigate why they fail. If you’re unable to diagnose this yourself, raise it as a bug report by following the guidelines in this document: Bug Reports.
  3. Write tests that demonstrate your bug or feature. Ensure that they fail.
  4. Make your change.
  5. Run the entire test suite again, confirming that all tests pass _including the ones you just added_.
  6. Send a GitHub Pull Request to the main repository’s `main` branch. GitHub Pull Requests are the expected method of code collaboration on this project.


The following sub-sections go into more detail on some of the points above.
### Code Review¶
Contributions will not be merged until they’ve been code reviewed. You should implement any code review feedback unless you strongly object to it. In the event that you object to the code review feedback, you should make your case clearly and calmly. If, after doing so, the feedback is judged to still apply, you must either apply the feedback or withdraw your contribution.
### Code Style¶
Requests uses a collection of tools to ensure the code base has a consistent style as it grows. We have these orchestrated using a tool called pre-commit. This can be installed locally and run over your changes prior to opening a PR, and will also be run as part of the CI approval process before a change is merged.
You can find the full list of formatting requirements specified in the .pre-commit-config.yaml at the top level directory of Requests.
### New Contributors¶
If you are new or relatively new to Open Source, welcome! Requests aims to be a gentle introduction to the world of Open Source. If you’re concerned about how best to contribute, please consider mailing a maintainer (listed above) and asking for help.
Please also check the Get Early Feedback section.
## Documentation Contributions¶
Documentation improvements are always welcome! The documentation files live in the `docs/` directory of the codebase. They’re written in reStructuredText, and use Sphinx to generate the full suite of documentation.
When contributing documentation, please do your best to follow the style of the documentation files. This means a soft-limit of 79 characters wide in your text files and a semi-formal, yet friendly and approachable, prose style.
When presenting Python code, use single-quoted strings (`'hello'` instead of `"hello"`).
## Bug Reports¶
Bug reports are hugely important! Before you raise one, though, please check through the GitHub issues, **both open and closed** , to confirm that the bug hasn’t been reported before. Duplicate bug reports are a huge drain on the time of other contributors, and should be avoided as much as possible.
## Feature Requests¶
Requests is in a perpetual feature freeze, only the BDFL can add or approve of new features. The maintainers believe that Requests is a feature-complete piece of software at this time.
One of the most important skills to have while maintaining a largely-used open source project is learning the ability to say “no” to suggested changes, while keeping an open ear and mind.
If you believe there is a feature missing, feel free to raise a feature request, but please do be aware that the overwhelming likelihood is that your feature request will not be accepted.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Contributor’s Guide
    * Be Cordial
    * Get Early Feedback
    * Contribution Suitability
    * Code Contributions
      * Steps for Submitting Code
      * Code Review
      * Code Style
      * New Contributors
    * Documentation Contributions
    * Bug Reports
    * Feature Requests


### Related Topics
  * Documentation overview
    * Previous: Developer Interface
    * Next: Authors


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Advanced Usage¶
This document covers some of Requests more advanced features.
## Session Objects¶
The Session object allows you to persist certain parameters across requests. It also persists cookies across all requests made from the Session instance, and will use `urllib3`’s connection pooling. So if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase (see HTTP persistent connection).
A Session object has all the methods of the main Requests API.
Let’s persist some cookies across requests:
```
s = requests.Session()
s.get('https://httpbin.org/cookies/set/sessioncookie/123456789')
r = s.get('https://httpbin.org/cookies')
print(r.text)
# '{"cookies": {"sessioncookie": "123456789"}}'

```

Sessions can also be used to provide default data to the request methods. This is done by providing data to the properties on a Session object:
```
s = requests.Session()
s.auth = ('user', 'pass')
s.headers.update({'x-test': 'true'})
# both 'x-test' and 'x-test2' are sent
s.get('https://httpbin.org/headers', headers={'x-test2': 'true'})

```

Any dictionaries that you pass to a request method will be merged with the session-level values that are set. The method-level parameters override session parameters.
Note, however, that method-level parameters will _not_ be persisted across requests, even if using a session. This example will only send the cookies with the first request, but not the second:
```
s = requests.Session()
r = s.get('https://httpbin.org/cookies', cookies={'from-my': 'browser'})
print(r.text)
# '{"cookies": {"from-my": "browser"}}'
r = s.get('https://httpbin.org/cookies')
print(r.text)
# '{"cookies": {}}'

```

If you want to manually add cookies to your session, use the Cookie utility functions to manipulate `Session.cookies`.
Sessions can also be used as context managers:
```
with requests.Session() as s:
  s.get('https://httpbin.org/cookies/set/sessioncookie/123456789')

```

This will make sure the session is closed as soon as the `with` block is exited, even if unhandled exceptions occurred.
Remove a Value From a Dict Parameter
Sometimes you’ll want to omit session-level keys from a dict parameter. To do this, you simply set that key’s value to `None` in the method-level parameter. It will automatically be omitted.
All values that are contained within a session are directly available to you. See the Session API Docs to learn more.
## Request and Response Objects¶
Whenever a call is made to `requests.get()` and friends, you are doing two major things. First, you are constructing a `Request` object which will be sent off to a server to request or query some resource. Second, a `Response` object is generated once Requests gets a response back from the server. The `Response` object contains all of the information returned by the server and also contains the `Request` object you created originally. Here is a simple request to get some very important information from Wikipedia’s servers:
```
>>> r = requests.get('https://en.wikipedia.org/wiki/Monty_Python')

```

If we want to access the headers the server sent back to us, we do this:
```
>>> r.headers
{'content-length': '56170', 'x-content-type-options': 'nosniff', 'x-cache':
'HIT from cp1006.eqiad.wmnet, MISS from cp1010.eqiad.wmnet', 'content-encoding':
'gzip', 'age': '3080', 'content-language': 'en', 'vary': 'Accept-Encoding,Cookie',
'server': 'Apache', 'last-modified': 'Wed, 13 Jun 2012 01:33:50 GMT',
'connection': 'close', 'cache-control': 'private, s-maxage=0, max-age=0,
must-revalidate', 'date': 'Thu, 14 Jun 2012 12:59:39 GMT', 'content-type':
'text/html; charset=UTF-8', 'x-cache-lookup': 'HIT from cp1006.eqiad.wmnet:3128,
MISS from cp1010.eqiad.wmnet:80'}

```

However, if we want to get the headers we sent the server, we simply access the request, and then the request’s headers:
```
>>> r.request.headers
{'Accept-Encoding': 'identity, deflate, compress, gzip',
'Accept': '*/*', 'User-Agent': 'python-requests/1.2.0'}

```

## Prepared Requests¶
Whenever you receive a `Response` object from an API call or a Session call, the `request` attribute is actually the `PreparedRequest` that was used. In some cases you may wish to do some extra work to the body or headers (or anything else really) before sending a request. The simple recipe for this is the following:
```
fromrequestsimport Request, Session
s = Session()
req = Request('POST', url, data=data, headers=headers)
prepped = req.prepare()
# do something with prepped.body
prepped.body = 'No, I want exactly this as the body.'
# do something with prepped.headers
del prepped.headers['Content-Type']
resp = s.send(prepped,
  stream=stream,
  verify=verify,
  proxies=proxies,
  cert=cert,
  timeout=timeout
)
print(resp.status_code)

```

Since you are not doing anything special with the `Request` object, you prepare it immediately and modify the `PreparedRequest` object. You then send that with the other parameters you would have sent to `requests.*` or `Session.*`.
However, the above code will lose some of the advantages of having a Requests `Session` object. In particular, `Session`-level state such as cookies will not get applied to your request. To get a `PreparedRequest` with that state applied, replace the call to `Request.prepare()` with a call to `Session.prepare_request()`, like this:
```
fromrequestsimport Request, Session
s = Session()
req = Request('GET', url, data=data, headers=headers)
prepped = s.prepare_request(req)
# do something with prepped.body
prepped.body = 'Seriously, send exactly these bytes.'
# do something with prepped.headers
prepped.headers['Keep-Dead'] = 'parrot'
resp = s.send(prepped,
  stream=stream,
  verify=verify,
  proxies=proxies,
  cert=cert,
  timeout=timeout
)
print(resp.status_code)

```

When you are using the prepared request flow, keep in mind that it does not take into account the environment. This can cause problems if you are using environment variables to change the behaviour of requests. For example: Self-signed SSL certificates specified in `REQUESTS_CA_BUNDLE` will not be taken into account. As a result an `SSL: CERTIFICATE_VERIFY_FAILED` is thrown. You can get around this behaviour by explicitly merging the environment settings into your session:
```
fromrequestsimport Request, Session
s = Session()
req = Request('GET', url)
prepped = s.prepare_request(req)
# Merge environment settings into session
settings = s.merge_environment_settings(prepped.url, {}, None, None, None)
resp = s.send(prepped, **settings)
print(resp.status_code)

```

## SSL Cert Verification¶
Requests verifies SSL certificates for HTTPS requests, just like a web browser. By default, SSL verification is enabled, and Requests will throw a SSLError if it’s unable to verify the certificate:
```
>>> requests.get('https://requestb.in')
requests.exceptions.SSLError: hostname 'requestb.in' doesn't match either of '*.herokuapp.com', 'herokuapp.com'

```

I don’t have SSL setup on this domain, so it throws an exception. Excellent. GitHub does though:
```
>>> requests.get('https://github.com')
<Response [200]>

```

You can pass `verify` the path to a CA_BUNDLE file or directory with certificates of trusted CAs:
```
>>> requests.get('https://github.com', verify='/path/to/certfile')

```

or persistent:
```
s = requests.Session()
s.verify = '/path/to/certfile'

```

Note
If `verify` is set to a path to a directory, the directory must have been processed using the `c_rehash` utility supplied with OpenSSL.
This list of trusted CAs can also be specified through the `REQUESTS_CA_BUNDLE` environment variable. If `REQUESTS_CA_BUNDLE` is not set, `CURL_CA_BUNDLE` will be used as fallback.
Requests can also ignore verifying the SSL certificate if you set `verify` to False:
```
>>> requests.get('https://kennethreitz.org', verify=False)
<Response [200]>

```

Note that when `verify` is set to `False`, requests will accept any TLS certificate presented by the server, and will ignore hostname mismatches and/or expired certificates, which will make your application vulnerable to man-in-the-middle (MitM) attacks. Setting verify to `False` may be useful during local development or testing.
By default, `verify` is set to True. Option `verify` only applies to host certs.
## Client Side Certificates¶
You can also specify a local cert to use as client side certificate, as a single file (containing the private key and the certificate) or as a tuple of both files’ paths:
```
>>> requests.get('https://kennethreitz.org', cert=('/path/client.cert', '/path/client.key'))
<Response [200]>

```

or persistent:
```
s = requests.Session()
s.cert = '/path/client.cert'

```

If you specify a wrong path or an invalid cert, you’ll get a SSLError:
```
>>> requests.get('https://kennethreitz.org', cert='/wrong_path/client.pem')
SSLError: [Errno 336265225] _ssl.c:347: error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib

```

Warning
The private key to your local certificate _must_ be unencrypted. Currently, Requests does not support using encrypted keys.
## CA Certificates¶
Requests uses certificates from the package certifi. This allows for users to update their trusted certificates without changing the version of Requests.
Before version 2.16, Requests bundled a set of root CAs that it trusted, sourced from the Mozilla trust store. The certificates were only updated once for each Requests version. When `certifi` was not installed, this led to extremely out-of-date certificate bundles when using significantly older versions of Requests.
For the sake of security we recommend upgrading certifi frequently!
## Body Content Workflow¶
By default, when you make a request, the body of the response is downloaded immediately. You can override this behaviour and defer downloading the response body until you access the `Response.content` attribute with the `stream` parameter:
```
tarball_url = 'https://github.com/psf/requests/tarball/main'
r = requests.get(tarball_url, stream=True)

```

At this point only the response headers have been downloaded and the connection remains open, hence allowing us to make content retrieval conditional:
```
if int(r.headers['content-length']) < TOO_LONG:
 content = r.content
 ...

```

You can further control the workflow by use of the `Response.iter_content()` and `Response.iter_lines()` methods. Alternatively, you can read the undecoded body from the underlying urllib3 `urllib3.HTTPResponse` at `Response.raw`.
If you set `stream` to `True` when making a request, Requests cannot release the connection back to the pool unless you consume all the data or call `Response.close`. This can lead to inefficiency with connections. If you find yourself partially reading request bodies (or not reading them at all) while using `stream=True`, you should make the request within a `with` statement to ensure it’s always closed:
```
with requests.get('https://httpbin.org/get', stream=True) as r:
  # Do things with the response here.

```

## Keep-Alive¶
Excellent news — thanks to urllib3, keep-alive is 100% automatic within a session! Any requests that you make within a session will automatically reuse the appropriate connection!
Note that connections are only released back to the pool for reuse once all body data has been read; be sure to either set `stream` to `False` or read the `content` property of the `Response` object.
## Streaming Uploads¶
Requests supports streaming uploads, which allow you to send large streams or files without reading them into memory. To stream and upload, simply provide a file-like object for your body:
```
with open('massive-body', 'rb') as f:
  requests.post('http://some.url/streamed', data=f)

```

Warning
It is strongly recommended that you open files in binary mode. This is because Requests may attempt to provide the `Content-Length` header for you, and if it does this value will be set to the number of _bytes_ in the file. Errors may occur if you open the file in _text mode_.
## Chunk-Encoded Requests¶
Requests also supports Chunked transfer encoding for outgoing and incoming requests. To send a chunk-encoded request, simply provide a generator (or any iterator without a length) for your body:
```
defgen():
  yield 'hi'
  yield 'there'
requests.post('http://some.url/chunked', data=gen())

```

For chunked encoded responses, it’s best to iterate over the data using `Response.iter_content()`. In an ideal situation you’ll have set `stream=True` on the request, in which case you can iterate chunk-by-chunk by calling `iter_content` with a `chunk_size` parameter of `None`. If you want to set a maximum size of the chunk, you can set a `chunk_size` parameter to any integer.
## POST Multiple Multipart-Encoded Files¶
You can send multiple files in one request. For example, suppose you want to upload image files to an HTML form with a multiple file field ‘images’:
```
<input type="file" name="images" multiple="true" required="true"/>

```

To do that, just set files to a list of tuples of `(form_field_name, file_info)`:
```
>>> url = 'https://httpbin.org/post'
>>> multiple_files = [
...   ('images', ('foo.png', open('foo.png', 'rb'), 'image/png')),
...   ('images', ('bar.png', open('bar.png', 'rb'), 'image/png'))]
>>> r = requests.post(url, files=multiple_files)
>>> r.text
{
 ...
 'files': {'images': 'data:image/png;base64,iVBORw ....'}
 'Content-Type': 'multipart/form-data; boundary=3131623adb2043caaeb5538cc7aa0b3a',
 ...
}

```

Warning
It is strongly recommended that you open files in binary mode. This is because Requests may attempt to provide the `Content-Length` header for you, and if it does this value will be set to the number of _bytes_ in the file. Errors may occur if you open the file in _text mode_.
## Event Hooks¶
Requests has a hook system that you can use to manipulate portions of the request process, or signal event handling.
Available hooks: 

`response`:
    
The response generated from a Request.
You can assign a hook function on a per-request basis by passing a `{hook_name: callback_function}` dictionary to the `hooks` request parameter:
```
hooks={'response': print_url}

```

That `callback_function` will receive a chunk of data as its first argument.
```
defprint_url(r, *args, **kwargs):
  print(r.url)

```

Your callback function must handle its own exceptions. Any unhandled exception won’t be passed silently and thus should be handled by the code calling Requests.
If the callback function returns a value, it is assumed that it is to replace the data that was passed in. If the function doesn’t return anything, nothing else is affected.
```
defrecord_hook(r, *args, **kwargs):
  r.hook_called = True
  return r

```

Let’s print some request method arguments at runtime:
```
>>> requests.get('https://httpbin.org/', hooks={'response': print_url})
https://httpbin.org/
<Response [200]>

```

You can add multiple hooks to a single request. Let’s call two hooks at once:
```
>>> r = requests.get('https://httpbin.org/', hooks={'response': [print_url, record_hook]})
>>> r.hook_called
True

```

You can also add hooks to a `Session` instance. Any hooks you add will then be called on every request made to the session. For example:
```
>>> s = requests.Session()
>>> s.hooks['response'].append(print_url)
>>> s.get('https://httpbin.org/')
 https://httpbin.org/
 <Response [200]>

```

A `Session` can have multiple hooks, which will be called in the order they are added.
## Custom Authentication¶
Requests allows you to specify your own authentication mechanism.
Any callable which is passed as the `auth` argument to a request method will have the opportunity to modify the request before it is dispatched.
Authentication implementations are subclasses of `AuthBase`, and are easy to define. Requests provides two common authentication scheme implementations in `requests.auth`: `HTTPBasicAuth` and `HTTPDigestAuth`.
Let’s pretend that we have a web service that will only respond if the `X-Pizza` header is set to a password value. Unlikely, but just go with it.
```
fromrequests.authimport AuthBase
classPizzaAuth(AuthBase):
"""Attaches HTTP Pizza Authentication to the given Request object."""
  def__init__(self, username):
    # setup any auth-related data here
    self.username = username
  def__call__(self, r):
    # modify and return the request
    r.headers['X-Pizza'] = self.username
    return r

```

Then, we can make a request using our Pizza Auth:
```
>>> requests.get('http://pizzabin.org/admin', auth=PizzaAuth('kenneth'))
<Response [200]>

```

## Streaming Requests¶
With `Response.iter_lines()` you can easily iterate over streaming APIs such as the Twitter Streaming API. Simply set `stream` to `True` and iterate over the response with `iter_lines`:
```
importjson
importrequests
r = requests.get('https://httpbin.org/stream/20', stream=True)
for line in r.iter_lines():
  # filter out keep-alive new lines
  if line:
    decoded_line = line.decode('utf-8')
    print(json.loads(decoded_line))

```

When using decode_unicode=True with `Response.iter_lines()` or `Response.iter_content()`, you’ll want to provide a fallback encoding in the event the server doesn’t provide one:
```
r = requests.get('https://httpbin.org/stream/20', stream=True)
if r.encoding is None:
  r.encoding = 'utf-8'
for line in r.iter_lines(decode_unicode=True):
  if line:
    print(json.loads(line))

```

Warning
`iter_lines` is not reentrant safe. Calling this method multiple times causes some of the received data being lost. In case you need to call it from multiple places, use the resulting iterator object instead:
```
lines = r.iter_lines()
# Save the first line for later or just skip it
first_line = next(lines)
for line in lines:
  print(line)

```

## Proxies¶
If you need to use a proxy, you can configure individual requests with the `proxies` argument to any request method:
```
importrequests
proxies = {
 'http': 'http://10.10.1.10:3128',
 'https': 'http://10.10.1.10:1080',
}
requests.get('http://example.org', proxies=proxies)

```

Alternatively you can configure it once for an entire `Session`:
```
importrequests
proxies = {
 'http': 'http://10.10.1.10:3128',
 'https': 'http://10.10.1.10:1080',
}
session = requests.Session()
session.proxies.update(proxies)
session.get('http://example.org')

```

Warning
Setting `session.proxies` may behave differently than expected. Values provided will be overwritten by environmental proxies (those returned by urllib.request.getproxies). To ensure the use of proxies in the presence of environmental proxies, explicitly specify the `proxies` argument on all individual requests as initially explained above.
See #2018 for details.
When the proxies configuration is not overridden per request as shown above, Requests relies on the proxy configuration defined by standard environment variables `http_proxy`, `https_proxy`, `no_proxy`, and `all_proxy`. Uppercase variants of these variables are also supported. You can therefore set them to configure Requests (only set the ones relevant to your needs):
```
$ export HTTP_PROXY="http://10.10.1.10:3128"
$ export HTTPS_PROXY="http://10.10.1.10:1080"
$ export ALL_PROXY="socks5://10.10.1.10:3434"
$ python
>>> import requests
>>> requests.get('http://example.org')

```

To use HTTP Basic Auth with your proxy, use the http://user:password@host/ syntax in any of the above configuration entries:
```
$ export HTTPS_PROXY="http://user:pass@10.10.1.10:1080"
$ python
>>> proxies = {'http': 'http://user:pass@10.10.1.10:3128/'}

```

Warning
Storing sensitive username and password information in an environment variable or a version-controlled file is a security risk and is highly discouraged.
To give a proxy for a specific scheme and host, use the scheme://hostname form for the key. This will match for any request to the given scheme and exact hostname.
```
proxies = {'http://10.20.1.128': 'http://10.10.1.10:5323'}

```

Note that proxy URLs must include the scheme.
Finally, note that using a proxy for https connections typically requires your local machine to trust the proxy’s root certificate. By default the list of certificates trusted by Requests can be found with:
```
fromrequests.utilsimport DEFAULT_CA_BUNDLE_PATH
print(DEFAULT_CA_BUNDLE_PATH)

```

You override this default certificate bundle by setting the `REQUESTS_CA_BUNDLE` (or `CURL_CA_BUNDLE`) environment variable to another file path:
```
$ export REQUESTS_CA_BUNDLE="/usr/local/myproxy_info/cacert.pem"
$ export https_proxy="http://10.10.1.10:1080"
$ python
>>> import requests
>>> requests.get('https://example.org')

```

### SOCKS¶
New in version 2.10.0.
In addition to basic HTTP proxies, Requests also supports proxies using the SOCKS protocol. This is an optional feature that requires that additional third-party libraries be installed before use.
You can get the dependencies for this feature from `pip`:
```
$python-mpipinstall'requests[socks]'

```

Once you’ve installed those dependencies, using a SOCKS proxy is just as easy as using a HTTP one:
```
proxies = {
  'http': 'socks5://user:pass@host:port',
  'https': 'socks5://user:pass@host:port'
}

```

Using the scheme `socks5` causes the DNS resolution to happen on the client, rather than on the proxy server. This is in line with curl, which uses the scheme to decide whether to do the DNS resolution on the client or proxy. If you want to resolve the domains on the proxy server, use `socks5h` as the scheme.
## Compliance¶
Requests is intended to be compliant with all relevant specifications and RFCs where that compliance will not cause difficulties for users. This attention to the specification can lead to some behaviour that may seem unusual to those not familiar with the relevant specification.
### Encodings¶
When you receive a response, Requests makes a guess at the encoding to use for decoding the response when you access the `Response.text` attribute. Requests will first check for an encoding in the HTTP header, and if none is present, will use charset_normalizer or chardet to attempt to guess the encoding.
If `chardet` is installed, `requests` uses it, however for python3 `chardet` is no longer a mandatory dependency. The `chardet` library is an LGPL-licenced dependency and some users of requests cannot depend on mandatory LGPL-licensed dependencies.
When you install `requests` without specifying `[use_chardet_on_py3]` extra, and `chardet` is not already installed, `requests` uses `charset-normalizer` (MIT-licensed) to guess the encoding.
The only time Requests will not guess the encoding is if no explicit charset is present in the HTTP headers **and** the `Content-Type` header contains `text`. In this situation, RFC 2616 specifies that the default charset must be `ISO-8859-1`. Requests follows the specification in this case. If you require a different encoding, you can manually set the `Response.encoding` property, or use the raw `Response.content`.
## HTTP Verbs¶
Requests provides access to almost the full range of HTTP verbs: GET, OPTIONS, HEAD, POST, PUT, PATCH and DELETE. The following provides detailed examples of using these various verbs in Requests, using the GitHub API.
We will begin with the verb most commonly used: GET. HTTP GET is an idempotent method that returns a resource from a given URL. As a result, it is the verb you ought to use when attempting to retrieve data from a web location. An example usage would be attempting to get information about a specific commit from GitHub. Suppose we wanted commit `a050faf` on Requests. We would get it like so:
```
>>> importrequests
>>> r = requests.get('https://api.github.com/repos/psf/requests/git/commits/a050faf084662f3a352dd1a941f2c7c9f886d4ad')

```

We should confirm that GitHub responded correctly. If it has, we want to work out what type of content it is. Do this like so:
```
>>> if r.status_code == requests.codes.ok:
...   print(r.headers['content-type'])
...
application/json; charset=utf-8

```

So, GitHub returns JSON. That’s great, we can use the `r.json` method to parse it into Python objects.
```
>>> commit_data = r.json()
>>> print(commit_data.keys())
['committer', 'author', 'url', 'tree', 'sha', 'parents', 'message']
>>> print(commit_data['committer'])
{'date': '2012-05-10T11:10:50-07:00', 'email': 'me@kennethreitz.com', 'name': 'Kenneth Reitz'}
>>> print(commit_data['message'])
makin' history

```

So far, so simple. Well, let’s investigate the GitHub API a little bit. Now, we could look at the documentation, but we might have a little more fun if we use Requests instead. We can take advantage of the Requests OPTIONS verb to see what kinds of HTTP methods are supported on the url we just used.
```
>>> verbs = requests.options(r.url)
>>> verbs.status_code
500

```

Uh, what? That’s unhelpful! Turns out GitHub, like many API providers, don’t actually implement the OPTIONS method. This is an annoying oversight, but it’s OK, we can just use the boring documentation. If GitHub had correctly implemented OPTIONS, however, they should return the allowed methods in the headers, e.g.
```
>>> verbs = requests.options('http://a-good-website.com/api/cats')
>>> print(verbs.headers['allow'])
GET,HEAD,POST,OPTIONS

```

Turning to the documentation, we see that the only other method allowed for commits is POST, which creates a new commit. As we’re using the Requests repo, we should probably avoid making ham-handed POSTS to it. Instead, let’s play with the Issues feature of GitHub.
This documentation was added in response to Issue #482. Given that this issue already exists, we will use it as an example. Let’s start by getting it.
```
>>> r = requests.get('https://api.github.com/repos/psf/requests/issues/482')
>>> r.status_code
200
>>> issue = json.loads(r.text)
>>> print(issue['title'])
Feature any http verb in docs
>>> print(issue['comments'])
3

```

Cool, we have three comments. Let’s take a look at the last of them.
```
>>> r = requests.get(r.url + '/comments')
>>> r.status_code
200
>>> comments = r.json()
>>> print(comments[0].keys())
['body', 'url', 'created_at', 'updated_at', 'user', 'id']
>>> print(comments[2]['body'])
Probably in the "advanced" section

```

Well, that seems like a silly place. Let’s post a comment telling the poster that he’s silly. Who is the poster, anyway?
```
>>> print(comments[2]['user']['login'])
kennethreitz

```

OK, so let’s tell this Kenneth guy that we think this example should go in the quickstart guide instead. According to the GitHub API doc, the way to do this is to POST to the thread. Let’s do it.
```
>>> body = json.dumps({u"body": u"Sounds great! I'll get right on it!"})
>>> url = u"https://api.github.com/repos/psf/requests/issues/482/comments"
>>> r = requests.post(url=url, data=body)
>>> r.status_code
404

```

Huh, that’s weird. We probably need to authenticate. That’ll be a pain, right? Wrong. Requests makes it easy to use many forms of authentication, including the very common Basic Auth.
```
>>> fromrequests.authimport HTTPBasicAuth
>>> auth = HTTPBasicAuth('fake@example.com', 'not_a_real_password')
>>> r = requests.post(url=url, data=body, auth=auth)
>>> r.status_code
201
>>> content = r.json()
>>> print(content['body'])
Sounds great! I'll get right on it.

```

Brilliant. Oh, wait, no! I meant to add that it would take me a while, because I had to go feed my cat. If only I could edit this comment! Happily, GitHub allows us to use another HTTP verb, PATCH, to edit this comment. Let’s do that.
```
>>> print(content[u"id"])
5804413
>>> body = json.dumps({u"body": u"Sounds great! I'll get right on it once I feed my cat."})
>>> url = u"https://api.github.com/repos/psf/requests/issues/comments/5804413"
>>> r = requests.patch(url=url, data=body, auth=auth)
>>> r.status_code
200

```

Excellent. Now, just to torture this Kenneth guy, I’ve decided to let him sweat and not tell him that I’m working on this. That means I want to delete this comment. GitHub lets us delete comments using the incredibly aptly named DELETE method. Let’s get rid of it.
```
>>> r = requests.delete(url=url, auth=auth)
>>> r.status_code
204
>>> r.headers['status']
'204 No Content'

```

Excellent. All gone. The last thing I want to know is how much of my ratelimit I’ve used. Let’s find out. GitHub sends that information in the headers, so rather than download the whole page I’ll send a HEAD request to get the headers.
```
>>> r = requests.head(url=url, auth=auth)
>>> print(r.headers)
...
'x-ratelimit-remaining': '4995'
'x-ratelimit-limit': '5000'
...

```

Excellent. Time to write a Python program that abuses the GitHub API in all kinds of exciting ways, 4995 more times.
## Custom Verbs¶
From time to time you may be working with a server that, for whatever reason, allows use or even requires use of HTTP verbs not covered above. One example of this would be the MKCOL method some WEBDAV servers use. Do not fret, these can still be used with Requests. These make use of the built-in `.request` method. For example:
```
>>> r = requests.request('MKCOL', url, data=data)
>>> r.status_code
200 # Assuming your call was correct

```

Utilising this, you can make use of any method verb that your server allows.
## Link Headers¶
Many HTTP APIs feature Link headers. They make APIs more self describing and discoverable.
GitHub uses these for pagination in their API, for example:
```
>>> url = 'https://api.github.com/users/kennethreitz/repos?page=1&per_page=10'
>>> r = requests.head(url=url)
>>> r.headers['link']
'<https://api.github.com/users/kennethreitz/repos?page=2&per_page=10>; rel="next", <https://api.github.com/users/kennethreitz/repos?page=6&per_page=10>; rel="last"'

```

Requests will automatically parse these link headers and make them easily consumable:
```
>>> r.links["next"]
{'url': 'https://api.github.com/users/kennethreitz/repos?page=2&per_page=10', 'rel': 'next'}
>>> r.links["last"]
{'url': 'https://api.github.com/users/kennethreitz/repos?page=7&per_page=10', 'rel': 'last'}

```

## Transport Adapters¶
As of v1.0.0, Requests has moved to a modular internal design. Part of the reason this was done was to implement Transport Adapters, originally described here. Transport Adapters provide a mechanism to define interaction methods for an HTTP service. In particular, they allow you to apply per-service configuration.
Requests ships with a single Transport Adapter, the `HTTPAdapter`. This adapter provides the default Requests interaction with HTTP and HTTPS using the powerful urllib3 library. Whenever a Requests `Session` is initialized, one of these is attached to the `Session` object for HTTP, and one for HTTPS.
Requests enables users to create and use their own Transport Adapters that provide specific functionality. Once created, a Transport Adapter can be mounted to a Session object, along with an indication of which web services it should apply to.
```
>>> s = requests.Session()
>>> s.mount('https://github.com/', MyAdapter())

```

The mount call registers a specific instance of a Transport Adapter to a prefix. Once mounted, any HTTP request made using that session whose URL starts with the given prefix will use the given Transport Adapter.
Note
The adapter will be chosen based on a longest prefix match. Be mindful prefixes such as `http://localhost` will also match `http://localhost.other.com` or `http://localhost@other.com`. It’s recommended to terminate full hostnames with a `/`.
Many of the details of implementing a Transport Adapter are beyond the scope of this documentation, but take a look at the next example for a simple SSL use- case. For more than that, you might look at subclassing the `BaseAdapter`.
### Example: Specific SSL Version¶
The Requests team has made a specific choice to use whatever SSL version is default in the underlying library (urllib3). Normally this is fine, but from time to time, you might find yourself needing to connect to a service-endpoint that uses a version that isn’t compatible with the default.
You can use Transport Adapters for this by taking most of the existing implementation of HTTPAdapter, and adding a parameter _ssl_version_ that gets passed-through to urllib3. We’ll make a Transport Adapter that instructs the library to use SSLv3:
```
importssl
fromurllib3.poolmanagerimport PoolManager
fromrequests.adaptersimport HTTPAdapter

classSsl3HttpAdapter(HTTPAdapter):
""""Transport adapter" that allows us to use SSLv3."""
  definit_poolmanager(self, connections, maxsize, block=False):
    self.poolmanager = PoolManager(
      num_pools=connections, maxsize=maxsize,
      block=block, ssl_version=ssl.PROTOCOL_SSLv3)

```

### Example: Automatic Retries¶
By default, Requests does not retry failed connections. However, it is possible to implement automatic retries with a powerful array of features, including backoff, within a Requests `Session` using the urllib3.util.Retry class:
```
fromurllib3.utilimport Retry
fromrequestsimport Session
fromrequests.adaptersimport HTTPAdapter
s = Session()
retries = Retry(
  total=3,
  backoff_factor=0.1,
  status_forcelist=[502, 503, 504],
  allowed_methods={'POST'},
)
s.mount('https://', HTTPAdapter(max_retries=retries))

```

## Blocking Or Non-Blocking?¶
With the default Transport Adapter in place, Requests does not provide any kind of non-blocking IO. The `Response.content` property will block until the entire response has been downloaded. If you require more granularity, the streaming features of the library (see Streaming Requests) allow you to retrieve smaller quantities of the response at a time. However, these calls will still block.
If you are concerned about the use of blocking IO, there are lots of projects out there that combine Requests with one of Python’s asynchronicity frameworks. Some excellent examples are requests-threads, grequests, requests-futures, and httpx.
## Header Ordering¶
In unusual circumstances you may want to provide headers in an ordered manner. If you pass an `OrderedDict` to the `headers` keyword argument, that will provide the headers with an ordering. _However_ , the ordering of the default headers used by Requests will be preferred, which means that if you override default headers in the `headers` keyword argument, they may appear out of order compared to other headers in that keyword argument.
If this is problematic, users should consider setting the default headers on a `Session` object, by setting `Session.headers` to a custom `OrderedDict`. That ordering will always be preferred.
## Timeouts¶
Most requests to external servers should have a timeout attached, in case the server is not responding in a timely manner. By default, requests do not time out unless a timeout value is set explicitly. Without a timeout, your code may hang for minutes or more.
The **connect** timeout is the number of seconds Requests will wait for your client to establish a connection to a remote machine (corresponding to the connect()) call on the socket. It’s a good practice to set connect timeouts to slightly larger than a multiple of 3, which is the default TCP packet retransmission window.
Once your client has connected to the server and sent the HTTP request, the **read** timeout is the number of seconds the client will wait for the server to send a response. (Specifically, it’s the number of seconds that the client will wait _between_ bytes sent from the server. In 99.9% of cases, this is the time before the server sends the first byte).
If you specify a single value for the timeout, like this:
```
r = requests.get('https://github.com', timeout=5)

```

The timeout value will be applied to both the `connect` and the `read` timeouts. Specify a tuple if you would like to set the values separately:
```
r = requests.get('https://github.com', timeout=(3.05, 27))

```

If the remote server is very slow, you can tell Requests to wait forever for a response, by passing None as a timeout value and then retrieving a cup of coffee.
```
r = requests.get('https://github.com', timeout=None)

```

Note
The connect timeout applies to each connection attempt to an IP address. If multiple addresses exist for a domain name, the underlying `urllib3` will try each address sequentially until one successfully connects. This may lead to an effective total connection timeout _multiple_ times longer than the specified time, e.g. an unresponsive server having both IPv4 and IPv6 addresses will have its perceived timeout _doubled_ , so take that into account when setting the connection timeout.
Note
Neither the connect nor read timeouts are wall clock. This means that if you start a request, and look at the time, and then look at the time when the request finishes or times out, the real-world time may be greater than what you specified.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Advanced Usage
    * Session Objects
    * Request and Response Objects
    * Prepared Requests
    * SSL Cert Verification
    * Client Side Certificates
    * CA Certificates
    * Body Content Workflow
    * Keep-Alive
    * Streaming Uploads
    * Chunk-Encoded Requests
    * POST Multiple Multipart-Encoded Files
    * Event Hooks
    * Custom Authentication
    * Streaming Requests
    * Proxies
      * SOCKS
    * Compliance
      * Encodings
    * HTTP Verbs
    * Custom Verbs
    * Link Headers
    * Transport Adapters
      * Example: Specific SSL Version
      * Example: Automatic Retries
    * Blocking Or Non-Blocking?
    * Header Ordering
    * Timeouts


### Related Topics
  * Documentation overview
    * Previous: Quickstart
    * Next: Authentication


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Authentication¶
This document discusses using various kinds of authentication with Requests.
Many web services require authentication, and there are many different types. Below, we outline various forms of authentication available in Requests, from the simple to the complex.
## Basic Authentication¶
Many web services that require authentication accept HTTP Basic Auth. This is the simplest kind, and Requests supports it straight out of the box.
Making requests with HTTP Basic Auth is very simple:
```
>>> fromrequests.authimport HTTPBasicAuth
>>> basic = HTTPBasicAuth('user', 'pass')
>>> requests.get('https://httpbin.org/basic-auth/user/pass', auth=basic)
<Response [200]>

```

In fact, HTTP Basic Auth is so common that Requests provides a handy shorthand for using it:
```
>>> requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))
<Response [200]>

```

Providing the credentials in a tuple like this is exactly the same as the `HTTPBasicAuth` example above.
### netrc Authentication¶
If no authentication method is given with the `auth` argument, Requests will attempt to get the authentication credentials for the URL’s hostname from the user’s netrc file. The netrc file overrides raw HTTP authentication headers set with headers=.
If credentials for the hostname are found, the request is sent with HTTP Basic Auth.
## Digest Authentication¶
Another very popular form of HTTP Authentication is Digest Authentication, and Requests supports this out of the box as well:
```
>>> fromrequests.authimport HTTPDigestAuth
>>> url = 'https://httpbin.org/digest-auth/auth/user/pass'
>>> requests.get(url, auth=HTTPDigestAuth('user', 'pass'))
<Response [200]>

```

## OAuth 1 Authentication¶
A common form of authentication for several web APIs is OAuth. The `requests-oauthlib` library allows Requests users to easily make OAuth 1 authenticated requests:
```
>>> importrequests
>>> fromrequests_oauthlibimport OAuth1
>>> url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
>>> auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET',
...        'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')
>>> requests.get(url, auth=auth)
<Response [200]>

```

For more information on how to OAuth flow works, please see the official OAuth website. For examples and documentation on requests-oauthlib, please see the requests_oauthlib repository on GitHub
## OAuth 2 and OpenID Connect Authentication¶
The `requests-oauthlib` library also handles OAuth 2, the authentication mechanism underpinning OpenID Connect. See the requests-oauthlib OAuth2 documentation for details of the various OAuth 2 credential management flows:
  * Web Application Flow
  * Mobile Application Flow
  * Legacy Application Flow
  * Backend Application Flow


## Other Authentication¶
Requests is designed to allow other forms of authentication to be easily and quickly plugged in. Members of the open-source community frequently write authentication handlers for more complicated or less commonly-used forms of authentication. Some of the best have been brought together under the Requests organization, including:
  * Kerberos
  * NTLM


If you want to use any of these forms of authentication, go straight to their GitHub page and follow the instructions.
## New Forms of Authentication¶
If you can’t find a good implementation of the form of authentication you want, you can implement it yourself. Requests makes it easy to add your own forms of authentication.
To do so, subclass `AuthBase` and implement the `__call__()` method:
```
>>> importrequests
>>> classMyAuth(requests.auth.AuthBase):
...   def__call__(self, r):
...     # Implement my authentication
...     return r
...
>>> url = 'https://httpbin.org/get'
>>> requests.get(url, auth=MyAuth())
<Response [200]>

```

When an authentication handler is attached to a request, it is called during request setup. The `__call__` method must therefore do whatever is required to make the authentication work. Some forms of authentication will additionally add hooks to provide further functionality.
Further examples can be found under the Requests organization and in the `auth.py` file.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Authentication
    * Basic Authentication
      * netrc Authentication
    * Digest Authentication
    * OAuth 1 Authentication
    * OAuth 2 and OpenID Connect Authentication
    * Other Authentication
    * New Forms of Authentication


### Related Topics
  * Documentation overview
    * Previous: Advanced Usage
    * Next: Recommended Packages and Extensions


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Installation of Requests¶
This part of the documentation covers the installation of Requests. The first step to using any software package is getting it properly installed.
## $ python -m pip install requests¶
To install Requests, simply run this simple command in your terminal of choice:
```
$ python -m pip install requests

```

## Get the Source Code¶
Requests is actively developed on GitHub, where the code is always available.
You can either clone the public repository:
```
$ git clone https://github.com/psf/requests.git

```

Or, download the tarball:
```
$ curl -OL https://github.com/psf/requests/tarball/main
# optionally, zipball is also available (for Windows users).

```

Once you have a copy of the source, you can embed it in your own Python package, or install it into your site-packages easily:
```
$ cd requests
$ python -m pip install .

```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Installation of Requests
    * $ python -m pip install requests
    * Get the Source Code


### Related Topics
  * Documentation overview
    * Previous: Requests: HTTP for Humans™
    * Next: Quickstart


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Quickstart¶
Eager to get started? This page gives a good introduction in how to get started with Requests.
First, make sure that:
  * Requests is installed
  * Requests is up-to-date


Let’s get started with some simple examples.
## Make a Request¶
Making a request with Requests is very simple.
Begin by importing the Requests module:
```
>>> importrequests

```

Now, let’s try to get a webpage. For this example, let’s get GitHub’s public timeline:
```
>>> r = requests.get('https://api.github.com/events')

```

Now, we have a `Response` object called `r`. We can get all the information we need from this object.
Requests’ simple API means that all forms of HTTP request are as obvious. For example, this is how you make an HTTP POST request:
```
>>> r = requests.post('https://httpbin.org/post', data={'key': 'value'})

```

Nice, right? What about the other HTTP request types: PUT, DELETE, HEAD and OPTIONS? These are all just as simple:
```
>>> r = requests.put('https://httpbin.org/put', data={'key': 'value'})
>>> r = requests.delete('https://httpbin.org/delete')
>>> r = requests.head('https://httpbin.org/get')
>>> r = requests.options('https://httpbin.org/get')

```

That’s all well and good, but it’s also only the start of what Requests can do.
## Passing Parameters In URLs¶
You often want to send some sort of data in the URL’s query string. If you were constructing the URL by hand, this data would be given as key/value pairs in the URL after a question mark, e.g. `httpbin.org/get?key=val`. Requests allows you to provide these arguments as a dictionary of strings, using the `params` keyword argument. As an example, if you wanted to pass `key1=value1` and `key2=value2` to `httpbin.org/get`, you would use the following code:
```
>>> payload = {'key1': 'value1', 'key2': 'value2'}
>>> r = requests.get('https://httpbin.org/get', params=payload)

```

You can see that the URL has been correctly encoded by printing the URL:
```
>>> print(r.url)
https://httpbin.org/get?key2=value2&key1=value1

```

Note that any dictionary key whose value is `None` will not be added to the URL’s query string.
You can also pass a list of items as a value:
```
>>> payload = {'key1': 'value1', 'key2': ['value2', 'value3']}
>>> r = requests.get('https://httpbin.org/get', params=payload)
>>> print(r.url)
https://httpbin.org/get?key1=value1&key2=value2&key2=value3

```

## Response Content¶
We can read the content of the server’s response. Consider the GitHub timeline again:
```
>>> importrequests
>>> r = requests.get('https://api.github.com/events')
>>> r.text
'[{"repository":{"open_issues":0,"url":"https://github.com/...

```

Requests will automatically decode content from the server. Most unicode charsets are seamlessly decoded.
When you make a request, Requests makes educated guesses about the encoding of the response based on the HTTP headers. The text encoding guessed by Requests is used when you access `r.text`. You can find out what encoding Requests is using, and change it, using the `r.encoding` property:
```
>>> r.encoding
'utf-8'
>>> r.encoding = 'ISO-8859-1'

```

If you change the encoding, Requests will use the new value of `r.encoding` whenever you call `r.text`. You might want to do this in any situation where you can apply special logic to work out what the encoding of the content will be. For example, HTML and XML have the ability to specify their encoding in their body. In situations like this, you should use `r.content` to find the encoding, and then set `r.encoding`. This will let you use `r.text` with the correct encoding.
Requests will also use custom encodings in the event that you need them. If you have created your own encoding and registered it with the `codecs` module, you can simply use the codec name as the value of `r.encoding` and Requests will handle the decoding for you.
## Binary Response Content¶
You can also access the response body as bytes, for non-text requests:
```
>>> r.content
b'[{"repository":{"open_issues":0,"url":"https://github.com/...

```

The `gzip` and `deflate` transfer-encodings are automatically decoded for you.
The `br` transfer-encoding is automatically decoded for you if a Brotli library like brotli or brotlicffi is installed.
For example, to create an image from binary data returned by a request, you can use the following code:
```
>>> fromPILimport Image
>>> fromioimport BytesIO
>>> i = Image.open(BytesIO(r.content))

```

## JSON Response Content¶
There’s also a builtin JSON decoder, in case you’re dealing with JSON data:
```
>>> importrequests
>>> r = requests.get('https://api.github.com/events')
>>> r.json()
[{'repository': {'open_issues': 0, 'url': 'https://github.com/...

```

In case the JSON decoding fails, `r.json()` raises an exception. For example, if the response gets a 204 (No Content), or if the response contains invalid JSON, attempting `r.json()` raises `requests.exceptions.JSONDecodeError`. This wrapper exception provides interoperability for multiple exceptions that may be thrown by different python versions and json serialization libraries.
It should be noted that the success of the call to `r.json()` does **not** indicate the success of the response. Some servers may return a JSON object in a failed response (e.g. error details with HTTP 500). Such JSON will be decoded and returned. To check that a request is successful, use `r.raise_for_status()` or check `r.status_code` is what you expect.
## Raw Response Content¶
In the rare case that you’d like to get the raw socket response from the server, you can access `r.raw`. If you want to do this, make sure you set `stream=True` in your initial request. Once you do, you can do this:
```
>>> r = requests.get('https://api.github.com/events', stream=True)
>>> r.raw
<urllib3.response.HTTPResponse object at 0x101194810>
>>> r.raw.read(10)
b'\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\x03'

```

In general, however, you should use a pattern like this to save what is being streamed to a file:
```
with open(filename, 'wb') as fd:
  for chunk in r.iter_content(chunk_size=128):
    fd.write(chunk)

```

Using `Response.iter_content` will handle a lot of what you would otherwise have to handle when using `Response.raw` directly. When streaming a download, the above is the preferred and recommended way to retrieve the content. Note that `chunk_size` can be freely adjusted to a number that may better fit your use cases.
Note
An important note about using `Response.iter_content` versus `Response.raw`. `Response.iter_content` will automatically decode the `gzip` and `deflate` transfer-encodings. `Response.raw` is a raw stream of bytes – it does not transform the response content. If you really need access to the bytes as they were returned, use `Response.raw`.
## Custom Headers¶
If you’d like to add HTTP headers to a request, simply pass in a `dict` to the `headers` parameter.
For example, we didn’t specify our user-agent in the previous example:
```
>>> url = 'https://api.github.com/some/endpoint'
>>> headers = {'user-agent': 'my-app/0.0.1'}
>>> r = requests.get(url, headers=headers)

```

Note: Custom headers are given less precedence than more specific sources of information. For instance:
  * Authorization headers set with headers= will be overridden if credentials are specified in `.netrc`, which in turn will be overridden by the `auth=` parameter. Requests will search for the netrc file at ~/.netrc, ~/_netrc, or at the path specified by the NETRC environment variable.
  * Authorization headers will be removed if you get redirected off-host.
  * Proxy-Authorization headers will be overridden by proxy credentials provided in the URL.
  * Content-Length headers will be overridden when we can determine the length of the content.


Furthermore, Requests does not change its behavior at all based on which custom headers are specified. The headers are simply passed on into the final request.
Note: All header values must be a `string`, bytestring, or unicode. While permitted, it’s advised to avoid passing unicode header values.
## More complicated POST requests¶
Typically, you want to send some form-encoded data — much like an HTML form. To do this, simply pass a dictionary to the `data` argument. Your dictionary of data will automatically be form-encoded when the request is made:
```
>>> payload = {'key1': 'value1', 'key2': 'value2'}
>>> r = requests.post('https://httpbin.org/post', data=payload)
>>> print(r.text)
{
 ...
 "form": {
  "key2": "value2",
  "key1": "value1"
 },
 ...
}

```

The `data` argument can also have multiple values for each key. This can be done by making `data` either a list of tuples or a dictionary with lists as values. This is particularly useful when the form has multiple elements that use the same key:
```
>>> payload_tuples = [('key1', 'value1'), ('key1', 'value2')]
>>> r1 = requests.post('https://httpbin.org/post', data=payload_tuples)
>>> payload_dict = {'key1': ['value1', 'value2']}
>>> r2 = requests.post('https://httpbin.org/post', data=payload_dict)
>>> print(r1.text)
{
 ...
 "form": {
  "key1": [
   "value1",
   "value2"
  ]
 },
 ...
}
>>> r1.text == r2.text
True

```

There are times that you may want to send data that is not form-encoded. If you pass in a `string` instead of a `dict`, that data will be posted directly.
For example, the GitHub API v3 accepts JSON-Encoded POST/PATCH data:
```
>>> importjson
>>> url = 'https://api.github.com/some/endpoint'
>>> payload = {'some': 'data'}
>>> r = requests.post(url, data=json.dumps(payload))

```

Please note that the above code will NOT add the `Content-Type` header (so in particular it will NOT set it to `application/json`).
If you need that header set and you don’t want to encode the `dict` yourself, you can also pass it directly using the `json` parameter (added in version 2.4.2) and it will be encoded automatically:
```
>>> url = 'https://api.github.com/some/endpoint'
>>> payload = {'some': 'data'}

```

```
>>> r = requests.post(url, json=payload)

```

Note, the `json` parameter is ignored if either `data` or `files` is passed.
## POST a Multipart-Encoded File¶
Requests makes it simple to upload Multipart-encoded files:
```
>>> url = 'https://httpbin.org/post'
>>> files = {'file': open('report.xls', 'rb')}
>>> r = requests.post(url, files=files)
>>> r.text
{
 ...
 "files": {
  "file": "<censored...binary...data>"
 },
 ...
}

```

You can set the filename, content_type and headers explicitly:
```
>>> url = 'https://httpbin.org/post'
>>> files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})}
>>> r = requests.post(url, files=files)
>>> r.text
{
 ...
 "files": {
  "file": "<censored...binary...data>"
 },
 ...
}

```

If you want, you can send strings to be received as files:
```
>>> url = 'https://httpbin.org/post'
>>> files = {'file': ('report.csv', 'some,data,to,send\nanother,row,to,send\n')}
>>> r = requests.post(url, files=files)
>>> r.text
{
 ...
 "files": {
  "file": "some,data,to,send\\nanother,row,to,send\\n"
 },
 ...
}

```

In the event you are posting a very large file as a `multipart/form-data` request, you may want to stream the request. By default, `requests` does not support this, but there is a separate package which does - `requests-toolbelt`. You should read the toolbelt’s documentation for more details about how to use it.
For sending multiple files in one request refer to the advanced section.
Warning
It is strongly recommended that you open files in binary mode. This is because Requests may attempt to provide the `Content-Length` header for you, and if it does this value will be set to the number of _bytes_ in the file. Errors may occur if you open the file in _text mode_.
## Response Status Codes¶
We can check the response status code:
```
>>> r = requests.get('https://httpbin.org/get')
>>> r.status_code
200

```

Requests also comes with a built-in status code lookup object for easy reference:
```
>>> r.status_code == requests.codes.ok
True

```

If we made a bad request (a 4XX client error or 5XX server error response), we can raise it with `Response.raise_for_status()`:
```
>>> bad_r = requests.get('https://httpbin.org/status/404')
>>> bad_r.status_code
404
>>> bad_r.raise_for_status()
Traceback (most recent call last):
 File "requests/models.py", line 832, in raise_for_status
raise http_error
requests.exceptions.HTTPError: 404 Client Error

```

But, since our `status_code` for `r` was `200`, when we call `raise_for_status()` we get:
```
>>> r.raise_for_status()
None

```

All is well.
## Response Headers¶
We can view the server’s response headers using a Python dictionary:
```
>>> r.headers
{
  'content-encoding': 'gzip',
  'transfer-encoding': 'chunked',
  'connection': 'close',
  'server': 'nginx/1.0.4',
  'x-runtime': '148ms',
  'etag': '"e1ca502697e5c9317743dc078f67693f"',
  'content-type': 'application/json'
}

```

The dictionary is special, though: it’s made just for HTTP headers. According to RFC 7230, HTTP Header names are case-insensitive.
So, we can access the headers using any capitalization we want:
```
>>> r.headers['Content-Type']
'application/json'
>>> r.headers.get('content-type')
'application/json'

```

It is also special in that the server could have sent the same header multiple times with different values, but requests combines them so they can be represented in the dictionary within a single mapping, as per RFC 7230:
> A recipient MAY combine multiple header fields with the same field name into one “field-name: field-value” pair, without changing the semantics of the message, by appending each subsequent field value to the combined field value in order, separated by a comma.
## Cookies¶
If a response contains some Cookies, you can quickly access them:
```
>>> url = 'http://example.com/some/cookie/setting/url'
>>> r = requests.get(url)
>>> r.cookies['example_cookie_name']
'example_cookie_value'

```

To send your own cookies to the server, you can use the `cookies` parameter:
```
>>> url = 'https://httpbin.org/cookies'
>>> cookies = dict(cookies_are='working')
>>> r = requests.get(url, cookies=cookies)
>>> r.text
'{"cookies": {"cookies_are": "working"}}'

```

Cookies are returned in a `RequestsCookieJar`, which acts like a `dict` but also offers a more complete interface, suitable for use over multiple domains or paths. Cookie jars can also be passed in to requests:
```
>>> jar = requests.cookies.RequestsCookieJar()
>>> jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')
>>> jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere')
>>> url = 'https://httpbin.org/cookies'
>>> r = requests.get(url, cookies=jar)
>>> r.text
'{"cookies": {"tasty_cookie": "yum"}}'

```

## Redirection and History¶
By default Requests will perform location redirection for all verbs except HEAD.
We can use the `history` property of the Response object to track redirection.
The `Response.history` list contains the `Response` objects that were created in order to complete the request. The list is sorted from the oldest to the most recent response.
For example, GitHub redirects all HTTP requests to HTTPS:
```
>>> r = requests.get('http://github.com/')
>>> r.url
'https://github.com/'
>>> r.status_code
200
>>> r.history
[<Response [301]>]

```

If you’re using GET, OPTIONS, POST, PUT, PATCH or DELETE, you can disable redirection handling with the `allow_redirects` parameter:
```
>>> r = requests.get('http://github.com/', allow_redirects=False)
>>> r.status_code
301
>>> r.history
[]

```

If you’re using HEAD, you can enable redirection as well:
```
>>> r = requests.head('http://github.com/', allow_redirects=True)
>>> r.url
'https://github.com/'
>>> r.history
[<Response [301]>]

```

## Timeouts¶
You can tell Requests to stop waiting for a response after a given number of seconds with the `timeout` parameter. Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely:
```
>>> requests.get('https://github.com/', timeout=0.001)
Traceback (most recent call last):
 File "<stdin>", line 1, in <module>
requests.exceptions.Timeout: HTTPConnectionPool(host='github.com', port=80): Request timed out. (timeout=0.001)

```

Note
`timeout` is not a time limit on the entire response download; rather, an exception is raised if the server has not issued a response for `timeout` seconds (more precisely, if no bytes have been received on the underlying socket for `timeout` seconds). If no timeout is specified explicitly, requests do not time out.
## Errors and Exceptions¶
In the event of a network problem (e.g. DNS failure, refused connection, etc), Requests will raise a `ConnectionError` exception.
`Response.raise_for_status()` will raise an `HTTPError` if the HTTP request returned an unsuccessful status code.
If a request times out, a `Timeout` exception is raised.
If a request exceeds the configured number of maximum redirections, a `TooManyRedirects` exception is raised.
All exceptions that Requests explicitly raises inherit from `requests.exceptions.RequestException`.
Ready for more? Check out the advanced section.
Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Table of Contents
  * Quickstart
    * Make a Request
    * Passing Parameters In URLs
    * Response Content
    * Binary Response Content
    * JSON Response Content
    * Raw Response Content
    * Custom Headers
    * More complicated POST requests
    * POST a Multipart-Encoded File
    * Response Status Codes
    * Response Headers
    * Cookies
    * Redirection and History
    * Timeouts
    * Errors and Exceptions


### Related Topics
  * Documentation overview
    * Previous: Installation of Requests
    * Next: Advanced Usage


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.adapters
```
"""
requests.adapters
~~~~~~~~~~~~~~~~~
This module contains the transport adapters that Requests uses to define
and maintain connections.
"""
importos.path
importsocket # noqa: F401
importtyping
importwarnings
fromurllib3.exceptionsimport ClosedPoolError, ConnectTimeoutError
fromurllib3.exceptionsimport HTTPError as _HTTPError
fromurllib3.exceptionsimport InvalidHeader as _InvalidHeader
fromurllib3.exceptionsimport (
  LocationValueError,
  MaxRetryError,
  NewConnectionError,
  ProtocolError,
)
fromurllib3.exceptionsimport ProxyError as _ProxyError
fromurllib3.exceptionsimport ReadTimeoutError, ResponseError
fromurllib3.exceptionsimport SSLError as _SSLError
fromurllib3.poolmanagerimport PoolManager, proxy_from_url
fromurllib3.utilimport Timeout as TimeoutSauce
fromurllib3.utilimport parse_url
fromurllib3.util.retryimport Retry
fromurllib3.util.ssl_import create_urllib3_context
from.authimport _basic_auth_str
from.compatimport basestring, urlparse
from.cookiesimport extract_cookies_to_jar
from.exceptionsimport (
  ConnectionError,
  ConnectTimeout,
  InvalidHeader,
  InvalidProxyURL,
  InvalidSchema,
  InvalidURL,
  ProxyError,
  ReadTimeout,
  RetryError,
  SSLError,
)
from.modelsimport Response
from.structuresimport CaseInsensitiveDict
from.utilsimport (
  DEFAULT_CA_BUNDLE_PATH,
  extract_zipped_paths,
  get_auth_from_url,
  get_encoding_from_headers,
  prepend_scheme_if_needed,
  select_proxy,
  urldefragauth,
)
try:
  fromurllib3.contrib.socksimport SOCKSProxyManager
except ImportError:
  defSOCKSProxyManager(*args, **kwargs):
    raise InvalidSchema("Missing dependencies for SOCKS support.")

if typing.TYPE_CHECKING:
  from.modelsimport PreparedRequest

DEFAULT_POOLBLOCK = False
DEFAULT_POOLSIZE = 10
DEFAULT_RETRIES = 0
DEFAULT_POOL_TIMEOUT = None

try:
  importssl # noqa: F401
  _preloaded_ssl_context = create_urllib3_context()
  _preloaded_ssl_context.load_verify_locations(
    extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)
  )
except ImportError:
  # Bypass default SSLContext creation when Python
  # interpreter isn't built with the ssl module.
  _preloaded_ssl_context = None

def_urllib3_request_context(
  request: "PreparedRequest",
  verify: "bool | str | None",
  client_cert: "typing.Tuple[str, str] | str | None",
  poolmanager: "PoolManager",
) -> "(typing.Dict[str, typing.Any], typing.Dict[str, typing.Any])":
  host_params = {}
  pool_kwargs = {}
  parsed_request_url = urlparse(request.url)
  scheme = parsed_request_url.scheme.lower()
  port = parsed_request_url.port
  # Determine if we have and should use our default SSLContext
  # to optimize performance on standard requests.
  poolmanager_kwargs = getattr(poolmanager, "connection_pool_kw", {})
  has_poolmanager_ssl_context = poolmanager_kwargs.get("ssl_context")
  should_use_default_ssl_context = (
    _preloaded_ssl_context is not None and not has_poolmanager_ssl_context
  )
  cert_reqs = "CERT_REQUIRED"
  if verify is False:
    cert_reqs = "CERT_NONE"
  elif verify is True and should_use_default_ssl_context:
    pool_kwargs["ssl_context"] = _preloaded_ssl_context
  elif isinstance(verify, str):
    if not os.path.isdir(verify):
      pool_kwargs["ca_certs"] = verify
    else:
      pool_kwargs["ca_cert_dir"] = verify
  pool_kwargs["cert_reqs"] = cert_reqs
  if client_cert is not None:
    if isinstance(client_cert, tuple) and len(client_cert) == 2:
      pool_kwargs["cert_file"] = client_cert[0]
      pool_kwargs["key_file"] = client_cert[1]
    else:
      # According to our docs, we allow users to specify just the client
      # cert path
      pool_kwargs["cert_file"] = client_cert
  host_params = {
    "scheme": scheme,
    "host": parsed_request_url.hostname,
    "port": port,
  }
  return host_params, pool_kwargs



[docs]
classBaseAdapter:
"""The Base Transport Adapter"""
  def__init__(self):
    super().__init__()


[docs]
  defsend(
    self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
  ):
"""Sends PreparedRequest object. Returns Response object.
    :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
    :param stream: (optional) Whether to stream the request content.
    :param timeout: (optional) How long to wait for the server to send
      data before giving up, as a float, or a :ref:`(connect timeout,
      read timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
      the server's TLS certificate, or a string, in which case it must be a path
      to a CA bundle to use
    :param cert: (optional) Any user-provided SSL certificate to be trusted.
    :param proxies: (optional) The proxies dictionary to apply to the request.
    """
    raise NotImplementedError




[docs]
  defclose(self):
"""Cleans up adapter specific items."""
    raise NotImplementedError






[docs]
classHTTPAdapter(BaseAdapter):
"""The built-in HTTP Adapter for urllib3.
  Provides a general-case interface for Requests sessions to contact HTTP and
  HTTPS urls by implementing the Transport Adapter interface. This class will
  usually be created by the :class:`Session <Session>` class under the
  covers.
  :param pool_connections: The number of urllib3 connection pools to cache.
  :param pool_maxsize: The maximum number of connections to save in the pool.
  :param max_retries: The maximum number of retries each connection
    should attempt. Note, this applies only to failed DNS lookups, socket
    connections and connection timeouts, never to requests where data has
    made it to the server. By default, Requests does not retry failed
    connections. If you need granular control over the conditions under
    which we retry a request, import urllib3's ``Retry`` class and pass
    that instead.
  :param pool_block: Whether the connection pool should block for connections.
  Usage::
   >>> import requests
   >>> s = requests.Session()
   >>> a = requests.adapters.HTTPAdapter(max_retries=3)
   >>> s.mount('http://', a)
  """
  __attrs__ = [
    "max_retries",
    "config",
    "_pool_connections",
    "_pool_maxsize",
    "_pool_block",
  ]
  def__init__(
    self,
    pool_connections=DEFAULT_POOLSIZE,
    pool_maxsize=DEFAULT_POOLSIZE,
    max_retries=DEFAULT_RETRIES,
    pool_block=DEFAULT_POOLBLOCK,
  ):
    if max_retries == DEFAULT_RETRIES:
      self.max_retries = Retry(0, read=False)
    else:
      self.max_retries = Retry.from_int(max_retries)
    self.config = {}
    self.proxy_manager = {}
    super().__init__()
    self._pool_connections = pool_connections
    self._pool_maxsize = pool_maxsize
    self._pool_block = pool_block
    self.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)
  def__getstate__(self):
    return {attr: getattr(self, attr, None) for attr in self.__attrs__}
  def__setstate__(self, state):
    # Can't handle by adding 'proxy_manager' to self.__attrs__ because
    # self.poolmanager uses a lambda function, which isn't pickleable.
    self.proxy_manager = {}
    self.config = {}
    for attr, value in state.items():
      setattr(self, attr, value)
    self.init_poolmanager(
      self._pool_connections, self._pool_maxsize, block=self._pool_block
    )


[docs]
  definit_poolmanager(
    self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs
  ):
"""Initializes a urllib3 PoolManager.
    This method should not be called from user code, and is only
    exposed for use when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param connections: The number of urllib3 connection pools to cache.
    :param maxsize: The maximum number of connections to save in the pool.
    :param block: Block when no free connections are available.
    :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.
    """
    # save these values for pickling
    self._pool_connections = connections
    self._pool_maxsize = maxsize
    self._pool_block = block
    self.poolmanager = PoolManager(
      num_pools=connections,
      maxsize=maxsize,
      block=block,
      **pool_kwargs,
    )




[docs]
  defproxy_manager_for(self, proxy, **proxy_kwargs):
"""Return urllib3 ProxyManager for the given proxy.
    This method should not be called from user code, and is only
    exposed for use when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param proxy: The proxy to return a urllib3 ProxyManager for.
    :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.
    :returns: ProxyManager
    :rtype: urllib3.ProxyManager
    """
    if proxy in self.proxy_manager:
      manager = self.proxy_manager[proxy]
    elif proxy.lower().startswith("socks"):
      username, password = get_auth_from_url(proxy)
      manager = self.proxy_manager[proxy] = SOCKSProxyManager(
        proxy,
        username=username,
        password=password,
        num_pools=self._pool_connections,
        maxsize=self._pool_maxsize,
        block=self._pool_block,
        **proxy_kwargs,
      )
    else:
      proxy_headers = self.proxy_headers(proxy)
      manager = self.proxy_manager[proxy] = proxy_from_url(
        proxy,
        proxy_headers=proxy_headers,
        num_pools=self._pool_connections,
        maxsize=self._pool_maxsize,
        block=self._pool_block,
        **proxy_kwargs,
      )
    return manager




[docs]
  defcert_verify(self, conn, url, verify, cert):
"""Verify a SSL certificate. This method should not be called from user
    code, and is only exposed for use when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param conn: The urllib3 connection object associated with the cert.
    :param url: The requested URL.
    :param verify: Either a boolean, in which case it controls whether we verify
      the server's TLS certificate, or a string, in which case it must be a path
      to a CA bundle to use
    :param cert: The SSL certificate to verify.
    """
    if url.lower().startswith("https") and verify:
      conn.cert_reqs = "CERT_REQUIRED"
      # Only load the CA certificates if 'verify' is a string indicating the CA bundle to use.
      # Otherwise, if verify is a boolean, we don't load anything since
      # the connection will be using a context with the default certificates already loaded,
      # and this avoids a call to the slow load_verify_locations()
      if verify is not True:
        # `verify` must be a str with a path then
        cert_loc = verify
        if not os.path.exists(cert_loc):
          raise OSError(
            f"Could not find a suitable TLS CA certificate bundle, "
            f"invalid path: {cert_loc}"
          )
        if not os.path.isdir(cert_loc):
          conn.ca_certs = cert_loc
        else:
          conn.ca_cert_dir = cert_loc
    else:
      conn.cert_reqs = "CERT_NONE"
      conn.ca_certs = None
      conn.ca_cert_dir = None
    if cert:
      if not isinstance(cert, basestring):
        conn.cert_file = cert[0]
        conn.key_file = cert[1]
      else:
        conn.cert_file = cert
        conn.key_file = None
      if conn.cert_file and not os.path.exists(conn.cert_file):
        raise OSError(
          f"Could not find the TLS certificate file, "
          f"invalid path: {conn.cert_file}"
        )
      if conn.key_file and not os.path.exists(conn.key_file):
        raise OSError(
          f"Could not find the TLS key file, invalid path: {conn.key_file}"
        )




[docs]
  defbuild_response(self, req, resp):
"""Builds a :class:`Response <requests.Response>` object from a urllib3
    response. This should not be called from user code, and is only exposed
    for use when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`
    :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
    :param resp: The urllib3 response object.
    :rtype: requests.Response
    """
    response = Response()
    # Fallback to None if there's no status_code, for whatever reason.
    response.status_code = getattr(resp, "status", None)
    # Make headers case-insensitive.
    response.headers = CaseInsensitiveDict(getattr(resp, "headers", {}))
    # Set encoding.
    response.encoding = get_encoding_from_headers(response.headers)
    response.raw = resp
    response.reason = response.raw.reason
    if isinstance(req.url, bytes):
      response.url = req.url.decode("utf-8")
    else:
      response.url = req.url
    # Add new cookies from the server.
    extract_cookies_to_jar(response.cookies, req, resp)
    # Give the Response some context.
    response.request = req
    response.connection = self
    return response




[docs]
  defbuild_connection_pool_key_attributes(self, request, verify, cert=None):
"""Build the PoolKey attributes used by urllib3 to return a connection.
    This looks at the PreparedRequest, the user-specified verify value,
    and the value of the cert parameter to determine what PoolKey values
    to use to select a connection from a given urllib3 Connection Pool.
    The SSL related pool key arguments are not consistently set. As of
    this writing, use the following to determine what keys may be in that
    dictionary:
    * If ``verify`` is ``True``, ``"ssl_context"`` will be set and will be the
     default Requests SSL Context
    * If ``verify`` is ``False``, ``"ssl_context"`` will not be set but
     ``"cert_reqs"`` will be set
    * If ``verify`` is a string, (i.e., it is a user-specified trust bundle)
     ``"ca_certs"`` will be set if the string is not a directory recognized
     by :py:func:`os.path.isdir`, otherwise ``"ca_certs_dir"`` will be
     set.
    * If ``"cert"`` is specified, ``"cert_file"`` will always be set. If
     ``"cert"`` is a tuple with a second item, ``"key_file"`` will also
     be present
    To override these settings, one may subclass this class, call this
    method and use the above logic to change parameters as desired. For
    example, if one wishes to use a custom :py:class:`ssl.SSLContext` one
    must both set ``"ssl_context"`` and based on what else they require,
    alter the other keys to ensure the desired behaviour.
    :param request:
      The PreparedReqest being sent over the connection.
    :type request:
      :class:`~requests.models.PreparedRequest`
    :param verify:
      Either a boolean, in which case it controls whether
      we verify the server's TLS certificate, or a string, in which case it
      must be a path to a CA bundle to use.
    :param cert:
      (optional) Any user-provided SSL certificate for client
      authentication (a.k.a., mTLS). This may be a string (i.e., just
      the path to a file which holds both certificate and key) or a
      tuple of length 2 with the certificate file path and key file
      path.
    :returns:
      A tuple of two dictionaries. The first is the "host parameters"
      portion of the Pool Key including scheme, hostname, and port. The
      second is a dictionary of SSLContext related parameters.
    """
    return _urllib3_request_context(request, verify, cert, self.poolmanager)




[docs]
  defget_connection_with_tls_context(self, request, verify, proxies=None, cert=None):
"""Returns a urllib3 connection for the given request and TLS settings.
    This should not be called from user code, and is only exposed for use
    when subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param request:
      The :class:`PreparedRequest <PreparedRequest>` object to be sent
      over the connection.
    :param verify:
      Either a boolean, in which case it controls whether we verify the
      server's TLS certificate, or a string, in which case it must be a
      path to a CA bundle to use.
    :param proxies:
      (optional) The proxies dictionary to apply to the request.
    :param cert:
      (optional) Any user-provided SSL certificate to be used for client
      authentication (a.k.a., mTLS).
    :rtype:
      urllib3.ConnectionPool
    """
    proxy = select_proxy(request.url, proxies)
    try:
      host_params, pool_kwargs = self.build_connection_pool_key_attributes(
        request,
        verify,
        cert,
      )
    except ValueError as e:
      raise InvalidURL(e, request=request)
    if proxy:
      proxy = prepend_scheme_if_needed(proxy, "http")
      proxy_url = parse_url(proxy)
      if not proxy_url.host:
        raise InvalidProxyURL(
          "Please check proxy URL. It is malformed "
          "and could be missing the host."
        )
      proxy_manager = self.proxy_manager_for(proxy)
      conn = proxy_manager.connection_from_host(
        **host_params, pool_kwargs=pool_kwargs
      )
    else:
      # Only scheme should be lower case
      conn = self.poolmanager.connection_from_host(
        **host_params, pool_kwargs=pool_kwargs
      )
    return conn




[docs]
  defget_connection(self, url, proxies=None):
"""DEPRECATED: Users should move to `get_connection_with_tls_context`
    for all subclasses of HTTPAdapter using Requests>=2.32.2.
    Returns a urllib3 connection for the given URL. This should not be
    called from user code, and is only exposed for use when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param url: The URL to connect to.
    :param proxies: (optional) A Requests-style dictionary of proxies used on this request.
    :rtype: urllib3.ConnectionPool
    """
    warnings.warn(
      (
        "`get_connection` has been deprecated in favor of "
        "`get_connection_with_tls_context`. Custom HTTPAdapter subclasses "
        "will need to migrate for Requests>=2.32.2. Please see "
        "https://github.com/psf/requests/pull/6710 for more details."
      ),
      DeprecationWarning,
    )
    proxy = select_proxy(url, proxies)
    if proxy:
      proxy = prepend_scheme_if_needed(proxy, "http")
      proxy_url = parse_url(proxy)
      if not proxy_url.host:
        raise InvalidProxyURL(
          "Please check proxy URL. It is malformed "
          "and could be missing the host."
        )
      proxy_manager = self.proxy_manager_for(proxy)
      conn = proxy_manager.connection_from_url(url)
    else:
      # Only scheme should be lower case
      parsed = urlparse(url)
      url = parsed.geturl()
      conn = self.poolmanager.connection_from_url(url)
    return conn




[docs]
  defclose(self):
"""Disposes of any internal state.
    Currently, this closes the PoolManager and any active ProxyManager,
    which closes any pooled connections.
    """
    self.poolmanager.clear()
    for proxy in self.proxy_manager.values():
      proxy.clear()




[docs]
  defrequest_url(self, request, proxies):
"""Obtain the url to use when making the final request.
    If the message is being sent through a HTTP proxy, the full URL has to
    be used. Otherwise, we should only use the path portion of the URL.
    This should not be called from user code, and is only exposed for use
    when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.
    :rtype: str
    """
    proxy = select_proxy(request.url, proxies)
    scheme = urlparse(request.url).scheme
    is_proxied_http_request = proxy and scheme != "https"
    using_socks_proxy = False
    if proxy:
      proxy_scheme = urlparse(proxy).scheme.lower()
      using_socks_proxy = proxy_scheme.startswith("socks")
    url = request.path_url
    if url.startswith("//"): # Don't confuse urllib3
      url = f"/{url.lstrip('/')}"
    if is_proxied_http_request and not using_socks_proxy:
      url = urldefragauth(request.url)
    return url




[docs]
  defadd_headers(self, request, **kwargs):
"""Add any headers needed by the connection. As of v2.0 this does
    nothing by default, but is left for overriding by users that subclass
    the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    This should not be called from user code, and is only exposed for use
    when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.
    :param kwargs: The keyword arguments from the call to send().
    """
    pass




[docs]
  defproxy_headers(self, proxy):
"""Returns a dictionary of the headers to add to any request sent
    through a proxy. This works with urllib3 magic to ensure that they are
    correctly sent to the proxy, rather than in a tunnelled request if
    CONNECT is being used.
    This should not be called from user code, and is only exposed for use
    when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
    :param proxy: The url of the proxy being used for this request.
    :rtype: dict
    """
    headers = {}
    username, password = get_auth_from_url(proxy)
    if username:
      headers["Proxy-Authorization"] = _basic_auth_str(username, password)
    return headers




[docs]
  defsend(
    self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
  ):
"""Sends PreparedRequest object. Returns Response object.
    :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
    :param stream: (optional) Whether to stream the request content.
    :param timeout: (optional) How long to wait for the server to send
      data before giving up, as a float, or a :ref:`(connect timeout,
      read timeout) <timeouts>` tuple.
    :type timeout: float or tuple or urllib3 Timeout object
    :param verify: (optional) Either a boolean, in which case it controls whether
      we verify the server's TLS certificate, or a string, in which case it
      must be a path to a CA bundle to use
    :param cert: (optional) Any user-provided SSL certificate to be trusted.
    :param proxies: (optional) The proxies dictionary to apply to the request.
    :rtype: requests.Response
    """
    try:
      conn = self.get_connection_with_tls_context(
        request, verify, proxies=proxies, cert=cert
      )
    except LocationValueError as e:
      raise InvalidURL(e, request=request)
    self.cert_verify(conn, request.url, verify, cert)
    url = self.request_url(request, proxies)
    self.add_headers(
      request,
      stream=stream,
      timeout=timeout,
      verify=verify,
      cert=cert,
      proxies=proxies,
    )
    chunked = not (request.body is None or "Content-Length" in request.headers)
    if isinstance(timeout, tuple):
      try:
        connect, read = timeout
        timeout = TimeoutSauce(connect=connect, read=read)
      except ValueError:
        raise ValueError(
          f"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, "
          f"or a single float to set both timeouts to the same value."
        )
    elif isinstance(timeout, TimeoutSauce):
      pass
    else:
      timeout = TimeoutSauce(connect=timeout, read=timeout)
    try:
      resp = conn.urlopen(
        method=request.method,
        url=url,
        body=request.body,
        headers=request.headers,
        redirect=False,
        assert_same_host=False,
        preload_content=False,
        decode_content=False,
        retries=self.max_retries,
        timeout=timeout,
        chunked=chunked,
      )
    except (ProtocolError, OSError) as err:
      raise ConnectionError(err, request=request)
    except MaxRetryError as e:
      if isinstance(e.reason, ConnectTimeoutError):
        # TODO: Remove this in 3.0.0: see #2811
        if not isinstance(e.reason, NewConnectionError):
          raise ConnectTimeout(e, request=request)
      if isinstance(e.reason, ResponseError):
        raise RetryError(e, request=request)
      if isinstance(e.reason, _ProxyError):
        raise ProxyError(e, request=request)
      if isinstance(e.reason, _SSLError):
        # This branch is for urllib3 v1.22 and later.
        raise SSLError(e, request=request)
      raise ConnectionError(e, request=request)
    except ClosedPoolError as e:
      raise ConnectionError(e, request=request)
    except _ProxyError as e:
      raise ProxyError(e)
    except (_SSLError, _HTTPError) as e:
      if isinstance(e, _SSLError):
        # This branch is for urllib3 versions earlier than v1.22
        raise SSLError(e, request=request)
      elif isinstance(e, ReadTimeoutError):
        raise ReadTimeout(e, request=request)
      elif isinstance(e, _InvalidHeader):
        raise InvalidHeader(e, request=request)
      else:
        raise
    return self.build_response(request, resp)




```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.api
```
"""
requests.api
~~~~~~~~~~~~
This module implements the Requests API.
:copyright: (c) 2012 by Kenneth Reitz.
:license: Apache2, see LICENSE for more details.
"""
from.import sessions



[docs]
defrequest(method, url, **kwargs):
"""Constructs and sends a :class:`Request <Request>`.
  :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
  :param url: URL for the new :class:`Request` object.
  :param params: (optional) Dictionary, list of tuples or bytes to send
    in the query string for the :class:`Request`.
  :param data: (optional) Dictionary, list of tuples, bytes, or file-like
    object to send in the body of the :class:`Request`.
  :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
  :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
  :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
  :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
    ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
    or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content_type'`` is a string
    defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
    to add for the file.
  :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
  :param timeout: (optional) How many seconds to wait for the server to send data
    before giving up, as a float, or a :ref:`(connect timeout, read
    timeout) <timeouts>` tuple.
  :type timeout: float or tuple
  :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
  :type allow_redirects: bool
  :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
  :param verify: (optional) Either a boolean, in which case it controls whether we verify
      the server's TLS certificate, or a string, in which case it must be a path
      to a CA bundle to use. Defaults to ``True``.
  :param stream: (optional) if ``False``, the response content will be immediately downloaded.
  :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  Usage::
   >>> import requests
   >>> req = requests.request('GET', 'https://httpbin.org/get')
   >>> req
   <Response [200]>
  """
  # By using the 'with' statement we are sure the session is closed, thus we
  # avoid leaving sockets open which can trigger a ResourceWarning in some
  # cases, and look like a memory leak in others.
  with sessions.Session() as session:
    return session.request(method=method, url=url, **kwargs)




[docs]
defget(url, params=None, **kwargs):
r"""Sends a GET request.
  :param url: URL for the new :class:`Request` object.
  :param params: (optional) Dictionary, list of tuples or bytes to send
    in the query string for the :class:`Request`.
  :param \*\*kwargs: Optional arguments that ``request`` takes.
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  """
  return request("get", url, params=params, **kwargs)


defoptions(url, **kwargs):
r"""Sends an OPTIONS request.
  :param url: URL for the new :class:`Request` object.
  :param \*\*kwargs: Optional arguments that ``request`` takes.
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  """
  return request("options", url, **kwargs)



[docs]
defhead(url, **kwargs):
r"""Sends a HEAD request.
  :param url: URL for the new :class:`Request` object.
  :param \*\*kwargs: Optional arguments that ``request`` takes. If
    `allow_redirects` is not provided, it will be set to `False` (as
    opposed to the default :meth:`request` behavior).
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  """
  kwargs.setdefault("allow_redirects", False)
  return request("head", url, **kwargs)




[docs]
defpost(url, data=None, json=None, **kwargs):
r"""Sends a POST request.
  :param url: URL for the new :class:`Request` object.
  :param data: (optional) Dictionary, list of tuples, bytes, or file-like
    object to send in the body of the :class:`Request`.
  :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
  :param \*\*kwargs: Optional arguments that ``request`` takes.
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  """
  return request("post", url, data=data, json=json, **kwargs)




[docs]
defput(url, data=None, **kwargs):
r"""Sends a PUT request.
  :param url: URL for the new :class:`Request` object.
  :param data: (optional) Dictionary, list of tuples, bytes, or file-like
    object to send in the body of the :class:`Request`.
  :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
  :param \*\*kwargs: Optional arguments that ``request`` takes.
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  """
  return request("put", url, data=data, **kwargs)




[docs]
defpatch(url, data=None, **kwargs):
r"""Sends a PATCH request.
  :param url: URL for the new :class:`Request` object.
  :param data: (optional) Dictionary, list of tuples, bytes, or file-like
    object to send in the body of the :class:`Request`.
  :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
  :param \*\*kwargs: Optional arguments that ``request`` takes.
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  """
  return request("patch", url, data=data, **kwargs)




[docs]
defdelete(url, **kwargs):
r"""Sends a DELETE request.
  :param url: URL for the new :class:`Request` object.
  :param \*\*kwargs: Optional arguments that ``request`` takes.
  :return: :class:`Response <Response>` object
  :rtype: requests.Response
  """
  return request("delete", url, **kwargs)


```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.auth
```
"""
requests.auth
~~~~~~~~~~~~~
This module contains the authentication handlers for Requests.
"""
importhashlib
importos
importre
importthreading
importtime
importwarnings
frombase64import b64encode
from._internal_utilsimport to_native_string
from.compatimport basestring, str, urlparse
from.cookiesimport extract_cookies_to_jar
from.utilsimport parse_dict_header
CONTENT_TYPE_FORM_URLENCODED = "application/x-www-form-urlencoded"
CONTENT_TYPE_MULTI_PART = "multipart/form-data"

def_basic_auth_str(username, password):
"""Returns a Basic Auth string."""
  # "I want us to put a big-ol' comment on top of it that
  # says that this behaviour is dumb but we need to preserve
  # it because people are relying on it."
  #  - Lukasa
  #
  # These are here solely to maintain backwards compatibility
  # for things like ints. This will be removed in 3.0.0.
  if not isinstance(username, basestring):
    warnings.warn(
      "Non-string usernames will no longer be supported in Requests "
      "3.0.0. Please convert the object you've passed in ({!r}) to "
      "a string or bytes object in the near future to avoid "
      "problems.".format(username),
      category=DeprecationWarning,
    )
    username = str(username)
  if not isinstance(password, basestring):
    warnings.warn(
      "Non-string passwords will no longer be supported in Requests "
      "3.0.0. Please convert the object you've passed in ({!r}) to "
      "a string or bytes object in the near future to avoid "
      "problems.".format(type(password)),
      category=DeprecationWarning,
    )
    password = str(password)
  # -- End Removal --
  if isinstance(username, str):
    username = username.encode("latin1")
  if isinstance(password, str):
    password = password.encode("latin1")
  authstr = "Basic " + to_native_string(
    b64encode(b":".join((username, password))).strip()
  )
  return authstr



[docs]
classAuthBase:
"""Base class that all auth implementations derive from"""
  def__call__(self, r):
    raise NotImplementedError("Auth hooks must be callable.")




[docs]
classHTTPBasicAuth(AuthBase):
"""Attaches HTTP Basic Authentication to the given Request object."""
  def__init__(self, username, password):
    self.username = username
    self.password = password
  def__eq__(self, other):
    return all(
      [
        self.username == getattr(other, "username", None),
        self.password == getattr(other, "password", None),
      ]
    )
  def__ne__(self, other):
    return not self == other
  def__call__(self, r):
    r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
    return r




[docs]
classHTTPProxyAuth(HTTPBasicAuth):
"""Attaches HTTP Proxy Authentication to a given Request object."""
  def__call__(self, r):
    r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
    return r




[docs]
classHTTPDigestAuth(AuthBase):
"""Attaches HTTP Digest Authentication to the given Request object."""
  def__init__(self, username, password):
    self.username = username
    self.password = password
    # Keep state in per-thread local storage
    self._thread_local = threading.local()
  definit_per_thread_state(self):
    # Ensure state is initialized just once per-thread
    if not hasattr(self._thread_local, "init"):
      self._thread_local.init = True
      self._thread_local.last_nonce = ""
      self._thread_local.nonce_count = 0
      self._thread_local.chal = {}
      self._thread_local.pos = None
      self._thread_local.num_401_calls = None
  defbuild_digest_header(self, method, url):
"""
    :rtype: str
    """
    realm = self._thread_local.chal["realm"]
    nonce = self._thread_local.chal["nonce"]
    qop = self._thread_local.chal.get("qop")
    algorithm = self._thread_local.chal.get("algorithm")
    opaque = self._thread_local.chal.get("opaque")
    hash_utf8 = None
    if algorithm is None:
      _algorithm = "MD5"
    else:
      _algorithm = algorithm.upper()
    # lambdas assume digest modules are imported at the top level
    if _algorithm == "MD5" or _algorithm == "MD5-SESS":
      defmd5_utf8(x):
        if isinstance(x, str):
          x = x.encode("utf-8")
        return hashlib.md5(x).hexdigest()
      hash_utf8 = md5_utf8
    elif _algorithm == "SHA":
      defsha_utf8(x):
        if isinstance(x, str):
          x = x.encode("utf-8")
        return hashlib.sha1(x).hexdigest()
      hash_utf8 = sha_utf8
    elif _algorithm == "SHA-256":
      defsha256_utf8(x):
        if isinstance(x, str):
          x = x.encode("utf-8")
        return hashlib.sha256(x).hexdigest()
      hash_utf8 = sha256_utf8
    elif _algorithm == "SHA-512":
      defsha512_utf8(x):
        if isinstance(x, str):
          x = x.encode("utf-8")
        return hashlib.sha512(x).hexdigest()
      hash_utf8 = sha512_utf8
    KD = lambda s, d: hash_utf8(f"{s}:{d}") # noqa:E731
    if hash_utf8 is None:
      return None
    # XXX not implemented yet
    entdig = None
    p_parsed = urlparse(url)
    #: path is request-uri defined in RFC 2616 which should not be empty
    path = p_parsed.path or "/"
    if p_parsed.query:
      path += f"?{p_parsed.query}"
    A1 = f"{self.username}:{realm}:{self.password}"
    A2 = f"{method}:{path}"
    HA1 = hash_utf8(A1)
    HA2 = hash_utf8(A2)
    if nonce == self._thread_local.last_nonce:
      self._thread_local.nonce_count += 1
    else:
      self._thread_local.nonce_count = 1
    ncvalue = f"{self._thread_local.nonce_count:08x}"
    s = str(self._thread_local.nonce_count).encode("utf-8")
    s += nonce.encode("utf-8")
    s += time.ctime().encode("utf-8")
    s += os.urandom(8)
    cnonce = hashlib.sha1(s).hexdigest()[:16]
    if _algorithm == "MD5-SESS":
      HA1 = hash_utf8(f"{HA1}:{nonce}:{cnonce}")
    if not qop:
      respdig = KD(HA1, f"{nonce}:{HA2}")
    elif qop == "auth" or "auth" in qop.split(","):
      noncebit = f"{nonce}:{ncvalue}:{cnonce}:auth:{HA2}"
      respdig = KD(HA1, noncebit)
    else:
      # XXX handle auth-int.
      return None
    self._thread_local.last_nonce = nonce
    # XXX should the partial digests be encoded too?
    base = (
      f'username="{self.username}", realm="{realm}", nonce="{nonce}", '
      f'uri="{path}", response="{respdig}"'
    )
    if opaque:
      base += f', opaque="{opaque}"'
    if algorithm:
      base += f', algorithm="{algorithm}"'
    if entdig:
      base += f', digest="{entdig}"'
    if qop:
      base += f', qop="auth", nc={ncvalue}, cnonce="{cnonce}"'
    return f"Digest {base}"
  defhandle_redirect(self, r, **kwargs):
"""Reset num_401_calls counter on redirects."""
    if r.is_redirect:
      self._thread_local.num_401_calls = 1
  defhandle_401(self, r, **kwargs):
"""
    Takes the given response and tries digest-auth, if needed.
    :rtype: requests.Response
    """
    # If response is not 4xx, do not auth
    # See https://github.com/psf/requests/issues/3772
    if not 400 <= r.status_code < 500:
      self._thread_local.num_401_calls = 1
      return r
    if self._thread_local.pos is not None:
      # Rewind the file position indicator of the body to where
      # it was to resend the request.
      r.request.body.seek(self._thread_local.pos)
    s_auth = r.headers.get("www-authenticate", "")
    if "digest" in s_auth.lower() and self._thread_local.num_401_calls < 2:
      self._thread_local.num_401_calls += 1
      pat = re.compile(r"digest ", flags=re.IGNORECASE)
      self._thread_local.chal = parse_dict_header(pat.sub("", s_auth, count=1))
      # Consume content and release the original connection
      # to allow our new request to reuse the same one.
      r.content
      r.close()
      prep = r.request.copy()
      extract_cookies_to_jar(prep._cookies, r.request, r.raw)
      prep.prepare_cookies(prep._cookies)
      prep.headers["Authorization"] = self.build_digest_header(
        prep.method, prep.url
      )
      _r = r.connection.send(prep, **kwargs)
      _r.history.append(r)
      _r.request = prep
      return _r
    self._thread_local.num_401_calls = 1
    return r
  def__call__(self, r):
    # Initialize per-thread state, if needed
    self.init_per_thread_state()
    # If we have a saved nonce, skip the 401
    if self._thread_local.last_nonce:
      r.headers["Authorization"] = self.build_digest_header(r.method, r.url)
    try:
      self._thread_local.pos = r.body.tell()
    except AttributeError:
      # In the case of HTTPDigestAuth being reused and the body of
      # the previous request was a file-like object, pos has the
      # file position of the previous body. Ensure it's set to
      # None.
      self._thread_local.pos = None
    r.register_hook("response", self.handle_401)
    r.register_hook("response", self.handle_redirect)
    self._thread_local.num_401_calls = 1
    return r
  def__eq__(self, other):
    return all(
      [
        self.username == getattr(other, "username", None),
        self.password == getattr(other, "password", None),
      ]
    )
  def__ne__(self, other):
    return not self == other


```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.cookies
```
"""
requests.cookies
~~~~~~~~~~~~~~~~
Compatibility code to be able to use `http.cookiejar.CookieJar` with requests.
requests.utils imports from here, so be careful with imports.
"""
importcalendar
importcopy
importtime
from._internal_utilsimport to_native_string
from.compatimport Morsel, MutableMapping, cookielib, urlparse, urlunparse
try:
  importthreading
except ImportError:
  importdummy_threadingasthreading

classMockRequest:
"""Wraps a `requests.Request` to mimic a `urllib2.Request`.
  The code in `http.cookiejar.CookieJar` expects this interface in order to correctly
  manage cookie policies, i.e., determine whether a cookie can be set, given the
  domains of the request and the cookie.
  The original request object is read-only. The client is responsible for collecting
  the new headers via `get_new_headers()` and interpreting them appropriately. You
  probably want `get_cookie_header`, defined below.
  """
  def__init__(self, request):
    self._r = request
    self._new_headers = {}
    self.type = urlparse(self._r.url).scheme
  defget_type(self):
    return self.type
  defget_host(self):
    return urlparse(self._r.url).netloc
  defget_origin_req_host(self):
    return self.get_host()
  defget_full_url(self):
    # Only return the response's URL if the user hadn't set the Host
    # header
    if not self._r.headers.get("Host"):
      return self._r.url
    # If they did set it, retrieve it and reconstruct the expected domain
    host = to_native_string(self._r.headers["Host"], encoding="utf-8")
    parsed = urlparse(self._r.url)
    # Reconstruct the URL as we expect it
    return urlunparse(
      [
        parsed.scheme,
        host,
        parsed.path,
        parsed.params,
        parsed.query,
        parsed.fragment,
      ]
    )
  defis_unverifiable(self):
    return True
  defhas_header(self, name):
    return name in self._r.headers or name in self._new_headers
  defget_header(self, name, default=None):
    return self._r.headers.get(name, self._new_headers.get(name, default))
  defadd_header(self, key, val):
"""cookiejar has no legitimate use for this method; add it back if you find one."""
    raise NotImplementedError(
      "Cookie headers should be added with add_unredirected_header()"
    )
  defadd_unredirected_header(self, name, value):
    self._new_headers[name] = value
  defget_new_headers(self):
    return self._new_headers
  @property
  defunverifiable(self):
    return self.is_unverifiable()
  @property
  deforigin_req_host(self):
    return self.get_origin_req_host()
  @property
  defhost(self):
    return self.get_host()

classMockResponse:
"""Wraps a `httplib.HTTPMessage` to mimic a `urllib.addinfourl`.
  ...what? Basically, expose the parsed HTTP headers from the server response
  the way `http.cookiejar` expects to see them.
  """
  def__init__(self, headers):
"""Make a MockResponse for `cookiejar` to read.
    :param headers: a httplib.HTTPMessage or analogous carrying the headers
    """
    self._headers = headers
  definfo(self):
    return self._headers
  defgetheaders(self, name):
    self._headers.getheaders(name)

defextract_cookies_to_jar(jar, request, response):
"""Extract the cookies from the response into a CookieJar.
  :param jar: http.cookiejar.CookieJar (not necessarily a RequestsCookieJar)
  :param request: our own requests.Request object
  :param response: urllib3.HTTPResponse object
  """
  if not (hasattr(response, "_original_response") and response._original_response):
    return
  # the _original_response field is the wrapped httplib.HTTPResponse object,
  req = MockRequest(request)
  # pull out the HTTPMessage with the headers and put it in the mock:
  res = MockResponse(response._original_response.msg)
  jar.extract_cookies(res, req)

defget_cookie_header(jar, request):
"""
  Produce an appropriate Cookie header string to be sent with `request`, or None.
  :rtype: str
  """
  r = MockRequest(request)
  jar.add_cookie_header(r)
  return r.get_new_headers().get("Cookie")

defremove_cookie_by_name(cookiejar, name, domain=None, path=None):
"""Unsets a cookie by name, by default over all domains and paths.
  Wraps CookieJar.clear(), is O(n).
  """
  clearables = []
  for cookie in cookiejar:
    if cookie.name != name:
      continue
    if domain is not None and domain != cookie.domain:
      continue
    if path is not None and path != cookie.path:
      continue
    clearables.append((cookie.domain, cookie.path, cookie.name))
  for domain, path, name in clearables:
    cookiejar.clear(domain, path, name)



[docs]
classCookieConflictError(RuntimeError):
"""There are two cookies that meet the criteria specified in the cookie jar.
  Use .get and .set and include domain and path args in order to be more specific.
  """




[docs]
classRequestsCookieJar(cookielib.CookieJar, MutableMapping):
"""Compatibility class; is a http.cookiejar.CookieJar, but exposes a dict
  interface.
  This is the CookieJar we create by default for requests and sessions that
  don't specify one, since some clients may expect response.cookies and
  session.cookies to support dict operations.
  Requests does not use the dict interface internally; it's just for
  compatibility with external client code. All requests code should work
  out of the box with externally provided instances of ``CookieJar``, e.g.
  ``LWPCookieJar`` and ``FileCookieJar``.
  Unlike a regular CookieJar, this class is pickleable.
  .. warning:: dictionary operations that are normally O(1) may be O(n).
  """


[docs]
  defget(self, name, default=None, domain=None, path=None):
"""Dict-like get() that also supports optional domain and path args in
    order to resolve naming collisions from using one cookie jar over
    multiple domains.
    .. warning:: operation is O(n), not O(1).
    """
    try:
      return self._find_no_duplicates(name, domain, path)
    except KeyError:
      return default




[docs]
  defset(self, name, value, **kwargs):
"""Dict-like set() that also supports optional domain and path args in
    order to resolve naming collisions from using one cookie jar over
    multiple domains.
    """
    # support client code that unsets cookies by assignment of a None value:
    if value is None:
      remove_cookie_by_name(
        self, name, domain=kwargs.get("domain"), path=kwargs.get("path")
      )
      return
    if isinstance(value, Morsel):
      c = morsel_to_cookie(value)
    else:
      c = create_cookie(name, value, **kwargs)
    self.set_cookie(c)
    return c




[docs]
  defiterkeys(self):
"""Dict-like iterkeys() that returns an iterator of names of cookies
    from the jar.
    .. seealso:: itervalues() and iteritems().
    """
    for cookie in iter(self):
      yield cookie.name




[docs]
  defkeys(self):
"""Dict-like keys() that returns a list of names of cookies from the
    jar.
    .. seealso:: values() and items().
    """
    return list(self.iterkeys())




[docs]
  defitervalues(self):
"""Dict-like itervalues() that returns an iterator of values of cookies
    from the jar.
    .. seealso:: iterkeys() and iteritems().
    """
    for cookie in iter(self):
      yield cookie.value




[docs]
  defvalues(self):
"""Dict-like values() that returns a list of values of cookies from the
    jar.
    .. seealso:: keys() and items().
    """
    return list(self.itervalues())




[docs]
  defiteritems(self):
"""Dict-like iteritems() that returns an iterator of name-value tuples
    from the jar.
    .. seealso:: iterkeys() and itervalues().
    """
    for cookie in iter(self):
      yield cookie.name, cookie.value




[docs]
  defitems(self):
"""Dict-like items() that returns a list of name-value tuples from the
    jar. Allows client-code to call ``dict(RequestsCookieJar)`` and get a
    vanilla python dict of key value pairs.
    .. seealso:: keys() and values().
    """
    return list(self.iteritems())




[docs]
  deflist_domains(self):
"""Utility method to list all the domains in the jar."""
    domains = []
    for cookie in iter(self):
      if cookie.domain not in domains:
        domains.append(cookie.domain)
    return domains




[docs]
  deflist_paths(self):
"""Utility method to list all the paths in the jar."""
    paths = []
    for cookie in iter(self):
      if cookie.path not in paths:
        paths.append(cookie.path)
    return paths




[docs]
  defmultiple_domains(self):
"""Returns True if there are multiple domains in the jar.
    Returns False otherwise.
    :rtype: bool
    """
    domains = []
    for cookie in iter(self):
      if cookie.domain is not None and cookie.domain in domains:
        return True
      domains.append(cookie.domain)
    return False # there is only one domain in jar




[docs]
  defget_dict(self, domain=None, path=None):
"""Takes as an argument an optional domain and path and returns a plain
    old Python dict of name-value pairs of cookies that meet the
    requirements.
    :rtype: dict
    """
    dictionary = {}
    for cookie in iter(self):
      if (domain is None or cookie.domain == domain) and (
        path is None or cookie.path == path
      ):
        dictionary[cookie.name] = cookie.value
    return dictionary


  def__contains__(self, name):
    try:
      return super().__contains__(name)
    except CookieConflictError:
      return True
  def__getitem__(self, name):
"""Dict-like __getitem__() for compatibility with client code. Throws
    exception if there are more than one cookie with name. In that case,
    use the more explicit get() method instead.
    .. warning:: operation is O(n), not O(1).
    """
    return self._find_no_duplicates(name)
  def__setitem__(self, name, value):
"""Dict-like __setitem__ for compatibility with client code. Throws
    exception if there is already a cookie of that name in the jar. In that
    case, use the more explicit set() method instead.
    """
    self.set(name, value)
  def__delitem__(self, name):
"""Deletes a cookie given a name. Wraps ``http.cookiejar.CookieJar``'s
    ``remove_cookie_by_name()``.
    """
    remove_cookie_by_name(self, name)


[docs]
  defset_cookie(self, cookie, *args, **kwargs):
    if (
      hasattr(cookie.value, "startswith")
      and cookie.value.startswith('"')
      and cookie.value.endswith('"')
    ):
      cookie.value = cookie.value.replace('\\"', "")
    return super().set_cookie(cookie, *args, **kwargs)




[docs]
  defupdate(self, other):
"""Updates this jar with cookies from another CookieJar or dict-like"""
    if isinstance(other, cookielib.CookieJar):
      for cookie in other:
        self.set_cookie(copy.copy(cookie))
    else:
      super().update(other)


  def_find(self, name, domain=None, path=None):
"""Requests uses this method internally to get cookie values.
    If there are conflicting cookies, _find arbitrarily chooses one.
    See _find_no_duplicates if you want an exception thrown if there are
    conflicting cookies.
    :param name: a string containing name of cookie
    :param domain: (optional) string containing domain of cookie
    :param path: (optional) string containing path of cookie
    :return: cookie.value
    """
    for cookie in iter(self):
      if cookie.name == name:
        if domain is None or cookie.domain == domain:
          if path is None or cookie.path == path:
            return cookie.value
    raise KeyError(f"name={name!r}, domain={domain!r}, path={path!r}")
  def_find_no_duplicates(self, name, domain=None, path=None):
"""Both ``__get_item__`` and ``get`` call this function: it's never
    used elsewhere in Requests.
    :param name: a string containing name of cookie
    :param domain: (optional) string containing domain of cookie
    :param path: (optional) string containing path of cookie
    :raises KeyError: if cookie is not found
    :raises CookieConflictError: if there are multiple cookies
      that match name and optionally domain and path
    :return: cookie.value
    """
    toReturn = None
    for cookie in iter(self):
      if cookie.name == name:
        if domain is None or cookie.domain == domain:
          if path is None or cookie.path == path:
            if toReturn is not None:
              # if there are multiple cookies that meet passed in criteria
              raise CookieConflictError(
                f"There are multiple cookies with name, {name!r}"
              )
            # we will eventually return this as long as no cookie conflict
            toReturn = cookie.value
    if toReturn:
      return toReturn
    raise KeyError(f"name={name!r}, domain={domain!r}, path={path!r}")
  def__getstate__(self):
"""Unlike a normal CookieJar, this class is pickleable."""
    state = self.__dict__.copy()
    # remove the unpickleable RLock object
    state.pop("_cookies_lock")
    return state
  def__setstate__(self, state):
"""Unlike a normal CookieJar, this class is pickleable."""
    self.__dict__.update(state)
    if "_cookies_lock" not in self.__dict__:
      self._cookies_lock = threading.RLock()


[docs]
  defcopy(self):
"""Return a copy of this RequestsCookieJar."""
    new_cj = RequestsCookieJar()
    new_cj.set_policy(self.get_policy())
    new_cj.update(self)
    return new_cj




[docs]
  defget_policy(self):
"""Return the CookiePolicy instance used."""
    return self._policy




def_copy_cookie_jar(jar):
  if jar is None:
    return None
  if hasattr(jar, "copy"):
    # We're dealing with an instance of RequestsCookieJar
    return jar.copy()
  # We're dealing with a generic CookieJar instance
  new_jar = copy.copy(jar)
  new_jar.clear()
  for cookie in jar:
    new_jar.set_cookie(copy.copy(cookie))
  return new_jar

defcreate_cookie(name, value, **kwargs):
"""Make a cookie from underspecified parameters.
  By default, the pair of `name` and `value` will be set for the domain ''
  and sent on every request (this is sometimes called a "supercookie").
  """
  result = {
    "version": 0,
    "name": name,
    "value": value,
    "port": None,
    "domain": "",
    "path": "/",
    "secure": False,
    "expires": None,
    "discard": True,
    "comment": None,
    "comment_url": None,
    "rest": {"HttpOnly": None},
    "rfc2109": False,
  }
  badargs = set(kwargs) - set(result)
  if badargs:
    raise TypeError(
      f"create_cookie() got unexpected keyword arguments: {list(badargs)}"
    )
  result.update(kwargs)
  result["port_specified"] = bool(result["port"])
  result["domain_specified"] = bool(result["domain"])
  result["domain_initial_dot"] = result["domain"].startswith(".")
  result["path_specified"] = bool(result["path"])
  return cookielib.Cookie(**result)

defmorsel_to_cookie(morsel):
"""Convert a Morsel object into a Cookie containing the one k/v pair."""
  expires = None
  if morsel["max-age"]:
    try:
      expires = int(time.time() + int(morsel["max-age"]))
    except ValueError:
      raise TypeError(f"max-age: {morsel['max-age']} must be integer")
  elif morsel["expires"]:
    time_template = "%a, %d-%b-%Y %H:%M:%S GMT"
    expires = calendar.timegm(time.strptime(morsel["expires"], time_template))
  return create_cookie(
    comment=morsel["comment"],
    comment_url=bool(morsel["comment"]),
    discard=False,
    domain=morsel["domain"],
    expires=expires,
    name=morsel.key,
    path=morsel["path"],
    port=None,
    rest={"HttpOnly": morsel["httponly"]},
    rfc2109=False,
    secure=bool(morsel["secure"]),
    value=morsel.value,
    version=morsel["version"] or 0,
  )



[docs]
defcookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):
"""Returns a CookieJar from a key/value dictionary.
  :param cookie_dict: Dict of key/values to insert into CookieJar.
  :param cookiejar: (optional) A cookiejar to add the cookies to.
  :param overwrite: (optional) If False, will not replace cookies
    already in the jar with new ones.
  :rtype: CookieJar
  """
  if cookiejar is None:
    cookiejar = RequestsCookieJar()
  if cookie_dict is not None:
    names_from_jar = [cookie.name for cookie in cookiejar]
    for name in cookie_dict:
      if overwrite or (name not in names_from_jar):
        cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))
  return cookiejar


defmerge_cookies(cookiejar, cookies):
"""Add cookies to cookiejar and returns a merged CookieJar.
  :param cookiejar: CookieJar object to add the cookies to.
  :param cookies: Dictionary or CookieJar object to be added.
  :rtype: CookieJar
  """
  if not isinstance(cookiejar, cookielib.CookieJar):
    raise ValueError("You can only merge into CookieJar")
  if isinstance(cookies, dict):
    cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)
  elif isinstance(cookies, cookielib.CookieJar):
    try:
      cookiejar.update(cookies)
    except AttributeError:
      for cookie_in_jar in cookies:
        cookiejar.set_cookie(cookie_in_jar)
  return cookiejar

```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.exceptions
```
"""
requests.exceptions
~~~~~~~~~~~~~~~~~~~
This module contains the set of Requests' exceptions.
"""
fromurllib3.exceptionsimport HTTPError as BaseHTTPError
from.compatimport JSONDecodeError as CompatJSONDecodeError



[docs]
classRequestException(IOError):
"""There was an ambiguous exception that occurred while handling your
  request.
  """
  def__init__(self, *args, **kwargs):
"""Initialize RequestException with `request` and `response` objects."""
    response = kwargs.pop("response", None)
    self.response = response
    self.request = kwargs.pop("request", None)
    if response is not None and not self.request and hasattr(response, "request"):
      self.request = self.response.request
    super().__init__(*args, **kwargs)


classInvalidJSONError(RequestException):
"""A JSON error occurred."""



[docs]
classJSONDecodeError(InvalidJSONError, CompatJSONDecodeError):
"""Couldn't decode the text into json"""
  def__init__(self, *args, **kwargs):
"""
    Construct the JSONDecodeError instance first with all
    args. Then use it's args to construct the IOError so that
    the json specific args aren't used as IOError specific args
    and the error message from JSONDecodeError is preserved.
    """
    CompatJSONDecodeError.__init__(self, *args)
    InvalidJSONError.__init__(self, *self.args, **kwargs)
  def__reduce__(self):
"""
    The __reduce__ method called when pickling the object must
    be the one from the JSONDecodeError (be it json/simplejson)
    as it expects all the arguments for instantiation, not just
    one like the IOError, and the MRO would by default call the
    __reduce__ method from the IOError due to the inheritance order.
    """
    return CompatJSONDecodeError.__reduce__(self)




[docs]
classHTTPError(RequestException):
"""An HTTP error occurred."""




[docs]
classConnectionError(RequestException):
"""A Connection error occurred."""


classProxyError(ConnectionError):
"""A proxy error occurred."""

classSSLError(ConnectionError):
"""An SSL error occurred."""



[docs]
classTimeout(RequestException):
"""The request timed out.
  Catching this error will catch both
  :exc:`~requests.exceptions.ConnectTimeout` and
  :exc:`~requests.exceptions.ReadTimeout` errors.
  """




[docs]
classConnectTimeout(ConnectionError, Timeout):
"""The request timed out while trying to connect to the remote server.
  Requests that produced this error are safe to retry.
  """




[docs]
classReadTimeout(Timeout):
"""The server did not send any data in the allotted amount of time."""


classURLRequired(RequestException):
"""A valid URL is required to make a request."""



[docs]
classTooManyRedirects(RequestException):
"""Too many redirects."""


classMissingSchema(RequestException, ValueError):
"""The URL scheme (e.g. http or https) is missing."""

classInvalidSchema(RequestException, ValueError):
"""The URL scheme provided is either invalid or unsupported."""

classInvalidURL(RequestException, ValueError):
"""The URL provided was somehow invalid."""

classInvalidHeader(RequestException, ValueError):
"""The header value provided was somehow invalid."""

classInvalidProxyURL(InvalidURL):
"""The proxy URL provided is invalid."""

classChunkedEncodingError(RequestException):
"""The server declared chunked encoding but sent an invalid chunk."""

classContentDecodingError(RequestException, BaseHTTPError):
"""Failed to decode response content."""

classStreamConsumedError(RequestException, TypeError):
"""The content for this response was already consumed."""

classRetryError(RequestException):
"""Custom retries logic failed"""

classUnrewindableBodyError(RequestException):
"""Requests encountered an error when trying to rewind a body."""

# Warnings

classRequestsWarning(Warning):
"""Base warning for Requests."""

classFileModeWarning(RequestsWarning, DeprecationWarning):
"""A file was opened in text mode, but Requests determined its binary length."""

classRequestsDependencyWarning(RequestsWarning):
"""An imported dependency doesn't match the expected version range."""

```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.models
```
"""
requests.models
~~~~~~~~~~~~~~~
This module contains the primary objects that power Requests.
"""
importdatetime
# Import encoding now, to avoid implicit import later.
# Implicit import within threads may cause LookupError when standard library is in a ZIP,
# such as in Embedded Python. See https://github.com/psf/requests/issues/3578.
importencodings.idna # noqa: F401
fromioimport UnsupportedOperation
fromurllib3.exceptionsimport (
  DecodeError,
  LocationParseError,
  ProtocolError,
  ReadTimeoutError,
  SSLError,
)
fromurllib3.fieldsimport RequestField
fromurllib3.filepostimport encode_multipart_formdata
fromurllib3.utilimport parse_url
from._internal_utilsimport to_native_string, unicode_is_ascii
from.authimport HTTPBasicAuth
from.compatimport (
  Callable,
  JSONDecodeError,
  Mapping,
  basestring,
  builtin_str,
  chardet,
  cookielib,
)
from.compatimport json as complexjson
from.compatimport urlencode, urlsplit, urlunparse
from.cookiesimport _copy_cookie_jar, cookiejar_from_dict, get_cookie_header
from.exceptionsimport (
  ChunkedEncodingError,
  ConnectionError,
  ContentDecodingError,
  HTTPError,
  InvalidJSONError,
  InvalidURL,
)
from.exceptionsimport JSONDecodeError as RequestsJSONDecodeError
from.exceptionsimport MissingSchema
from.exceptionsimport SSLError as RequestsSSLError
from.exceptionsimport StreamConsumedError
from.hooksimport default_hooks
from.status_codesimport codes
from.structuresimport CaseInsensitiveDict
from.utilsimport (
  check_header_validity,
  get_auth_from_url,
  guess_filename,
  guess_json_utf,
  iter_slices,
  parse_header_links,
  requote_uri,
  stream_decode_response_unicode,
  super_len,
  to_key_val_list,
)
#: The set of HTTP status codes that indicate an automatically
#: processable redirect.
REDIRECT_STATI = (
  codes.moved, # 301
  codes.found, # 302
  codes.other, # 303
  codes.temporary_redirect, # 307
  codes.permanent_redirect, # 308
)
DEFAULT_REDIRECT_LIMIT = 30
CONTENT_CHUNK_SIZE = 10 * 1024
ITER_CHUNK_SIZE = 512

classRequestEncodingMixin:
  @property
  defpath_url(self):
"""Build the path URL to use."""
    url = []
    p = urlsplit(self.url)
    path = p.path
    if not path:
      path = "/"
    url.append(path)
    query = p.query
    if query:
      url.append("?")
      url.append(query)
    return "".join(url)
  @staticmethod
  def_encode_params(data):
"""Encode parameters in a piece of data.
    Will successfully encode parameters when passed as a dict or a list of
    2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
    if parameters are supplied as a dict.
    """
    if isinstance(data, (str, bytes)):
      return data
    elif hasattr(data, "read"):
      return data
    elif hasattr(data, "__iter__"):
      result = []
      for k, vs in to_key_val_list(data):
        if isinstance(vs, basestring) or not hasattr(vs, "__iter__"):
          vs = [vs]
        for v in vs:
          if v is not None:
            result.append(
              (
                k.encode("utf-8") if isinstance(k, str) else k,
                v.encode("utf-8") if isinstance(v, str) else v,
              )
            )
      return urlencode(result, doseq=True)
    else:
      return data
  @staticmethod
  def_encode_files(files, data):
"""Build the body for a multipart/form-data request.
    Will successfully encode files when passed as a dict or a list of
    tuples. Order is retained if data is a list of tuples but arbitrary
    if parameters are supplied as a dict.
    The tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)
    or 4-tuples (filename, fileobj, contentype, custom_headers).
    """
    if not files:
      raise ValueError("Files must be provided.")
    elif isinstance(data, basestring):
      raise ValueError("Data must not be a string.")
    new_fields = []
    fields = to_key_val_list(data or {})
    files = to_key_val_list(files or {})
    for field, val in fields:
      if isinstance(val, basestring) or not hasattr(val, "__iter__"):
        val = [val]
      for v in val:
        if v is not None:
          # Don't call str() on bytestrings: in Py3 it all goes wrong.
          if not isinstance(v, bytes):
            v = str(v)
          new_fields.append(
            (
              field.decode("utf-8")
              if isinstance(field, bytes)
              else field,
              v.encode("utf-8") if isinstance(v, str) else v,
            )
          )
    for k, v in files:
      # support for explicit filename
      ft = None
      fh = None
      if isinstance(v, (tuple, list)):
        if len(v) == 2:
          fn, fp = v
        elif len(v) == 3:
          fn, fp, ft = v
        else:
          fn, fp, ft, fh = v
      else:
        fn = guess_filename(v) or k
        fp = v
      if isinstance(fp, (str, bytes, bytearray)):
        fdata = fp
      elif hasattr(fp, "read"):
        fdata = fp.read()
      elif fp is None:
        continue
      else:
        fdata = fp
      rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)
      rf.make_multipart(content_type=ft)
      new_fields.append(rf)
    body, content_type = encode_multipart_formdata(new_fields)
    return body, content_type

classRequestHooksMixin:
  defregister_hook(self, event, hook):
"""Properly register a hook."""
    if event not in self.hooks:
      raise ValueError(f'Unsupported event specified, with event name "{event}"')
    if isinstance(hook, Callable):
      self.hooks[event].append(hook)
    elif hasattr(hook, "__iter__"):
      self.hooks[event].extend(h for h in hook if isinstance(h, Callable))
  defderegister_hook(self, event, hook):
"""Deregister a previously registered hook.
    Returns True if the hook existed, False if not.
    """
    try:
      self.hooks[event].remove(hook)
      return True
    except ValueError:
      return False



[docs]
classRequest(RequestHooksMixin):
"""A user-created :class:`Request <Request>` object.
  Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.
  :param method: HTTP method to use.
  :param url: URL to send.
  :param headers: dictionary of headers to send.
  :param files: dictionary of {filename: fileobject} files to multipart upload.
  :param data: the body to attach to the request. If a dictionary or
    list of tuples ``[(key, value)]`` is provided, form-encoding will
    take place.
  :param json: json for the body to attach to the request (if files or data is not specified).
  :param params: URL parameters to append to the URL. If a dictionary or
    list of tuples ``[(key, value)]`` is provided, form-encoding will
    take place.
  :param auth: Auth handler or (user, pass) tuple.
  :param cookies: dictionary or CookieJar of cookies to attach to this request.
  :param hooks: dictionary of callback hooks, for internal usage.
  Usage::
   >>> import requests
   >>> req = requests.Request('GET', 'https://httpbin.org/get')
   >>> req.prepare()
   <PreparedRequest [GET]>
  """
  def__init__(
    self,
    method=None,
    url=None,
    headers=None,
    files=None,
    data=None,
    params=None,
    auth=None,
    cookies=None,
    hooks=None,
    json=None,
  ):
    # Default empty dicts for dict params.
    data = [] if data is None else data
    files = [] if files is None else files
    headers = {} if headers is None else headers
    params = {} if params is None else params
    hooks = {} if hooks is None else hooks
    self.hooks = default_hooks()
    for k, v in list(hooks.items()):
      self.register_hook(event=k, hook=v)
    self.method = method
    self.url = url
    self.headers = headers
    self.files = files
    self.data = data
    self.json = json
    self.params = params
    self.auth = auth
    self.cookies = cookies
  def__repr__(self):
    return f"<Request [{self.method}]>"


[docs]
  defprepare(self):
"""Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it."""
    p = PreparedRequest()
    p.prepare(
      method=self.method,
      url=self.url,
      headers=self.headers,
      files=self.files,
      data=self.data,
      json=self.json,
      params=self.params,
      auth=self.auth,
      cookies=self.cookies,
      hooks=self.hooks,
    )
    return p






[docs]
classPreparedRequest(RequestEncodingMixin, RequestHooksMixin):
"""The fully mutable :class:`PreparedRequest <PreparedRequest>` object,
  containing the exact bytes that will be sent to the server.
  Instances are generated from a :class:`Request <Request>` object, and
  should not be instantiated manually; doing so may produce undesirable
  effects.
  Usage::
   >>> import requests
   >>> req = requests.Request('GET', 'https://httpbin.org/get')
   >>> r = req.prepare()
   >>> r
   <PreparedRequest [GET]>
   >>> s = requests.Session()
   >>> s.send(r)
   <Response [200]>
  """
  def__init__(self):
    #: HTTP verb to send to the server.
    self.method = None
    #: HTTP URL to send the request to.
    self.url = None
    #: dictionary of HTTP headers.
    self.headers = None
    # The `CookieJar` used to create the Cookie header will be stored here
    # after prepare_cookies is called
    self._cookies = None
    #: request body to send to the server.
    self.body = None
    #: dictionary of callback hooks, for internal usage.
    self.hooks = default_hooks()
    #: integer denoting starting position of a readable file-like body.
    self._body_position = None


[docs]
  defprepare(
    self,
    method=None,
    url=None,
    headers=None,
    files=None,
    data=None,
    params=None,
    auth=None,
    cookies=None,
    hooks=None,
    json=None,
  ):
"""Prepares the entire request with the given parameters."""
    self.prepare_method(method)
    self.prepare_url(url, params)
    self.prepare_headers(headers)
    self.prepare_cookies(cookies)
    self.prepare_body(data, files, json)
    self.prepare_auth(auth, url)
    # Note that prepare_auth must be last to enable authentication schemes
    # such as OAuth to work on a fully prepared request.
    # This MUST go after prepare_auth. Authenticators could add a hook
    self.prepare_hooks(hooks)


  def__repr__(self):
    return f"<PreparedRequest [{self.method}]>"
  defcopy(self):
    p = PreparedRequest()
    p.method = self.method
    p.url = self.url
    p.headers = self.headers.copy() if self.headers is not None else None
    p._cookies = _copy_cookie_jar(self._cookies)
    p.body = self.body
    p.hooks = self.hooks
    p._body_position = self._body_position
    return p


[docs]
  defprepare_method(self, method):
"""Prepares the given HTTP method."""
    self.method = method
    if self.method is not None:
      self.method = to_native_string(self.method.upper())


  @staticmethod
  def_get_idna_encoded_host(host):
    importidna
    try:
      host = idna.encode(host, uts46=True).decode("utf-8")
    except idna.IDNAError:
      raise UnicodeError
    return host


[docs]
  defprepare_url(self, url, params):
"""Prepares the given HTTP URL."""
    #: Accept objects that have string representations.
    #: We're unable to blindly call unicode/str functions
    #: as this will include the bytestring indicator (b'')
    #: on python 3.x.
    #: https://github.com/psf/requests/pull/2238
    if isinstance(url, bytes):
      url = url.decode("utf8")
    else:
      url = str(url)
    # Remove leading whitespaces from url
    url = url.lstrip()
    # Don't do any URL preparation for non-HTTP schemes like `mailto`,
    # `data` etc to work around exceptions from `url_parse`, which
    # handles RFC 3986 only.
    if ":" in url and not url.lower().startswith("http"):
      self.url = url
      return
    # Support for unicode domain names and paths.
    try:
      scheme, auth, host, port, path, query, fragment = parse_url(url)
    except LocationParseError as e:
      raise InvalidURL(*e.args)
    if not scheme:
      raise MissingSchema(
        f"Invalid URL {url!r}: No scheme supplied. "
        f"Perhaps you meant https://{url}?"
      )
    if not host:
      raise InvalidURL(f"Invalid URL {url!r}: No host supplied")
    # In general, we want to try IDNA encoding the hostname if the string contains
    # non-ASCII characters. This allows users to automatically get the correct IDNA
    # behaviour. For strings containing only ASCII characters, we need to also verify
    # it doesn't start with a wildcard (*), before allowing the unencoded hostname.
    if not unicode_is_ascii(host):
      try:
        host = self._get_idna_encoded_host(host)
      except UnicodeError:
        raise InvalidURL("URL has an invalid label.")
    elif host.startswith(("*", ".")):
      raise InvalidURL("URL has an invalid label.")
    # Carefully reconstruct the network location
    netloc = auth or ""
    if netloc:
      netloc += "@"
    netloc += host
    if port:
      netloc += f":{port}"
    # Bare domains aren't valid URLs.
    if not path:
      path = "/"
    if isinstance(params, (str, bytes)):
      params = to_native_string(params)
    enc_params = self._encode_params(params)
    if enc_params:
      if query:
        query = f"{query}&{enc_params}"
      else:
        query = enc_params
    url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))
    self.url = url




[docs]
  defprepare_headers(self, headers):
"""Prepares the given HTTP headers."""
    self.headers = CaseInsensitiveDict()
    if headers:
      for header in headers.items():
        # Raise exception on invalid header value.
        check_header_validity(header)
        name, value = header
        self.headers[to_native_string(name)] = value




[docs]
  defprepare_body(self, data, files, json=None):
"""Prepares the given HTTP body data."""
    # Check if file, fo, generator, iterator.
    # If not, run through normal process.
    # Nottin' on you.
    body = None
    content_type = None
    if not data and json is not None:
      # urllib3 requires a bytes-like body. Python 2's json.dumps
      # provides this natively, but Python 3 gives a Unicode string.
      content_type = "application/json"
      try:
        body = complexjson.dumps(json, allow_nan=False)
      except ValueError as ve:
        raise InvalidJSONError(ve, request=self)
      if not isinstance(body, bytes):
        body = body.encode("utf-8")
    is_stream = all(
      [
        hasattr(data, "__iter__"),
        not isinstance(data, (basestring, list, tuple, Mapping)),
      ]
    )
    if is_stream:
      try:
        length = super_len(data)
      except (TypeError, AttributeError, UnsupportedOperation):
        length = None
      body = data
      if getattr(body, "tell", None) is not None:
        # Record the current file position before reading.
        # This will allow us to rewind a file in the event
        # of a redirect.
        try:
          self._body_position = body.tell()
        except OSError:
          # This differentiates from None, allowing us to catch
          # a failed `tell()` later when trying to rewind the body
          self._body_position = object()
      if files:
        raise NotImplementedError(
          "Streamed bodies and files are mutually exclusive."
        )
      if length:
        self.headers["Content-Length"] = builtin_str(length)
      else:
        self.headers["Transfer-Encoding"] = "chunked"
    else:
      # Multi-part file uploads.
      if files:
        (body, content_type) = self._encode_files(files, data)
      else:
        if data:
          body = self._encode_params(data)
          if isinstance(data, basestring) or hasattr(data, "read"):
            content_type = None
          else:
            content_type = "application/x-www-form-urlencoded"
      self.prepare_content_length(body)
      # Add content-type if it wasn't explicitly provided.
      if content_type and ("content-type" not in self.headers):
        self.headers["Content-Type"] = content_type
    self.body = body




[docs]
  defprepare_content_length(self, body):
"""Prepare Content-Length header based on request method and body"""
    if body is not None:
      length = super_len(body)
      if length:
        # If length exists, set it. Otherwise, we fallback
        # to Transfer-Encoding: chunked.
        self.headers["Content-Length"] = builtin_str(length)
    elif (
      self.method not in ("GET", "HEAD")
      and self.headers.get("Content-Length") is None
    ):
      # Set Content-Length to 0 for methods that can have a body
      # but don't provide one. (i.e. not GET or HEAD)
      self.headers["Content-Length"] = "0"




[docs]
  defprepare_auth(self, auth, url=""):
"""Prepares the given HTTP auth data."""
    # If no Auth is explicitly provided, extract it from the URL first.
    if auth is None:
      url_auth = get_auth_from_url(self.url)
      auth = url_auth if any(url_auth) else None
    if auth:
      if isinstance(auth, tuple) and len(auth) == 2:
        # special-case basic HTTP auth
        auth = HTTPBasicAuth(*auth)
      # Allow auth to make its changes.
      r = auth(self)
      # Update self to reflect the auth changes.
      self.__dict__.update(r.__dict__)
      # Recompute Content-Length
      self.prepare_content_length(self.body)




[docs]
  defprepare_cookies(self, cookies):
"""Prepares the given HTTP cookie data.
    This function eventually generates a ``Cookie`` header from the
    given cookies using cookielib. Due to cookielib's design, the header
    will not be regenerated if it already exists, meaning this function
    can only be called once for the life of the
    :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls
    to ``prepare_cookies`` will have no actual effect, unless the "Cookie"
    header is removed beforehand.
    """
    if isinstance(cookies, cookielib.CookieJar):
      self._cookies = cookies
    else:
      self._cookies = cookiejar_from_dict(cookies)
    cookie_header = get_cookie_header(self._cookies, self)
    if cookie_header is not None:
      self.headers["Cookie"] = cookie_header




[docs]
  defprepare_hooks(self, hooks):
"""Prepares the given hooks."""
    # hooks can be passed as None to the prepare method and to this
    # method. To prevent iterating over None, simply use an empty list
    # if hooks is False-y
    hooks = hooks or []
    for event in hooks:
      self.register_hook(event, hooks[event])






[docs]
classResponse:
"""The :class:`Response <Response>` object, which contains a
  server's response to an HTTP request.
  """
  __attrs__ = [
    "_content",
    "status_code",
    "headers",
    "url",
    "history",
    "encoding",
    "reason",
    "cookies",
    "elapsed",
    "request",
  ]
  def__init__(self):
    self._content = False
    self._content_consumed = False
    self._next = None
    #: Integer Code of responded HTTP Status, e.g. 404 or 200.
    self.status_code = None
    #: Case-insensitive Dictionary of Response Headers.
    #: For example, ``headers['content-encoding']`` will return the
    #: value of a ``'Content-Encoding'`` response header.
    self.headers = CaseInsensitiveDict()
    #: File-like object representation of response (for advanced usage).
    #: Use of ``raw`` requires that ``stream=True`` be set on the request.
    #: This requirement does not apply for use internally to Requests.
    self.raw = None
    #: Final URL location of Response.
    self.url = None
    #: Encoding to decode with when accessing r.text.
    self.encoding = None
    #: A list of :class:`Response <Response>` objects from
    #: the history of the Request. Any redirect responses will end
    #: up here. The list is sorted from the oldest to the most recent request.
    self.history = []
    #: Textual reason of responded HTTP Status, e.g. "Not Found" or "OK".
    self.reason = None
    #: A CookieJar of Cookies the server sent back.
    self.cookies = cookiejar_from_dict({})
    #: The amount of time elapsed between sending the request
    #: and the arrival of the response (as a timedelta).
    #: This property specifically measures the time taken between sending
    #: the first byte of the request and finishing parsing the headers. It
    #: is therefore unaffected by consuming the response content or the
    #: value of the ``stream`` keyword argument.
    self.elapsed = datetime.timedelta(0)
    #: The :class:`PreparedRequest <PreparedRequest>` object to which this
    #: is a response.
    self.request = None
  def__enter__(self):
    return self
  def__exit__(self, *args):
    self.close()
  def__getstate__(self):
    # Consume everything; accessing the content attribute makes
    # sure the content has been fully read.
    if not self._content_consumed:
      self.content
    return {attr: getattr(self, attr, None) for attr in self.__attrs__}
  def__setstate__(self, state):
    for name, value in state.items():
      setattr(self, name, value)
    # pickled objects do not have .raw
    setattr(self, "_content_consumed", True)
    setattr(self, "raw", None)
  def__repr__(self):
    return f"<Response [{self.status_code}]>"
  def__bool__(self):
"""Returns True if :attr:`status_code` is less than 400.
    This attribute checks if the status code of the response is between
    400 and 600 to see if there was a client error or a server error. If
    the status code, is between 200 and 400, this will return True. This
    is **not** a check to see if the response code is ``200 OK``.
    """
    return self.ok
  def__nonzero__(self):
"""Returns True if :attr:`status_code` is less than 400.
    This attribute checks if the status code of the response is between
    400 and 600 to see if there was a client error or a server error. If
    the status code, is between 200 and 400, this will return True. This
    is **not** a check to see if the response code is ``200 OK``.
    """
    return self.ok
  def__iter__(self):
"""Allows you to use a response as an iterator."""
    return self.iter_content(128)
  @property
  defok(self):
"""Returns True if :attr:`status_code` is less than 400, False if not.
    This attribute checks if the status code of the response is between
    400 and 600 to see if there was a client error or a server error. If
    the status code is between 200 and 400, this will return True. This
    is **not** a check to see if the response code is ``200 OK``.
    """
    try:
      self.raise_for_status()
    except HTTPError:
      return False
    return True
  @property
  defis_redirect(self):
"""True if this Response is a well-formed HTTP redirect that could have
    been processed automatically (by :meth:`Session.resolve_redirects`).
    """
    return "location" in self.headers and self.status_code in REDIRECT_STATI
  @property
  defis_permanent_redirect(self):
"""True if this Response one of the permanent versions of redirect."""
    return "location" in self.headers and self.status_code in (
      codes.moved_permanently,
      codes.permanent_redirect,
    )
  @property
  defnext(self):
"""Returns a PreparedRequest for the next request in a redirect chain, if there is one."""
    return self._next
  @property
  defapparent_encoding(self):
"""The apparent encoding, provided by the charset_normalizer or chardet libraries."""
    if chardet is not None:
      return chardet.detect(self.content)["encoding"]
    else:
      # If no character detection library is available, we'll fall back
      # to a standard Python utf-8 str.
      return "utf-8"


[docs]
  defiter_content(self, chunk_size=1, decode_unicode=False):
"""Iterates over the response data. When stream=True is set on the
    request, this avoids reading the content at once into memory for
    large responses. The chunk size is the number of bytes it should
    read into memory. This is not necessarily the length of each item
    returned as decoding can take place.
    chunk_size must be of type int or None. A value of None will
    function differently depending on the value of `stream`.
    stream=True will read data as it arrives in whatever size the
    chunks are received. If stream=False, data is returned as
    a single chunk.
    If decode_unicode is True, content will be decoded using the best
    available encoding based on the response.
    """
    defgenerate():
      # Special case for urllib3.
      if hasattr(self.raw, "stream"):
        try:
          yield from self.raw.stream(chunk_size, decode_content=True)
        except ProtocolError as e:
          raise ChunkedEncodingError(e)
        except DecodeError as e:
          raise ContentDecodingError(e)
        except ReadTimeoutError as e:
          raise ConnectionError(e)
        except SSLError as e:
          raise RequestsSSLError(e)
      else:
        # Standard file-like object.
        while True:
          chunk = self.raw.read(chunk_size)
          if not chunk:
            break
          yield chunk
      self._content_consumed = True
    if self._content_consumed and isinstance(self._content, bool):
      raise StreamConsumedError()
    elif chunk_size is not None and not isinstance(chunk_size, int):
      raise TypeError(
        f"chunk_size must be an int, it is instead a {type(chunk_size)}."
      )
    # simulate reading small chunks of the content
    reused_chunks = iter_slices(self._content, chunk_size)
    stream_chunks = generate()
    chunks = reused_chunks if self._content_consumed else stream_chunks
    if decode_unicode:
      chunks = stream_decode_response_unicode(chunks, self)
    return chunks




[docs]
  defiter_lines(
    self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=False, delimiter=None
  ):
"""Iterates over the response data, one line at a time. When
    stream=True is set on the request, this avoids reading the
    content at once into memory for large responses.
    .. note:: This method is not reentrant safe.
    """
    pending = None
    for chunk in self.iter_content(
      chunk_size=chunk_size, decode_unicode=decode_unicode
    ):
      if pending is not None:
        chunk = pending + chunk
      if delimiter:
        lines = chunk.split(delimiter)
      else:
        lines = chunk.splitlines()
      if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:
        pending = lines.pop()
      else:
        pending = None
      yield from lines
    if pending is not None:
      yield pending


  @property
  defcontent(self):
"""Content of the response, in bytes."""
    if self._content is False:
      # Read the contents.
      if self._content_consumed:
        raise RuntimeError("The content for this response was already consumed")
      if self.status_code == 0 or self.raw is None:
        self._content = None
      else:
        self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""
    self._content_consumed = True
    # don't need to release the connection; that's been handled by urllib3
    # since we exhausted the data.
    return self._content
  @property
  deftext(self):
"""Content of the response, in unicode.
    If Response.encoding is None, encoding will be guessed using
    ``charset_normalizer`` or ``chardet``.
    The encoding of the response content is determined based solely on HTTP
    headers, following RFC 2616 to the letter. If you can take advantage of
    non-HTTP knowledge to make a better guess at the encoding, you should
    set ``r.encoding`` appropriately before accessing this property.
    """
    # Try charset from content-type
    content = None
    encoding = self.encoding
    if not self.content:
      return ""
    # Fallback to auto-detected encoding.
    if self.encoding is None:
      encoding = self.apparent_encoding
    # Decode unicode from given encoding.
    try:
      content = str(self.content, encoding, errors="replace")
    except (LookupError, TypeError):
      # A LookupError is raised if the encoding was not found which could
      # indicate a misspelling or similar mistake.
      #
      # A TypeError can be raised if encoding is None
      #
      # So we try blindly encoding.
      content = str(self.content, errors="replace")
    return content


[docs]
  defjson(self, **kwargs):
r"""Decodes the JSON response body (if any) as a Python object.
    This may return a dictionary, list, etc. depending on what is in the response.
    :param \*\*kwargs: Optional arguments that ``json.loads`` takes.
    :raises requests.exceptions.JSONDecodeError: If the response body does not
      contain valid json.
    """
    if not self.encoding and self.content and len(self.content) > 3:
      # No encoding set. JSON RFC 4627 section 3 states we should expect
      # UTF-8, -16 or -32. Detect which one to use; If the detection or
      # decoding fails, fall back to `self.text` (using charset_normalizer to make
      # a best guess).
      encoding = guess_json_utf(self.content)
      if encoding is not None:
        try:
          return complexjson.loads(self.content.decode(encoding), **kwargs)
        except UnicodeDecodeError:
          # Wrong UTF codec detected; usually because it's not UTF-8
          # but some other 8-bit codec. This is an RFC violation,
          # and the server didn't bother to tell us what codec *was*
          # used.
          pass
        except JSONDecodeError as e:
          raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
    try:
      return complexjson.loads(self.text, **kwargs)
    except JSONDecodeError as e:
      # Catch JSON-related errors and raise as requests.JSONDecodeError
      # This aliases json.JSONDecodeError and simplejson.JSONDecodeError
      raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)


  @property
  deflinks(self):
"""Returns the parsed header links of the response, if any."""
    header = self.headers.get("link")
    resolved_links = {}
    if header:
      links = parse_header_links(header)
      for link in links:
        key = link.get("rel") or link.get("url")
        resolved_links[key] = link
    return resolved_links


[docs]
  defraise_for_status(self):
"""Raises :class:`HTTPError`, if one occurred."""
    http_error_msg = ""
    if isinstance(self.reason, bytes):
      # We attempt to decode utf-8 first because some servers
      # choose to localize their reason strings. If the string
      # isn't utf-8, we fall back to iso-8859-1 for all other
      # encodings. (See PR #3538)
      try:
        reason = self.reason.decode("utf-8")
      except UnicodeDecodeError:
        reason = self.reason.decode("iso-8859-1")
    else:
      reason = self.reason
    if 400 <= self.status_code < 500:
      http_error_msg = (
        f"{self.status_code} Client Error: {reason} for url: {self.url}"
      )
    elif 500 <= self.status_code < 600:
      http_error_msg = (
        f"{self.status_code} Server Error: {reason} for url: {self.url}"
      )
    if http_error_msg:
      raise HTTPError(http_error_msg, response=self)




[docs]
  defclose(self):
"""Releases the connection back to the pool. Once this method has been
    called the underlying ``raw`` object must not be accessed again.
    *Note: Should not normally need to be called explicitly.*
    """
    if not self._content_consumed:
      self.raw.close()
    release_conn = getattr(self.raw, "release_conn", None)
    if release_conn is not None:
      release_conn()




```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.sessions
```
"""
requests.sessions
~~~~~~~~~~~~~~~~~
This module provides a Session object to manage and persist settings across
requests (cookies, auth, proxies).
"""
importos
importsys
importtime
fromcollectionsimport OrderedDict
fromdatetimeimport timedelta
from._internal_utilsimport to_native_string
from.adaptersimport HTTPAdapter
from.authimport _basic_auth_str
from.compatimport Mapping, cookielib, urljoin, urlparse
from.cookiesimport (
  RequestsCookieJar,
  cookiejar_from_dict,
  extract_cookies_to_jar,
  merge_cookies,
)
from.exceptionsimport (
  ChunkedEncodingError,
  ContentDecodingError,
  InvalidSchema,
  TooManyRedirects,
)
from.hooksimport default_hooks, dispatch_hook
# formerly defined here, reexposed here for backward compatibility
from.modelsimport ( # noqa: F401
  DEFAULT_REDIRECT_LIMIT,
  REDIRECT_STATI,
  PreparedRequest,
  Request,
)
from.status_codesimport codes
from.structuresimport CaseInsensitiveDict
from.utilsimport ( # noqa: F401
  DEFAULT_PORTS,
  default_headers,
  get_auth_from_url,
  get_environ_proxies,
  get_netrc_auth,
  requote_uri,
  resolve_proxies,
  rewind_body,
  should_bypass_proxies,
  to_key_val_list,
)
# Preferred clock, based on which one is more accurate on a given system.
if sys.platform == "win32":
  preferred_clock = time.perf_counter
else:
  preferred_clock = time.time

defmerge_setting(request_setting, session_setting, dict_class=OrderedDict):
"""Determines appropriate setting for a given request, taking into account
  the explicit setting on that request, and the setting in the session. If a
  setting is a dictionary, they will be merged together using `dict_class`
  """
  if session_setting is None:
    return request_setting
  if request_setting is None:
    return session_setting
  # Bypass if not a dictionary (e.g. verify)
  if not (
    isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)
  ):
    return request_setting
  merged_setting = dict_class(to_key_val_list(session_setting))
  merged_setting.update(to_key_val_list(request_setting))
  # Remove keys that are set to None. Extract keys first to avoid altering
  # the dictionary during iteration.
  none_keys = [k for (k, v) in merged_setting.items() if v is None]
  for key in none_keys:
    del merged_setting[key]
  return merged_setting

defmerge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):
"""Properly merges both requests and session hooks.
  This is necessary because when request_hooks == {'response': []}, the
  merge breaks Session hooks entirely.
  """
  if session_hooks is None or session_hooks.get("response") == []:
    return request_hooks
  if request_hooks is None or request_hooks.get("response") == []:
    return session_hooks
  return merge_setting(request_hooks, session_hooks, dict_class)

classSessionRedirectMixin:
  defget_redirect_target(self, resp):
"""Receives a Response. Returns a redirect URI or ``None``"""
    # Due to the nature of how requests processes redirects this method will
    # be called at least once upon the original response and at least twice
    # on each subsequent redirect response (if any).
    # If a custom mixin is used to handle this logic, it may be advantageous
    # to cache the redirect location onto the response object as a private
    # attribute.
    if resp.is_redirect:
      location = resp.headers["location"]
      # Currently the underlying http module on py3 decode headers
      # in latin1, but empirical evidence suggests that latin1 is very
      # rarely used with non-ASCII characters in HTTP headers.
      # It is more likely to get UTF8 header rather than latin1.
      # This causes incorrect handling of UTF8 encoded location headers.
      # To solve this, we re-encode the location in latin1.
      location = location.encode("latin1")
      return to_native_string(location, "utf8")
    return None
  defshould_strip_auth(self, old_url, new_url):
"""Decide whether Authorization header should be removed when redirecting"""
    old_parsed = urlparse(old_url)
    new_parsed = urlparse(new_url)
    if old_parsed.hostname != new_parsed.hostname:
      return True
    # Special case: allow http -> https redirect when using the standard
    # ports. This isn't specified by RFC 7235, but is kept to avoid
    # breaking backwards compatibility with older versions of requests
    # that allowed any redirects on the same host.
    if (
      old_parsed.scheme == "http"
      and old_parsed.port in (80, None)
      and new_parsed.scheme == "https"
      and new_parsed.port in (443, None)
    ):
      return False
    # Handle default port usage corresponding to scheme.
    changed_port = old_parsed.port != new_parsed.port
    changed_scheme = old_parsed.scheme != new_parsed.scheme
    default_port = (DEFAULT_PORTS.get(old_parsed.scheme, None), None)
    if (
      not changed_scheme
      and old_parsed.port in default_port
      and new_parsed.port in default_port
    ):
      return False
    # Standard case: root URI must match
    return changed_port or changed_scheme
  defresolve_redirects(
    self,
    resp,
    req,
    stream=False,
    timeout=None,
    verify=True,
    cert=None,
    proxies=None,
    yield_requests=False,
    **adapter_kwargs,
  ):
"""Receives a Response. Returns a generator of Responses or Requests."""
    hist = [] # keep track of history
    url = self.get_redirect_target(resp)
    previous_fragment = urlparse(req.url).fragment
    while url:
      prepared_request = req.copy()
      # Update history and keep track of redirects.
      # resp.history must ignore the original request in this loop
      hist.append(resp)
      resp.history = hist[1:]
      try:
        resp.content # Consume socket so it can be released
      except (ChunkedEncodingError, ContentDecodingError, RuntimeError):
        resp.raw.read(decode_content=False)
      if len(resp.history) >= self.max_redirects:
        raise TooManyRedirects(
          f"Exceeded {self.max_redirects} redirects.", response=resp
        )
      # Release the connection back into the pool.
      resp.close()
      # Handle redirection without scheme (see: RFC 1808 Section 4)
      if url.startswith("//"):
        parsed_rurl = urlparse(resp.url)
        url = ":".join([to_native_string(parsed_rurl.scheme), url])
      # Normalize url case and attach previous fragment if needed (RFC 7231 7.1.2)
      parsed = urlparse(url)
      if parsed.fragment == "" and previous_fragment:
        parsed = parsed._replace(fragment=previous_fragment)
      elif parsed.fragment:
        previous_fragment = parsed.fragment
      url = parsed.geturl()
      # Facilitate relative 'location' headers, as allowed by RFC 7231.
      # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')
      # Compliant with RFC3986, we percent encode the url.
      if not parsed.netloc:
        url = urljoin(resp.url, requote_uri(url))
      else:
        url = requote_uri(url)
      prepared_request.url = to_native_string(url)
      self.rebuild_method(prepared_request, resp)
      # https://github.com/psf/requests/issues/1084
      if resp.status_code not in (
        codes.temporary_redirect,
        codes.permanent_redirect,
      ):
        # https://github.com/psf/requests/issues/3490
        purged_headers = ("Content-Length", "Content-Type", "Transfer-Encoding")
        for header in purged_headers:
          prepared_request.headers.pop(header, None)
        prepared_request.body = None
      headers = prepared_request.headers
      headers.pop("Cookie", None)
      # Extract any cookies sent on the response to the cookiejar
      # in the new request. Because we've mutated our copied prepared
      # request, use the old one that we haven't yet touched.
      extract_cookies_to_jar(prepared_request._cookies, req, resp.raw)
      merge_cookies(prepared_request._cookies, self.cookies)
      prepared_request.prepare_cookies(prepared_request._cookies)
      # Rebuild auth and proxy information.
      proxies = self.rebuild_proxies(prepared_request, proxies)
      self.rebuild_auth(prepared_request, resp)
      # A failed tell() sets `_body_position` to `object()`. This non-None
      # value ensures `rewindable` will be True, allowing us to raise an
      # UnrewindableBodyError, instead of hanging the connection.
      rewindable = prepared_request._body_position is not None and (
        "Content-Length" in headers or "Transfer-Encoding" in headers
      )
      # Attempt to rewind consumed file-like object.
      if rewindable:
        rewind_body(prepared_request)
      # Override the original request.
      req = prepared_request
      if yield_requests:
        yield req
      else:
        resp = self.send(
          req,
          stream=stream,
          timeout=timeout,
          verify=verify,
          cert=cert,
          proxies=proxies,
          allow_redirects=False,
          **adapter_kwargs,
        )
        extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)
        # extract redirect url, if any, for the next loop
        url = self.get_redirect_target(resp)
        yield resp
  defrebuild_auth(self, prepared_request, response):
"""When being redirected we may want to strip authentication from the
    request to avoid leaking credentials. This method intelligently removes
    and reapplies authentication where possible to avoid credential loss.
    """
    headers = prepared_request.headers
    url = prepared_request.url
    if "Authorization" in headers and self.should_strip_auth(
      response.request.url, url
    ):
      # If we get redirected to a new host, we should strip out any
      # authentication headers.
      del headers["Authorization"]
    # .netrc might have more auth for us on our new host.
    new_auth = get_netrc_auth(url) if self.trust_env else None
    if new_auth is not None:
      prepared_request.prepare_auth(new_auth)
  defrebuild_proxies(self, prepared_request, proxies):
"""This method re-evaluates the proxy configuration by considering the
    environment variables. If we are redirected to a URL covered by
    NO_PROXY, we strip the proxy configuration. Otherwise, we set missing
    proxy keys for this URL (in case they were stripped by a previous
    redirect).
    This method also replaces the Proxy-Authorization header where
    necessary.
    :rtype: dict
    """
    headers = prepared_request.headers
    scheme = urlparse(prepared_request.url).scheme
    new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)
    if "Proxy-Authorization" in headers:
      del headers["Proxy-Authorization"]
    try:
      username, password = get_auth_from_url(new_proxies[scheme])
    except KeyError:
      username, password = None, None
    # urllib3 handles proxy authorization for us in the standard adapter.
    # Avoid appending this to TLS tunneled requests where it may be leaked.
    if not scheme.startswith("https") and username and password:
      headers["Proxy-Authorization"] = _basic_auth_str(username, password)
    return new_proxies
  defrebuild_method(self, prepared_request, response):
"""When being redirected we may want to change the method of the request
    based on certain specs or browser behavior.
    """
    method = prepared_request.method
    # https://tools.ietf.org/html/rfc7231#section-6.4.4
    if response.status_code == codes.see_other and method != "HEAD":
      method = "GET"
    # Do what the browsers do, despite standards...
    # First, turn 302s into GETs.
    if response.status_code == codes.found and method != "HEAD":
      method = "GET"
    # Second, if a POST is responded to with a 301, turn it into a GET.
    # This bizarre behaviour is explained in Issue 1704.
    if response.status_code == codes.moved and method == "POST":
      method = "GET"
    prepared_request.method = method



[docs]
classSession(SessionRedirectMixin):
"""A Requests session.
  Provides cookie persistence, connection-pooling, and configuration.
  Basic Usage::
   >>> import requests
   >>> s = requests.Session()
   >>> s.get('https://httpbin.org/get')
   <Response [200]>
  Or as a context manager::
   >>> with requests.Session() as s:
   ...   s.get('https://httpbin.org/get')
   <Response [200]>
  """
  __attrs__ = [
    "headers",
    "cookies",
    "auth",
    "proxies",
    "hooks",
    "params",
    "verify",
    "cert",
    "adapters",
    "stream",
    "trust_env",
    "max_redirects",
  ]
  def__init__(self):
    #: A case-insensitive dictionary of headers to be sent on each
    #: :class:`Request <Request>` sent from this
    #: :class:`Session <Session>`.
    self.headers = default_headers()
    #: Default Authentication tuple or object to attach to
    #: :class:`Request <Request>`.
    self.auth = None
    #: Dictionary mapping protocol or protocol and host to the URL of the proxy
    #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to
    #: be used on each :class:`Request <Request>`.
    self.proxies = {}
    #: Event-handling hooks.
    self.hooks = default_hooks()
    #: Dictionary of querystring data to attach to each
    #: :class:`Request <Request>`. The dictionary values may be lists for
    #: representing multivalued query parameters.
    self.params = {}
    #: Stream response content default.
    self.stream = False
    #: SSL Verification default.
    #: Defaults to `True`, requiring requests to verify the TLS certificate at the
    #: remote end.
    #: If verify is set to `False`, requests will accept any TLS certificate
    #: presented by the server, and will ignore hostname mismatches and/or
    #: expired certificates, which will make your application vulnerable to
    #: man-in-the-middle (MitM) attacks.
    #: Only set this to `False` for testing.
    self.verify = True
    #: SSL client certificate default, if String, path to ssl client
    #: cert file (.pem). If Tuple, ('cert', 'key') pair.
    self.cert = None
    #: Maximum number of redirects allowed. If the request exceeds this
    #: limit, a :class:`TooManyRedirects` exception is raised.
    #: This defaults to requests.models.DEFAULT_REDIRECT_LIMIT, which is
    #: 30.
    self.max_redirects = DEFAULT_REDIRECT_LIMIT
    #: Trust environment settings for proxy configuration, default
    #: authentication and similar.
    self.trust_env = True
    #: A CookieJar containing all currently outstanding cookies set on this
    #: session. By default it is a
    #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but
    #: may be any other ``cookielib.CookieJar`` compatible object.
    self.cookies = cookiejar_from_dict({})
    # Default connection adapters.
    self.adapters = OrderedDict()
    self.mount("https://", HTTPAdapter())
    self.mount("http://", HTTPAdapter())
  def__enter__(self):
    return self
  def__exit__(self, *args):
    self.close()


[docs]
  defprepare_request(self, request):
"""Constructs a :class:`PreparedRequest <PreparedRequest>` for
    transmission and returns it. The :class:`PreparedRequest` has settings
    merged from the :class:`Request <Request>` instance and those of the
    :class:`Session`.
    :param request: :class:`Request` instance to prepare with this
      session's settings.
    :rtype: requests.PreparedRequest
    """
    cookies = request.cookies or {}
    # Bootstrap CookieJar.
    if not isinstance(cookies, cookielib.CookieJar):
      cookies = cookiejar_from_dict(cookies)
    # Merge with session cookies
    merged_cookies = merge_cookies(
      merge_cookies(RequestsCookieJar(), self.cookies), cookies
    )
    # Set environment's basic authentication if not explicitly set.
    auth = request.auth
    if self.trust_env and not auth and not self.auth:
      auth = get_netrc_auth(request.url)
    p = PreparedRequest()
    p.prepare(
      method=request.method.upper(),
      url=request.url,
      files=request.files,
      data=request.data,
      json=request.json,
      headers=merge_setting(
        request.headers, self.headers, dict_class=CaseInsensitiveDict
      ),
      params=merge_setting(request.params, self.params),
      auth=merge_setting(auth, self.auth),
      cookies=merged_cookies,
      hooks=merge_hooks(request.hooks, self.hooks),
    )
    return p




[docs]
  defrequest(
    self,
    method,
    url,
    params=None,
    data=None,
    headers=None,
    cookies=None,
    files=None,
    auth=None,
    timeout=None,
    allow_redirects=True,
    proxies=None,
    hooks=None,
    stream=None,
    verify=None,
    cert=None,
    json=None,
  ):
"""Constructs a :class:`Request <Request>`, prepares it and sends it.
    Returns :class:`Response <Response>` object.
    :param method: method for the new :class:`Request` object.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary or bytes to be sent in the query
      string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
      object to send in the body of the :class:`Request`.
    :param json: (optional) json to send in the body of the
      :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the
      :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the
      :class:`Request`.
    :param files: (optional) Dictionary of ``'filename': file-like-objects``
      for multipart encoding upload.
    :param auth: (optional) Auth tuple or callable to enable
      Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How long to wait for the server to send
      data before giving up, as a float, or a :ref:`(connect timeout,
      read timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Set to True by default.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol or protocol and
      hostname to the URL of the proxy.
    :param hooks: (optional) Dictionary mapping hook name to one event or
      list of events, event must be callable.
    :param stream: (optional) whether to immediately download the response
      content. Defaults to ``False``.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
      the server's TLS certificate, or a string, in which case it must be a path
      to a CA bundle to use. Defaults to ``True``. When set to
      ``False``, requests will accept any TLS certificate presented by
      the server, and will ignore hostname mismatches and/or expired
      certificates, which will make your application vulnerable to
      man-in-the-middle (MitM) attacks. Setting verify to ``False``
      may be useful during local development or testing.
    :param cert: (optional) if String, path to ssl client cert file (.pem).
      If Tuple, ('cert', 'key') pair.
    :rtype: requests.Response
    """
    # Create the Request.
    req = Request(
      method=method.upper(),
      url=url,
      headers=headers,
      files=files,
      data=data or {},
      json=json,
      params=params or {},
      auth=auth,
      cookies=cookies,
      hooks=hooks,
    )
    prep = self.prepare_request(req)
    proxies = proxies or {}
    settings = self.merge_environment_settings(
      prep.url, proxies, stream, verify, cert
    )
    # Send the request.
    send_kwargs = {
      "timeout": timeout,
      "allow_redirects": allow_redirects,
    }
    send_kwargs.update(settings)
    resp = self.send(prep, **send_kwargs)
    return resp




[docs]
  defget(self, url, **kwargs):
r"""Sends a GET request. Returns :class:`Response` object.
    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :rtype: requests.Response
    """
    kwargs.setdefault("allow_redirects", True)
    return self.request("GET", url, **kwargs)




[docs]
  defoptions(self, url, **kwargs):
r"""Sends a OPTIONS request. Returns :class:`Response` object.
    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :rtype: requests.Response
    """
    kwargs.setdefault("allow_redirects", True)
    return self.request("OPTIONS", url, **kwargs)




[docs]
  defhead(self, url, **kwargs):
r"""Sends a HEAD request. Returns :class:`Response` object.
    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :rtype: requests.Response
    """
    kwargs.setdefault("allow_redirects", False)
    return self.request("HEAD", url, **kwargs)




[docs]
  defpost(self, url, data=None, json=None, **kwargs):
r"""Sends a POST request. Returns :class:`Response` object.
    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
      object to send in the body of the :class:`Request`.
    :param json: (optional) json to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :rtype: requests.Response
    """
    return self.request("POST", url, data=data, json=json, **kwargs)




[docs]
  defput(self, url, data=None, **kwargs):
r"""Sends a PUT request. Returns :class:`Response` object.
    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
      object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :rtype: requests.Response
    """
    return self.request("PUT", url, data=data, **kwargs)




[docs]
  defpatch(self, url, data=None, **kwargs):
r"""Sends a PATCH request. Returns :class:`Response` object.
    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
      object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :rtype: requests.Response
    """
    return self.request("PATCH", url, data=data, **kwargs)




[docs]
  defdelete(self, url, **kwargs):
r"""Sends a DELETE request. Returns :class:`Response` object.
    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :rtype: requests.Response
    """
    return self.request("DELETE", url, **kwargs)




[docs]
  defsend(self, request, **kwargs):
"""Send a given PreparedRequest.
    :rtype: requests.Response
    """
    # Set defaults that the hooks can utilize to ensure they always have
    # the correct parameters to reproduce the previous request.
    kwargs.setdefault("stream", self.stream)
    kwargs.setdefault("verify", self.verify)
    kwargs.setdefault("cert", self.cert)
    if "proxies" not in kwargs:
      kwargs["proxies"] = resolve_proxies(request, self.proxies, self.trust_env)
    # It's possible that users might accidentally send a Request object.
    # Guard against that specific failure case.
    if isinstance(request, Request):
      raise ValueError("You can only send PreparedRequests.")
    # Set up variables needed for resolve_redirects and dispatching of hooks
    allow_redirects = kwargs.pop("allow_redirects", True)
    stream = kwargs.get("stream")
    hooks = request.hooks
    # Get the appropriate adapter to use
    adapter = self.get_adapter(url=request.url)
    # Start time (approximately) of the request
    start = preferred_clock()
    # Send the request
    r = adapter.send(request, **kwargs)
    # Total elapsed time of the request (approximately)
    elapsed = preferred_clock() - start
    r.elapsed = timedelta(seconds=elapsed)
    # Response manipulation hooks
    r = dispatch_hook("response", hooks, r, **kwargs)
    # Persist cookies
    if r.history:
      # If the hooks create history then we want those cookies too
      for resp in r.history:
        extract_cookies_to_jar(self.cookies, resp.request, resp.raw)
    extract_cookies_to_jar(self.cookies, request, r.raw)
    # Resolve redirects if allowed.
    if allow_redirects:
      # Redirect resolving generator.
      gen = self.resolve_redirects(r, request, **kwargs)
      history = [resp for resp in gen]
    else:
      history = []
    # Shuffle things around if there's history.
    if history:
      # Insert the first (original) request at the start
      history.insert(0, r)
      # Get the last request made
      r = history.pop()
      r.history = history
    # If redirects aren't being followed, store the response on the Request for Response.next().
    if not allow_redirects:
      try:
        r._next = next(
          self.resolve_redirects(r, request, yield_requests=True, **kwargs)
        )
      except StopIteration:
        pass
    if not stream:
      r.content
    return r




[docs]
  defmerge_environment_settings(self, url, proxies, stream, verify, cert):
"""
    Check the environment and merge it with some settings.
    :rtype: dict
    """
    # Gather clues from the surrounding environment.
    if self.trust_env:
      # Set environment's proxies.
      no_proxy = proxies.get("no_proxy") if proxies is not None else None
      env_proxies = get_environ_proxies(url, no_proxy=no_proxy)
      for k, v in env_proxies.items():
        proxies.setdefault(k, v)
      # Look for requests environment configuration
      # and be compatible with cURL.
      if verify is True or verify is None:
        verify = (
          os.environ.get("REQUESTS_CA_BUNDLE")
          or os.environ.get("CURL_CA_BUNDLE")
          or verify
        )
    # Merge all the kwargs.
    proxies = merge_setting(proxies, self.proxies)
    stream = merge_setting(stream, self.stream)
    verify = merge_setting(verify, self.verify)
    cert = merge_setting(cert, self.cert)
    return {"proxies": proxies, "stream": stream, "verify": verify, "cert": cert}




[docs]
  defget_adapter(self, url):
"""
    Returns the appropriate connection adapter for the given URL.
    :rtype: requests.adapters.BaseAdapter
    """
    for prefix, adapter in self.adapters.items():
      if url.lower().startswith(prefix.lower()):
        return adapter
    # Nothing matches :-/
    raise InvalidSchema(f"No connection adapters were found for {url!r}")




[docs]
  defclose(self):
"""Closes all adapters and as such the session"""
    for v in self.adapters.values():
      v.close()




[docs]
  defmount(self, prefix, adapter):
"""Registers a connection adapter to a prefix.
    Adapters are sorted in descending order by prefix length.
    """
    self.adapters[prefix] = adapter
    keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]
    for key in keys_to_move:
      self.adapters[key] = self.adapters.pop(key)


  def__getstate__(self):
    state = {attr: getattr(self, attr, None) for attr in self.__attrs__}
    return state
  def__setstate__(self, state):
    for attr, value in state.items():
      setattr(self, attr, value)


defsession():
"""
  Returns a :class:`Session` for context-management.
  .. deprecated:: 1.0.0
    This method has been deprecated since version 1.0.0 and is only kept for
    backwards compatibility. New code should use :class:`~requests.sessions.Session`
    to create a session. This may be removed at a future date.
  :rtype: Session
  """
  return Session()

```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# Source code for requests.utils
```
"""
requests.utils
~~~~~~~~~~~~~~
This module provides utility functions that are used within Requests
that are also useful for external consumption.
"""
importcodecs
importcontextlib
importio
importos
importre
importsocket
importstruct
importsys
importtempfile
importwarnings
importzipfile
fromcollectionsimport OrderedDict
fromurllib3.utilimport make_headers, parse_url
from.import certs
from.__version__import __version__
# to_native_string is unused here, but imported here for backwards compatibility
from._internal_utilsimport ( # noqa: F401
  _HEADER_VALIDATORS_BYTE,
  _HEADER_VALIDATORS_STR,
  HEADER_VALIDATORS,
  to_native_string,
)
from.compatimport (
  Mapping,
  basestring,
  bytes,
  getproxies,
  getproxies_environment,
  integer_types,
  is_urllib3_1,
)
from.compatimport parse_http_list as _parse_list_header
from.compatimport (
  proxy_bypass,
  proxy_bypass_environment,
  quote,
  str,
  unquote,
  urlparse,
  urlunparse,
)
from.cookiesimport cookiejar_from_dict
from.exceptionsimport (
  FileModeWarning,
  InvalidHeader,
  InvalidURL,
  UnrewindableBodyError,
)
from.structuresimport CaseInsensitiveDict
NETRC_FILES = (".netrc", "_netrc")
DEFAULT_CA_BUNDLE_PATH = certs.where()
DEFAULT_PORTS = {"http": 80, "https": 443}
# Ensure that ', ' is used to preserve previous delimiter behavior.
DEFAULT_ACCEPT_ENCODING = ", ".join(
  re.split(r",\s*", make_headers(accept_encoding=True)["accept-encoding"])
)

if sys.platform == "win32":
  # provide a proxy_bypass version on Windows without DNS lookups
  defproxy_bypass_registry(host):
    try:
      importwinreg
    except ImportError:
      return False
    try:
      internetSettings = winreg.OpenKey(
        winreg.HKEY_CURRENT_USER,
        r"Software\Microsoft\Windows\CurrentVersion\Internet Settings",
      )
      # ProxyEnable could be REG_SZ or REG_DWORD, normalizing it
      proxyEnable = int(winreg.QueryValueEx(internetSettings, "ProxyEnable")[0])
      # ProxyOverride is almost always a string
      proxyOverride = winreg.QueryValueEx(internetSettings, "ProxyOverride")[0]
    except (OSError, ValueError):
      return False
    if not proxyEnable or not proxyOverride:
      return False
    # make a check value list from the registry entry: replace the
    # '<local>' string by the localhost entry and the corresponding
    # canonical entry.
    proxyOverride = proxyOverride.split(";")
    # filter out empty strings to avoid re.match return true in the following code.
    proxyOverride = filter(None, proxyOverride)
    # now check if we match one of the registry values.
    for test in proxyOverride:
      if test == "<local>":
        if "." not in host:
          return True
      test = test.replace(".", r"\.") # mask dots
      test = test.replace("*", r".*") # change glob sequence
      test = test.replace("?", r".") # change glob char
      if re.match(test, host, re.I):
        return True
    return False
  defproxy_bypass(host): # noqa
"""Return True, if the host should be bypassed.
    Checks proxy settings gathered from the environment, if specified,
    or the registry.
    """
    if getproxies_environment():
      return proxy_bypass_environment(host)
    else:
      return proxy_bypass_registry(host)

defdict_to_sequence(d):
"""Returns an internal sequence dictionary update."""
  if hasattr(d, "items"):
    d = d.items()
  return d

defsuper_len(o):
  total_length = None
  current_position = 0
  if not is_urllib3_1 and isinstance(o, str):
    # urllib3 2.x+ treats all strings as utf-8 instead
    # of latin-1 (iso-8859-1) like http.client.
    o = o.encode("utf-8")
  if hasattr(o, "__len__"):
    total_length = len(o)
  elif hasattr(o, "len"):
    total_length = o.len
  elif hasattr(o, "fileno"):
    try:
      fileno = o.fileno()
    except (io.UnsupportedOperation, AttributeError):
      # AttributeError is a surprising exception, seeing as how we've just checked
      # that `hasattr(o, 'fileno')`. It happens for objects obtained via
      # `Tarfile.extractfile()`, per issue 5229.
      pass
    else:
      total_length = os.fstat(fileno).st_size
      # Having used fstat to determine the file length, we need to
      # confirm that this file was opened up in binary mode.
      if "b" not in o.mode:
        warnings.warn(
          (
            "Requests has determined the content-length for this "
            "request using the binary size of the file: however, the "
            "file has been opened in text mode (i.e. without the 'b' "
            "flag in the mode). This may lead to an incorrect "
            "content-length. In Requests 3.0, support will be removed "
            "for files in text mode."
          ),
          FileModeWarning,
        )
  if hasattr(o, "tell"):
    try:
      current_position = o.tell()
    except OSError:
      # This can happen in some weird situations, such as when the file
      # is actually a special file descriptor like stdin. In this
      # instance, we don't know what the length is, so set it to zero and
      # let requests chunk it instead.
      if total_length is not None:
        current_position = total_length
    else:
      if hasattr(o, "seek") and total_length is None:
        # StringIO and BytesIO have seek but no usable fileno
        try:
          # seek to end of file
          o.seek(0, 2)
          total_length = o.tell()
          # seek back to current position to support
          # partially read file-like objects
          o.seek(current_position or 0)
        except OSError:
          total_length = 0
  if total_length is None:
    total_length = 0
  return max(0, total_length - current_position)

defget_netrc_auth(url, raise_errors=False):
"""Returns the Requests tuple auth for a given url from netrc."""
  netrc_file = os.environ.get("NETRC")
  if netrc_file is not None:
    netrc_locations = (netrc_file,)
  else:
    netrc_locations = (f"~/{f}" for f in NETRC_FILES)
  try:
    fromnetrcimport NetrcParseError, netrc
    netrc_path = None
    for f in netrc_locations:
      try:
        loc = os.path.expanduser(f)
      except KeyError:
        # os.path.expanduser can fail when $HOME is undefined and
        # getpwuid fails. See https://bugs.python.org/issue20164 &
        # https://github.com/psf/requests/issues/1846
        return
      if os.path.exists(loc):
        netrc_path = loc
        break
    # Abort early if there isn't one.
    if netrc_path is None:
      return
    ri = urlparse(url)
    # Strip port numbers from netloc. This weird `if...encode`` dance is
    # used for Python 3.2, which doesn't support unicode literals.
    splitstr = b":"
    if isinstance(url, str):
      splitstr = splitstr.decode("ascii")
    host = ri.netloc.split(splitstr)[0]
    try:
      _netrc = netrc(netrc_path).authenticators(host)
      if _netrc:
        # Return with login / password
        login_i = 0 if _netrc[0] else 1
        return (_netrc[login_i], _netrc[2])
    except (NetrcParseError, OSError):
      # If there was a parsing error or a permissions issue reading the file,
      # we'll just skip netrc auth unless explicitly asked to raise errors.
      if raise_errors:
        raise
  # App Engine hackiness.
  except (ImportError, AttributeError):
    pass

defguess_filename(obj):
"""Tries to guess the filename of the given object."""
  name = getattr(obj, "name", None)
  if name and isinstance(name, basestring) and name[0] != "<" and name[-1] != ">":
    return os.path.basename(name)

defextract_zipped_paths(path):
"""Replace nonexistent paths that look like they refer to a member of a zip
  archive with the location of an extracted copy of the target, or else
  just return the provided path unchanged.
  """
  if os.path.exists(path):
    # this is already a valid path, no need to do anything further
    return path
  # find the first valid part of the provided path and treat that as a zip archive
  # assume the rest of the path is the name of a member in the archive
  archive, member = os.path.split(path)
  while archive and not os.path.exists(archive):
    archive, prefix = os.path.split(archive)
    if not prefix:
      # If we don't check for an empty prefix after the split (in other words, archive remains unchanged after the split),
      # we _can_ end up in an infinite loop on a rare corner case affecting a small number of users
      break
    member = "/".join([prefix, member])
  if not zipfile.is_zipfile(archive):
    return path
  zip_file = zipfile.ZipFile(archive)
  if member not in zip_file.namelist():
    return path
  # we have a valid zip archive and a valid member of that archive
  tmp = tempfile.gettempdir()
  extracted_path = os.path.join(tmp, member.split("/")[-1])
  if not os.path.exists(extracted_path):
    # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition
    with atomic_open(extracted_path) as file_handler:
      file_handler.write(zip_file.read(member))
  return extracted_path

@contextlib.contextmanager
defatomic_open(filename):
"""Write a file to the disk in an atomic fashion"""
  tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))
  try:
    with os.fdopen(tmp_descriptor, "wb") as tmp_handler:
      yield tmp_handler
    os.replace(tmp_name, filename)
  except BaseException:
    os.remove(tmp_name)
    raise

deffrom_key_val_list(value):
"""Take an object and test to see if it can be represented as a
  dictionary. Unless it can not be represented as such, return an
  OrderedDict, e.g.,
  ::
    >>> from_key_val_list([('key', 'val')])
    OrderedDict([('key', 'val')])
    >>> from_key_val_list('string')
    Traceback (most recent call last):
    ...
    ValueError: cannot encode objects that are not 2-tuples
    >>> from_key_val_list({'key': 'val'})
    OrderedDict([('key', 'val')])
  :rtype: OrderedDict
  """
  if value is None:
    return None
  if isinstance(value, (str, bytes, bool, int)):
    raise ValueError("cannot encode objects that are not 2-tuples")
  return OrderedDict(value)

defto_key_val_list(value):
"""Take an object and test to see if it can be represented as a
  dictionary. If it can be, return a list of tuples, e.g.,
  ::
    >>> to_key_val_list([('key', 'val')])
    [('key', 'val')]
    >>> to_key_val_list({'key': 'val'})
    [('key', 'val')]
    >>> to_key_val_list('string')
    Traceback (most recent call last):
    ...
    ValueError: cannot encode objects that are not 2-tuples
  :rtype: list
  """
  if value is None:
    return None
  if isinstance(value, (str, bytes, bool, int)):
    raise ValueError("cannot encode objects that are not 2-tuples")
  if isinstance(value, Mapping):
    value = value.items()
  return list(value)

# From mitsuhiko/werkzeug (used with permission).
defparse_list_header(value):
"""Parse lists as described by RFC 2068 Section 2.
  In particular, parse comma-separated lists where the elements of
  the list may include quoted-strings. A quoted-string could
  contain a comma. A non-quoted string could have quotes in the
  middle. Quotes are removed automatically after parsing.
  It basically works like :func:`parse_set_header` just that items
  may appear multiple times and case sensitivity is preserved.
  The return value is a standard :class:`list`:
  >>> parse_list_header('token, "quoted value"')
  ['token', 'quoted value']
  To create a header from the :class:`list` again, use the
  :func:`dump_header` function.
  :param value: a string with a list header.
  :return: :class:`list`
  :rtype: list
  """
  result = []
  for item in _parse_list_header(value):
    if item[:1] == item[-1:] == '"':
      item = unquote_header_value(item[1:-1])
    result.append(item)
  return result

# From mitsuhiko/werkzeug (used with permission).
defparse_dict_header(value):
"""Parse lists of key, value pairs as described by RFC 2068 Section 2 and
  convert them into a python dict:
  >>> d = parse_dict_header('foo="is a fish", bar="as well"')
  >>> type(d) is dict
  True
  >>> sorted(d.items())
  [('bar', 'as well'), ('foo', 'is a fish')]
  If there is no value for a key it will be `None`:
  >>> parse_dict_header('key_without_value')
  {'key_without_value': None}
  To create a header from the :class:`dict` again, use the
  :func:`dump_header` function.
  :param value: a string with a dict header.
  :return: :class:`dict`
  :rtype: dict
  """
  result = {}
  for item in _parse_list_header(value):
    if "=" not in item:
      result[item] = None
      continue
    name, value = item.split("=", 1)
    if value[:1] == value[-1:] == '"':
      value = unquote_header_value(value[1:-1])
    result[name] = value
  return result

# From mitsuhiko/werkzeug (used with permission).
defunquote_header_value(value, is_filename=False):
r"""Unquotes a header value. (Reversal of :func:`quote_header_value`).
  This does not use the real unquoting but what browsers are actually
  using for quoting.
  :param value: the header value to unquote.
  :rtype: str
  """
  if value and value[0] == value[-1] == '"':
    # this is not the real unquoting, but fixing this so that the
    # RFC is met will result in bugs with internet explorer and
    # probably some other browsers as well. IE for example is
    # uploading files with "C:\foo\bar.txt" as filename
    value = value[1:-1]
    # if this is a filename and the starting characters look like
    # a UNC path, then just return the value without quotes. Using the
    # replace sequence below on a UNC path has the effect of turning
    # the leading double slash into a single slash and then
    # _fix_ie_filename() doesn't work correctly. See #458.
    if not is_filename or value[:2] != "\\\\":
      return value.replace("\\\\", "\\").replace('\\"', '"')
  return value



[docs]
defdict_from_cookiejar(cj):
"""Returns a key/value dictionary from a CookieJar.
  :param cj: CookieJar object to extract cookies from.
  :rtype: dict
  """
  cookie_dict = {cookie.name: cookie.value for cookie in cj}
  return cookie_dict




[docs]
defadd_dict_to_cookiejar(cj, cookie_dict):
"""Returns a CookieJar from a key/value dictionary.
  :param cj: CookieJar to insert cookies into.
  :param cookie_dict: Dict of key/values to insert into CookieJar.
  :rtype: CookieJar
  """
  return cookiejar_from_dict(cookie_dict, cj)




[docs]
defget_encodings_from_content(content):
"""Returns encodings from given content string.
  :param content: bytestring to extract encodings from.
  """
  warnings.warn(
    (
      "In requests 3.0, get_encodings_from_content will be removed. For "
      "more information, please see the discussion on issue #2266. (This"
      " warning should only appear once.)"
    ),
    DeprecationWarning,
  )
  charset_re = re.compile(r'<meta.*?charset=["\']*(.+?)["\'>]', flags=re.I)
  pragma_re = re.compile(r'<meta.*?content=["\']*;?charset=(.+?)["\'>]', flags=re.I)
  xml_re = re.compile(r'^<\?xml.*?encoding=["\']*(.+?)["\'>]')
  return (
    charset_re.findall(content)
    + pragma_re.findall(content)
    + xml_re.findall(content)
  )


def_parse_content_type_header(header):
"""Returns content type and parameters from given header
  :param header: string
  :return: tuple containing content type and dictionary of
     parameters
  """
  tokens = header.split(";")
  content_type, params = tokens[0].strip(), tokens[1:]
  params_dict = {}
  items_to_strip = "\"' "
  for param in params:
    param = param.strip()
    if param:
      key, value = param, True
      index_of_equals = param.find("=")
      if index_of_equals != -1:
        key = param[:index_of_equals].strip(items_to_strip)
        value = param[index_of_equals + 1 :].strip(items_to_strip)
      params_dict[key.lower()] = value
  return content_type, params_dict



[docs]
defget_encoding_from_headers(headers):
"""Returns encodings from given HTTP Header Dict.
  :param headers: dictionary to extract encoding from.
  :rtype: str
  """
  content_type = headers.get("content-type")
  if not content_type:
    return None
  content_type, params = _parse_content_type_header(content_type)
  if "charset" in params:
    return params["charset"].strip("'\"")
  if "text" in content_type:
    return "ISO-8859-1"
  if "application/json" in content_type:
    # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
    return "utf-8"


defstream_decode_response_unicode(iterator, r):
"""Stream decodes an iterator."""
  if r.encoding is None:
    yield from iterator
    return
  decoder = codecs.getincrementaldecoder(r.encoding)(errors="replace")
  for chunk in iterator:
    rv = decoder.decode(chunk)
    if rv:
      yield rv
  rv = decoder.decode(b"", final=True)
  if rv:
    yield rv

defiter_slices(string, slice_length):
"""Iterate over slices of a string."""
  pos = 0
  if slice_length is None or slice_length <= 0:
    slice_length = len(string)
  while pos < len(string):
    yield string[pos : pos + slice_length]
    pos += slice_length



[docs]
defget_unicode_from_response(r):
"""Returns the requested content back in unicode.
  :param r: Response object to get unicode content from.
  Tried:
  1. charset from content-type
  2. fall back and replace all unicode characters
  :rtype: str
  """
  warnings.warn(
    (
      "In requests 3.0, get_unicode_from_response will be removed. For "
      "more information, please see the discussion on issue #2266. (This"
      " warning should only appear once.)"
    ),
    DeprecationWarning,
  )
  tried_encodings = []
  # Try charset from content-type
  encoding = get_encoding_from_headers(r.headers)
  if encoding:
    try:
      return str(r.content, encoding)
    except UnicodeError:
      tried_encodings.append(encoding)
  # Fall back:
  try:
    return str(r.content, encoding, errors="replace")
  except TypeError:
    return r.content


# The unreserved URI characters (RFC 3986)
UNRESERVED_SET = frozenset(
  "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz" + "0123456789-._~"
)

defunquote_unreserved(uri):
"""Un-escape any percent-escape sequences in a URI that are unreserved
  characters. This leaves all reserved, illegal and non-ASCII bytes encoded.
  :rtype: str
  """
  parts = uri.split("%")
  for i in range(1, len(parts)):
    h = parts[i][0:2]
    if len(h) == 2 and h.isalnum():
      try:
        c = chr(int(h, 16))
      except ValueError:
        raise InvalidURL(f"Invalid percent-escape sequence: '{h}'")
      if c in UNRESERVED_SET:
        parts[i] = c + parts[i][2:]
      else:
        parts[i] = f"%{parts[i]}"
    else:
      parts[i] = f"%{parts[i]}"
  return "".join(parts)

defrequote_uri(uri):
"""Re-quote the given URI.
  This function passes the given URI through an unquote/quote cycle to
  ensure that it is fully and consistently quoted.
  :rtype: str
  """
  safe_with_percent = "!#$%&'()*+,/:;=?@[]~"
  safe_without_percent = "!#$&'()*+,/:;=?@[]~"
  try:
    # Unquote only the unreserved characters
    # Then quote only illegal characters (do not quote reserved,
    # unreserved, or '%')
    return quote(unquote_unreserved(uri), safe=safe_with_percent)
  except InvalidURL:
    # We couldn't unquote the given URI, so let's try quoting it, but
    # there may be unquoted '%'s in the URI. We need to make sure they're
    # properly quoted so they do not cause issues elsewhere.
    return quote(uri, safe=safe_without_percent)

defaddress_in_network(ip, net):
"""This function allows you to check if an IP belongs to a network subnet
  Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
       returns False if ip = 192.168.1.1 and net = 192.168.100.0/24
  :rtype: bool
  """
  ipaddr = struct.unpack("=L", socket.inet_aton(ip))[0]
  netaddr, bits = net.split("/")
  netmask = struct.unpack("=L", socket.inet_aton(dotted_netmask(int(bits))))[0]
  network = struct.unpack("=L", socket.inet_aton(netaddr))[0] & netmask
  return (ipaddr & netmask) == (network & netmask)

defdotted_netmask(mask):
"""Converts mask from /xx format to xxx.xxx.xxx.xxx
  Example: if mask is 24 function returns 255.255.255.0
  :rtype: str
  """
  bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1
  return socket.inet_ntoa(struct.pack(">I", bits))

defis_ipv4_address(string_ip):
"""
  :rtype: bool
  """
  try:
    socket.inet_aton(string_ip)
  except OSError:
    return False
  return True

defis_valid_cidr(string_network):
"""
  Very simple check of the cidr format in no_proxy variable.
  :rtype: bool
  """
  if string_network.count("/") == 1:
    try:
      mask = int(string_network.split("/")[1])
    except ValueError:
      return False
    if mask < 1 or mask > 32:
      return False
    try:
      socket.inet_aton(string_network.split("/")[0])
    except OSError:
      return False
  else:
    return False
  return True

@contextlib.contextmanager
defset_environ(env_name, value):
"""Set the environment variable 'env_name' to 'value'
  Save previous value, yield, and then restore the previous value stored in
  the environment variable 'env_name'.
  If 'value' is None, do nothing"""
  value_changed = value is not None
  if value_changed:
    old_value = os.environ.get(env_name)
    os.environ[env_name] = value
  try:
    yield
  finally:
    if value_changed:
      if old_value is None:
        del os.environ[env_name]
      else:
        os.environ[env_name] = old_value

defshould_bypass_proxies(url, no_proxy):
"""
  Returns whether we should bypass proxies or not.
  :rtype: bool
  """
  # Prioritize lowercase environment variables over uppercase
  # to keep a consistent behaviour with other http projects (curl, wget).
  defget_proxy(key):
    return os.environ.get(key) or os.environ.get(key.upper())
  # First check whether no_proxy is defined. If it is, check that the URL
  # we're getting isn't in the no_proxy list.
  no_proxy_arg = no_proxy
  if no_proxy is None:
    no_proxy = get_proxy("no_proxy")
  parsed = urlparse(url)
  if parsed.hostname is None:
    # URLs don't always have hostnames, e.g. file:/// urls.
    return True
  if no_proxy:
    # We need to check whether we match here. We need to see if we match
    # the end of the hostname, both with and without the port.
    no_proxy = (host for host in no_proxy.replace(" ", "").split(",") if host)
    if is_ipv4_address(parsed.hostname):
      for proxy_ip in no_proxy:
        if is_valid_cidr(proxy_ip):
          if address_in_network(parsed.hostname, proxy_ip):
            return True
        elif parsed.hostname == proxy_ip:
          # If no_proxy ip was defined in plain IP notation instead of cidr notation &
          # matches the IP of the index
          return True
    else:
      host_with_port = parsed.hostname
      if parsed.port:
        host_with_port += f":{parsed.port}"
      for host in no_proxy:
        if parsed.hostname.endswith(host) or host_with_port.endswith(host):
          # The URL does match something in no_proxy, so we don't want
          # to apply the proxies on this URL.
          return True
  with set_environ("no_proxy", no_proxy_arg):
    # parsed.hostname can be `None` in cases such as a file URI.
    try:
      bypass = proxy_bypass(parsed.hostname)
    except (TypeError, socket.gaierror):
      bypass = False
  if bypass:
    return True
  return False

defget_environ_proxies(url, no_proxy=None):
"""
  Return a dict of environment proxies.
  :rtype: dict
  """
  if should_bypass_proxies(url, no_proxy=no_proxy):
    return {}
  else:
    return getproxies()

defselect_proxy(url, proxies):
"""Select a proxy for the url, if applicable.
  :param url: The url being for the request
  :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
  """
  proxies = proxies or {}
  urlparts = urlparse(url)
  if urlparts.hostname is None:
    return proxies.get(urlparts.scheme, proxies.get("all"))
  proxy_keys = [
    urlparts.scheme + "://" + urlparts.hostname,
    urlparts.scheme,
    "all://" + urlparts.hostname,
    "all",
  ]
  proxy = None
  for proxy_key in proxy_keys:
    if proxy_key in proxies:
      proxy = proxies[proxy_key]
      break
  return proxy

defresolve_proxies(request, proxies, trust_env=True):
"""This method takes proxy information from a request and configuration
  input to resolve a mapping of target proxies. This will consider settings
  such as NO_PROXY to strip proxy configurations.
  :param request: Request or PreparedRequest
  :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
  :param trust_env: Boolean declaring whether to trust environment configs
  :rtype: dict
  """
  proxies = proxies if proxies is not None else {}
  url = request.url
  scheme = urlparse(url).scheme
  no_proxy = proxies.get("no_proxy")
  new_proxies = proxies.copy()
  if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):
    environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)
    proxy = environ_proxies.get(scheme, environ_proxies.get("all"))
    if proxy:
      new_proxies.setdefault(scheme, proxy)
  return new_proxies

defdefault_user_agent(name="python-requests"):
"""
  Return a string representing the default user agent.
  :rtype: str
  """
  return f"{name}/{__version__}"

defdefault_headers():
"""
  :rtype: requests.structures.CaseInsensitiveDict
  """
  return CaseInsensitiveDict(
    {
      "User-Agent": default_user_agent(),
      "Accept-Encoding": DEFAULT_ACCEPT_ENCODING,
      "Accept": "*/*",
      "Connection": "keep-alive",
    }
  )

defparse_header_links(value):
"""Return a list of parsed link headers proxies.
  i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"
  :rtype: list
  """
  links = []
  replace_chars = " '\""
  value = value.strip(replace_chars)
  if not value:
    return links
  for val in re.split(", *<", value):
    try:
      url, params = val.split(";", 1)
    except ValueError:
      url, params = val, ""
    link = {"url": url.strip("<> '\"")}
    for param in params.split(";"):
      try:
        key, value = param.split("=")
      except ValueError:
        break
      link[key.strip(replace_chars)] = value.strip(replace_chars)
    links.append(link)
  return links

# Null bytes; no need to recreate these on each call to guess_json_utf
_null = "\x00".encode("ascii") # encoding to ASCII for Python 3
_null2 = _null * 2
_null3 = _null * 3

defguess_json_utf(data):
"""
  :rtype: str
  """
  # JSON always starts with two ASCII characters, so detection is as
  # easy as counting the nulls and from their location and count
  # determine the encoding. Also detect a BOM, if present.
  sample = data[:4]
  if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):
    return "utf-32" # BOM included
  if sample[:3] == codecs.BOM_UTF8:
    return "utf-8-sig" # BOM included, MS style (discouraged)
  if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
    return "utf-16" # BOM included
  nullcount = sample.count(_null)
  if nullcount == 0:
    return "utf-8"
  if nullcount == 2:
    if sample[::2] == _null2: # 1st and 3rd are null
      return "utf-16-be"
    if sample[1::2] == _null2: # 2nd and 4th are null
      return "utf-16-le"
    # Did not detect 2 valid UTF-16 ascii-range characters
  if nullcount == 3:
    if sample[:3] == _null3:
      return "utf-32-be"
    if sample[1:] == _null3:
      return "utf-32-le"
    # Did not detect a valid UTF-32 ascii-range character
  return None

defprepend_scheme_if_needed(url, new_scheme):
"""Given a URL that may or may not have a scheme, prepend the given scheme.
  Does not replace a present scheme with the one provided as an argument.
  :rtype: str
  """
  parsed = parse_url(url)
  scheme, auth, host, port, path, query, fragment = parsed
  # A defect in urlparse determines that there isn't a netloc present in some
  # urls. We previously assumed parsing was overly cautious, and swapped the
  # netloc and path. Due to a lack of tests on the original defect, this is
  # maintained with parse_url for backwards compatibility.
  netloc = parsed.netloc
  if not netloc:
    netloc, path = path, netloc
  if auth:
    # parse_url doesn't provide the netloc with auth
    # so we'll add it ourselves.
    netloc = "@".join([auth, netloc])
  if scheme is None:
    scheme = new_scheme
  if path is None:
    path = ""
  return urlunparse((scheme, netloc, path, "", query, fragment))

defget_auth_from_url(url):
"""Given a url with authentication components, extract them into a tuple of
  username,password.
  :rtype: (str,str)
  """
  parsed = urlparse(url)
  try:
    auth = (unquote(parsed.username), unquote(parsed.password))
  except (AttributeError, TypeError):
    auth = ("", "")
  return auth

defcheck_header_validity(header):
"""Verifies that header parts don't contain leading whitespace
  reserved characters, or return characters.
  :param header: tuple, in the format (name, value).
  """
  name, value = header
  _validate_header_part(header, name, 0)
  _validate_header_part(header, value, 1)

def_validate_header_part(header, header_part, header_validator_index):
  if isinstance(header_part, str):
    validator = _HEADER_VALIDATORS_STR[header_validator_index]
  elif isinstance(header_part, bytes):
    validator = _HEADER_VALIDATORS_BYTE[header_validator_index]
  else:
    raise InvalidHeader(
      f"Header part ({header_part!r}) from {header} "
      f"must be of type str or bytes, not {type(header_part)}"
    )
  if not validator.match(header_part):
    header_kind = "name" if header_validator_index == 0 else "value"
    raise InvalidHeader(
      f"Invalid leading whitespace, reserved character(s), or return "
      f"character(s) in header {header_kind}: {header_part!r}"
    )

defurldefragauth(url):
"""
  Given a url remove the fragment and the authentication part.
  :rtype: str
  """
  scheme, netloc, path, params, query, fragment = urlparse(url)
  # see func:`prepend_scheme_if_needed`
  if not netloc:
    netloc, path = path, netloc
  netloc = netloc.rsplit("@", 1)[-1]
  return urlunparse((scheme, netloc, path, params, query, ""))

defrewind_body(prepared_request):
"""Move file pointer back to its recorded starting position
  so it can be read again on redirect.
  """
  body_seek = getattr(prepared_request.body, "seek", None)
  if body_seek is not None and isinstance(
    prepared_request._body_position, integer_types
  ):
    try:
      body_seek(prepared_request._body_position)
    except OSError:
      raise UnrewindableBodyError(
        "An error occurred when rewinding request body for redirect."
      )
  else:
    raise UnrewindableBodyError("Unable to rewind request body for redirect.")

```

Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview
    * Module code


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)


---

# All modules for which code is available
  * requests.adapters
  * requests.api
  * requests.auth
  * requests.cookies
  * requests.exceptions
  * requests.models
  * requests.sessions
  * requests.utils


Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 
### Useful Links
  * Quickstart
  * Advanced Usage
  * API Reference
  * Release History
  * Contributors Guide
  * Recommended Packages and Extensions
  * Requests @ GitHub
  * Requests @ PyPI
  * Issue Tracker


### Related Topics
  * Documentation overview


### Quick search
©MMXVIX. A Kenneth Reitz Project. 
![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png)
