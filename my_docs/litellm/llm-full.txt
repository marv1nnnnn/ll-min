Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM - Getting Started


On this page
# LiteLLM - Getting Started
https://github.com/BerriAI/litellm
## **Call 100+ LLMs using the OpenAI Input/Output Format**​
  * Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints
  * Consistent output, text responses will always be available at `['choices'][0]['message']['content']`
  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router
  * Track spend & set budgets per project LiteLLM Proxy Server


## How to use LiteLLM​
You can use litellm through either:
  1. LiteLLM Proxy Server - Server (LLM Gateway) to call 100+ LLMs, load balance, cost tracking across projects
  2. LiteLLM python SDK - Python Client to call 100+ LLMs, load balance, cost tracking


### **When to use LiteLLM Proxy Server (LLM Gateway)**​
tip
Use LiteLLM Proxy Server if you want a **central service (LLM Gateway) to access multiple LLMs**
Typically used by Gen AI Enablement / ML PLatform Teams
  * LiteLLM Proxy gives you a unified interface to access multiple LLMs (100+ LLMs)
  * Track LLM Usage and setup guardrails
  * Customize Logging, Guardrails, Caching per project


### **When to use LiteLLM Python SDK**​
tip
Use LiteLLM Python SDK if you want to use LiteLLM in your **python code**
Typically used by developers building llm projects
  * LiteLLM SDK gives you a unified interface to access multiple LLMs (100+ LLMs) 
  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router


## **LiteLLM Python SDK**​
### Basic usage​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
```
pip install litellm  

```

  * OpenAI
  * Anthropic
  * xAI
  * VertexAI
  * NVIDIA
  * HuggingFace
  * Azure OpenAI
  * Ollama
  * Openrouter


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="openai/gpt-4o",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="anthropic/claude-3-sonnet-20240229",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["XAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="xai/grok-2-latest",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
# auth: run 'gcloud auth application-default'  
os.environ["VERTEXAI_PROJECT"] = "hardy-device-386718"  
os.environ["VERTEXAI_LOCATION"] = "us-central1"  
  
response = completion(  
 model="vertex_ai/gemini-1.5-pro",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"  
os.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"  
  
response = completion(  
 model="nvidia_nim/<model_name>",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
os.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"  
  
# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints  
response = completion(  
 model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 api_base="https://my-endpoint.huggingface.cloud"  
)  
  
print(response)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
response = completion(  
 "azure/<your_deployment_name>",  
 messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
  
response = completion(  
      model="ollama/llama2",  
      messages = [{ "content": "Hello, how are you?","role": "user"}],  
      api_base="http://localhost:11434"  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"  
  
response = completion(  
 model="openrouter/google/palm-2-chat-bison",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
)  

```

### Response Format (OpenAI Format)​
```
{  
  "id": "chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885",  
  "created": 1734366691,  
  "model": "claude-3-sonnet-20240229",  
  "object": "chat.completion",  
  "system_fingerprint": null,  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": "Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?",  
        "role": "assistant",  
        "tool_calls": null,  
        "function_call": null  
      }  
    }  
  ],  
  "usage": {  
    "completion_tokens": 43,  
    "prompt_tokens": 13,  
    "total_tokens": 56,  
    "completion_tokens_details": null,  
    "prompt_tokens_details": {  
      "audio_tokens": null,  
      "cached_tokens": 0  
    },  
    "cache_creation_input_tokens": 0,  
    "cache_read_input_tokens": 0  
  }  
}  

```

### Streaming​
Set `stream=True` in the `completion` args. 
  * OpenAI
  * Anthropic
  * xAI
  * VertexAI
  * NVIDIA
  * HuggingFace
  * Azure OpenAI
  * Ollama
  * Openrouter


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="openai/gpt-4o",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="anthropic/claude-3-sonnet-20240229",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["XAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="xai/grok-2-latest",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
# auth: run 'gcloud auth application-default'  
os.environ["VERTEX_PROJECT"] = "hardy-device-386718"  
os.environ["VERTEX_LOCATION"] = "us-central1"  
  
response = completion(  
 model="vertex_ai/gemini-1.5-pro",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"  
os.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"  
  
response = completion(  
 model="nvidia_nim/<model_name>",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
os.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"  
  
# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints  
response = completion(  
 model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 api_base="https://my-endpoint.huggingface.cloud",  
 stream=True,  
)  
  
print(response)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
response = completion(  
 "azure/<your_deployment_name>",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
  
response = completion(  
      model="ollama/llama2",  
      messages = [{ "content": "Hello, how are you?","role": "user"}],  
      api_base="http://localhost:11434",  
      stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"  
  
response = completion(  
 model="openrouter/google/palm-2-chat-bison",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

### Streaming Response Format (OpenAI Format)​
```
{  
  "id": "chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697",  
  "created": 1734366925,  
  "model": "claude-3-sonnet-20240229",  
  "object": "chat.completion.chunk",  
  "system_fingerprint": null,  
  "choices": [  
    {  
      "finish_reason": null,  
      "index": 0,  
      "delta": {  
        "content": "Hello",  
        "role": "assistant",  
        "function_call": null,  
        "tool_calls": null,  
        "audio": null  
      },  
      "logprobs": null  
    }  
  ]  
}  

```

### Exception handling​
LiteLLM maps exceptions across all supported providers to the OpenAI exceptions. All our exceptions inherit from OpenAI's exception types, so any error-handling you have for that, should work out of the box with LiteLLM.
```
from openai.error import OpenAIError  
from litellm import completion  
  
os.environ["ANTHROPIC_API_KEY"] = "bad-key"  
try:  
  # some code  
  completion(model="claude-instant-1", messages=[{"role": "user", "content": "Hey, how's it going?"}])  
except OpenAIError as e:  
  print(e)  

```

### Logging Observability - Log LLM Input/Output (Docs)​
LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, Helicone, Promptlayer, Traceloop, Slack
```
from litellm import completion  
  
## set env variables for logging tools (API key set up is not required when using MLflow)  
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key" # get your public key at https://app.lunary.ai/settings  
os.environ["HELICONE_API_KEY"] = "your-helicone-key"  
os.environ["LANGFUSE_PUBLIC_KEY"] = ""  
os.environ["LANGFUSE_SECRET_KEY"] = ""  
  
os.environ["OPENAI_API_KEY"]  
  
# set callbacks  
litellm.success_callback = ["lunary", "mlflow", "langfuse", "helicone"] # log input/output to lunary, mlflow, langfuse, helicone  
  
#openai call  
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}])  

```

### Track Costs, Usage, Latency for streaming​
Use a callback function for this - more info on custom callbacks: https://docs.litellm.ai/docs/observability/custom_callback
```
import litellm  
  
# track_cost_callback  
def track_cost_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  try:  
   response_cost = kwargs.get("response_cost", 0)  
   print("streaming response_cost", response_cost)  
  except:  
    pass  
# set callback  
litellm.success_callback = [track_cost_callback] # set custom callback function  
  
# litellm.completion() call  
response = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hi 👋 - i'm openai"  
    }  
  ],  
  stream=True  
)  

```

## **LiteLLM Proxy Server (LLM Gateway)**​
Track spend across multiple projects/people
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)
The proxy provides:
  1. Hooks for auth
  2. Hooks for logging
  3. Cost tracking
  4. Rate Limiting


### 📖 Proxy Endpoints - Swagger Docs​
Go here for a complete tutorial with keys + rate limits - **here**
### Quick Start Proxy - CLI​
```
pip install 'litellm[proxy]'  

```

#### Step 1: Start litellm proxy​
  * pip package
  * Docker container


```
$ litellm --model huggingface/bigcode/starcoder  
  
#INFO: Proxy running on http://0.0.0.0:4000  

```

Step 1. CREATE config.yaml 
Example `litellm_config.yaml`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/<your-azure-model-deployment>  
   api_base: os.environ/AZURE_API_BASE # runs os.getenv("AZURE_API_BASE")  
   api_key: os.environ/AZURE_API_KEY # runs os.getenv("AZURE_API_KEY")  
   api_version: "2023-07-01-preview"  

```

Step 2. RUN Docker Image
```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e AZURE_API_KEY=d6*********** \  
  -e AZURE_API_BASE=https://openai-***********/ \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm:main-latest \  
  --config /app/config.yaml --detailed_debug  

```

#### Step 2: Make ChatCompletions Request to Proxy​
```
import openai # openai v1.0.0+  
client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  

```

## More details​
  * exception mapping
  * retries + model fallbacks for completion()
  * proxy virtual keys & spend management
  * E2E Tutorial for LiteLLM Proxy Server


Next
LiteLLM Proxy Server
  * **Call 100+ LLMs using the OpenAI Input/Output Format**
  * How to use LiteLLM
    * **When to use LiteLLM Proxy Server (LLM Gateway)**
    * **When to use LiteLLM Python SDK**
  * **LiteLLM Python SDK**
    * Basic usage
    * Response Format (OpenAI Format)
    * Streaming
    * Streaming Response Format (OpenAI Format)
    * Exception handling
    * Logging Observability - Log LLM Input/Output (Docs)
    * Track Costs, Usage, Latency for streaming
  * **LiteLLM Proxy Server (LLM Gateway)**
    * 📖 Proxy Endpoints - Swagger Docs
    * Quick Start Proxy - CLI
  * More details


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Your Docusaurus site did not load properly.
A very common reason is a wrong site baseUrl configuration.
Current configured baseUrl = / (default value)
We suggest trying baseUrl = /
Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# LiteLLM - Getting Started
https://github.com/BerriAI/litellm
## **Call 100+ LLMs using the OpenAI Input/Output Format**​
  * Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints
  * Consistent output, text responses will always be available at `['choices'][0]['message']['content']`
  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router
  * Track spend & set budgets per project LiteLLM Proxy Server


## How to use LiteLLM​
You can use litellm through either:
  1. LiteLLM Proxy Server - Server (LLM Gateway) to call 100+ LLMs, load balance, cost tracking across projects
  2. LiteLLM python SDK - Python Client to call 100+ LLMs, load balance, cost tracking


### **When to use LiteLLM Proxy Server (LLM Gateway)**​
tip
Use LiteLLM Proxy Server if you want a **central service (LLM Gateway) to access multiple LLMs**
Typically used by Gen AI Enablement / ML PLatform Teams
  * LiteLLM Proxy gives you a unified interface to access multiple LLMs (100+ LLMs)
  * Track LLM Usage and setup guardrails
  * Customize Logging, Guardrails, Caching per project


### **When to use LiteLLM Python SDK**​
tip
Use LiteLLM Python SDK if you want to use LiteLLM in your **python code**
Typically used by developers building llm projects
  * LiteLLM SDK gives you a unified interface to access multiple LLMs (100+ LLMs) 
  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router


## **LiteLLM Python SDK**​
### Basic usage​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
```
pip install litellm  

```

  * OpenAI
  * Anthropic
  * VertexAI
  * NVIDIA
  * HuggingFace
  * Azure OpenAI
  * Ollama
  * Openrouter


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="claude-2",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
# auth: run 'gcloud auth application-default'  
os.environ["VERTEX_PROJECT"] = "hardy-device-386718"  
os.environ["VERTEX_LOCATION"] = "us-central1"  
  
response = completion(  
 model="chat-bison",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"  
os.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"  
  
response = completion(  
 model="nvidia_nim/<model_name>",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
os.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"  
  
# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints  
response = completion(  
 model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 api_base="https://my-endpoint.huggingface.cloud"  
)  
  
print(response)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
response = completion(  
 "azure/<your_deployment_name>",  
 messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
  
response = completion(  
      model="ollama/llama2",  
      messages = [{ "content": "Hello, how are you?","role": "user"}],  
      api_base="http://localhost:11434"  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"  
  
response = completion(  
 model="openrouter/google/palm-2-chat-bison",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
)  

```

### Streaming​
Set `stream=True` in the `completion` args. 
  * OpenAI
  * Anthropic
  * VertexAI
  * NVIDIA
  * HuggingFace
  * Azure OpenAI
  * Ollama
  * Openrouter


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="claude-2",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
# auth: run 'gcloud auth application-default'  
os.environ["VERTEX_PROJECT"] = "hardy-device-386718"  
os.environ["VERTEX_LOCATION"] = "us-central1"  
  
response = completion(  
 model="chat-bison",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"  
os.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"  
  
response = completion(  
 model="nvidia_nim/<model_name>",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
os.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"  
  
# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints  
response = completion(  
 model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 api_base="https://my-endpoint.huggingface.cloud",  
 stream=True,  
)  
  
print(response)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
response = completion(  
 "azure/<your_deployment_name>",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
  
response = completion(  
      model="ollama/llama2",  
      messages = [{ "content": "Hello, how are you?","role": "user"}],  
      api_base="http://localhost:11434",  
      stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"  
  
response = completion(  
 model="openrouter/google/palm-2-chat-bison",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

### Exception handling​
LiteLLM maps exceptions across all supported providers to the OpenAI exceptions. All our exceptions inherit from OpenAI's exception types, so any error-handling you have for that, should work out of the box with LiteLLM.
```
from openai.error import OpenAIError  
from litellm import completion  
  
os.environ["ANTHROPIC_API_KEY"] = "bad-key"  
try:  
  # some code  
  completion(model="claude-instant-1", messages=[{"role": "user", "content": "Hey, how's it going?"}])  
except OpenAIError as e:  
  print(e)  

```

### Logging Observability - Log LLM Input/Output (Docs)​
LiteLLM exposes pre defined callbacks to send data to MLflow, Lunary, Langfuse, Helicone, Promptlayer, Traceloop, Slack
```
from litellm import completion  
  
## set env variables for logging tools (API key set up is not required when using MLflow)  
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key" # get your key at https://app.lunary.ai/settings  
os.environ["HELICONE_API_KEY"] = "your-helicone-key"  
os.environ["LANGFUSE_PUBLIC_KEY"] = ""  
os.environ["LANGFUSE_SECRET_KEY"] = ""  
  
os.environ["OPENAI_API_KEY"]  
  
# set callbacks  
litellm.success_callback = ["lunary", "mlflow", "langfuse", "helicone"] # log input/output to lunary, mlflow, langfuse, helicone  
  
#openai call  
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}])  

```

### Track Costs, Usage, Latency for streaming​
Use a callback function for this - more info on custom callbacks: https://docs.litellm.ai/docs/observability/custom_callback
```
import litellm  
  
# track_cost_callback  
def track_cost_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  try:  
   response_cost = kwargs.get("response_cost", 0)  
   print("streaming response_cost", response_cost)  
  except:  
    pass  
# set callback  
litellm.success_callback = [track_cost_callback] # set custom callback function  
  
# litellm.completion() call  
response = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hi 👋 - i'm openai"  
    }  
  ],  
  stream=True  
)  

```

## **LiteLLM Proxy Server (LLM Gateway)**​
Track spend across multiple projects/people
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)
The proxy provides:
  1. Hooks for auth
  2. Hooks for logging
  3. Cost tracking
  4. Rate Limiting


### 📖 Proxy Endpoints - Swagger Docs​
Go here for a complete tutorial with keys + rate limits - **here**
### Quick Start Proxy - CLI​
```
pip install 'litellm[proxy]'  

```

#### Step 1: Start litellm proxy​
  * pip package
  * Docker container


```
$ litellm --model huggingface/bigcode/starcoder  
  
#INFO: Proxy running on http://0.0.0.0:4000  

```

### Step 1. CREATE config.yaml​
Example `litellm_config.yaml`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/<your-azure-model-deployment>  
   api_base: os.environ/AZURE_API_BASE # runs os.getenv("AZURE_API_BASE")  
   api_key: os.environ/AZURE_API_KEY # runs os.getenv("AZURE_API_KEY")  
   api_version: "2023-07-01-preview"  

```

### Step 2. RUN Docker Image​
```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e AZURE_API_KEY=d6*********** \  
  -e AZURE_API_BASE=https://openai-***********/ \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm:main-latest \  
  --config /app/config.yaml --detailed_debug  

```

#### Step 2: Make ChatCompletions Request to Proxy​
```
import openai # openai v1.0.0+  
client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  

```

## More details​
  * exception mapping
  * E2E Tutorial for LiteLLM Proxy Server
  * proxy virtual keys & spend management


  * **Call 100+ LLMs using the OpenAI Input/Output Format**
  * How to use LiteLLM
    * **When to use LiteLLM Proxy Server (LLM Gateway)**
    * **When to use LiteLLM Python SDK**
  * **LiteLLM Python SDK**
    * Basic usage
    * Streaming
    * Exception handling
    * Logging Observability - Log LLM Input/Output (Docs)
    * Track Costs, Usage, Latency for streaming
  * **LiteLLM Proxy Server (LLM Gateway)**
    * 📖 Proxy Endpoints - Swagger Docs
    * Quick Start Proxy - CLI
    * Step 1. CREATE config.yaml
    * Step 2. RUN Docker Image
  * More details


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM - Getting Started


On this page
# LiteLLM - Getting Started
https://github.com/BerriAI/litellm
## **Call 100+ LLMs using the OpenAI Input/Output Format**​
  * Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints
  * Consistent output, text responses will always be available at `['choices'][0]['message']['content']`
  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router
  * Track spend & set budgets per project LiteLLM Proxy Server


## How to use LiteLLM​
You can use litellm through either:
  1. LiteLLM Proxy Server - Server (LLM Gateway) to call 100+ LLMs, load balance, cost tracking across projects
  2. LiteLLM python SDK - Python Client to call 100+ LLMs, load balance, cost tracking


### **When to use LiteLLM Proxy Server (LLM Gateway)**​
tip
Use LiteLLM Proxy Server if you want a **central service (LLM Gateway) to access multiple LLMs**
Typically used by Gen AI Enablement / ML PLatform Teams
  * LiteLLM Proxy gives you a unified interface to access multiple LLMs (100+ LLMs)
  * Track LLM Usage and setup guardrails
  * Customize Logging, Guardrails, Caching per project


### **When to use LiteLLM Python SDK**​
tip
Use LiteLLM Python SDK if you want to use LiteLLM in your **python code**
Typically used by developers building llm projects
  * LiteLLM SDK gives you a unified interface to access multiple LLMs (100+ LLMs) 
  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router


## **LiteLLM Python SDK**​
### Basic usage​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
```
pip install litellm  

```

  * OpenAI
  * Anthropic
  * xAI
  * VertexAI
  * NVIDIA
  * HuggingFace
  * Azure OpenAI
  * Ollama
  * Openrouter


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="openai/gpt-4o",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="anthropic/claude-3-sonnet-20240229",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["XAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="xai/grok-2-latest",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
# auth: run 'gcloud auth application-default'  
os.environ["VERTEXAI_PROJECT"] = "hardy-device-386718"  
os.environ["VERTEXAI_LOCATION"] = "us-central1"  
  
response = completion(  
 model="vertex_ai/gemini-1.5-pro",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"  
os.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"  
  
response = completion(  
 model="nvidia_nim/<model_name>",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
import os  
  
os.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"  
  
# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints  
response = completion(  
 model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 api_base="https://my-endpoint.huggingface.cloud"  
)  
  
print(response)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
response = completion(  
 "azure/<your_deployment_name>",  
 messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

```
from litellm import completion  
  
response = completion(  
      model="ollama/llama2",  
      messages = [{ "content": "Hello, how are you?","role": "user"}],  
      api_base="http://localhost:11434"  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"  
  
response = completion(  
 model="openrouter/google/palm-2-chat-bison",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
)  

```

### Response Format (OpenAI Format)​
```
{  
  "id": "chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885",  
  "created": 1734366691,  
  "model": "claude-3-sonnet-20240229",  
  "object": "chat.completion",  
  "system_fingerprint": null,  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": "Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?",  
        "role": "assistant",  
        "tool_calls": null,  
        "function_call": null  
      }  
    }  
  ],  
  "usage": {  
    "completion_tokens": 43,  
    "prompt_tokens": 13,  
    "total_tokens": 56,  
    "completion_tokens_details": null,  
    "prompt_tokens_details": {  
      "audio_tokens": null,  
      "cached_tokens": 0  
    },  
    "cache_creation_input_tokens": 0,  
    "cache_read_input_tokens": 0  
  }  
}  

```

### Streaming​
Set `stream=True` in the `completion` args. 
  * OpenAI
  * Anthropic
  * xAI
  * VertexAI
  * NVIDIA
  * HuggingFace
  * Azure OpenAI
  * Ollama
  * Openrouter


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="openai/gpt-4o",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="anthropic/claude-3-sonnet-20240229",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["XAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="xai/grok-2-latest",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
# auth: run 'gcloud auth application-default'  
os.environ["VERTEX_PROJECT"] = "hardy-device-386718"  
os.environ["VERTEX_LOCATION"] = "us-central1"  
  
response = completion(  
 model="vertex_ai/gemini-1.5-pro",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"  
os.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"  
  
response = completion(  
 model="nvidia_nim/<model_name>",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
 stream=True,  
)  

```

```
from litellm import completion  
import os  
  
os.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"  
  
# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints  
response = completion(  
 model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 api_base="https://my-endpoint.huggingface.cloud",  
 stream=True,  
)  
  
print(response)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
response = completion(  
 "azure/<your_deployment_name>",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

```
from litellm import completion  
  
response = completion(  
      model="ollama/llama2",  
      messages = [{ "content": "Hello, how are you?","role": "user"}],  
      api_base="http://localhost:11434",  
      stream=True,  
)  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"  
  
response = completion(  
 model="openrouter/google/palm-2-chat-bison",  
 messages = [{ "content": "Hello, how are you?","role": "user"}],  
 stream=True,  
)  

```

### Streaming Response Format (OpenAI Format)​
```
{  
  "id": "chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697",  
  "created": 1734366925,  
  "model": "claude-3-sonnet-20240229",  
  "object": "chat.completion.chunk",  
  "system_fingerprint": null,  
  "choices": [  
    {  
      "finish_reason": null,  
      "index": 0,  
      "delta": {  
        "content": "Hello",  
        "role": "assistant",  
        "function_call": null,  
        "tool_calls": null,  
        "audio": null  
      },  
      "logprobs": null  
    }  
  ]  
}  

```

### Exception handling​
LiteLLM maps exceptions across all supported providers to the OpenAI exceptions. All our exceptions inherit from OpenAI's exception types, so any error-handling you have for that, should work out of the box with LiteLLM.
```
from openai.error import OpenAIError  
from litellm import completion  
  
os.environ["ANTHROPIC_API_KEY"] = "bad-key"  
try:  
  # some code  
  completion(model="claude-instant-1", messages=[{"role": "user", "content": "Hey, how's it going?"}])  
except OpenAIError as e:  
  print(e)  

```

### Logging Observability - Log LLM Input/Output (Docs)​
LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, Helicone, Promptlayer, Traceloop, Slack
```
from litellm import completion  
  
## set env variables for logging tools (API key set up is not required when using MLflow)  
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key" # get your public key at https://app.lunary.ai/settings  
os.environ["HELICONE_API_KEY"] = "your-helicone-key"  
os.environ["LANGFUSE_PUBLIC_KEY"] = ""  
os.environ["LANGFUSE_SECRET_KEY"] = ""  
  
os.environ["OPENAI_API_KEY"]  
  
# set callbacks  
litellm.success_callback = ["lunary", "mlflow", "langfuse", "helicone"] # log input/output to lunary, mlflow, langfuse, helicone  
  
#openai call  
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}])  

```

### Track Costs, Usage, Latency for streaming​
Use a callback function for this - more info on custom callbacks: https://docs.litellm.ai/docs/observability/custom_callback
```
import litellm  
  
# track_cost_callback  
def track_cost_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  try:  
   response_cost = kwargs.get("response_cost", 0)  
   print("streaming response_cost", response_cost)  
  except:  
    pass  
# set callback  
litellm.success_callback = [track_cost_callback] # set custom callback function  
  
# litellm.completion() call  
response = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hi 👋 - i'm openai"  
    }  
  ],  
  stream=True  
)  

```

## **LiteLLM Proxy Server (LLM Gateway)**​
Track spend across multiple projects/people
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)
The proxy provides:
  1. Hooks for auth
  2. Hooks for logging
  3. Cost tracking
  4. Rate Limiting


### 📖 Proxy Endpoints - Swagger Docs​
Go here for a complete tutorial with keys + rate limits - **here**
### Quick Start Proxy - CLI​
```
pip install 'litellm[proxy]'  

```

#### Step 1: Start litellm proxy​
  * pip package
  * Docker container


```
$ litellm --model huggingface/bigcode/starcoder  
  
#INFO: Proxy running on http://0.0.0.0:4000  

```

Step 1. CREATE config.yaml 
Example `litellm_config.yaml`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/<your-azure-model-deployment>  
   api_base: os.environ/AZURE_API_BASE # runs os.getenv("AZURE_API_BASE")  
   api_key: os.environ/AZURE_API_KEY # runs os.getenv("AZURE_API_KEY")  
   api_version: "2023-07-01-preview"  

```

Step 2. RUN Docker Image
```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e AZURE_API_KEY=d6*********** \  
  -e AZURE_API_BASE=https://openai-***********/ \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm:main-latest \  
  --config /app/config.yaml --detailed_debug  

```

#### Step 2: Make ChatCompletions Request to Proxy​
```
import openai # openai v1.0.0+  
client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  

```

## More details​
  * exception mapping
  * retries + model fallbacks for completion()
  * proxy virtual keys & spend management
  * E2E Tutorial for LiteLLM Proxy Server


Next
LiteLLM Proxy Server
  * **Call 100+ LLMs using the OpenAI Input/Output Format**
  * How to use LiteLLM
    * **When to use LiteLLM Proxy Server (LLM Gateway)**
    * **When to use LiteLLM Python SDK**
  * **LiteLLM Python SDK**
    * Basic usage
    * Response Format (OpenAI Format)
    * Streaming
    * Streaming Response Format (OpenAI Format)
    * Exception handling
    * Logging Observability - Log LLM Input/Output (Docs)
    * Track Costs, Usage, Latency for streaming
  * **LiteLLM Proxy Server (LLM Gateway)**
    * 📖 Proxy Endpoints - Swagger Docs
    * Quick Start Proxy - CLI
  * More details


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
    * Benchmarks
    * LiteLLM Proxy - 1K RPS Load test on locust
    * LiteLLM SDK vs OpenAI
    * Multi-Instance TPM/RPM (litellm.Router)
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Load Testing
  * Benchmarks


On this page
# Benchmarks
Benchmarks for LiteLLM Gateway (Proxy Server) tested against a fake OpenAI endpoint.
Use this config for testing:
**Note:** we're currently migrating to aiohttp which has 10x higher throughput. We recommend using the `aiohttp_openai/` provider for load testing.
```
model_list:  
 - model_name: "fake-openai-endpoint"  
  litellm_params:  
   model: aiohttp_openai/any  
   api_base: https://your-fake-openai-endpoint.com/chat/completions  
   api_key: "test"  

```

### 1 Instance LiteLLM Proxy​
In these tests the median latency of directly calling the fake-openai-endpoint is 60ms.
Metric| Litellm Proxy (1 Instance)  
---|---  
RPS| 475  
Median Latency (ms)| 100  
Latency overhead added by LiteLLM Proxy| 40ms  
#### Key Findings​
  * Single instance: 475 RPS @ 100ms latency
  * 2 LiteLLM instances: 950 RPS @ 100ms latency
  * 4 LiteLLM instances: 1900 RPS @ 100ms latency


### 2 Instances​
**Adding 1 instance, will double the RPS and maintain the`100ms-110ms` median latency.**
Metric| Litellm Proxy (2 Instances)  
---|---  
Median Latency (ms)| 100  
RPS| 950  
## Machine Spec used for testing​
Each machine deploying LiteLLM had the following specs:
  * 2 CPU
  * 4GB RAM


## Logging Callbacks​
### GCS Bucket Logging​
Using GCS Bucket has **no impact on latency, RPS compared to Basic Litellm Proxy**
Metric| Basic Litellm Proxy| LiteLLM Proxy with GCS Bucket Logging  
---|---|---  
RPS| 1133.2| 1137.3  
Median Latency (ms)| 140| 138  
### LangSmith logging​
Using LangSmith has **no impact on latency, RPS compared to Basic Litellm Proxy**
Metric| Basic Litellm Proxy| LiteLLM Proxy with LangSmith  
---|---|---  
RPS| 1133.2| 1135  
Median Latency (ms)| 140| 132  
## Locust Settings​
  * 2500 Users
  * 100 user Ramp Up


Previous
Custom Prompt Management
Next
LiteLLM Proxy - 1K RPS Load test on locust
  * 1 Instance LiteLLM Proxy
  * 2 Instances
  * Machine Spec used for testing
  * Logging Callbacks
    * GCS Bucket Logging
    * LangSmith logging
  * Locust Settings


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Exception Mapping


On this page
# Exception Mapping
LiteLLM maps exceptions across all providers to their OpenAI counterparts.
All exceptions can be imported from `litellm` - e.g. `from litellm import BadRequestError`
## LiteLLM Exceptions​
Status Code| Error Type| Inherits from| Description  
---|---|---|---  
400| BadRequestError| openai.BadRequestError|   
400| UnsupportedParamsError| litellm.BadRequestError| Raised when unsupported params are passed  
400| ContextWindowExceededError| litellm.BadRequestError| Special error type for context window exceeded error messages - enables context window fallbacks  
400| ContentPolicyViolationError| litellm.BadRequestError| Special error type for content policy violation error messages - enables content policy fallbacks  
400| InvalidRequestError| openai.BadRequestError| Deprecated error, use BadRequestError instead  
401| AuthenticationError| openai.AuthenticationError|   
403| PermissionDeniedError| openai.PermissionDeniedError|   
404| NotFoundError| openai.NotFoundError| raise when invalid models passed, example gpt-8  
408| Timeout| openai.APITimeoutError| Raised when a timeout occurs  
422| UnprocessableEntityError| openai.UnprocessableEntityError|   
429| RateLimitError| openai.RateLimitError|   
500| APIConnectionError| openai.APIConnectionError| If any unmapped error is returned, we return this error  
500| APIError| openai.APIError| Generic 500-status code error  
503| ServiceUnavailableError| openai.APIStatusError| If provider returns a service unavailable error, this error is raised  
>=500| InternalServerError| openai.InternalServerError| If any unmapped 500-status code error is returned, this error is raised  
N/A| APIResponseValidationError| openai.APIResponseValidationError| If Rules are used, and request/response fails a rule, this error is raised  
N/A| BudgetExceededError| Exception| Raised for proxy, when budget is exceeded  
N/A| JSONSchemaValidationError| litellm.APIResponseValidationError| Raised when response does not match expected json schema - used if `response_schema` param passed in with `enforce_validation=True`  
N/A| MockException| Exception| Internal exception, raised by mock_completion class. Do not use directly  
N/A| OpenAIError| openai.OpenAIError| Deprecated internal exception, inherits from openai.OpenAIError.  
Base case we return APIConnectionError
All our exceptions inherit from OpenAI's exception types, so any error-handling you have for that, should work out of the box with LiteLLM. 
For all cases, the exception returned inherits from the original OpenAI Exception but contains 3 additional attributes: 
  * status_code - the http status code of the exception
  * message - the error message
  * llm_provider - the provider raising the exception


## Usage​
```
import litellm  
import openai  
  
try:  
  response = litellm.completion(  
        model="gpt-4",  
        messages=[  
          {  
            "role": "user",  
            "content": "hello, write a 20 pageg essay"  
          }  
        ],  
        timeout=0.01, # this will raise a timeout exception  
      )  
except openai.APITimeoutError as e:  
  print("Passed: Raised correct exception. Got openai.APITimeoutError\nGood Job", e)  
  print(type(e))  
  pass  

```

## Usage - Catching Streaming Exceptions​
```
import litellm  
try:  
  response = litellm.completion(  
    model="gpt-3.5-turbo",  
    messages=[  
      {  
        "role": "user",  
        "content": "hello, write a 20 pg essay"  
      }  
    ],  
    timeout=0.0001, # this will raise an exception  
    stream=True,  
  )  
  for chunk in response:  
    print(chunk)  
except openai.APITimeoutError as e:  
  print("Passed: Raised correct exception. Got openai.APITimeoutError\nGood Job", e)  
  print(type(e))  
  pass  
except Exception as e:  
  print(f"Did not raise error `openai.APITimeoutError`. Instead raised error type: {type(e)}, Error: {e}")  
  

```

## Usage - Should you retry exception?​
```
import litellm  
import openai  
  
try:  
  response = litellm.completion(  
        model="gpt-4",  
        messages=[  
          {  
            "role": "user",  
            "content": "hello, write a 20 pageg essay"  
          }  
        ],  
        timeout=0.01, # this will raise a timeout exception  
      )  
except openai.APITimeoutError as e:  
  should_retry = litellm._should_retry(e.status_code)  
  print(f"should_retry: {should_retry}")  

```

## Details​
To see how it's implemented - check out the code
Create an issue **or** make a PR if you want to improve the exception mapping. 
**Note** For OpenAI and Azure we return the original exception (since they're of the OpenAI Error type). But we add the 'llm_provider' attribute to them. See code
## Custom mapping list​
Base case - we return `litellm.APIConnectionError` exception (inherits from openai's APIConnectionError exception).
custom_llm_provider| Timeout| ContextWindowExceededError| BadRequestError| NotFoundError| ContentPolicyViolationError| AuthenticationError| APIError| RateLimitError| ServiceUnavailableError| PermissionDeniedError| UnprocessableEntityError  
---|---|---|---|---|---|---|---|---|---|---|---  
openai| ✓| ✓| ✓| | ✓| ✓| | | | |   
watsonx| | | | | | | | ✓| | |   
text-completion-openai| ✓| ✓| ✓| | ✓| ✓| | | | |   
custom_openai| ✓| ✓| ✓| | ✓| ✓| | | | |   
openai_compatible_providers| ✓| ✓| ✓| | ✓| ✓| | | | |   
anthropic| ✓| ✓| ✓| ✓| | ✓| | | ✓| ✓|   
replicate| ✓| ✓| ✓| ✓| | ✓| | ✓| ✓| |   
bedrock| ✓| ✓| ✓| ✓| | ✓| | ✓| ✓| ✓|   
sagemaker| | ✓| ✓| | | | | | | |   
vertex_ai| ✓| | ✓| | | | ✓| | | | ✓  
palm| ✓| ✓| | | | | ✓| | | |   
gemini| ✓| ✓| | | | | ✓| | | |   
cloudflare| | | ✓| | | ✓| | | | |   
cohere| | ✓| ✓| | | ✓| | | ✓| |   
cohere_chat| | ✓| ✓| | | ✓| | | ✓| |   
huggingface| ✓| ✓| ✓| | | ✓| | ✓| ✓| |   
ai21| ✓| ✓| ✓| ✓| | ✓| | ✓| | |   
nlp_cloud| ✓| ✓| ✓| | | ✓| ✓| ✓| ✓| |   
together_ai| ✓| ✓| ✓| | | ✓| | | | |   
aleph_alpha| | | ✓| | | ✓| | | | |   
ollama| ✓| | ✓| | | | | | ✓| |   
ollama_chat| ✓| | ✓| | | | | | ✓| |   
vllm| | | | | | ✓| ✓| | | |   
azure| ✓| ✓| ✓| ✓| ✓| ✓| | | ✓| |   
  * "✓" indicates that the specified `custom_llm_provider` can raise the corresponding exception.
  * Empty cells indicate the lack of association or that the provider does not raise that particular exception type as indicated by the function.


> For a deeper understanding of these exceptions, you can check out this implementation for additional insights.
The `ContextWindowExceededError` is a sub-class of `InvalidRequestError`. It was introduced to provide more granularity for exception-handling scenarios. Please refer to this issue to learn more.
Contributions to improve exception mapping are welcome
Previous
Snowflake
Next
Provider-specific Params
  * LiteLLM Exceptions
  * Usage
  * Usage - Catching Streaming Exceptions
  * Usage - Should you retry exception?
  * Details
  * Custom mapping list


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
    * Data Privacy and Security
    * Data Retention Policy
    * Migration Policy
    * ❤️ 🚅 Projects built on LiteLLM
    * PII Masking - LiteLLM Gateway (Deprecated Version)
    * Code Quality
    * Rules
    * [DEPRECATED] Team-based Routing
    * [DEPRECATED] Region-based Routing
    * [OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
  * Support & Talk with founders


  *   * Extras
  * Data Privacy and Security


On this page
# Data Privacy and Security
At LiteLLM, **safeguarding your data privacy and security** is our top priority. We recognize the critical importance of the data you share with us and handle it with the highest level of diligence.
With LiteLLM Cloud, we handle:
  * Deployment
  * Scaling
  * Upgrades and security patches
  * Ensuring high availability


## Security Measures​
### LiteLLM Cloud​
  * We encrypt all data stored using your `LITELLM_MASTER_KEY` and in transit using TLS.
  * Our database and application run on GCP, AWS infrastructure, partly managed by NeonDB.
    * US data region: Northern California (AWS/GCP `us-west-1`) & Virginia (AWS `us-east-1`)
    * EU data region Germany/Frankfurt (AWS/GCP `eu-central-1`)
  * All users have access to SSO (Single Sign-On) through OAuth 2.0 with Google, Okta, Microsoft, KeyCloak. 
  * Audit Logs with retention policy
  * Control Allowed IP Addresses that can access your Cloud LiteLLM Instance


### Self-hosted Instances LiteLLM​
  * **No data or telemetry is stored on LiteLLM Servers when you self-host**
  * For installation and configuration, see: Self-hosting guide
  * **Telemetry** : We run no telemetry when you self-host LiteLLM


For security inquiries, please contact us at support@berri.ai
## **Security Certifications**​
**Certification**| **Status**  
---|---  
SOC 2 Type I| Certified. Report available upon request on Enterprise plan.  
SOC 2 Type II| In progress. Certificate available by April 15th, 2025  
ISO 27001| Certified. Report available upon request on Enterprise  
## Supported Data Regions for LiteLLM Cloud​
LiteLLM supports the following data regions:
  * US, Northern California (AWS/GCP `us-west-1`)
  * Europe, Frankfurt, Germany (AWS/GCP `eu-central-1`)


All data, user accounts, and infrastructure are completely separated between these two regions
## Collection of Personal Data​
### For Self-hosted LiteLLM Users:​
  * No personal data is collected or transmitted to LiteLLM servers when you self-host our software.
  * Any data generated or processed remains entirely within your own infrastructure.


### For LiteLLM Cloud Users:​
  * LiteLLM Cloud tracks LLM usage data - We do not access or store the message / response content of your API requests or responses. You can see the fields tracked here


**How to Use and Share the Personal Data**
  * Only proxy admins can view their usage data, and they can only see the usage data of their organization.
  * Proxy admins have the ability to invite other users / admins to their server to view their own usage data
  * LiteLLM Cloud does not sell or share any usage data with any third parties.


## Cookies Information, Security, and Privacy​
### For Self-hosted LiteLLM Users:​
  * Cookie data remains within your own infrastructure.
  * LiteLLM uses minimal cookies, solely for the purpose of allowing Proxy users to access the LiteLLM Admin UI.
  * These cookies are stored in your web browser after you log in.
  * We do not use cookies for advertising, tracking, or any purpose beyond maintaining your login session.
  * The only cookies used are essential for maintaining user authentication and session management for the app UI.
  * Session cookies expire when you close your browser, logout or after 24 hours.
  * LiteLLM does not use any third-party cookies.
  * The Admin UI accesses the cookie to authenticate your login session.
  * The cookie is stored as JWT and is not accessible to any other part of the system.
  * We (LiteLLM) do not access or share this cookie data for any other purpose.


### For LiteLLM Cloud Users:​
  * LiteLLM uses minimal cookies, solely for the purpose of allowing Proxy users to access the LiteLLM Admin UI.
  * These cookies are stored in your web browser after you log in.
  * We do not use cookies for advertising, tracking, or any purpose beyond maintaining your login session.
  * The only cookies used are essential for maintaining user authentication and session management for the app UI.
  * Session cookies expire when you close your browser, logout or after 24 hours.
  * LiteLLM does not use any third-party cookies.
  * The Admin UI accesses the cookie to authenticate your login session.
  * The cookie is stored as JWT and is not accessible to any other part of the system.
  * We (LiteLLM) do not access or share this cookie data for any other purpose.


## Security Vulnerability Reporting Guidelines​
We value the security community's role in protecting our systems and users. To report a security vulnerability:
  * Email support@berri.ai with details
  * Include steps to reproduce the issue
  * Provide any relevant additional information


We'll review all reports promptly. Note that we don't currently offer a bug bounty program.
## Vulnerability Scanning​
  * LiteLLM runs `grype` security scans on all built Docker images.
    * See `grype litellm` check on ci/cd. 
    * Current Status: ✅ Passing. 0 High/Critical severity vulnerabilities found.


## Legal/Compliance FAQs​
### Procurement Options​
  1. Invoicing
  2. AWS Marketplace
  3. Azure Marketplace


### Vendor Information​
Legal Entity Name: Berrie AI Incorporated
Company Phone Number: 7708783106 
Point of contact email address for security incidents: krrish@berri.ai
Point of contact email address for general security-related questions: krrish@berri.ai
Has the Vendor been audited / certified? 
  * SOC 2 Type I. Certified. Report available upon request on Enterprise plan.
  * SOC 2 Type II. In progress. Certificate available by April 15th, 2025.
  * ISO 27001. Certified. Report available upon request on Enterprise plan.


Has an information security management system been implemented? 
  * Yes - CodeQL and a comprehensive ISMS covering multiple security domains.


Is logging of key events - auth, creation, update changes occurring? 
  * Yes - we have audit logs


Does the Vendor have an established Cybersecurity incident management program? 
  * Yes, Incident Response Policy available upon request.


Does the vendor have a vulnerability disclosure policy in place? Yes
Does the vendor perform vulnerability scans? 
  * Yes, regular vulnerability scans are conducted as detailed in the Vulnerability Scanning section.


Signer Name: Krish Amit Dholakia
Signer Email: krrish@berri.ai
Previous
Contributing - UI
Next
Data Retention Policy
  * Security Measures
    * LiteLLM Cloud
    * Self-hosted Instances LiteLLM
  * **Security Certifications**
  * Supported Data Regions for LiteLLM Cloud
  * Collection of Personal Data
    * For Self-hosted LiteLLM Users:
    * For LiteLLM Cloud Users:
  * Cookies Information, Security, and Privacy
    * For Self-hosted LiteLLM Users:
    * For LiteLLM Cloud Users:
  * Security Vulnerability Reporting Guidelines
  * Vulnerability Scanning
  * Legal/Compliance FAQs
    * Procurement Options
    * Vendor Information


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
    * Contributing Code
    * Adding Providers
    * Contributing to Documentation
    * Contributing - UI
  * Extras
  * Support & Talk with founders


  *   * Contributing
  * Contributing Code


On this page
# Contributing Code
## **Checklist before submitting a PR**​
Here are the core requirements for any PR submitted to LiteLLM
  * Add testing, **Adding at least 1 test is a hard requirement** - see details
  * Ensure your PR passes the following tests:
    * Unit Tests
    * Formatting / Linting Tests
  * Keep scope as isolated as possible. As a general rule, your changes should address 1 specific problem at a time


## Quick start​
## 1. Setup your local dev environment​
Here's how to modify the repo locally:
Step 1: Clone the repo
```
git clone https://github.com/BerriAI/litellm.git  

```

Step 2: Install dev dependencies:
```
poetry install --with dev --extras proxy  

```

That's it, your local dev environment is ready!
## 2. Adding Testing to your PR​
  * Add your test to the `tests/litellm/` directory
  * This directory 1:1 maps the the `litellm/` directory, and can only contain mocked tests.
  * Do not add real llm api calls to this directory.


### 2.1 File Naming Convention for `tests/litellm/`​
The `tests/litellm/` directory follows the same directory structure as `litellm/`.
  * `litellm/proxy/test_caching_routes.py` maps to `litellm/proxy/caching_routes.py`
  * `test_{filename}.py` maps to `litellm/{filename}.py`


## 3. Running Unit Tests​
run the following command on the root of the litellm directory
```
make test-unit  

```

## 3.5 Running Linting Tests​
run the following command on the root of the litellm directory
```
make lint  

```

LiteLLM uses mypy for linting. On ci/cd we also run `black` for formatting.
## 4. Submit a PR with your changes!​
  * push your fork to your GitHub repo
  * submit a PR from there


## Advanced​
### Building LiteLLM Docker Image​
Some people might want to build the LiteLLM docker image themselves. Follow these instructions if you want to build / run the LiteLLM Docker Image yourself.
Step 1: Clone the repo
```
git clone https://github.com/BerriAI/litellm.git  

```

Step 2: Build the Docker Image
Build using Dockerfile.non_root
```
docker build -f docker/Dockerfile.non_root -t litellm_test_image .  

```

Step 3: Run the Docker Image
Make sure config.yaml is present in the root directory. This is your litellm proxy config file.
```
docker run \  
  -v $(pwd)/proxy_config.yaml:/app/config.yaml \  
  -e DATABASE_URL="postgresql://xxxxxxxx" \  
  -e LITELLM_MASTER_KEY="sk-1234" \  
  -p 4000:4000 \  
  litellm_test_image \  
  --config /app/config.yaml --detailed_debug  

```

Previous
Model Fallbacks w/ LiteLLM
Next
Directory Structure
  * **Checklist before submitting a PR**
  * Quick start
  * 1. Setup your local dev environment
  * 2. Adding Testing to your PR
    * 2.1 File Naming Convention for `tests/litellm/`
  * 3. Running Unit Tests
  * 3.5 Running Linting Tests
  * 4. Submit a PR with your changes!
  * Advanced
    * Building LiteLLM Docker Image


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Reliability - Retries, Fallbacks


On this page
# Reliability - Retries, Fallbacks
LiteLLM helps prevent failed requests in 2 ways: 
  * Retries
  * Fallbacks: Context Window + General


## Helper utils​
LiteLLM supports the following functions for reliability:
  * `litellm.longer_context_model_fallback_dict`: Dictionary which has a mapping for those models which have larger equivalents 
  * `num_retries`: use tenacity retries
  * `completion()` with fallbacks: switch between models/keys/api bases in case of errors. 


## Retry failed requests​
Call it in completion like this `completion(..num_retries=2)`.
Here's a quick look at how you can use it: 
```
from litellm import completion  
  
user_message = "Hello, whats the weather in San Francisco??"  
messages = [{"content": user_message, "role": "user"}]  
  
# normal call   
response = completion(  
      model="gpt-3.5-turbo",  
      messages=messages,  
      num_retries=2  
    )  

```

## Fallbacks (SDK)​
info
See how to do on PROXY
### Context Window Fallbacks (SDK)​
```
from litellm import completion  
  
fallback_dict = {"gpt-3.5-turbo": "gpt-3.5-turbo-16k"}  
messages = [{"content": "how does a court case get to the Supreme Court?" * 500, "role": "user"}]  
  
completion(model="gpt-3.5-turbo", messages=messages, context_window_fallback_dict=fallback_dict)  

```

### Fallbacks - Switch Models/API Keys/API Bases (SDK)​
LLM APIs can be unstable, completion() with fallbacks ensures you'll always get a response from your calls
#### Usage​
To use fallback models with `completion()`, specify a list of models in the `fallbacks` parameter. 
The `fallbacks` list should include the primary model you want to use, followed by additional models that can be used as backups in case the primary model fails to provide a response.
#### switch models​
```
response = completion(model="bad-model", messages=messages,   
  fallbacks=["gpt-3.5-turbo" "command-nightly"])  

```

#### switch api keys/bases (E.g. azure deployment)​
Switch between different keys for the same azure deployment, or use another deployment as well. 
```
api_key="bad-key"  
response = completion(model="azure/gpt-4", messages=messages, api_key=api_key,  
  fallbacks=[{"api_key": "good-key-1"}, {"api_key": "good-key-2", "api_base": "good-api-base-2"}])  

```

Check out this section for implementation details
## Implementation Details (SDK)​
### Fallbacks​
#### Output from calls​
```
Completion with 'bad-model': got exception Unable to map your input to a model. Check your input - {'model': 'bad-model'  
  
  
  
completion call gpt-3.5-turbo  
{  
 "id": "chatcmpl-7qTmVRuO3m3gIBg4aTmAumV1TmQhB",  
 "object": "chat.completion",  
 "created": 1692741891,  
 "model": "gpt-3.5-turbo-0613",  
 "choices": [  
  {  
   "index": 0,  
   "message": {  
    "role": "assistant",  
    "content": "I apologize, but as an AI, I do not have the capability to provide real-time weather updates. However, you can easily check the current weather in San Francisco by using a search engine or checking a weather website or app."  
   },  
   "finish_reason": "stop"  
  }  
 ],  
 "usage": {  
  "prompt_tokens": 16,  
  "completion_tokens": 46,  
  "total_tokens": 62  
 }  
}  
  

```

#### How does fallbacks work​
When you pass `fallbacks` to `completion`, it makes the first `completion` call using the primary model specified as `model` in `completion(model=model)`. If the primary model fails or encounters an error, it automatically tries the `fallbacks` models in the specified order. This ensures a response even if the primary model is unavailable.
#### Key components of Model Fallbacks implementation:​
  * Looping through `fallbacks`
  * Cool-Downs for rate-limited models


#### Looping through `fallbacks`​
Allow `45seconds` for each request. In the 45s this function tries calling the primary model set as `model`. If model fails it loops through the backup `fallbacks` models and attempts to get a response in the allocated `45s` time set here: 
```
while response == None and time.time() - start_time < 45:  
    for model in fallbacks:  

```

#### Cool-Downs for rate-limited models​
If a model API call leads to an error - allow it to cooldown for `60s`
```
except Exception as e:  
 print(f"got exception {e} for model {model}")  
 rate_limited_models.add(model)  
 model_expiration_times[model] = (  
   time.time() + 60  
 ) # cool down this selected model  
 pass  

```

Before making an LLM API call we check if the selected model is in `rate_limited_models`, if so skip making the API call
```
if (  
 model in rate_limited_models  
): # check if model is currently cooling down  
 if (  
   model_expiration_times.get(model)  
   and time.time() >= model_expiration_times[model]  
 ):  
   rate_limited_models.remove(  
     model  
   ) # check if it's been 60s of cool down and remove model  
 else:  
   continue # skip model  
  

```

#### Full code of completion with fallbacks()​
```
  
  response = None  
  rate_limited_models = set()  
  model_expiration_times = {}  
  start_time = time.time()  
  fallbacks = [kwargs["model"]] + kwargs["fallbacks"]  
  del kwargs["fallbacks"] # remove fallbacks so it's not recursive  
  
  while response == None and time.time() - start_time < 45:  
    for model in fallbacks:  
      # loop thru all models  
      try:  
        if (  
          model in rate_limited_models  
        ): # check if model is currently cooling down  
          if (  
            model_expiration_times.get(model)  
            and time.time() >= model_expiration_times[model]  
          ):  
            rate_limited_models.remove(  
              model  
            ) # check if it's been 60s of cool down and remove model  
          else:  
            continue # skip model  
  
        # delete model from kwargs if it exists  
        if kwargs.get("model"):  
          del kwargs["model"]  
  
        print("making completion call", model)  
        response = litellm.completion(**kwargs, model=model)  
  
        if response != None:  
          return response  
  
      except Exception as e:  
        print(f"got exception {e} for model {model}")  
        rate_limited_models.add(model)  
        model_expiration_times[model] = (  
          time.time() + 60  
        ) # cool down this selected model  
        pass  
  return response  

```

Previous
Mock Completion() Responses - Save Testing Costs 💰
Next
Supported Endpoints
  * Helper utils
  * Retry failed requests
  * Fallbacks (SDK)
    * Context Window Fallbacks (SDK)
    * Fallbacks - Switch Models/API Keys/API Bases (SDK)
  * Implementation Details (SDK)
    * Fallbacks


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
      * Input Params
      * Output
      * Usage
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /chat/completions
  * Output


On this page
# Output
## Format​
Here's the exact json output and type you can expect from all litellm `completion` calls for all models
```
{  
 'choices': [  
  {  
   'finish_reason': str,   # String: 'stop'  
   'index': int,       # Integer: 0  
   'message': {       # Dictionary [str, str]  
    'role': str,      # String: 'assistant'  
    'content': str     # String: "default message"  
   }  
  }  
 ],  
 'created': str,        # String: None  
 'model': str,         # String: None  
 'usage': {          # Dictionary [str, int]  
  'prompt_tokens': int,    # Integer  
  'completion_tokens': int,  # Integer  
  'total_tokens': int     # Integer  
 }  
}  
  

```

You can access the response as a dictionary or as a class object, just as OpenAI allows you
```
print(response.choices[0].message.content)  
print(response['choices'][0]['message']['content'])  

```

Here's what an example response looks like 
```
{  
 'choices': [  
   {  
    'finish_reason': 'stop',  
    'index': 0,  
    'message': {  
      'role': 'assistant',  
      'content': " I'm doing well, thank you for asking. I am Claude, an AI assistant created by Anthropic."  
    }  
   }  
  ],  
 'created': 1691429984.3852863,  
 'model': 'claude-instant-1',  
 'usage': {'prompt_tokens': 18, 'completion_tokens': 23, 'total_tokens': 41}  
}  

```

## Additional Attributes​
You can also access information like latency. 
```
from litellm import completion  
import os  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
messages=[{"role": "user", "content": "Hey!"}]  
  
response = completion(model="claude-2", messages=messages)  
  
print(response.response_ms) # 616.25# 616.25  

```

Previous
Input Params
Next
Usage
  * Format
  * Additional Attributes


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
On this page
# Hosted LiteLLM Proxy
LiteLLM maintains the proxy, so you can focus on your core products. 
## **Get Onboarded**​
This is in alpha. Schedule a call with us, and we'll give you a hosted proxy within 30 minutes. 
**🚨 Schedule Call**
### **Status** : Alpha​
Our proxy is already used in production by customers. 
See our status page for **live reliability**
### **Benefits**​
  * **No Maintenance, No Infra** : We'll maintain the proxy, and spin up any additional infrastructure (e.g.: separate server for spend logs) to make sure you can load balance + track spend across multiple LLM projects. 
  * **Reliable** : Our hosted proxy is tested on 1k requests per second, making it reliable for high load.
  * **Secure** : LiteLLM is currently undergoing SOC-2 compliance, to make sure your data is as secure as possible.


## Data Privacy & Security​
You can find our data privacy & security policy for cloud litellm here
## Supported data regions for LiteLLM Cloud​
You can find supported data regions litellm here
### Pricing​
Pricing is based on usage. We can figure out a price that works for your team, on the call. 
**🚨 Schedule Call**
## **Screenshots**​
### 1. Create keys​
### 2. Add Models​
### 3. Track spend​
### 4. Configure load balancing​
#### **🚨 Schedule Call**​
## Feature List​
  * Easy way to add/remove models
  * 100% uptime even when models are added/removed
  * custom callback webhooks
  * your domain name with HTTPS
  * Ability to create/delete User API keys
  * Reasonable set monthly cost


  * **Get Onboarded**
    * **Status** : Alpha
    * **Benefits**
  * Data Privacy & Security
  * Supported data regions for LiteLLM Cloud
    * Pricing
  * **Screenshots**
    * 1. Create keys
    * 2. Add Models
    * 3. Track spend
    * 4. Configure load balancing
  * Feature List


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
On this page
# Enterprise
For companies that need SSO, user management and professional support for LiteLLM Proxy
info
Get free 7-day trial key here
Includes all enterprise features.
**Procurement available via AWS / Azure Marketplace**
This covers: 
  * **Enterprise Features**
  * ✅ **Feature Prioritization**
  * ✅ **Custom Integrations**
  * ✅ **Professional Support - Dedicated discord + slack**


Deployment Options:
**Self-Hosted**
  1. Manage Yourself - you can deploy our Docker Image or build a custom image from our pip package, and manage your own infrastructure. In this case, we would give you a license key + provide support via a dedicated support channel. 
  2. We Manage - you give us subscription access on your AWS/Azure/GCP account, and we manage the deployment.


**Managed**
You can use our cloud product where we setup a dedicated instance for you. 
## Frequently Asked Questions​
### SLA's + Professional Support​
Professional Support can assist with LLM/Provider integrations, deployment, upgrade management, and LLM Provider troubleshooting. We can’t solve your own infrastructure-related issues but we will guide you to fix them.
  * 1 hour for Sev0 issues - 100% production traffic is failing
  * 6 hours for Sev1 - <100% production traffic is failing
  * 24h for Sev2-Sev3 between 7am – 7pm PT (Monday through Saturday) - setup issues e.g. Redis working on our end, but not on your infrastructure.
  * 72h SLA for patching vulnerabilities in the software. 


**We can offer custom SLAs** based on your needs and the severity of the issue
### What’s the cost of the Self-Managed Enterprise edition?​
Self-Managed Enterprise deployments require our team to understand your exact needs. Get in touch with us to learn more
### How does deployment with Enterprise License work?​
You just deploy our docker image and get an enterprise license key to add to your environment to unlock additional functionality (SSO, Prometheus metrics, etc.). 
```
LITELLM_LICENSE="eyJ..."  

```

No data leaves your environment. 
## Data Security / Legal / Compliance FAQs​
Data Security / Legal / Compliance FAQs
  * Frequently Asked Questions
    * SLA's + Professional Support
    * What’s the cost of the Self-Managed Enterprise edition?
    * How does deployment with Enterprise License work?
  * Data Security / Legal / Compliance FAQs


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * 🖇️ AgentOps - LLM Observability Platform


On this page
# 🖇️ AgentOps - LLM Observability Platform
tip
This is community maintained. Please make an issue if you run into a bug: https://github.com/BerriAI/litellm
AgentOps is an observability platform that enables tracing and monitoring of LLM calls, providing detailed insights into your AI operations.
## Using AgentOps with LiteLLM​
LiteLLM provides `success_callbacks` and `failure_callbacks`, allowing you to easily integrate AgentOps for comprehensive tracing and monitoring of your LLM operations.
### Integration​
Use just a few lines of code to instantly trace your responses **across all providers** with AgentOps: Get your AgentOps API Keys from https://app.agentops.ai/
```
import litellm  
  
# Configure LiteLLM to use AgentOps  
litellm.success_callback = ["agentops"]  
  
# Make your LLM calls as usual  
response = litellm.completion(  
  model="gpt-3.5-turbo",  
  messages=[{"role": "user", "content": "Hello, how are you?"}],  
)  

```

Complete Code:
```
import os  
from litellm import completion  
  
# Set env variables  
os.environ["OPENAI_API_KEY"] = "your-openai-key"  
os.environ["AGENTOPS_API_KEY"] = "your-agentops-api-key"  
  
# Configure LiteLLM to use AgentOps  
litellm.success_callback = ["agentops"]  
  
# OpenAI call  
response = completion(  
  model="gpt-4",  
  messages=[{"role": "user", "content": "Hi 👋 - I'm OpenAI"}],  
)  
  
print(response)  

```

### Configuration Options​
The AgentOps integration can be configured through environment variables:
  * `AGENTOPS_API_KEY` (str, optional): Your AgentOps API key
  * `AGENTOPS_ENVIRONMENT` (str, optional): Deployment environment (defaults to "production")
  * `AGENTOPS_SERVICE_NAME` (str, optional): Service name for tracing (defaults to "agentops")


### Advanced Usage​
You can configure additional settings through environment variables:
```
import os  
  
# Configure AgentOps settings  
os.environ["AGENTOPS_API_KEY"] = "your-agentops-api-key"  
os.environ["AGENTOPS_ENVIRONMENT"] = "staging"  
os.environ["AGENTOPS_SERVICE_NAME"] = "my-service"  
  
# Enable AgentOps tracing  
litellm.success_callback = ["agentops"]  

```

### Support​
For issues or questions, please refer to:
  * AgentOps Documentation
  * LiteLLM Documentation


Previous
Multi-Instance TPM/RPM (litellm.Router)
Next
🪢 Langfuse - Logging LLM Input/Output
  * Using AgentOps with LiteLLM
    * Integration
    * Configuration Options
    * Advanced Usage
    * Support


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
On this page
# Callbacks
## Use Callbacks to send Output Data to Posthog, Sentry etc​
liteLLM provides `input_callbacks`, `success_callbacks` and `failure_callbacks`, making it easy for you to send data to a particular provider depending on the status of your responses.
liteLLM supports:
  * Custom Callback Functions
  * Lunary
  * Langfuse
  * LangSmith
  * Helicone
  * Traceloop
  * Athina
  * Sentry
  * PostHog
  * Slack


This is **not** an extensive list. Please check the dropdown for all logging integrations.
### Quick Start​
```
from litellm import completion  
  
# set callbacks  
litellm.input_callback=["sentry"] # for sentry breadcrumbing - logs the input being sent to the api  
litellm.success_callback=["posthog", "helicone", "langfuse", "lunary", "athina"]  
litellm.failure_callback=["sentry", "lunary", "langfuse"]  
  
## set env variables  
os.environ['LUNARY_PUBLIC_KEY'] = ""  
os.environ['SENTRY_DSN'], os.environ['SENTRY_API_TRACE_RATE']= ""  
os.environ['POSTHOG_API_KEY'], os.environ['POSTHOG_API_URL'] = "api-key", "api-url"  
os.environ["HELICONE_API_KEY"] = ""  
os.environ["TRACELOOP_API_KEY"] = ""  
os.environ["LUNARY_PUBLIC_KEY"] = ""  
os.environ["ATHINA_API_KEY"] = ""  
os.environ["LANGFUSE_PUBLIC_KEY"] = ""  
os.environ["LANGFUSE_SECRET_KEY"] = ""  
os.environ["LANGFUSE_HOST"] = ""  
  
response = completion(model="gpt-3.5-turbo", messages=messages)  

```

  * Use Callbacks to send Output Data to Posthog, Sentry etc
    * Quick Start


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
    * Prompt Management
    * Custom Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * [Beta] Prompt Management
  * Prompt Management


On this page
# Prompt Management
info
This feature is currently in beta, and might change unexpectedly. We expect this to be more stable by next month (February 2025). 
Run experiments or change the specific model (e.g. from gpt-4o to gpt4o-mini finetune) from your prompt management tool (e.g. Langfuse) instead of making changes in the application. 
Supported Integrations| Link  
---|---  
Langfuse| Get Started  
Humanloop| Get Started  
## Quick Start​
  * SDK
  * PROXY


```
import os   
import litellm  
  
os.environ["LANGFUSE_PUBLIC_KEY"] = "public_key" # [OPTIONAL] set here or in `.completion`  
os.environ["LANGFUSE_SECRET_KEY"] = "secret_key" # [OPTIONAL] set here or in `.completion`  
  
litellm.set_verbose = True # see raw request to provider  
  
resp = litellm.completion(  
  model="langfuse/gpt-3.5-turbo",  
  prompt_id="test-chat-prompt",  
  prompt_variables={"user_message": "this is used"}, # [OPTIONAL]  
  messages=[{"role": "user", "content": "<IGNORED>"}],  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: my-langfuse-model  
  litellm_params:  
   model: langfuse/openai-model  
   prompt_id: "<langfuse_prompt_id>"  
   api_key: os.environ/OPENAI_API_KEY  
 - model_name: openai-model  
  litellm_params:  
   model: openai/gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  

```

  1. Start the proxy


```
litellm --config config.yaml --detailed_debug  

```

  1. Test it! 


  * CURL
  * OpenAI Python SDK


```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "my-langfuse-model",  
  "messages": [  
    {  
      "role": "user",  
      "content": "THIS WILL BE IGNORED"  
    }  
  ],  
  "prompt_variables": {  
    "key": "this is used"  
  }  
}'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
    "prompt_variables": { # [OPTIONAL]  
      "key": "this is used"  
    }  
  }  
)  
  
print(response)  

```

**Expected Logs:**
```
POST Request Sent from LiteLLM:  
curl -X POST \  
https://api.openai.com/v1/ \  
-d '{'model': 'gpt-3.5-turbo', 'messages': <YOUR LANGFUSE PROMPT TEMPLATE>}'  

```

## How to set model​
### Set the model on LiteLLM​
You can do `langfuse/<litellm_model_name>`
  * SDK
  * PROXY


```
litellm.completion(  
  model="langfuse/gpt-3.5-turbo", # or `langfuse/anthropic/claude-3-5-sonnet`  
  ...  
)  

```

```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: langfuse/gpt-3.5-turbo # OR langfuse/anthropic/claude-3-5-sonnet  
   prompt_id: <langfuse_prompt_id>  
   api_key: os.environ/OPENAI_API_KEY  

```

### Set the model in Langfuse​
If the model is specified in the Langfuse config, it will be used.
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_key: os.environ/AZURE_API_KEY  
   api_base: os.environ/AZURE_API_BASE  

```

## What is 'prompt_variables'?​
  * `prompt_variables`: A dictionary of variables that will be used to replace parts of the prompt.


## What is 'prompt_id'?​
  * `prompt_id`: The ID of the prompt that will be used for the request.


## What will the formatted prompt look like?​
### `/chat/completions` messages​
The `messages` field sent in by the client is ignored. 
The Langfuse prompt will replace the `messages` field.
To replace parts of the prompt, use the `prompt_variables` field. See how prompt variables are used
If the Langfuse prompt is a string, it will be sent as a user message (not all providers support system messages).
If the Langfuse prompt is a list, it will be sent as is (Langfuse chat prompts are OpenAI compatible).
## Architectural Overview​
## API Reference​
These are the params you can pass to the `litellm.completion` function in SDK and `litellm_params` in config.yaml
```
prompt_id: str # required  
prompt_variables: Optional[dict] # optional  
langfuse_public_key: Optional[str] # optional  
langfuse_secret: Optional[str] # optional  
langfuse_secret_key: Optional[str] # optional  
langfuse_host: Optional[str] # optional  

```

Previous
Instructor - Function Calling
Next
Custom Prompt Management
  * Quick Start
  * How to set model
    * Set the model on LiteLLM
    * Set the model in Langfuse
  * What is 'prompt_variables'?
  * What is 'prompt_id'?
  * What will the formatted prompt look like?
    * `/chat/completions` messages
  * Architectural Overview
  * API Reference


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
      * Virtual Keys
      * OIDC - JWT-based Auth
      * [Beta] Service Accounts
      * Role-based Access Controls (RBAC)
      * Custom Auth
      * IP Address Filtering
      * Email Notifications
      * Attribute Management changes to Users
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Authentication
  * Virtual Keys


On this page
# Virtual Keys
Track Spend, and control model access via virtual keys for the proxy
info
  * 🔑 UI to Generate, Edit, Delete Keys (with SSO)
  * Deploy LiteLLM Proxy with Key Management
  * Dockerfile.database for LiteLLM Proxy + Key Management


## Setup​
Requirements: 
  * Need a postgres database (e.g. Supabase, Neon, etc)
  * Set `DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname>` in your env 
  * Set a `master key`, this is your Proxy Admin key - you can use this to create other keys (🚨 must start with `sk-`).
    * **Set on config.yaml** set your master key under `general_settings:master_key`, example below
    * **Set env variable** set `LITELLM_MASTER_KEY`


(the proxy Dockerfile checks if the `DATABASE_URL` is set and then initializes the DB connection)
```
export DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname>  

```

You can then generate keys by hitting the `/key/generate` endpoint.
**See code**
## **Quick Start - Generate a Key**​
**Step 1: Save postgres db url**
```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
    model: ollama/llama2  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
    model: ollama/llama2  
  
general_settings:   
 master_key: sk-1234   
 database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # 👈 KEY CHANGE  

```

**Step 2: Start litellm**
```
litellm --config /path/to/config.yaml  

```

**Step 3: Generate keys**
```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "metadata": {"user": "ishaan@berri.ai"}}'  

```

## Spend Tracking​
Get spend per:
  * key - via `/key/info` Swagger
  * user - via `/user/info` Swagger
  * team - via `/team/info` Swagger
  * ⏳ end-users - via `/end_user/info` - Comment on this issue for end-user cost tracking


**How is it calculated?**
The cost per model is stored here and calculated by the `completion_cost` function.
**How is it tracking?**
Spend is automatically tracked for the key in the "LiteLLM_VerificationTokenTable". If the key has an attached 'user_id' or 'team_id', the spend for that user is tracked in the "LiteLLM_UserTable", and team in the "LiteLLM_TeamTable".
  * Key Spend
  * User Spend
  * Team Spend


You can get spend for a key by using the `/key/info` endpoint. 
```
curl 'http://0.0.0.0:4000/key/info?key=<user-key>' \  
   -X GET \  
   -H 'Authorization: Bearer <your-master-key>'  

```

This is automatically updated (in USD) when calls are made to /completions, /chat/completions, /embeddings using litellm's completion_cost() function. **See Code**. 
**Sample response**
```
{  
  "key": "sk-tXL0wt5-lOOVK9sfY2UacA",  
  "info": {  
    "token": "sk-tXL0wt5-lOOVK9sfY2UacA",  
    "spend": 0.0001065, # 👈 SPEND  
    "expires": "2023-11-24T23:19:11.131000Z",  
    "models": [  
      "gpt-3.5-turbo",  
      "gpt-4",  
      "claude-2"  
    ],  
    "aliases": {  
      "mistral-7b": "gpt-3.5-turbo"  
    },  
    "config": {}  
  }  
}  

```

**1. Create a user**
```
curl --location 'http://localhost:4000/user/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{user_email: "krrish@berri.ai"}'   

```

**Expected Response**
```
{  
  ...  
  "expires": "2023-12-22T09:53:13.861000Z",  
  "user_id": "my-unique-id", # 👈 unique id  
  "max_budget": 0.0  
}  

```

**2. Create a key for that user**
```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "user_id": "my-unique-id"}'  

```

Returns a key - `sk-...`.
**3. See spend for user**
```
curl 'http://0.0.0.0:4000/user/info?user_id=my-unique-id' \  
   -X GET \  
   -H 'Authorization: Bearer <your-master-key>'  

```

Expected Response
```
{  
 ...  
 "spend": 0 # 👈 SPEND  
}  

```

Use teams, if you want keys to be owned by multiple people (e.g. for a production app).
**1. Create a team**
```
curl --location 'http://localhost:4000/team/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{"team_alias": "my-awesome-team"}'   

```

**Expected Response**
```
{  
  ...  
  "expires": "2023-12-22T09:53:13.861000Z",  
  "team_id": "my-unique-id", # 👈 unique id  
  "max_budget": 0.0  
}  

```

**2. Create a key for that team**
```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "team_id": "my-unique-id"}'  

```

Returns a key - `sk-...`.
**3. See spend for team**
```
curl 'http://0.0.0.0:4000/team/info?team_id=my-unique-id' \  
   -X GET \  
   -H 'Authorization: Bearer <your-master-key>'  

```

Expected Response
```
{  
 ...  
 "spend": 0 # 👈 SPEND  
}  

```

## Model Aliases​
If a user is expected to use a given model (i.e. gpt3-5), and you want to:
  * try to upgrade the request (i.e. GPT4)
  * or downgrade it (i.e. Mistral)


Here's how you can do that: 
**Step 1: Create a model group in config.yaml (save model name, api keys, etc.)**
```
model_list:  
 - model_name: my-free-tier  
  litellm_params:  
    model: huggingface/HuggingFaceH4/zephyr-7b-beta  
    api_base: http://0.0.0.0:8001  
 - model_name: my-free-tier  
  litellm_params:  
    model: huggingface/HuggingFaceH4/zephyr-7b-beta  
    api_base: http://0.0.0.0:8002  
 - model_name: my-free-tier  
  litellm_params:  
    model: huggingface/HuggingFaceH4/zephyr-7b-beta  
    api_base: http://0.0.0.0:8003  
 - model_name: my-paid-tier  
  litellm_params:  
    model: gpt-4  
    api_key: my-api-key  

```

**Step 2: Generate a key**
```
curl -X POST "https://0.0.0.0:4000/key/generate" \  
-H "Authorization: Bearer <your-master-key>" \  
-H "Content-Type: application/json" \  
-d '{  
  "models": ["my-free-tier"],   
  "aliases": {"gpt-3.5-turbo": "my-free-tier"}, # 👈 KEY CHANGE  
  "duration": "30min"  
}'  

```

  * **How to upgrade / downgrade request?** Change the alias mapping


**Step 3: Test the key**
```
curl -X POST "https://0.0.0.0:4000/key/generate" \  
-H "Authorization: Bearer <user-key>" \  
-H "Content-Type: application/json" \  
-d '{  
  "model": "gpt-3.5-turbo",   
  "messages": [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ]  
}'  

```

## Advanced​
### Pass LiteLLM Key in custom header​
Use this to make LiteLLM proxy look for the virtual key in a custom header instead of the default `"Authorization"` header
**Step 1** Define `litellm_key_header_name` name on litellm config.yaml
```
model_list:  
 - model_name: fake-openai-endpoint  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
general_settings:   
 master_key: sk-1234   
 litellm_key_header_name: "X-Litellm-Key" # 👈 Key Change  
  

```

**Step 2** Test it
In this request, litellm will use the Virtual key in the `X-Litellm-Key` header
  * curl
  * OpenAI Python SDK


```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "X-Litellm-Key: Bearer sk-1234" \  
 -H "Authorization: Bearer bad-key" \  
 -d '{  
  "model": "fake-openai-endpoint",  
  "messages": [  
   {"role": "user", "content": "Hello, Claude gm!"}  
  ]  
 }'  

```

**Expected Response**
Expect to see a successful response from the litellm proxy since the key passed in `X-Litellm-Key` is valid
```
{"id":"chatcmpl-f9b2b79a7c30477ab93cd0e717d1773e","choices":[{"finish_reason":"stop","index":0,"message":{"content":"\n\nHello there, how may I assist you today?","role":"assistant","tool_calls":null,"function_call":null}}],"created":1677652288,"model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_44709d6fcb","usage":{"completion_tokens":12,"prompt_tokens":9,"total_tokens":21}  

```

```
client = openai.OpenAI(  
  api_key="not-used",  
  base_url="https://api-gateway-url.com/llmservc/api/litellmp",  
  default_headers={  
    "Authorization": f"Bearer {API_GATEWAY_TOKEN}", # (optional) For your API Gateway  
    "X-Litellm-Key": f"Bearer sk-1234"       # For LiteLLM Proxy  
  }  
)  

```

### Enable/Disable Virtual Keys​
**Disable Keys**
```
curl -L -X POST 'http://0.0.0.0:4000/key/block' \  
-H 'Authorization: Bearer LITELLM_MASTER_KEY' \  
-H 'Content-Type: application/json' \  
-d '{"key": "KEY-TO-BLOCK"}'  

```

Expected Response: 
```
{  
 ...  
 "blocked": true  
}  

```

**Enable Keys**
```
curl -L -X POST 'http://0.0.0.0:4000/key/unblock' \  
-H 'Authorization: Bearer LITELLM_MASTER_KEY' \  
-H 'Content-Type: application/json' \  
-d '{"key": "KEY-TO-UNBLOCK"}'  

```

```
{  
 ...  
 "blocked": false  
}  

```

### Custom /key/generate​
If you need to add custom logic before generating a Proxy API Key (Example Validating `team_id`)
#### 1. Write a custom `custom_generate_key_fn`​
The input to the custom_generate_key_fn function is a single parameter: `data` (Type: GenerateKeyRequest)
The output of your `custom_generate_key_fn` should be a dictionary with the following structure
```
{  
  "decision": False,  
  "message": "This violates LiteLLM Proxy Rules. No team id provided.",  
}  
  

```

  * decision (Type: bool): A boolean value indicating whether the key generation is allowed (True) or not (False).
  * message (Type: str, Optional): An optional message providing additional information about the decision. This field is included when the decision is False.


```
async def custom_generate_key_fn(data: GenerateKeyRequest)-> dict:  
    """  
    Asynchronous function for generating a key based on the input data.  
  
    Args:  
      data (GenerateKeyRequest): The input data for key generation.  
  
    Returns:  
      dict: A dictionary containing the decision and an optional message.  
      {  
        "decision": False,  
        "message": "This violates LiteLLM Proxy Rules. No team id provided.",  
      }  
    """  
      
    # decide if a key should be generated or not  
    print("using custom auth function!")  
    data_json = data.json() # type: ignore  
  
    # Unpacking variables  
    team_id = data_json.get("team_id")  
    duration = data_json.get("duration")  
    models = data_json.get("models")  
    aliases = data_json.get("aliases")  
    config = data_json.get("config")  
    spend = data_json.get("spend")  
    user_id = data_json.get("user_id")  
    max_parallel_requests = data_json.get("max_parallel_requests")  
    metadata = data_json.get("metadata")  
    tpm_limit = data_json.get("tpm_limit")  
    rpm_limit = data_json.get("rpm_limit")  
  
    if team_id is not None and team_id == "litellm-core-infra@gmail.com":  
      # only team_id="litellm-core-infra@gmail.com" can make keys  
      return {  
        "decision": True,  
      }  
    else:  
      print("Failed custom auth")  
      return {  
        "decision": False,  
        "message": "This violates LiteLLM Proxy Rules. No team id provided.",  
      }  

```

#### 2. Pass the filepath (relative to the config.yaml)​
Pass the filepath to the config.yaml 
e.g. if they're both in the same dir - `./config.yaml` and `./custom_auth.py`, this is what it looks like:
```
model_list:   
 - model_name: "openai-model"  
  litellm_params:   
   model: "gpt-3.5-turbo"  
  
litellm_settings:  
 drop_params: True  
 set_verbose: True  
  
general_settings:  
 custom_key_generate: custom_auth.custom_generate_key_fn  

```

### Upperbound /key/generate params​
Use this, if you need to set default upperbounds for `max_budget`, `budget_duration` or any `key/generate` param per key. 
Set `litellm_settings:upperbound_key_generate_params`:
```
litellm_settings:  
 upperbound_key_generate_params:  
  max_budget: 100 # Optional[float], optional): upperbound of $100, for all /key/generate requests  
  budget_duration: "10d" # Optional[str], optional): upperbound of 10 days for budget_duration values  
  duration: "30d" # Optional[str], optional): upperbound of 30 days for all /key/generate requests  
  max_parallel_requests: 1000 # (Optional[int], optional): Max number of requests that can be made in parallel. Defaults to None.  
  tpm_limit: 1000 #(Optional[int], optional): Tpm limit. Defaults to None.  
  rpm_limit: 1000 #(Optional[int], optional): Rpm limit. Defaults to None.  

```

**Expected Behavior**
  * Send a `/key/generate` request with `max_budget=200`
  * Key will be created with `max_budget=100` since 100 is the upper bound


### Default /key/generate params​
Use this, if you need to control the default `max_budget` or any `key/generate` param per key. 
When a `/key/generate` request does not specify `max_budget`, it will use the `max_budget` specified in `default_key_generate_params`
Set `litellm_settings:default_key_generate_params`:
```
litellm_settings:  
 default_key_generate_params:  
  max_budget: 1.5000  
  models: ["azure-gpt-3.5"]  
  duration:   # blank means `null`  
  metadata: {"setting":"default"}  
  team_id: "core-infra"  

```

### ✨ Key Rotations​
info
This is an Enterprise feature.
Enterprise Pricing
Get free 7-day trial key
Rotate an existing API Key, while optionally updating its parameters.
```
  
curl 'http://localhost:4000/key/sk-1234/regenerate' \  
 -X POST \  
 -H 'Authorization: Bearer sk-1234' \  
 -H 'Content-Type: application/json' \  
 -d '{  
  "max_budget": 100,  
  "metadata": {  
   "team": "core-infra"  
  },  
  "models": [  
   "gpt-4",  
   "gpt-3.5-turbo"  
  ]  
 }'  
  

```

**Read More**
  * Write rotated keys to secrets manager


**👉 API REFERENCE DOCS**
### Temporary Budget Increase​
Use the `/key/update` endpoint to increase the budget of an existing key. 
```
curl -L -X POST 'http://localhost:4000/key/update' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{"key": "sk-b3Z3Lqdb_detHXSUp4ol4Q", "temp_budget_increase": 100, "temp_budget_expiry": "10d"}'  

```

API Reference
### Restricting Key Generation​
Use this to control who can generate keys. Useful when letting others create keys on the UI. 
```
litellm_settings:  
 key_generation_settings:  
  team_key_generation:  
   allowed_team_member_roles: ["admin"]  
   required_params: ["tags"] # require team admins to set tags for cost-tracking when generating a team key  
  personal_key_generation: # maps to 'Default Team' on UI   
   allowed_user_roles: ["proxy_admin"]  

```

#### Spec​
```
key_generation_settings: Optional[StandardKeyGenerationConfig] = None  

```

#### Types​
```
class StandardKeyGenerationConfig(TypedDict, total=False):  
  team_key_generation: TeamUIKeyGenerationConfig  
  personal_key_generation: PersonalUIKeyGenerationConfig  
  
class TeamUIKeyGenerationConfig(TypedDict):  
  allowed_team_member_roles: List[str] # either 'user' or 'admin'  
  required_params: List[str] # require params on `/key/generate` to be set if a team key (team_id in request) is being generated  
  
  
class PersonalUIKeyGenerationConfig(TypedDict):  
  allowed_user_roles: List[LitellmUserRoles]   
  required_params: List[str] # require params on `/key/generate` to be set if a personal key (no team_id in request) is being generated  
  
  
class LitellmUserRoles(str, enum.Enum):  
  """  
  Admin Roles:  
  PROXY_ADMIN: admin over the platform  
  PROXY_ADMIN_VIEW_ONLY: can login, view all own keys, view all spend  
  ORG_ADMIN: admin over a specific organization, can create teams, users only within their organization  
  
  Internal User Roles:  
  INTERNAL_USER: can login, view/create/delete their own keys, view their spend  
  INTERNAL_USER_VIEW_ONLY: can login, view their own keys, view their own spend  
  
  
  Team Roles:  
  TEAM: used for JWT auth  
  
  
  Customer Roles:  
  CUSTOMER: External users -> these are customers  
  
  """  
  
  # Admin Roles  
  PROXY_ADMIN = "proxy_admin"  
  PROXY_ADMIN_VIEW_ONLY = "proxy_admin_viewer"  
  
  # Organization admins  
  ORG_ADMIN = "org_admin"  
  
  # Internal User Roles  
  INTERNAL_USER = "internal_user"  
  INTERNAL_USER_VIEW_ONLY = "internal_user_viewer"  
  
  # Team Roles  
  TEAM = "team"  
  
  # Customer Roles - External users of proxy  
  CUSTOMER = "customer"  

```

## **Next Steps - Set Budgets, Rate Limits per Virtual Key**​
Follow this doc to set budgets, rate limiters per virtual key with LiteLLM
## Endpoint Reference (Spec)​
### Keys​
#### **👉 API REFERENCE DOCS**​
### Users​
#### **👉 API REFERENCE DOCS**​
### Teams​
#### **👉 API REFERENCE DOCS**​
Previous
Model Discovery
Next
OIDC - JWT-based Auth
  * Setup
  * **Quick Start - Generate a Key**
  * Spend Tracking
  * Model Aliases
  * Advanced
    * Pass LiteLLM Key in custom header
    * Enable/Disable Virtual Keys
    * Custom /key/generate
    * Upperbound /key/generate params
    * Default /key/generate params
    * ✨ Key Rotations
    * Temporary Budget Increase
    * Restricting Key Generation
  * **Next Steps - Set Budgets, Rate Limits per Virtual Key**
  * Endpoint Reference (Spec)
    * Keys
    * Users
    * Teams


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Getting Started - E2E Tutorial


On this page
# Getting Started - E2E Tutorial
End-to-End tutorial for LiteLLM Proxy to:
  * Add an Azure OpenAI model 
  * Make a successful /chat/completion call 
  * Generate a virtual key 
  * Set RPM limit on virtual key 


## Pre-Requisites​
  * Install LiteLLM Docker Image **OR** LiteLLM CLI (pip package)


  * Docker
  * LiteLLM CLI (pip package)


```
docker pull ghcr.io/berriai/litellm:main-latest  

```

**See all docker images**
```
$ pip install 'litellm[proxy]'  

```

## 1. Add a model​
Control LiteLLM Proxy with a config.yaml file.
Setup your config.yaml with your azure model.
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/my_azure_deployment  
   api_base: os.environ/AZURE_API_BASE  
   api_key: "os.environ/AZURE_API_KEY"  
   api_version: "2024-07-01-preview" # [OPTIONAL] litellm uses the latest azure api_version by default  

```

* * *
### Model List Specification​
  * **`model_name`**(`str`) - This field should contain the name of the model as received.
  * **`litellm_params`**(`dict`) See All LiteLLM Params
    * **`model`**(`str`) - Specifies the model name to be sent to `litellm.acompletion` / `litellm.aembedding`, etc. This is the identifier used by LiteLLM to route to the correct model + provider logic on the backend. 
    * **`api_key`**(`str`) - The API key required for authentication. It can be retrieved from an environment variable using `os.environ/`.
    * **`api_base`**(`str`) - The API base for your azure deployment.
    * **`api_version`**(`str`) - The API Version to use when calling Azure's OpenAI API. Get the latest Inference API version here.


### Useful Links​
  * **All Supported LLM API Providers (OpenAI/Bedrock/Vertex/etc.)**
  * **Full Config.Yaml Spec**
  * **Pass provider-specific params**


## 2. Make a successful /chat/completion call​
LiteLLM Proxy is 100% OpenAI-compatible. Test your azure model via the `/chat/completions` route.
### 2.1 Start Proxy​
Save your config.yaml from step 1. as `litellm_config.yaml`.
  * Docker
  * LiteLLM CLI (pip package)


```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e AZURE_API_KEY=d6*********** \  
  -e AZURE_API_BASE=https://openai-***********/ \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm:main-latest \  
  --config /app/config.yaml --detailed_debug  
  
# RUNNING on http://0.0.0.0:4000  

```

```
$ litellm --config /app/config.yaml --detailed_debug  

```

Confirm your config.yaml got mounted correctly
```
Loaded config YAML (api_key and environment_variables are not shown):  
{  
"model_list": [  
{  
"model_name ...  

```

### 2.2 Make Call​
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   {  
    "role": "user",  
    "content": "how can I solve 8x + 7 = -23"  
   }  
  ]  
}'  

```

**Expected Response**
```
{  
  "id": "chatcmpl-2076f062-3095-4052-a520-7c321c115c68",  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": "I am gpt-3.5-turbo",  
        "role": "assistant",  
        "tool_calls": null,  
        "function_call": null  
      }  
    }  
  ],  
  "created": 1724962831,  
  "model": "gpt-3.5-turbo",  
  "object": "chat.completion",  
  "system_fingerprint": null,  
  "usage": {  
    "completion_tokens": 20,  
    "prompt_tokens": 10,  
    "total_tokens": 30  
  }  
}  

```

### Useful Links​
  * All Supported LLM API Providers (OpenAI/Bedrock/Vertex/etc.)
  * Call LiteLLM Proxy via OpenAI SDK, Langchain, etc.
  * All API Endpoints Swagger
  * Other/Non-Chat Completion Endpoints
  * Pass-through for VertexAI, Bedrock, etc.


## 3. Generate a virtual key​
Track Spend, and control model access via virtual keys for the proxy
### 3.1 Set up a Database​
**Requirements**
  * Need a postgres database (e.g. Supabase, Neon, etc)


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/my_azure_deployment  
   api_base: os.environ/AZURE_API_BASE  
   api_key: "os.environ/AZURE_API_KEY"  
   api_version: "2024-07-01-preview" # [OPTIONAL] litellm uses the latest azure api_version by default  
  
general_settings:   
 master_key: sk-1234   
 database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # 👈 KEY CHANGE  

```

Save config.yaml as `litellm_config.yaml` (used in 3.2).
* * *
**What is`general_settings`?**
These are settings for the LiteLLM Proxy Server. 
See All General Settings here.
  1. **`master_key`**(`str`)
     * **Description** : 
       * Set a `master key`, this is your Proxy Admin key - you can use this to create other keys (🚨 must start with `sk-`).
     * **Usage** : 
       * **Set on config.yaml** set your master key under `general_settings:master_key`, example - `master_key: sk-1234`
       * **Set env variable** set `LITELLM_MASTER_KEY`
  2. **`database_url`**(str)
     * **Description** : 
       * Set a `database_url`, this is the connection to your Postgres DB, which is used by litellm for generating keys, users, teams.
     * **Usage** : 
       * **Set on config.yaml** set your master key under `general_settings:database_url`, example - `database_url: "postgresql://..."`
       * Set `DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname>` in your env 


### 3.2 Start Proxy​
```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e AZURE_API_KEY=d6*********** \  
  -e AZURE_API_BASE=https://openai-***********/ \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm:main-latest \  
  --config /app/config.yaml --detailed_debug  

```

### 3.3 Create Key w/ RPM Limit​
Create a key with `rpm_limit: 1`. This will only allow 1 request per minute for calls to proxy with this key.
```
curl -L -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "rpm_limit": 1  
}'  

```

**See full API Spec**
**Expected Response**
```
{  
  "key": "sk-12..."  
}  

```

### 3.4 Test it!​
**Use your virtual key from step 3.3**
1st call - Expect to work! 
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-12...' \  
-d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   {  
    "role": "user",  
    "content": "how can I solve 8x + 7 = -23"  
   }  
  ]  
}'  

```

**Expected Response**
```
{  
  "id": "chatcmpl-2076f062-3095-4052-a520-7c321c115c68",  
  "choices": [  
    ...  
}  

```

2nd call - Expect to fail! 
**Why did this call fail?**
We set the virtual key's requests per minute (RPM) limit to 1. This has now been crossed.
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-12...' \  
-d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   {  
    "role": "user",  
    "content": "how can I solve 8x + 7 = -23"  
   }  
  ]  
}'  

```

**Expected Response**
```
{  
 "error": {  
  "message": "Max parallel request limit reached. Hit limit for api_key: daa1b272072a4c6841470a488c5dad0f298ff506e1cc935f4a181eed90c182ad. tpm_limit: 100, current_tpm: 29, rpm_limit: 1, current_rpm: 2.",  
  "type": "None",  
  "param": "None",  
  "code": "429"  
 }  
}  

```

### Useful Links​
  * Creating Virtual Keys
  * Key Management API Endpoints Swagger
  * Set Budgets / Rate Limits per key/user/teams
  * Dynamic TPM/RPM Limits for keys


## Troubleshooting​
### Non-root docker image?​
If you need to run the docker image as a non-root user, use this.
### SSL Verification Issue / Connection Error.​
If you see 
```
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)  

```

OR
```
Connection Error.  

```

You can disable ssl verification with: 
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/my_azure_deployment  
   api_base: os.environ/AZURE_API_BASE  
   api_key: "os.environ/AZURE_API_KEY"  
   api_version: "2024-07-01-preview"  
  
litellm_settings:  
  ssl_verify: false # 👈 KEY CHANGE  

```

### (DB) All connection attempts failed​
If you see:
```
httpx.ConnectError: All connection attempts failed                                      
                                                               
ERROR:  Application startup failed. Exiting.                                        
3:21:43 - LiteLLM Proxy:ERROR: utils.py:2207 - Error getting LiteLLM_SpendLogs row count: All connection attempts failed   

```

This might be a DB permission issue. 
  1. Validate db user permission issue 


Try creating a new database. 
```
STATEMENT: CREATE DATABASE "litellm"  

```

If you get:
```
ERROR: permission denied to create   

```

This indicates you have a permission issue. 
  1. Grant permissions to your DB user


It should look something like this:
```
psql -U postgres  

```

```
CREATE DATABASE litellm;  

```

On CloudSQL, this is:
```
GRANT ALL PRIVILEGES ON DATABASE litellm TO your_username;  

```

**What is`litellm_settings`?**
LiteLLM Proxy uses the LiteLLM Python SDK for handling LLM API calls. 
`litellm_settings` are module-level params for the LiteLLM Python SDK (equivalent to doing `litellm.<some_param>` on the SDK). You can see all params here
## Support & Talk with founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


![Chat on WhatsApp](https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square) ![Chat on Discord](https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square)
Previous
LiteLLM Proxy Server
Next
Overview
  * Pre-Requisites
  * 1. Add a model
    * Model List Specification
    * Useful Links
  * 2. Make a successful /chat/completion call
    * 2.1 Start Proxy
    * 2.2 Make Call
    * Useful Links
  * 3. Generate a virtual key
    * 3.1 Set up a Database
    * 3.2 Start Proxy
    * 3.3 Create Key w/ RPM Limit
    * 3.4 Test it!
    * Useful Links
  * Troubleshooting
    * Non-root docker image?
    * SSL Verification Issue / Connection Error.
    * (DB) All connection attempts failed
  * Support & Talk with founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers


# Providers
Learn how to deploy + call models from different providers on LiteLLM
## 📄️ OpenAI
LiteLLM supports OpenAI Chat + Embedding calls.
## 📄️ OpenAI (Text Completion)
LiteLLM supports OpenAI text completion models
## 📄️ OpenAI-Compatible Endpoints
To call models hosted behind an openai proxy, make 2 changes:
## 📄️ Azure OpenAI
Overview
## 📄️ Azure AI Studio
LiteLLM supports all models on Azure AI Studio
## 📄️ AI/ML API
Getting started with the AI/ML API is simple. Follow these steps to set up your integration:
## 📄️ VertexAI [Anthropic, Gemini, Model Garden]
Overview
## 🗃️ Google AI Studio
2 items
## 📄️ Anthropic
LiteLLM supports all anthropic models.
## 📄️ AWS Sagemaker
LiteLLM supports All Sagemaker Huggingface Jumpstart Models
## 📄️ AWS Bedrock
ALL Bedrock models (Anthropic, Meta, Deepseek, Mistral, Amazon, etc.) are Supported
## 📄️ LiteLLM Proxy (LLM Gateway)
| Property | Details |
## 📄️ Mistral AI API
https://docs.mistral.ai/api/
## 📄️ Codestral API [Mistral AI]
Codestral is available in select code-completion plugins but can also be queried directly. See the documentation for more details.
## 📄️ Cohere
API KEYS
## 📄️ Anyscale
https://app.endpoints.anyscale.com/
## 📄️ Hugging Face
LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.
## 📄️ Databricks
LiteLLM supports all models on Databricks
## 📄️ Deepgram
LiteLLM supports Deepgram's /listen endpoint.
## 📄️ IBM watsonx.ai
LiteLLM supports all IBM watsonx.ai foundational models and embeddings.
## 📄️ Predibase
LiteLLM supports all models on Predibase
## 📄️ Nvidia NIM
https://docs.api.nvidia.com/nim/reference/
## 📄️ xAI
https://docs.x.ai/docs
## 📄️ LM Studio
https://lmstudio.ai/docs/basics/server
## 📄️ Cerebras
https://inference-docs.cerebras.ai/api-reference/chat-completions
## 📄️ Volcano Engine (Volcengine)
https://www.volcengine.com/docs/82379/1263482
## 📄️ Triton Inference Server
LiteLLM supports Embedding Models on Triton Inference Servers
## 📄️ Ollama
LiteLLM supports all models from Ollama
## 📄️ Perplexity AI (pplx-api)
https://www.perplexity.ai
## 📄️ FriendliAI
We support ALL FriendliAI models, just set friendliai/ as a prefix when sending completion requests
## 📄️ Galadriel
https://docs.galadriel.com/api-reference/chat-completion-API
## 📄️ Topaz
| Property | Details |
## 📄️ Groq
https://groq.com/
## 📄️ 🆕 Github
https://github.com/marketplace/models
## 📄️ Deepseek
https://deepseek.com/
## 📄️ Fireworks AI
We support ALL Fireworks AI models, just set fireworks_ai/ as a prefix when sending completion requests
## 📄️ Clarifai
Anthropic, OpenAI, Mistral, Llama and Gemini LLMs are Supported on Clarifai.
## 📄️ VLLM
LiteLLM supports all models on VLLM.
## 📄️ Infinity
| Property | Details |
## 📄️ Xinference [Xorbits Inference]
https://inference.readthedocs.io/en/latest/index.html
## 📄️ Cloudflare Workers AI
https://developers.cloudflare.com/workers-ai/models/text-generation/
## 📄️ DeepInfra
https://deepinfra.com/
## 📄️ AI21
LiteLLM supports the following AI21 models:
## 📄️ NLP Cloud
LiteLLM supports all LLMs on NLP Cloud.
## 📄️ Replicate
LiteLLM supports all models on Replicate
## 📄️ Together AI
LiteLLM supports all models on Together AI.
## 📄️ Voyage AI
https://docs.voyageai.com/embeddings/
## 📄️ Jina AI
https://jina.ai/embeddings/
## 📄️ Aleph Alpha
LiteLLM supports all models from Aleph Alpha.
## 📄️ Baseten
LiteLLM supports any Text-Gen-Interface models on Baseten.
## 📄️ OpenRouter
LiteLLM supports all the text / chat / vision models from OpenRouter
## 📄️ Sambanova
https://cloud.sambanova.ai/
## 📄️ Custom API Server (Custom Format)
Call your custom torch-serve / internal LLM APIs via LiteLLM
## 📄️ Petals
Petals//github.com/bigscience-workshop/petals
## 📄️ Snowflake
| Property | Details |
Previous
Caching
Next
OpenAI
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
      * 💰 Budgets, Rate Limits
      * ✨ Temporary Budget Increase
      * ✨ Budget / Rate Limit Tiers
      * 💰 Setting Team Budgets
      * 🙋‍♂️ Customers / End-User Budgets
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Budgets + Rate Limits
  * 💰 Budgets, Rate Limits


On this page
# 💰 Budgets, Rate Limits
Requirements: 
  * Need to a postgres database (e.g. Supabase, Neon, etc) **See Setup**


## Set Budgets​
### Global Proxy​
Apply a budget across all calls on the proxy
**Step 1. Modify config.yaml**
```
general_settings:  
 master_key: sk-1234  
  
litellm_settings:  
 # other litellm settings  
 max_budget: 0 # (float) sets max budget as $0 USD  
 budget_duration: 30d # (str) frequency of reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").  

```

**Step 2. Start proxy**
```
litellm /path/to/config.yaml  

```

**Step 3. Send test call**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Autherization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```

### Team​
You can:
  * Add budgets to Teams


info
**Step-by step tutorial on setting, resetting budgets on Teams here (API or using Admin UI)**
👉 https://docs.litellm.ai/docs/proxy/team_budgets
#### **Add budgets to teams**​
```
curl --location 'http://localhost:4000/team/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "team_alias": "my-new-team_4",  
 "members_with_roles": [{"role": "admin", "user_id": "5c4a0aa3-a1e1-43dc-bd87-3c2da8382a3a"}],  
 "rpm_limit": 99  
}'   

```

**See Swagger**
**Sample Response**
```
{  
  "team_alias": "my-new-team_4",  
  "team_id": "13e83b19-f851-43fe-8e93-f96e21033100",  
  "admins": [],  
  "members": [],  
  "members_with_roles": [  
    {  
      "role": "admin",  
      "user_id": "5c4a0aa3-a1e1-43dc-bd87-3c2da8382a3a"  
    }  
  ],  
  "metadata": {},  
  "tpm_limit": null,  
  "rpm_limit": 99,  
  "max_budget": null,  
  "models": [],  
  "spend": 0.0,  
  "max_parallel_requests": null,  
  "budget_duration": null,  
  "budget_reset_at": null  
}  

```

#### **Add budget duration to teams**​
`budget_duration`: Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").
```
curl 'http://0.0.0.0:4000/team/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "team_alias": "my-new-team_4",  
 "members_with_roles": [{"role": "admin", "user_id": "5c4a0aa3-a1e1-43dc-bd87-3c2da8382a3a"}],  
 "budget_duration": 10s,  
}'  

```

### Team Members​
Use this when you want to budget a users spend within a Team 
#### Step 1. Create User​
Create a user with `user_id=ishaan`
```
curl --location 'http://0.0.0.0:4000/user/new' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "user_id": "ishaan"  
}'  

```

#### Step 2. Add User to an existing Team - set `max_budget_in_team`​
Set `max_budget_in_team` when adding a User to a team. We use the same `user_id` we set in Step 1
```
curl -X POST 'http://0.0.0.0:4000/team/member_add' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{"team_id": "e8d1460f-846c-45d7-9b43-55f3cc52ac32", "max_budget_in_team": 0.000000000001, "member": {"role": "user", "user_id": "ishaan"}}'  

```

#### Step 3. Create a Key for Team member from Step 1​
Set `user_id=ishaan` from step 1
```
curl --location 'http://0.0.0.0:4000/key/generate' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "user_id": "ishaan",  
    "team_id": "e8d1460f-846c-45d7-9b43-55f3cc52ac32"  
}'  

```

Response from `/key/generate`
We use the `key` from this response in Step 4
```
{"key":"sk-RV-l2BJEZ_LYNChSx2EueQ", "models":[],"spend":0.0,"max_budget":null,"user_id":"ishaan","team_id":"e8d1460f-846c-45d7-9b43-55f3cc52ac32","max_parallel_requests":null,"metadata":{},"tpm_limit":null,"rpm_limit":null,"budget_duration":null,"allowed_cache_controls":[],"soft_budget":null,"key_alias":null,"duration":null,"aliases":{},"config":{},"permissions":{},"model_max_budget":{},"key_name":null,"expires":null,"token_id":null}%   

```

#### Step 4. Make /chat/completions requests for Team member​
Use the key from step 3 for this request. After 2-3 requests expect to see The following error `ExceededBudget: Crossed spend within team`
```
curl --location 'http://localhost:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-RV-l2BJEZ_LYNChSx2EueQ' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "llama3",  
  "messages": [  
    {  
    "role": "user",  
    "content": "tes4"  
    }  
  ]  
}'  

```

### Internal User​
Apply a budget across all calls an internal user (key owner) can make on the proxy. 
info
For most use-cases, we recommend setting team-member budgets
LiteLLM exposes a `/user/new` endpoint to create budgets for this.
You can:
  * Add budgets to users **Jump**
  * Add budget durations, to reset spend **Jump**


By default the `max_budget` is set to `null` and is not checked for keys
#### **Add budgets to users**​
```
curl --location 'http://localhost:4000/user/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{"models": ["azure-models"], "max_budget": 0, "user_id": "krrish3@berri.ai"}'   

```

**See Swagger**
**Sample Response**
```
{  
  "key": "sk-YF2OxDbrgd1y2KgwxmEA2w",  
  "expires": "2023-12-22T09:53:13.861000Z",  
  "user_id": "krrish3@berri.ai",  
  "max_budget": 0.0  
}  

```

#### **Add budget duration to users**​
`budget_duration`: Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").
```
curl 'http://0.0.0.0:4000/user/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "team_id": "core-infra", # [OPTIONAL]  
 "max_budget": 10,  
 "budget_duration": 10s,  
}'  

```

#### Create new keys for existing user​
Now you can just call `/key/generate` with that user_id (i.e. krrish3@berri.ai) and:
  * **Budget Check** : krrish3@berri.ai's budget (i.e. $10) will be checked for this key
  * **Spend Tracking** : spend for this key will update krrish3@berri.ai's spend as well


```
curl --location 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data '{"models": ["azure-models"], "user_id": "krrish3@berri.ai"}'  

```

### Virtual Key​
Apply a budget on a key.
You can:
  * Add budgets to keys **Jump**
  * Add budget durations, to reset spend **Jump**


**Expected Behaviour**
  * Costs Per key get auto-populated in `LiteLLM_VerificationToken` Table
  * After the key crosses it's `max_budget`, requests fail
  * If duration set, spend is reset at the end of the duration


By default the `max_budget` is set to `null` and is not checked for keys
#### **Add budgets to keys**​
```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "team_id": "core-infra", # [OPTIONAL]  
 "max_budget": 10,  
}'  

```

Example Request to `/chat/completions` when key has crossed budget
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
 --header 'Content-Type: application/json' \  
 --header 'Authorization: Bearer <generated-key>' \  
 --data ' {  
 "model": "azure-gpt-3.5",  
 "user": "e09b4da8-ed80-4b05-ac93-e16d9eb56fca",  
 "messages": [  
   {  
   "role": "user",  
   "content": "respond in 50 lines"  
   }  
 ],  
}'  

```

Expected Response from `/chat/completions` when key has crossed budget
```
{  
 "detail":"Authentication Error, ExceededTokenBudget: Current spend for token: 7.2e-05; Max Budget for Token: 2e-07"  
}    

```

#### **Add budget duration to keys**​
`budget_duration`: Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").
```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "team_id": "core-infra", # [OPTIONAL]  
 "max_budget": 10,  
 "budget_duration": 10s,  
}'  

```

### ✨ Virtual Key (Model Specific)​
Apply model specific budgets on a key. Example: 
  * Budget for `gpt-4o` is $0.0000001, for time period `1d` for `key = "sk-12345"`
  * Budget for `gpt-4o-mini` is $10, for time period `30d` for `key = "sk-12345"`


info
✨ This is an Enterprise only feature Get Started with Enterprise here
The spec for `model_max_budget` is **`Dict[str, GenericBudgetInfo]`**
```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "model_max_budget": {"gpt-4o": {"budget_limit": "0.0000001", "time_period": "1d"}}  
}'  

```

#### Make a test request​
We expect the first request to succeed, and the second request to fail since we cross the budget for `gpt-4o` on the Virtual Key
**Langchain, OpenAI SDK Usage Examples**
  * Successful Call 
  * Unsuccessful call


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer <sk-generated-key>' \  
--data ' {  
   "model": "gpt-4o",  
   "messages": [  
    {  
     "role": "user",  
     "content": "testing request"  
    }  
   ]  
  }  
'  

```

Expect this to fail since since we cross the budget `model=gpt-4o` on the Virtual Key
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer <sk-generated-key>' \  
--data ' {  
   "model": "gpt-4o",  
   "messages": [  
    {  
     "role": "user",  
     "content": "testing request"  
    }  
   ]  
  }  
'  

```

Expected response on failure
```
{  
  "error": {  
    "message": "LiteLLM Virtual Key: 9769f3f6768a199f76cc29xxxx, key_alias: None, exceeded budget for model=gpt-4o",  
    "type": "budget_exceeded",  
    "param": null,  
    "code": "400"  
  }  
}  

```

### Customers​
Use this to budget `user` passed to `/chat/completions`, **without needing to create a key for every user**
**Step 1. Modify config.yaml** Define `litellm.max_end_user_budget`
```
general_settings:  
 master_key: sk-1234  
  
litellm_settings:  
 max_end_user_budget: 0.0001 # budget for 'user' passed to /chat/completions  

```

  1. Make a /chat/completions call, pass 'user' - First call Works 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
    --header 'Content-Type: application/json' \  
    --header 'Authorization: Bearer sk-zi5onDRdHGD24v0Zdn7VBA' \  
    --data ' {  
    "model": "azure-gpt-3.5",  
    "user": "ishaan3",  
    "messages": [  
      {  
      "role": "user",  
      "content": "what time is it"  
      }  
    ]  
    }'  

```

  1. Make a /chat/completions call, pass 'user' - Call Fails, since 'ishaan3' over budget


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
    --header 'Content-Type: application/json' \  
    --header 'Authorization: Bearer sk-zi5onDRdHGD24v0Zdn7VBA' \  
    --data ' {  
    "model": "azure-gpt-3.5",  
    "user": "ishaan3",  
    "messages": [  
      {  
      "role": "user",  
      "content": "what time is it"  
      }  
    ]  
    }'  

```

Error
```
{"error":{"message":"Budget has been exceeded: User ishaan3 has exceeded their budget. Current spend: 0.0008869999999999999; Max Budget: 0.0001","type":"auth_error","param":"None","code":401}}%          

```

## Reset Budgets​
Reset budgets across keys/internal users/teams/customers
`budget_duration`: Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").
  * Internal Users
  * Keys
  * Teams


```
curl 'http://0.0.0.0:4000/user/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "max_budget": 10,  
 "budget_duration": 10s, # 👈 KEY CHANGE  
}'  

```

```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "max_budget": 10,  
 "budget_duration": 10s, # 👈 KEY CHANGE  
}'  

```

```
curl 'http://0.0.0.0:4000/team/new' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{  
 "max_budget": 10,  
 "budget_duration": 10s, # 👈 KEY CHANGE  
}'  

```

**Note:** By default, the server checks for resets every 10 minutes, to minimize DB calls.
To change this, set `proxy_budget_rescheduler_min_time` and `proxy_budget_rescheduler_max_time`
E.g.: Check every 1 seconds
```
general_settings:   
 proxy_budget_rescheduler_min_time: 1  
 proxy_budget_rescheduler_max_time: 1  

```

## Set Rate Limits​
You can set: 
  * tpm limits (tokens per minute)
  * rpm limits (requests per minute)
  * max parallel requests
  * rpm / tpm limits per model for a given key


  * Per Team
  * Per Internal User
  * Per Key
  * Per API Key Per model
  * For customers


Use `/team/new` or `/team/update`, to persist rate limits across multiple keys for a team.
```
curl --location 'http://0.0.0.0:4000/team/new' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{"team_id": "my-prod-team", "max_parallel_requests": 10, "tpm_limit": 20, "rpm_limit": 4}'   

```

**See Swagger**
**Expected Response**
```
{  
  "key": "sk-sA7VDkyhlQ7m8Gt77Mbt3Q",  
  "expires": "2024-01-19T01:21:12.816168",  
  "team_id": "my-prod-team",  
}  

```

Use `/user/new` or `/user/update`, to persist rate limits across multiple keys for internal users.
```
curl --location 'http://0.0.0.0:4000/user/new' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{"user_id": "krrish@berri.ai", "max_parallel_requests": 10, "tpm_limit": 20, "rpm_limit": 4}'   

```

**See Swagger**
**Expected Response**
```
{  
  "key": "sk-sA7VDkyhlQ7m8Gt77Mbt3Q",  
  "expires": "2024-01-19T01:21:12.816168",  
  "user_id": "krrish@berri.ai",  
}  

```

Use `/key/generate`, if you want them for just that key.
```
curl --location 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{"max_parallel_requests": 10, "tpm_limit": 20, "rpm_limit": 4}'   

```

**Expected Response**
```
{  
  "key": "sk-ulGNRXWtv7M0lFnnsQk0wQ",  
  "expires": "2024-01-18T20:48:44.297973",  
  "user_id": "78c2c8fc-c233-43b9-b0c3-eb931da27b84" // 👈 auto-generated  
}  

```

**Set rate limits per model per api key**
Set `model_rpm_limit` and `model_tpm_limit` to set rate limits per model per api key
Here `gpt-4` is the `model_name` set on the litellm config.yaml
```
curl --location 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{"model_rpm_limit": {"gpt-4": 2}, "model_tpm_limit": {"gpt-4":}}'   

```

**Expected Response**
```
{  
  "key": "sk-ulGNRXWtv7M0lFnnsQk0wQ",  
  "expires": "2024-01-18T20:48:44.297973",  
}  

```

**Verify Model Rate Limits set correctly for this key**
**Make /chat/completions request check if`x-litellm-key-remaining-requests-gpt-4` returned**
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-ulGNRXWtv7M0lFnnsQk0wQ" \  
 -d '{  
  "model": "gpt-4",  
  "messages": [  
   {"role": "user", "content": "Hello, Claude!ss eho ares"}  
  ]  
 }'  

```

**Expected headers**
```
x-litellm-key-remaining-requests-gpt-4: 1  
x-litellm-key-remaining-tokens-gpt-4: 179  

```

These headers indicate:
  * 1 request remaining for the GPT-4 model for key=`sk-ulGNRXWtv7M0lFnnsQk0wQ`
  * 179 tokens remaining for the GPT-4 model for key=`sk-ulGNRXWtv7M0lFnnsQk0wQ`


info
You can also create a budget id for a customer on the UI, under the 'Rate Limits' tab.
Use this to set rate limits for `user` passed to `/chat/completions`, without needing to create a key for every user
#### Step 1. Create Budget​
Set a `tpm_limit` on the budget (You can also pass `rpm_limit` if needed)
```
curl --location 'http://0.0.0.0:4000/budget/new' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{  
  "budget_id" : "free-tier",  
  "tpm_limit": 5  
}'  

```

#### Step 2. Create `Customer` with Budget​
We use `budget_id="free-tier"` from Step 1 when creating this new customers
```
curl --location 'http://0.0.0.0:4000/customer/new' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{  
  "user_id" : "palantir",  
  "budget_id": "free-tier"  
}'  

```

#### Step 3. Pass `user_id` id in `/chat/completions` requests​
Pass the `user_id` from Step 2 as `user="palantir"`
```
curl --location 'http://localhost:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "llama3",  
  "user": "palantir",  
  "messages": [  
    {  
    "role": "user",  
    "content": "gm"  
    }  
  ]  
}'  

```

## Set default budget for ALL internal users​
Use this to set a default budget for users who you give keys to.
This will apply when a user has `user_role="internal_user"` (set this via `/user/new` or `/user/update`). 
This will NOT apply if a key has a team_id (team budgets will apply then). Tell us how we can improve this!
  1. Define max budget in your config.yaml


```
model_list:   
 - model_name: "gpt-3.5-turbo"  
  litellm_params:  
   model: gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  
  
litellm_settings:  
 max_internal_user_budget: 0 # amount in USD  
 internal_user_budget_duration: "1mo" # reset every month  

```

  1. Create key for user 


```
curl -L -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{}'  

```

Expected Response: 
```
{  
 ...  
 "key": "sk-X53RdxnDhzamRwjKXR4IHg"  
}  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-X53RdxnDhzamRwjKXR4IHg' \  
-d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [{"role": "user", "content": "Hey, how's it going?"}]  
}'  

```

Expected Response: 
```
{  
  "error": {  
    "message": "ExceededBudget: User=<user_id> over budget. Spend=3.7e-05, Budget=0.0",  
    "type": "budget_exceeded",  
    "param": null,  
    "code": "400"  
  }  
}  

```

## Grant Access to new model​
Use model access groups to give users access to select models, and add new ones to it over time (e.g. mistral, llama-2, etc.). 
Difference between doing this with `/key/generate` vs. `/user/new`? If you do it on `/user/new` it'll persist across multiple keys generated for that user.
**Step 1. Assign model, access group in config.yaml**
```
model_list:  
 - model_name: text-embedding-ada-002  
  litellm_params:  
   model: azure/azure-embedding-model  
   api_base: "os.environ/AZURE_API_BASE"  
   api_key: "os.environ/AZURE_API_KEY"  
   api_version: "2023-07-01-preview"  
  model_info:  
   access_groups: ["beta-models"] # 👈 Model Access Group  

```

**Step 2. Create key with access group**
```
curl --location 'http://localhost:4000/user/new' \  
-H 'Authorization: Bearer <your-master-key>' \  
-H 'Content-Type: application/json' \  
-d '{"models": ["beta-models"], # 👈 Model Access Group  
      "max_budget": 0}'  

```

## Create new keys for existing internal user​
Just include user_id in the `/key/generate` request.
```
curl --location 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data '{"models": ["azure-models"], "user_id": "krrish@berri.ai"}'  

```

## API Specification​
### `GenericBudgetInfo`​
A Pydantic model that defines budget information with a time period and limit.
```
class GenericBudgetInfo(BaseModel):  
  budget_limit: float # The maximum budget amount in USD  
  time_period: str  # Duration string like "1d", "30d", etc.  

```

#### Fields:​
  * `budget_limit` (float): The maximum budget amount in USD
  * `time_period` (str): Duration string specifying the time period for the budget. Supported formats:
    * Seconds: "30s"
    * Minutes: "30m" 
    * Hours: "30h"
    * Days: "30d"


#### Example:​
```
{  
 "budget_limit": "0.0001",  
 "time_period": "1d"  
}  

```

Previous
Billing
Next
✨ Temporary Budget Increase
  * Set Budgets
    * Global Proxy
    * Team
    * Team Members
    * Internal User
    * Virtual Key
    * ✨ Virtual Key (Model Specific)
    * Customers
  * Reset Budgets
  * Set Rate Limits
  * Set default budget for ALL internal users
  * Grant Access to new model
  * Create new keys for existing internal user
  * API Specification
    * `GenericBudgetInfo`


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Custom Callbacks


On this page
# Custom Callbacks
info
**For PROXY** Go Here
## Callback Class​
You can create a custom callback class to precisely log events as they occur in litellm. 
```
import litellm  
from litellm.integrations.custom_logger import CustomLogger  
from litellm import completion, acompletion  
  
class MyCustomHandler(CustomLogger):  
  def log_pre_api_call(self, model, messages, kwargs):   
    print(f"Pre-API Call")  
    
  def log_post_api_call(self, kwargs, response_obj, start_time, end_time):   
    print(f"Post-API Call")  
    
  
  def log_success_event(self, kwargs, response_obj, start_time, end_time):   
    print(f"On Success")  
  
  def log_failure_event(self, kwargs, response_obj, start_time, end_time):   
    print(f"On Failure")  
    
  #### ASYNC #### - for acompletion/aembeddings  
  
  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Success")  
  
  async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Failure")  
  
customHandler = MyCustomHandler()  
  
litellm.callbacks = [customHandler]  
  
## sync   
response = completion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],  
               stream=True)  
for chunk in response:   
  continue  
  
  
## async  
import asyncio   
  
def async completion():  
  response = await acompletion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],  
               stream=True)  
  async for chunk in response:   
    continue  
asyncio.run(completion())  

```

## Callback Functions​
If you just want to log on a specific event (e.g. on input) - you can use callback functions. 
You can set custom callbacks to trigger for:
  * `litellm.input_callback` - Track inputs/transformed inputs before making the LLM API call
  * `litellm.success_callback` - Track inputs/outputs after making LLM API call
  * `litellm.failure_callback` - Track inputs/outputs + exceptions for litellm calls


## Defining a Custom Callback Function​
Create a custom callback function that takes specific arguments:
```
def custom_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  # Your custom code here  
  print("LITELLM: in custom callback function")  
  print("kwargs", kwargs)  
  print("completion_response", completion_response)  
  print("start_time", start_time)  
  print("end_time", end_time)  

```

### Setting the custom callback function​
```
import litellm  
litellm.success_callback = [custom_callback]  

```

## Using Your Custom Callback Function​
```
import litellm  
from litellm import completion  
  
# Assign the custom callback function  
litellm.success_callback = [custom_callback]  
  
response = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hi 👋 - i'm openai"  
    }  
  ]  
)  
  
print(response)  
  

```

## Async Callback Functions​
We recommend using the Custom Logger class for async.
```
from litellm.integrations.custom_logger import CustomLogger  
from litellm import acompletion   
  
class MyCustomHandler(CustomLogger):  
  #### ASYNC ####   
    
  
  
  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Success")  
  
  async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Failure")  
  
import asyncio   
customHandler = MyCustomHandler()  
  
litellm.callbacks = [customHandler]  
  
def async completion():  
  response = await acompletion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],  
               stream=True)  
  async for chunk in response:   
    continue  
asyncio.run(completion())  

```

**Functions**
If you just want to pass in an async function for logging. 
LiteLLM currently supports just async success callback functions for async completion/embedding calls. 
```
import asyncio, litellm   
  
async def async_test_logging_fn(kwargs, completion_obj, start_time, end_time):  
  print(f"On Async Success!")  
  
async def test_chat_openai():  
  try:  
    # litellm.set_verbose = True  
    litellm.success_callback = [async_test_logging_fn]  
    response = await litellm.acompletion(model="gpt-3.5-turbo",  
               messages=[{  
                 "role": "user",  
                 "content": "Hi 👋 - i'm openai"  
               }],  
               stream=True)  
    async for chunk in response:   
      continue  
  except Exception as e:  
    print(e)  
    pytest.fail(f"An error occurred - {str(e)}")  
  
asyncio.run(test_chat_openai())  

```

info
We're actively trying to expand this to other event types. Tell us if you need this!
## What's in kwargs?​
Notice we pass in a kwargs argument to custom callback. 
```
def custom_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  # Your custom code here  
  print("LITELLM: in custom callback function")  
  print("kwargs", kwargs)  
  print("completion_response", completion_response)  
  print("start_time", start_time)  
  print("end_time", end_time)  

```

This is a dictionary containing all the model-call details (the params we receive, the values we send to the http endpoint, the response we receive, stacktrace in case of errors, etc.). 
This is all logged in the model_call_details via our Logger.
Here's exactly what you can expect in the kwargs dictionary:
```
### DEFAULT PARAMS ###   
"model": self.model,  
"messages": self.messages,  
"optional_params": self.optional_params, # model-specific params passed in  
"litellm_params": self.litellm_params, # litellm-specific params passed in (e.g. metadata passed to completion call)  
"start_time": self.start_time, # datetime object of when call was started  
  
### PRE-API CALL PARAMS ### (check via kwargs["log_event_type"]="pre_api_call")  
"input" = input # the exact prompt sent to the LLM API  
"api_key" = api_key # the api key used for that LLM API   
"additional_args" = additional_args # any additional details for that API call (e.g. contains optional params sent)  
  
### POST-API CALL PARAMS ### (check via kwargs["log_event_type"]="post_api_call")  
"original_response" = original_response # the original http response received (saved via response.text)  
  
### ON-SUCCESS PARAMS ### (check via kwargs["log_event_type"]="successful_api_call")  
"complete_streaming_response" = complete_streaming_response # the complete streamed response (only set if `completion(..stream=True)`)  
"end_time" = end_time # datetime object of when call was completed  
  
### ON-FAILURE PARAMS ### (check via kwargs["log_event_type"]="failed_api_call")  
"exception" = exception # the Exception raised  
"traceback_exception" = traceback_exception # the traceback generated via `traceback.format_exc()`  
"end_time" = end_time # datetime object of when call was completed  

```

### Cache hits​
Cache hits are logged in success events as `kwarg["cache_hit"]`. 
Here's an example of accessing it: 
```
import litellm  
from litellm.integrations.custom_logger import CustomLogger  
from litellm import completion, acompletion, Cache  
  
class MyCustomHandler(CustomLogger):  
 async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):   
   print(f"On Success")  
   print(f"Value of Cache hit: {kwargs['cache_hit']"})  
  
async def test_async_completion_azure_caching():  
 customHandler_caching = MyCustomHandler()  
 litellm.cache = Cache(type="redis", host=os.environ['REDIS_HOST'], port=os.environ['REDIS_PORT'], password=os.environ['REDIS_PASSWORD'])  
 litellm.callbacks = [customHandler_caching]  
 unique_time = time.time()  
 response1 = await litellm.acompletion(model="azure/chatgpt-v-2",  
             messages=[{  
               "role": "user",  
               "content": f"Hi 👋 - i'm async azure {unique_time}"  
             }],  
             caching=True)  
 await asyncio.sleep(1)  
 print(f"customHandler_caching.states pre-cache hit: {customHandler_caching.states}")  
 response2 = await litellm.acompletion(model="azure/chatgpt-v-2",  
             messages=[{  
               "role": "user",  
               "content": f"Hi 👋 - i'm async azure {unique_time}"  
             }],  
             caching=True)  
 await asyncio.sleep(1) # success callbacks are done in parallel  
 print(f"customHandler_caching.states post-cache hit: {customHandler_caching.states}")  
 assert len(customHandler_caching.errors) == 0  
 assert len(customHandler_caching.states) == 4 # pre, post, success, success  

```

### Get complete streaming response​
LiteLLM will pass you the complete streaming response in the final streaming chunk as part of the kwargs for your custom callback function.
```
# litellm.set_verbose = False  
    def custom_callback(  
      kwargs,         # kwargs to completion  
      completion_response,  # response from completion  
      start_time, end_time  # start/end time  
    ):  
      # print(f"streaming response: {completion_response}")  
      if "complete_streaming_response" in kwargs:   
        print(f"Complete Streaming Response: {kwargs['complete_streaming_response']}")  
      
    # Assign the custom callback function  
    litellm.success_callback = [custom_callback]  
  
    response = completion(model="claude-instant-1", messages=messages, stream=True)  
    for idx, chunk in enumerate(response):   
      pass  

```

### Log additional metadata​
LiteLLM accepts a metadata dictionary in the completion call. You can pass additional metadata into your completion call via `completion(..., metadata={"key": "value"})`. 
Since this is a litellm-specific param, it's accessible via kwargs["litellm_params"]
```
from litellm import completion  
import os, litellm  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
def custom_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  print(kwargs["litellm_params"]["metadata"])  
    
  
# Assign the custom callback function  
litellm.success_callback = [custom_callback]  
  
response = litellm.completion(model="gpt-3.5-turbo", messages=messages, metadata={"hello": "world"})  

```

## Examples​
### Custom Callback to track costs for Streaming + Non-Streaming​
By default, the response cost is accessible in the logging object via `kwargs["response_cost"]` on success (sync + async)
```
  
# Step 1. Write your custom callback function  
def track_cost_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  try:  
    response_cost = kwargs["response_cost"] # litellm calculates response cost for you  
    print("regular response_cost", response_cost)  
  except:  
    pass  
  
# Step 2. Assign the custom callback function  
litellm.success_callback = [track_cost_callback]  
  
# Step 3. Make litellm.completion call  
response = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hi 👋 - i'm openai"  
    }  
  ]  
)  
  
print(response)  

```

### Custom Callback to log transformed Input to LLMs​
```
def get_transformed_inputs(  
  kwargs,  
):  
  params_to_model = kwargs["additional_args"]["complete_input_dict"]  
  print("params to model", params_to_model)  
  
litellm.input_callback = [get_transformed_inputs]  
  
def test_chat_openai():  
  try:  
    response = completion(model="claude-2",  
               messages=[{  
                 "role": "user",  
                 "content": "Hi 👋 - i'm openai"  
               }])  
  
    print(response)  
  
  except Exception as e:  
    print(e)  
    pass  

```

#### Output​
```
params to model {'model': 'claude-2', 'prompt': "\n\nHuman: Hi 👋 - i'm openai\n\nAssistant: ", 'max_tokens_to_sample': 256}  

```

### Custom Callback to write to Mixpanel​
```
import mixpanel  
import litellm  
from litellm import completion  
  
def custom_callback(  
  kwargs,         # kwargs to completion  
  completion_response,  # response from completion  
  start_time, end_time  # start/end time  
):  
  # Your custom code here  
  mixpanel.track("LLM Response", {"llm_response": completion_response})  
  
  
# Assign the custom callback function  
litellm.success_callback = [custom_callback]  
  
response = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hi 👋 - i'm openai"  
    }  
  ]  
)  
  
print(response)  
  

```

Previous
Raw Request/Response Logging
Next
Humanloop
  * Callback Class
  * Callback Functions
  * Defining a Custom Callback Function
    * Setting the custom callback function
  * Using Your Custom Callback Function
  * Async Callback Functions
  * What's in kwargs?
    * Cache hits
    * Get complete streaming response
    * Log additional metadata
  * Examples
    * Custom Callback to track costs for Streaming + Non-Streaming
    * Custom Callback to log transformed Input to LLMs
    * Custom Callback to write to Mixpanel


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
      * Logging
      * StandardLoggingPayload Specification
      * Team/Key Based Logging
      * 📈 Prometheus metrics
      * Alerting / Webhooks
      * PagerDuty Alerting
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Logging, Alerting, Metrics
  * Logging


On this page
# Logging
Log Proxy input, output, and exceptions using:
  * Langfuse
  * OpenTelemetry
  * GCS, s3, Azure (Blob) Buckets
  * Lunary
  * MLflow
  * Custom Callbacks
  * Langsmith
  * DataDog
  * DynamoDB
  * etc.


## Getting the LiteLLM Call ID​
LiteLLM generates a unique `call_id` for each request. This `call_id` can be used to track the request across the system. This can be very useful for finding the info for a particular request in a logging system like one of the systems mentioned in this page.
```
curl -i -sSL --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
   "model": "gpt-3.5-turbo",  
   "messages": [{"role": "user", "content": "what llm are you"}]  
  }' | grep 'x-litellm'  

```

The output of this is:
```
x-litellm-call-id: b980db26-9512-45cc-b1da-c511a363b83f  
x-litellm-model-id: cb41bc03f4c33d310019bae8c5afdb1af0a8f97b36a234405a9807614988457c  
x-litellm-model-api-base: https://x-example-1234.openai.azure.com  
x-litellm-version: 1.40.21  
x-litellm-response-cost: 2.85e-05  
x-litellm-key-tpm-limit: None  
x-litellm-key-rpm-limit: None  

```

A number of these headers could be useful for troubleshooting, but the `x-litellm-call-id` is the one that is most useful for tracking a request across components in your system, including in logging tools.
## Logging Features​
### Conditional Logging by Virtual Keys, Teams​
Use this to:
  1. Conditionally enable logging for some virtual keys/teams
  2. Set different logging providers for different virtual keys/teams


👉 **Get Started** - Team/Key Based Logging
### Redacting UserAPIKeyInfo​
Redact information about the user api key (hashed token, user_id, team id, etc.), from logs. 
Currently supported for Langfuse, OpenTelemetry, Logfire, ArizeAI logging.
```
litellm_settings:   
 callbacks: ["langfuse"]  
 redact_user_api_key_info: true  

```

### Redact Messages, Response Content​
Set `litellm.turn_off_message_logging=True` This will prevent the messages and responses from being logged to your logging provider, but request metadata - e.g. spend, will still be tracked.
  * Global
  * Per Request


**1. Setup config.yaml**
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 success_callback: ["langfuse"]  
 turn_off_message_logging: True # 👈 Key Change  

```

**2. Send request**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
}'  

```

info
Dynamic request message redaction is in BETA. 
Pass in a request header to enable message redaction for a request.
```
x-litellm-enable-message-redaction: true  

```

Example config.yaml
**1. Setup config.yaml**
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  

```

**2. Setup per request header**
```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-zV5HlSIm8ihj1F9C_ZbB1g' \  
-H 'x-litellm-enable-message-redaction: true' \  
-d '{  
 "model": "gpt-3.5-turbo-testing",  
 "messages": [  
  {  
   "role": "user",  
   "content": "Hey, how'\''s it going 1234?"  
  }  
 ]  
}'  

```

**3. Check Logging Tool + Spend Logs**
**Logging Tool**
**Spend Logs**
### Disable Message Redaction​
If you have `litellm.turn_on_message_logging` turned on, you can override it for specific requests by setting a request header `LiteLLM-Disable-Message-Redaction: true`.
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --header 'LiteLLM-Disable-Message-Redaction: true' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
}'  

```

### Turn off all tracking/logging​
For some use cases, you may want to turn off all tracking/logging. You can do this by passing `no-log=True` in the request body.
info
Disable this by setting `global_disable_no_log_param:true` in your config.yaml file.
```
litellm_settings:  
 global_disable_no_log_param: True  

```

  * Curl Request
  * OpenAI


```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer <litellm-api-key>' \  
-d '{  
  "model": "openai/gpt-3.5-turbo",  
  "messages": [  
   {  
    "role": "user",  
    "content": [  
     {  
      "type": "text",  
      "text": "What'\''s in this image?"  
     }  
    ]  
   }  
  ],  
  "max_tokens": 300,  
  "no-log": true # 👈 Key Change  
}'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
   "no-log": True # 👈 Key Change  
  }  
)  
  
print(response)  

```

**Expected Console Log**
```
LiteLLM.Info: "no-log request, skipping logging"  

```

## What gets logged?​
Found under `kwargs["standard_logging_object"]`. This is a standard payload, logged for every response.
👉 **Standard Logging Payload Specification**
## Langfuse​
We will use the `--config` to set `litellm.success_callback = ["langfuse"]` this will log all successful LLM calls to langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your environment
**Step 1** Install langfuse
```
pip install langfuse>=2.0.0  

```

**Step 2** : Create a `config.yaml` file and set `litellm_settings`: `success_callback`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 success_callback: ["langfuse"]  

```

**Step 3** : Set required env variables for logging to langfuse
```
export LANGFUSE_PUBLIC_KEY="pk_kk"  
export LANGFUSE_SECRET_KEY="sk_ss"  
# Optional, defaults to https://cloud.langfuse.com  
export LANGFUSE_HOST="https://xxx.langfuse.com"  

```

**Step 4** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --debug  

```

Test Request
```
litellm --test  

```

Expected output on Langfuse
### Logging Metadata to Langfuse​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


Pass `metadata` as part of the request body
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "metadata": {  
    "generation_name": "ishaan-test-generation",  
    "generation_id": "gen-id22",  
    "trace_id": "trace-id22",  
    "trace_user_id": "user-id2"  
  }  
}'  

```

Set `extra_body={"metadata": { }}` to `metadata` you want to pass
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
    "metadata": {  
      "generation_name": "ishaan-generation-openai-client",  
      "generation_id": "openai-client-gen-id22",  
      "trace_id": "openai-client-trace-id22",  
      "trace_user_id": "openai-client-user-id2"  
    }  
  }  
)  
  
print(response)  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "gpt-3.5-turbo",  
  temperature=0.1,  
  extra_body={  
    "metadata": {  
      "generation_name": "ishaan-generation-langchain-client",  
      "generation_id": "langchain-client-gen-id22",  
      "trace_id": "langchain-client-trace-id22",  
      "trace_user_id": "langchain-client-user-id2"  
    }  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

### Custom Tags​
Set `tags` as part of your request body
  * OpenAI Python v1.0.0+
  * Curl Request
  * Langchain


```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="llama3",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  user="palantir",  
  extra_body={  
    "metadata": {  
      "tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"]  
    }  
  }  
)  
  
print(response)  

```

Pass `metadata` as part of the request body
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --header 'Authorization: Bearer sk-1234' \  
  --data '{  
  "model": "llama3",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "user": "palantir",  
  "metadata": {  
    "tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"]  
  }  
}'  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
import os  
  
os.environ["OPENAI_API_KEY"] = "sk-1234"  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "llama3",  
  user="palantir",  
  extra_body={  
    "metadata": {  
      "tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"]  
    }  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

### LiteLLM Tags - `cache_hit`, `cache_key`​
Use this if you want to control which LiteLLM-specific fields are logged as tags by the LiteLLM proxy. By default LiteLLM Proxy logs no LiteLLM-specific fields
LiteLLM specific field| Description| Example Value  
---|---|---  
`cache_hit`| Indicates whether a cache hit occurred (True) or not (False)| `true`, `false`  
`cache_key`| The Cache key used for this request| `d2b758c****`  
`proxy_base_url`| The base URL for the proxy server, the value of env var `PROXY_BASE_URL` on your server| `https://proxy.example.com`  
`user_api_key_alias`| An alias for the LiteLLM Virtual Key.| `prod-app1`  
`user_api_key_user_id`| The unique ID associated with a user's API key.| `user_123`, `user_456`  
`user_api_key_user_email`| The email associated with a user's API key.| `user@example.com`, `admin@example.com`  
`user_api_key_team_alias`| An alias for a team associated with an API key.| `team_alpha`, `dev_team`  
**Usage**
Specify `langfuse_default_tags` to control what litellm fields get logged on Langfuse
Example config.yaml 
```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 success_callback: ["langfuse"]  
  
 # 👇 Key Change  
 langfuse_default_tags: ["cache_hit", "cache_key", "proxy_base_url", "user_api_key_alias", "user_api_key_user_id", "user_api_key_user_email", "user_api_key_team_alias", "semantic-similarity", "proxy_base_url"]  

```

### View POST sent from LiteLLM to provider​
Use this when you want to view the RAW curl request sent from LiteLLM to the LLM API 
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


Pass `metadata` as part of the request body
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "metadata": {  
    "log_raw_request": true  
  }  
}'  

```

Set `extra_body={"metadata": {"log_raw_request": True }}` to `metadata` you want to pass
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
    "metadata": {  
      "log_raw_request": True  
    }  
  }  
)  
  
print(response)  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "gpt-3.5-turbo",  
  temperature=0.1,  
  extra_body={  
    "metadata": {  
      "log_raw_request": True  
    }  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

**Expected Output on Langfuse**
You will see `raw_request` in your Langfuse Metadata. This is the RAW CURL command sent from LiteLLM to your LLM API provider
## OpenTelemetry​
info
[Optional] Customize OTEL Service Name and OTEL TRACER NAME by setting the following variables in your environment
```
OTEL_TRACER_NAME=<your-trace-name>   # default="litellm"  
OTEL_SERVICE_NAME=<your-service-name>` # default="litellm"  

```

  * Log to console
  * Log to Honeycomb
  * Log to Traceloop Cloud
  * Log to OTEL HTTP Collector
  * Log to OTEL GRPC Collector


**Step 1:** Set callbacks and env vars
Add the following to your env
```
OTEL_EXPORTER="console"  

```

Add `otel` as a callback on your `litellm_config.yaml`
```
litellm_settings:  
 callbacks: ["otel"]  

```

**Step 2** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --detailed_debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
  }'  

```

**Step 3** : **Expect to see the following logged on your server logs / console**
This is the Span from OTEL Logging
```
{  
  "name": "litellm-acompletion",  
  "context": {  
    "trace_id": "0x8d354e2346060032703637a0843b20a3",  
    "span_id": "0xd8d3476a2eb12724",  
    "trace_state": "[]"  
  },  
  "kind": "SpanKind.INTERNAL",  
  "parent_id": null,  
  "start_time": "2024-06-04T19:46:56.415888Z",  
  "end_time": "2024-06-04T19:46:56.790278Z",  
  "status": {  
    "status_code": "OK"  
  },  
  "attributes": {  
    "model": "llama3-8b-8192"  
  },  
  "events": [],  
  "links": [],  
  "resource": {  
    "attributes": {  
      "service.name": "litellm"  
    },  
    "schema_url": ""  
  }  
}  

```

#### Quick Start - Log to Honeycomb​
**Step 1:** Set callbacks and env vars
Add the following to your env
```
OTEL_EXPORTER="otlp_http"  
OTEL_ENDPOINT="https://api.honeycomb.io/v1/traces"  
OTEL_HEADERS="x-honeycomb-team=<your-api-key>"  

```

Add `otel` as a callback on your `litellm_config.yaml`
```
litellm_settings:  
 callbacks: ["otel"]  

```

**Step 2** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --detailed_debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
  }'  

```

#### Quick Start - Log to Traceloop​
**Step 1:** Add the following to your env
```
OTEL_EXPORTER="otlp_http"  
OTEL_ENDPOINT="https://api.traceloop.com"  
OTEL_HEADERS="Authorization=Bearer%20<your-api-key>"  

```

**Step 2:** Add `otel` as a callbacks
```
litellm_settings:  
 callbacks: ["otel"]  

```

**Step 3** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --detailed_debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
  }'  

```

#### Quick Start - Log to OTEL Collector​
**Step 1:** Set callbacks and env vars
Add the following to your env
```
OTEL_EXPORTER="otlp_http"  
OTEL_ENDPOINT="http://0.0.0.0:4317"  
OTEL_HEADERS="x-honeycomb-team=<your-api-key>" # Optional  

```

Add `otel` as a callback on your `litellm_config.yaml`
```
litellm_settings:  
 callbacks: ["otel"]  

```

**Step 2** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --detailed_debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
  }'  

```

#### Quick Start - Log to OTEL GRPC Collector​
**Step 1:** Set callbacks and env vars
Add the following to your env
```
OTEL_EXPORTER="otlp_grpc"  
OTEL_ENDPOINT="http:/0.0.0.0:4317"  
OTEL_HEADERS="x-honeycomb-team=<your-api-key>" # Optional  

```

Add `otel` as a callback on your `litellm_config.yaml`
```
litellm_settings:  
 callbacks: ["otel"]  

```

**Step 2** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --detailed_debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
  }'  

```

**🎉 Expect to see this trace logged in your OTEL collector**
### Redacting Messages, Response Content​
Set `message_logging=False` for `otel`, no messages / response will be logged
```
litellm_settings:  
 callbacks: ["otel"]  
  
## 👇 Key Change  
callback_settings:  
 otel:  
  message_logging: False  

```

### Traceparent Header​
##### Context propagation across Services `Traceparent HTTP Header`​
❓ Use this when you want to **pass information about the incoming request in a distributed tracing system**
✅ Key change: Pass the **`traceparent`header** in your requests. Read more about traceparent headers here
```
traceparent: 00-80e1afed08e019fc1110464cfa66635c-7a085853722dc6d2-01  

```

Example Usage
  1. Make Request to LiteLLM Proxy with `traceparent` header


```
import openai  
import uuid  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
example_traceparent = f"00-80e1afed08e019fc1110464cfa66635c-02e80198930058d4-01"  
extra_headers = {  
  "traceparent": example_traceparent  
}  
_trace_id = example_traceparent.split("-")[1]  
  
print("EXTRA HEADERS: ", extra_headers)  
print("Trace ID: ", _trace_id)  
  
response = client.chat.completions.create(  
  model="llama3",  
  messages=[  
    {"role": "user", "content": "this is a test request, write a short poem"}  
  ],  
  extra_headers=extra_headers,  
)  
  
print(response)  

```

```
# EXTRA HEADERS: {'traceparent': '00-80e1afed08e019fc1110464cfa66635c-02e80198930058d4-01'}  
# Trace ID: 80e1afed08e019fc1110464cfa66635c  

```

  1. Lookup Trace ID on OTEL Logger


Search for Trace=`80e1afed08e019fc1110464cfa66635c` on your OTEL Collector
##### Forwarding `Traceparent HTTP Header` to LLM APIs​
Use this if you want to forward the traceparent headers to your self hosted LLMs like vLLM
Set `forward_traceparent_to_llm_provider: True` in your `config.yaml`. This will forward the `traceparent` header to your LLM API
danger
Only use this for self hosted LLMs, this can cause Bedrock, VertexAI calls to fail
```
litellm_settings:  
 forward_traceparent_to_llm_provider: True  

```

## Google Cloud Storage Buckets​
Log LLM Logs to Google Cloud Storage Buckets
info
✨ This is an Enterprise only feature Get Started with Enterprise here
Property| Details  
---|---  
Description| Log LLM Input/Output to cloud storage buckets  
Load Test Benchmarks| Benchmarks  
Google Docs on Cloud Storage| Google Cloud Storage  
#### Usage​
  1. Add `gcs_bucket` to LiteLLM Config.yaml


```
model_list:  
- litellm_params:  
  api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  api_key: my-fake-key  
  model: openai/my-fake-model  
 model_name: fake-openai-endpoint  
  
litellm_settings:  
 callbacks: ["gcs_bucket"] # 👈 KEY CHANGE # 👈 KEY CHANGE  

```

  1. Set required env variables


```
GCS_BUCKET_NAME="<your-gcs-bucket-name>"  
GCS_PATH_SERVICE_ACCOUNT="/Users/ishaanjaffer/Downloads/adroit-crow-413218-a956eef1a2a8.json" # Add path to service account.json  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

#### Expected Logs on GCS Buckets​
#### Fields Logged on GCS Buckets​
**The standard logging object is logged on GCS Bucket**
#### Getting `service_account.json` from Google Cloud Console​
  1. Go to Google Cloud Console
  2. Search for IAM & Admin
  3. Click on Service Accounts
  4. Select a Service Account
  5. Click on 'Keys' -> Add Key -> Create New Key -> JSON
  6. Save the JSON file and add the path to `GCS_PATH_SERVICE_ACCOUNT`


## Google Cloud Storage - PubSub Topic​
Log LLM Logs/SpendLogs to Google Cloud Storage PubSub Topic
info
✨ This is an Enterprise only feature Get Started with Enterprise here
Property| Details  
---|---  
Description| Log LiteLLM `SpendLogs Table` to Google Cloud Storage PubSub Topic  
When to use `gcs_pubsub`?
  * If your LiteLLM Database has crossed 1M+ spend logs and you want to send `SpendLogs` to a PubSub Topic that can be consumed by GCS BigQuery


#### Usage​
  1. Add `gcs_pubsub` to LiteLLM Config.yaml


```
model_list:  
- litellm_params:  
  api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  api_key: my-fake-key  
  model: openai/my-fake-model  
 model_name: fake-openai-endpoint  
  
litellm_settings:  
 callbacks: ["gcs_pubsub"] # 👈 KEY CHANGE # 👈 KEY CHANGE  

```

  1. Set required env variables


```
GCS_PUBSUB_TOPIC_ID="litellmDB"  
GCS_PUBSUB_PROJECT_ID="reliableKeys"  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

## s3 Buckets​
We will use the `--config` to set 
  * `litellm.success_callback = ["s3"]`


This will log all successful LLM calls to s3 Bucket
**Step 1** Set AWS Credentials in .env
```
AWS_ACCESS_KEY_ID = ""  
AWS_SECRET_ACCESS_KEY = ""  
AWS_REGION_NAME = ""  

```

**Step 2** : Create a `config.yaml` file and set `litellm_settings`: `success_callback`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 success_callback: ["s3"]  
 s3_callback_params:  
  s3_bucket_name: logs-bucket-litellm  # AWS Bucket Name for S3  
  s3_region_name: us-west-2       # AWS Region Name for S3  
  s3_aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID # us os.environ/<variable name> to pass environment variables. This is AWS Access Key ID for S3  
  s3_aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY # AWS Secret Access Key for S3  
  s3_path: my-test-path # [OPTIONAL] set path in bucket you want to write logs to  
  s3_endpoint_url: https://s3.amazonaws.com # [OPTIONAL] S3 endpoint URL, if you want to use Backblaze/cloudflare s3 buckets  

```

**Step 3** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "Azure OpenAI GPT-4 East",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
  }'  

```

Your logs should be available on the specified s3 Bucket
### Team Alias Prefix in Object Key​
**This is a preview feature**
You can add the team alias to the object key by setting the `team_alias` in the `config.yaml` file. This will prefix the object key with the team alias.
```
litellm_settings:  
 callbacks: ["s3"]  
 enable_preview_features: true  
 s3_callback_params:  
  s3_bucket_name: logs-bucket-litellm  
  s3_region_name: us-west-2  
  s3_aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
  s3_aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
  s3_path: my-test-path  
  s3_endpoint_url: https://s3.amazonaws.com  
  s3_use_team_prefix: true  

```

On s3 bucket, you will see the object key as `my-test-path/my-team-alias/...`
## Azure Blob Storage​
Log LLM Logs to Azure Data Lake Storage
info
✨ This is an Enterprise only feature Get Started with Enterprise here
Property| Details  
---|---  
Description| Log LLM Input/Output to Azure Blob Storage (Bucket)  
Azure Docs on Data Lake Storage| Azure Data Lake Storage  
#### Usage​
  1. Add `azure_storage` to LiteLLM Config.yaml


```
model_list:  
 - model_name: fake-openai-endpoint  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["azure_storage"] # 👈 KEY CHANGE # 👈 KEY CHANGE  

```

  1. Set required env variables


```
# Required Environment Variables for Azure Storage  
AZURE_STORAGE_ACCOUNT_NAME="litellm2" # The name of the Azure Storage Account to use for logging  
AZURE_STORAGE_FILE_SYSTEM="litellm-logs" # The name of the Azure Storage File System to use for logging. (Typically the Container name)  
  
# Authentication Variables  
# Option 1: Use Storage Account Key  
AZURE_STORAGE_ACCOUNT_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # The Azure Storage Account Key to use for Authentication  
  
# Option 2: Use Tenant ID + Client ID + Client Secret  
AZURE_STORAGE_TENANT_ID="985efd7cxxxxxxxxxx" # The Application Tenant ID to use for Authentication  
AZURE_STORAGE_CLIENT_ID="abe66585xxxxxxxxxx" # The Application Client ID to use for Authentication  
AZURE_STORAGE_CLIENT_SECRET="uMS8Qxxxxxxxxxx" # The Application Client Secret to use for Authentication  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

#### Expected Logs on Azure Data Lake Storage​
#### Fields Logged on Azure Data Lake Storage​
**The standard logging object is logged on Azure Data Lake Storage**
## DataDog​
LiteLLM Supports logging to the following Datdog Integrations:
  * `datadog` Datadog Logs
  * `datadog_llm_observability` Datadog LLM Observability
  * `ddtrace-run` Datadog Tracing


  * Datadog Logs
  * Datadog LLM Observability


We will use the `--config` to set `litellm.callbacks = ["datadog"]` this will log all successful LLM calls to DataDog
**Step 1** : Create a `config.yaml` file and set `litellm_settings`: `success_callback`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 callbacks: ["datadog"] # logs llm success + failure logs on datadog  
 service_callback: ["datadog"] # logs redis, postgres failures on datadog  

```

```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 callbacks: ["datadog_llm_observability"] # logs llm success logs on datadog  

```

**Step 2** : Set Required env variables for datadog
```
DD_API_KEY="5f2d0f310***********" # your datadog API Key  
DD_SITE="us5.datadoghq.com"    # your datadog base url  
DD_SOURCE="litellm_dev"    # [OPTIONAL] your datadog source. use to differentiate dev vs. prod deployments  

```

**Step 3** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "metadata": {  
    "your-custom-metadata": "custom-field",  
  }  
}'  

```

Expected output on Datadog
#### Datadog Tracing​
Use `ddtrace-run` to enable Datadog Tracing on litellm proxy
Pass `USE_DDTRACE=true` to the docker run command. When `USE_DDTRACE=true`, the proxy will run `ddtrace-run litellm` as the `ENTRYPOINT` instead of just `litellm`
```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e USE_DDTRACE=true \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm:main-latest \  
  --config /app/config.yaml --detailed_debug  

```

### Set DD variables (`DD_SERVICE` etc)​
LiteLLM supports customizing the following Datadog environment variables
Environment Variable| Description| Default Value| Required  
---|---|---|---  
`DD_API_KEY`| Your Datadog API key for authentication| None| ✅ Yes  
`DD_SITE`| Your Datadog site (e.g., "us5.datadoghq.com")| None| ✅ Yes  
`DD_ENV`| Environment tag for your logs (e.g., "production", "staging")| "unknown"| ❌ No  
`DD_SERVICE`| Service name for your logs| "litellm-server"| ❌ No  
`DD_SOURCE`| Source name for your logs| "litellm"| ❌ No  
`DD_VERSION`| Version tag for your logs| "unknown"| ❌ No  
`HOSTNAME`| Hostname tag for your logs| ""| ❌ No  
`POD_NAME`| Pod name tag (useful for Kubernetes deployments)| "unknown"| ❌ No  
## Lunary​
#### Step1: Install dependencies and set your environment variables​
Install the dependencies
```
pip install litellm lunary  

```

Get you Lunary public key from from https://app.lunary.ai/settings
```
export LUNARY_PUBLIC_KEY="<your-public-key>"  

```

#### Step 2: Create a `config.yaml` and set `lunary` callbacks​
```
model_list:  
 - model_name: "*"  
  litellm_params:  
   model: "*"  
litellm_settings:  
 success_callback: ["lunary"]  
 failure_callback: ["lunary"]  

```

#### Step 3: Start the LiteLLM proxy​
```
litellm --config config.yaml  

```

#### Step 4: Make a request​
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-d '{  
  "model": "gpt-4o",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   {  
    "role": "user",  
    "content": "how can I solve 8x + 7 = -23"  
   }  
  ]  
}'  

```

## MLflow​
#### Step1: Install dependencies​
Install the dependencies.
```
pip install litellm mlflow  

```

#### Step 2: Create a `config.yaml` with `mlflow` callback​
```
model_list:  
 - model_name: "*"  
  litellm_params:  
   model: "*"  
litellm_settings:  
 success_callback: ["mlflow"]  
 failure_callback: ["mlflow"]  

```

#### Step 3: Start the LiteLLM proxy​
```
litellm --config config.yaml  

```

#### Step 4: Make a request​
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-d '{  
  "model": "gpt-4o-mini",  
  "messages": [  
   {  
    "role": "user",  
    "content": "What is the capital of France?"  
   }  
  ]  
}'  

```

#### Step 5: Review traces​
Run the following command to start MLflow UI and review recorded traces.
```
mlflow ui  

```

## Custom Callback Class [Async]​
Use this when you want to run custom callbacks in `python`
#### Step 1 - Create your custom `litellm` callback class​
We use `litellm.integrations.custom_logger` for this, **more details about litellm custom callbackshere**
Define your custom callback class in a python file.
Here's an example custom logger for tracking `key, user, model, prompt, response, tokens, cost`. We create a file called `custom_callbacks.py` and initialize `proxy_handler_instance`
```
from litellm.integrations.custom_logger import CustomLogger  
import litellm  
  
# This file includes the custom callbacks for LiteLLM Proxy  
# Once defined, these can be passed in proxy_config.yaml  
class MyCustomHandler(CustomLogger):  
  def log_pre_api_call(self, model, messages, kwargs):   
    print(f"Pre-API Call")  
    
  def log_post_api_call(self, kwargs, response_obj, start_time, end_time):   
    print(f"Post-API Call")  
      
  def log_success_event(self, kwargs, response_obj, start_time, end_time):   
    print("On Success")  
  
  def log_failure_event(self, kwargs, response_obj, start_time, end_time):   
    print(f"On Failure")  
  
  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Success!")  
    # log: key, user, model, prompt, response, tokens, cost  
    # Access kwargs passed to litellm.completion()  
    model = kwargs.get("model", None)  
    messages = kwargs.get("messages", None)  
    user = kwargs.get("user", None)  
  
    # Access litellm_params passed to litellm.completion(), example access `metadata`  
    litellm_params = kwargs.get("litellm_params", {})  
    metadata = litellm_params.get("metadata", {})  # headers passed to LiteLLM proxy, can be found here  
  
    # Calculate cost using litellm.completion_cost()  
    cost = litellm.completion_cost(completion_response=response_obj)  
    response = response_obj  
    # tokens used in response   
    usage = response_obj["usage"]  
  
    print(  
      f"""  
        Model: {model},  
        Messages: {messages},  
        User: {user},  
        Usage: {usage},  
        Cost: {cost},  
        Response: {response}  
        Proxy Metadata: {metadata}  
      """  
    )  
    return  
  
  async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):   
    try:  
      print(f"On Async Failure !")  
      print("\nkwargs", kwargs)  
      # Access kwargs passed to litellm.completion()  
      model = kwargs.get("model", None)  
      messages = kwargs.get("messages", None)  
      user = kwargs.get("user", None)  
  
      # Access litellm_params passed to litellm.completion(), example access `metadata`  
      litellm_params = kwargs.get("litellm_params", {})  
      metadata = litellm_params.get("metadata", {})  # headers passed to LiteLLM proxy, can be found here  
  
      # Access Exceptions & Traceback  
      exception_event = kwargs.get("exception", None)  
      traceback_event = kwargs.get("traceback_exception", None)  
  
      # Calculate cost using litellm.completion_cost()  
      cost = litellm.completion_cost(completion_response=response_obj)  
      print("now checking response obj")  
        
      print(  
        f"""  
          Model: {model},  
          Messages: {messages},  
          User: {user},  
          Cost: {cost},  
          Response: {response_obj}  
          Proxy Metadata: {metadata}  
          Exception: {exception_event}  
          Traceback: {traceback_event}  
        """  
      )  
    except Exception as e:  
      print(f"Exception: {e}")  
  
proxy_handler_instance = MyCustomHandler()  
  
# Set litellm.callbacks = [proxy_handler_instance] on the proxy  
# need to set litellm.callbacks = [proxy_handler_instance] # on the proxy  

```

#### Step 2 - Pass your custom callback class in `config.yaml`​
We pass the custom callback class defined in **Step1** to the config.yaml. Set `callbacks` to `python_filename.logger_instance_name`
In the config below, we pass
  * python_filename: `custom_callbacks.py`
  * logger_instance_name: `proxy_handler_instance`. This is defined in Step 1


`callbacks: custom_callbacks.proxy_handler_instance`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
  
litellm_settings:  
 callbacks: custom_callbacks.proxy_handler_instance # sets litellm.callbacks = [proxy_handler_instance]  
  

```

#### Step 3 - Start proxy + test request​
```
litellm --config proxy_config.yaml  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "good morning good sir"  
    }  
  ],  
  "user": "ishaan-app",  
  "temperature": 0.2  
  }'  

```

#### Resulting Log on Proxy​
```
On Success  
  Model: gpt-3.5-turbo,  
  Messages: [{'role': 'user', 'content': 'good morning good sir'}],  
  User: ishaan-app,  
  Usage: {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21},  
  Cost: 3.65e-05,  
  Response: {'id': 'chatcmpl-8S8avKJ1aVBg941y5xzGMSKrYCMvN', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Good morning! How can I assist you today?', 'role': 'assistant'}}], 'created': 1701716913, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21}}  
  Proxy Metadata: {'user_api_key': None, 'headers': Headers({'host': '0.0.0.0:4000', 'user-agent': 'curl/7.88.1', 'accept': '*/*', 'authorization': 'Bearer sk-1234', 'content-length': '199', 'content-type': 'application/x-www-form-urlencoded'}), 'model_group': 'gpt-3.5-turbo', 'deployment': 'gpt-3.5-turbo-ModelID-gpt-3.5-turbo'}  

```

#### Logging Proxy Request Object, Header, Url​
Here's how you can access the `url`, `headers`, `request body` sent to the proxy for each request
```
class MyCustomHandler(CustomLogger):  
  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Success!")  
  
    litellm_params = kwargs.get("litellm_params", None)  
    proxy_server_request = litellm_params.get("proxy_server_request")  
    print(proxy_server_request)  

```

**Expected Output**
```
{  
 "url": "http://testserver/chat/completions",  
 "method": "POST",  
 "headers": {  
  "host": "testserver",  
  "accept": "*/*",  
  "accept-encoding": "gzip, deflate",  
  "connection": "keep-alive",  
  "user-agent": "testclient",  
  "authorization": "Bearer None",  
  "content-length": "105",  
  "content-type": "application/json"  
 },  
 "body": {  
  "model": "Azure OpenAI GPT-4 Canada",  
  "messages": [  
   {  
    "role": "user",  
    "content": "hi"  
   }  
  ],  
  "max_tokens": 10  
 }  
}  

```

#### Logging `model_info` set in config.yaml​
Here is how to log the `model_info` set in your proxy `config.yaml`. Information on setting `model_info` on config.yaml
```
class MyCustomHandler(CustomLogger):  
  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Success!")  
  
    litellm_params = kwargs.get("litellm_params", None)  
    model_info = litellm_params.get("model_info")  
    print(model_info)  

```

**Expected Output**
```
{'mode': 'embedding', 'input_cost_per_token': 0.002}  

```

##### Logging responses from proxy​
Both `/chat/completions` and `/embeddings` responses are available as `response_obj`
**Note: for`/chat/completions` , both `stream=True` and `non stream` responses are available as `response_obj`**
```
class MyCustomHandler(CustomLogger):  
  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):  
    print(f"On Async Success!")  
    print(response_obj)  
  

```

**Expected Output /chat/completion [for both`stream` and `non-stream` responses]**
```
ModelResponse(  
  id='chatcmpl-8Tfu8GoMElwOZuj2JlHBhNHG01PPo',  
  choices=[  
    Choices(  
      finish_reason='stop',  
      index=0,  
      message=Message(  
        content='As an AI language model, I do not have a physical body and therefore do not possess any degree or educational qualifications. My knowledge and abilities come from the programming and algorithms that have been developed by my creators.',  
        role='assistant'  
      )  
    )  
  ],  
  created=1702083284,  
  model='chatgpt-v-2',  
  object='chat.completion',  
  system_fingerprint=None,  
  usage=Usage(  
    completion_tokens=42,  
    prompt_tokens=5,  
    total_tokens=47  
  )  
)  

```

**Expected Output /embeddings**
```
{  
  'model': 'ada',  
  'data': [  
    {  
      'embedding': [  
        -0.035126980394124985, -0.020624293014407158, -0.015343423001468182,  
        -0.03980357199907303, -0.02750781551003456, 0.02111034281551838,  
        -0.022069307044148445, -0.019442008808255196, -0.00955679826438427,  
        -0.013143060728907585, 0.029583381488919258, -0.004725852981209755,  
        -0.015198921784758568, -0.014069183729588985, 0.00897879246622324,  
        0.01521205808967352,  
        # ... (truncated for brevity)  
      ]  
    }  
  ]  
}  

```

## Custom Callback APIs [Async]​
info
This is an Enterprise only feature Get Started with Enterprise here
Use this if you:
  * Want to use custom callbacks written in a non Python programming language
  * Want your callbacks to run on a different microservice


#### Step 1. Create your generic logging API endpoint​
Set up a generic API endpoint that can receive data in JSON format. The data will be included within a "data" field. 
Your server should support the following Request format:
```
curl --location https://your-domain.com/log-event \  
   --request POST \  
   --header "Content-Type: application/json" \  
   --data '{  
    "data": {  
     "id": "chatcmpl-8sgE89cEQ4q9biRtxMvDfQU1O82PT",  
     "call_type": "acompletion",  
     "cache_hit": "None",  
     "startTime": "2024-02-15 16:18:44.336280",  
     "endTime": "2024-02-15 16:18:45.045539",  
     "model": "gpt-3.5-turbo",  
     "user": "ishaan-2",  
     "modelParameters": "{'temperature': 0.7, 'max_tokens': 10, 'user': 'ishaan-2', 'extra_body': {}}",  
     "messages": "[{'role': 'user', 'content': 'This is a test'}]",  
     "response": "ModelResponse(id='chatcmpl-8sgE89cEQ4q9biRtxMvDfQU1O82PT', choices=[Choices(finish_reason='length', index=0, message=Message(content='Great! How can I assist you with this test', role='assistant'))], created=1708042724, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=10, prompt_tokens=11, total_tokens=21))",  
     "usage": "Usage(completion_tokens=10, prompt_tokens=11, total_tokens=21)",  
     "metadata": "{}",  
     "cost": "3.65e-05"  
    }  
   }'  

```

Reference FastAPI Python Server
Here's a reference FastAPI Server that is compatible with LiteLLM Proxy:
```
# this is an example endpoint to receive data from litellm  
from fastapi import FastAPI, HTTPException, Request  
  
app = FastAPI()  
  
  
@app.post("/log-event")  
async def log_event(request: Request):  
  try:  
    print("Received /log-event request")  
    # Assuming the incoming request has JSON data  
    data = await request.json()  
    print("Received request data:")  
    print(data)  
  
    # Your additional logic can go here  
    # For now, just printing the received data  
  
    return {"message": "Request received successfully"}  
  except Exception as e:  
    print(f"Error processing request: {str(e)}")  
    import traceback  
  
    traceback.print_exc()  
    raise HTTPException(status_code=500, detail="Internal Server Error")  
  
  
if __name__ == "__main__":  
  import uvicorn  
  uvicorn.run(app, host="127.0.0.1", port=4000)  

```

#### Step 2. Set your `GENERIC_LOGGER_ENDPOINT` to the endpoint + route we should send callback logs to​
```
os.environ["GENERIC_LOGGER_ENDPOINT"] = "http://localhost:4000/log-event"  

```

#### Step 3. Create a `config.yaml` file and set `litellm_settings`: `success_callback` = ["generic"]​
Example litellm proxy config.yaml
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 success_callback: ["generic"]  

```

Start the LiteLLM Proxy and make a test request to verify the logs reached your callback API 
## Langsmith​
  1. Set `success_callback: ["langsmith"]` on litellm config.yaml


If you're using a custom LangSmith instance, you can set the `LANGSMITH_BASE_URL` environment variable to point to your instance.
```
litellm_settings:  
 success_callback: ["langsmith"]  
  
environment_variables:  
 LANGSMITH_API_KEY: "lsv2_pt_xxxxxxxx"  
 LANGSMITH_PROJECT: "litellm-proxy"  
  
 LANGSMITH_BASE_URL: "https://api.smith.langchain.com" # (Optional - only needed if you have a custom Langsmith instance)  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "Hello, Claude gm!"  
    }  
   ],  
  }  
'  

```

Expect to see your log on Langfuse
## Arize AI​
  1. Set `success_callback: ["arize"]` on litellm config.yaml


```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["arize"]  
  
environment_variables:  
  ARIZE_SPACE_KEY: "d0*****"  
  ARIZE_API_KEY: "141a****"  
  ARIZE_ENDPOINT: "https://otlp.arize.com/v1" # OPTIONAL - your custom arize GRPC api endpoint  
  ARIZE_HTTP_ENDPOINT: "https://otlp.arize.com/v1" # OPTIONAL - your custom arize HTTP api endpoint. Set either this or ARIZE_ENDPOINT  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "Hello, Claude gm!"  
    }  
   ],  
  }  
'  

```

Expect to see your log on Langfuse
## Langtrace​
  1. Set `success_callback: ["langtrace"]` on litellm config.yaml


```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["langtrace"]  
  
environment_variables:  
  LANGTRACE_API_KEY: "141a****"  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "Hello, Claude gm!"  
    }  
   ],  
  }  
'  

```

## Galileo​
[BETA]
Log LLM I/O on www.rungalileo.io
info
Beta Integration
**Required Env Variables**
```
export GALILEO_BASE_URL="" # For most users, this is the same as their console URL except with the word 'console' replaced by 'api' (e.g. http://www.console.galileo.myenterprise.com -> http://www.api.galileo.myenterprise.com)  
export GALILEO_PROJECT_ID=""  
export GALILEO_USERNAME=""  
export GALILEO_PASSWORD=""  

```

#### Quick Start​
  1. Add to Config.yaml


```
model_list:  
- litellm_params:  
  api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  api_key: my-fake-key  
  model: openai/my-fake-model  
 model_name: fake-openai-endpoint  
  
litellm_settings:  
 success_callback: ["galileo"] # 👈 KEY CHANGE  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

🎉 That's it - Expect to see your Logs on your Galileo Dashboard
## OpenMeter​
Bill customers according to their LLM API usage with OpenMeter
**Required Env Variables**
```
# from https://openmeter.cloud  
export OPENMETER_API_ENDPOINT="" # defaults to https://openmeter.cloud  
export OPENMETER_API_KEY=""  

```

##### Quick Start​
  1. Add to Config.yaml


```
model_list:  
- litellm_params:  
  api_base: https://openai-function-calling-workers.tasslexyz.workers.dev/  
  api_key: my-fake-key  
  model: openai/my-fake-model  
 model_name: fake-openai-endpoint  
  
litellm_settings:  
 success_callback: ["openmeter"] # 👈 KEY CHANGE  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

## DynamoDB​
We will use the `--config` to set 
  * `litellm.success_callback = ["dynamodb"]`
  * `litellm.dynamodb_table_name = "your-table-name"`


This will log all successful LLM calls to DynamoDB
**Step 1** Set AWS Credentials in .env
```
AWS_ACCESS_KEY_ID = ""  
AWS_SECRET_ACCESS_KEY = ""  
AWS_REGION_NAME = ""  

```

**Step 2** : Create a `config.yaml` file and set `litellm_settings`: `success_callback`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 success_callback: ["dynamodb"]  
 dynamodb_table_name: your-table-name  

```

**Step 3** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "Azure OpenAI GPT-4 East",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
  }'  

```

Your logs should be available on DynamoDB
#### Data Logged to DynamoDB /chat/completions​
```
{  
 "id": {  
  "S": "chatcmpl-8W15J4480a3fAQ1yQaMgtsKJAicen"  
 },  
 "call_type": {  
  "S": "acompletion"  
 },  
 "endTime": {  
  "S": "2023-12-15 17:25:58.424118"  
 },  
 "messages": {  
  "S": "[{'role': 'user', 'content': 'This is a test'}]"  
 },  
 "metadata": {  
  "S": "{}"  
 },  
 "model": {  
  "S": "gpt-3.5-turbo"  
 },  
 "modelParameters": {  
  "S": "{'temperature': 0.7, 'max_tokens': 100, 'user': 'ishaan-2'}"  
 },  
 "response": {  
  "S": "ModelResponse(id='chatcmpl-8W15J4480a3fAQ1yQaMgtsKJAicen', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Great! What can I assist you with?', role='assistant'))], created=1702641357, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20))"  
 },  
 "startTime": {  
  "S": "2023-12-15 17:25:56.047035"  
 },  
 "usage": {  
  "S": "Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20)"  
 },  
 "user": {  
  "S": "ishaan-2"  
 }  
}  

```

#### Data logged to DynamoDB /embeddings​
```
{  
 "id": {  
  "S": "4dec8d4d-4817-472d-9fc6-c7a6153eb2ca"  
 },  
 "call_type": {  
  "S": "aembedding"  
 },  
 "endTime": {  
  "S": "2023-12-15 17:25:59.890261"  
 },  
 "messages": {  
  "S": "['hi']"  
 },  
 "metadata": {  
  "S": "{}"  
 },  
 "model": {  
  "S": "text-embedding-ada-002"  
 },  
 "modelParameters": {  
  "S": "{'user': 'ishaan-2'}"  
 },  
 "response": {  
  "S": "EmbeddingResponse(model='text-embedding-ada-002-v2', data=[{'embedding': [-0.03503197431564331, -0.020601635798811913, -0.015375726856291294,  
 }  
}  

```

## Sentry​
If api calls fail (llm/database) you can log those to Sentry: 
**Step 1** Install Sentry
```
pip install --upgrade sentry-sdk  

```

**Step 2** : Save your Sentry_DSN and add `litellm_settings`: `failure_callback`
```
export SENTRY_DSN="your-sentry-dsn"  

```

```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 # other settings  
 failure_callback: ["sentry"]  
general_settings:   
 database_url: "my-bad-url" # set a fake url to trigger a sentry exception  

```

**Step 3** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --debug  

```

Test Request
```
litellm --test  

```

## Athina​
Athina allows you to log LLM Input/Output for monitoring, analytics, and observability.
We will use the `--config` to set `litellm.success_callback = ["athina"]` this will log all successful LLM calls to athina
**Step 1** Set Athina API key
```
ATHINA_API_KEY = "your-athina-api-key"  

```

**Step 2** : Create a `config.yaml` file and set `litellm_settings`: `success_callback`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
litellm_settings:  
 success_callback: ["athina"]  

```

**Step 3** : Start the proxy, make a test request
Start proxy
```
litellm --config config.yaml --debug  

```

Test Request
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "which llm are you"  
    }  
  ]  
  }'  

```

Previous
🙋‍♂️ Customers / End-User Budgets
Next
StandardLoggingPayload Specification
  * Getting the LiteLLM Call ID
  * Logging Features
    * Conditional Logging by Virtual Keys, Teams
    * Redacting UserAPIKeyInfo
    * Redact Messages, Response Content
    * Disable Message Redaction
    * Turn off all tracking/logging
  * What gets logged?
  * Langfuse
    * Logging Metadata to Langfuse
    * Custom Tags
    * LiteLLM Tags - `cache_hit`, `cache_key`
    * View POST sent from LiteLLM to provider
  * OpenTelemetry
    * Redacting Messages, Response Content
    * Traceparent Header
  * Google Cloud Storage Buckets
  * Google Cloud Storage - PubSub Topic
  * s3 Buckets
    * Team Alias Prefix in Object Key
  * Azure Blob Storage
  * DataDog
    * Set DD variables (`DD_SERVICE` etc)
  * Lunary
  * MLflow
  * Custom Callback Class Async
  * Custom Callback APIs Async
  * Langsmith
  * Arize AI
  * Langtrace
  * Galileo
  * OpenMeter
  * DynamoDB
  * Sentry
  * Athina


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
    * Contributing Code
    * Adding Providers
      * Directory Structure
      * Add Rerank Provider
    * Contributing to Documentation
    * Contributing - UI
  * Extras
  * Support & Talk with founders


  *   * Contributing
  * Adding Providers
  * Directory Structure


# Directory Structure
When adding a new provider, you need to create a directory for the provider that follows the following structure:
```
litellm/llms/  
└── provider_name/  
  ├── completion/ # use when endpoint is equivalent to openai's `/v1/completions`  
  │  ├── handler.py  
  │  └── transformation.py  
  ├── chat/ # use when endpoint is equivalent to openai's `/v1/chat/completions`  
  │  ├── handler.py  
  │  └── transformation.py  
  ├── embed/ # use when endpoint is equivalent to openai's `/v1/embeddings`  
  │  ├── handler.py  
  │  └── transformation.py  
  ├── audio_transcription/ # use when endpoint is equivalent to openai's `/v1/audio/transcriptions`  
  │  ├── handler.py  
  │  └── transformation.py  
  └── rerank/ # use when endpoint is equivalent to cohere's `/rerank` endpoint.  
    ├── handler.py  
    └── transformation.py  

```

Previous
Contributing Code
Next
Add Rerank Provider
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Select provider: All Providers
max_input_tokens >=
max_output_tokens >=


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
    * Router - Load Balancing
    * [BETA] Request Prioritization
    * Proxy - Load Balancing
    * Fallbacks
    * Timeouts
    * Tag Based Routing
    * Budget Routing
    * Provider specific Wildcard routing
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Routing, Loadbalancing & Fallbacks


# Routing, Loadbalancing & Fallbacks
Learn how to load balance, route, and set fallbacks for your LLM requests
## 📄️ Router - Load Balancing
LiteLLM manages:
## 📄️ [BETA] Request Prioritization
Beta feature. Use for testing only.
## 📄️ Proxy - Load Balancing
Load balance multiple instances of the same model
## 📄️ Fallbacks
If a call fails after num_retries, fallback to another model group.
## 📄️ Timeouts
The timeout set in router is for the entire length of the call, and is passed down to the completion() call level as well.
## 📄️ Tag Based Routing
Route requests based on tags.
## 📄️ Budget Routing
LiteLLM Supports setting the following budgets:
## 📄️ Provider specific Wildcard routing
Proxy all models from a provider
Previous
/moderations
Next
Router - Load Balancing
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server


# LiteLLM Proxy Server (LLM Gateway)
OpenAI Proxy Server (LLM Gateway) to call 100+ LLMs in a unified interface & track spend, set budgets per virtual key/user
## 📄️ Getting Started - E2E Tutorial
End-to-End tutorial for LiteLLM Proxy to:
## 🗃️ Config.yaml
3 items
## 🗃️ Setup & Deployment
9 items
## 📄️ Demo App
Here is a demo of the proxy. To log in pass in:
## 🗃️ Architecture
7 items
## 🔗 All Endpoints (Swagger)
## 📄️ ✨ Enterprise Features
To get a license, get in touch with us here
## 🗃️ Making LLM Requests
5 items
## 🗃️ Authentication
8 items
## 🗃️ Model Access
2 items
## 🗃️ Admin UI
8 items
## 🗃️ Spend Tracking
3 items
## 🗃️ Budgets + Rate Limits
5 items
## 🔗 Load Balancing, Routing, Fallbacks
## 🗃️ Logging, Alerting, Metrics
6 items
## 🗃️ [Beta] Guardrails
10 items
## 🗃️ Secret Managers
2 items
## 🗃️ Create Custom Plugins
Modify requests, responses, and more
## 📄️ Caching
For OpenAI/Anthropic Prompt Caching, go here
Previous
LiteLLM - Getting Started
Next
Getting Started - E2E Tutorial
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints


# Supported Endpoints
Learn how to deploy + call models from different providers on LiteLLM
## 🗃️ /chat/completions
3 items
## 📄️ /responses [Beta]
LiteLLM provides a BETA endpoint in the spec of OpenAI's /responses API
## 📄️ /completions
Usage
## 📄️ /embeddings
Quick Start
## 📄️ /v1/messages [BETA]
Use LiteLLM to call all your LLM APIs in the Anthropic v1/messages format.
## 📄️ /mcp [BETA] - Model Context Protocol
Expose MCP tools on LiteLLM Proxy Server
## 🗃️ /images
2 items
## 🗃️ /audio
2 items
## 🗃️ Pass-through Endpoints (Anthropic SDK, etc.)
12 items
## 📄️ /rerank
LiteLLM Follows the cohere api request / response for the rerank api
## 📄️ /assistants
Covers Threads, Messages, Assistants.
## 🗃️ /files
2 items
## 📄️ /batches
Covers Batches, Files
## 📄️ /realtime
Use this to loadbalance across Azure + OpenAI.
## 📄️ /fine_tuning
This is an Enterprise only endpoint Get Started with Enterprise here
## 📄️ /moderations
Usage
Previous
Reliability - Retries, Fallbacks
Next
/chat/completions
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * Setting API Keys, Base, Version


On this page
# Setting API Keys, Base, Version
LiteLLM allows you to specify the following:
  * API Key
  * API Base
  * API Version
  * API Type
  * Project
  * Location
  * Token


Useful Helper functions: 
  * `check_valid_key()`
  * `get_valid_models()`


You can set the API configs using:
  * Environment Variables
  * litellm variables `litellm.api_key`
  * Passing args to `completion()`


## Environment Variables​
### Setting API Keys​
Set the liteLLM API key or specific provider key:
```
import os   
  
# Set OpenAI API key  
os.environ["OPENAI_API_KEY"] = "Your API Key"  
os.environ["ANTHROPIC_API_KEY"] = "Your API Key"  
os.environ["XAI_API_KEY"] = "Your API Key"  
os.environ["REPLICATE_API_KEY"] = "Your API Key"  
os.environ["TOGETHERAI_API_KEY"] = "Your API Key"  

```

### Setting API Base, API Version, API Type​
```
# for azure openai  
os.environ['AZURE_API_BASE'] = "https://openai-gpt-4-test2-v-12.openai.azure.com/"  
os.environ['AZURE_API_VERSION'] = "2023-05-15" # [OPTIONAL]  
os.environ['AZURE_API_TYPE'] = "azure" # [OPTIONAL]  
  
# for openai  
os.environ['OPENAI_API_BASE'] = "https://openai-gpt-4-test2-v-12.openai.azure.com/"  

```

### Setting Project, Location, Token​
For cloud providers:
  * Azure
  * Bedrock
  * GCP
  * Watson AI 


you might need to set additional parameters. LiteLLM provides a common set of params, that we map across all providers. 
| LiteLLM param| Watson| Vertex AI| Azure| Bedrock  
---|---|---|---|---|---  
Project| project| watsonx_project| vertex_project| n/a| n/a  
Region| region_name| watsonx_region_name| vertex_location| n/a| aws_region_name  
Token| token| watsonx_token or token| n/a| azure_ad_token| n/a  
If you want, you can call them by their provider-specific params as well. 
## litellm variables​
### litellm.api_key​
This variable is checked for all providers
```
import litellm  
# openai call  
litellm.api_key = "sk-OpenAIKey"  
response = litellm.completion(messages=messages, model="gpt-3.5-turbo")  
  
# anthropic call  
litellm.api_key = "sk-AnthropicKey"  
response = litellm.completion(messages=messages, model="claude-2")  

```

### litellm.provider_key (example litellm.openai_key)​
```
litellm.openai_key = "sk-OpenAIKey"  
response = litellm.completion(messages=messages, model="gpt-3.5-turbo")  
  
# anthropic call  
litellm.anthropic_key = "sk-AnthropicKey"  
response = litellm.completion(messages=messages, model="claude-2")  

```

### litellm.api_base​
```
import litellm  
litellm.api_base = "https://hosted-llm-api.co"  
response = litellm.completion(messages=messages, model="gpt-3.5-turbo")  

```

### litellm.api_version​
```
import litellm  
litellm.api_version = "2023-05-15"  
response = litellm.completion(messages=messages, model="gpt-3.5-turbo")  

```

### litellm.organization​
```
import litellm  
litellm.organization = "LiteLlmOrg"  
response = litellm.completion(messages=messages, model="gpt-3.5-turbo")  

```

## Passing Args to completion() (or any litellm endpoint - `transcription`, `embedding`, `text_completion`, etc)​
You can pass the API key within `completion()` call:
### api_key​
```
from litellm import completion  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
response = completion("command-nightly", messages, api_key="Your-Api-Key")  

```

### api_base​
```
from litellm import completion  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
response = completion("command-nightly", messages, api_base="https://hosted-llm-api.co")  

```

### api_version​
```
from litellm import completion  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
response = completion("command-nightly", messages, api_version="2023-02-15")  

```

## Helper Functions​
### `check_valid_key()`​
Check if a user submitted a valid key for the model they're trying to call. 
```
key = "bad-key"  
response = check_valid_key(model="gpt-3.5-turbo", api_key=key)  
assert(response == False)  

```

### `get_valid_models()`​
This helper reads the .env and returns a list of supported llms for user
```
old_environ = os.environ  
os.environ = {'OPENAI_API_KEY': 'temp'} # mock set only openai key in environ  
  
valid_models = get_valid_models()  
print(valid_models)  
  
# list of openai supported llms on litellm  
expected_models = litellm.open_ai_chat_completion_models + litellm.open_ai_text_completion_models  
  
assert(valid_models == expected_models)  
  
# reset replicate env key  
os.environ = old_environ  

```

### `get_valid_models(check_provider_endpoint: True)`​
This helper will check the provider's endpoint for valid models.
Currently implemented for:
  * OpenAI (if OPENAI_API_KEY is set)
  * Fireworks AI (if FIREWORKS_AI_API_KEY is set)
  * LiteLLM Proxy (if LITELLM_PROXY_API_KEY is set)
  * Gemini (if GEMINI_API_KEY is set)
  * XAI (if XAI_API_KEY is set) 
  * Anthropic (if ANTHROPIC_API_KEY is set)


You can also specify a custom provider to check:
**All providers** :
```
from litellm import get_valid_models  
  
valid_models = get_valid_models(check_provider_endpoint=True)  
print(valid_models)  

```

**Specific provider** :
```
from litellm import get_valid_models  
  
valid_models = get_valid_models(check_provider_endpoint=True, custom_llm_provider="openai")  
print(valid_models)  

```

### `validate_environment(model: str)`​
This helper tells you if you have all the required environment variables for a model, and if not - what's missing. 
```
from litellm import validate_environment  
  
print(validate_environment("openai/gpt-3.5-turbo"))  

```

Previous
Provider specific Wildcard routing
Next
Completion Token Usage & Cost
  * Environment Variables
    * Setting API Keys
    * Setting API Base, API Version, API Type
    * Setting Project, Location, Token
  * litellm variables
    * litellm.api_key
    * litellm.provider_key (example litellm.openai_key)
    * litellm.api_base
    * litellm.api_version
    * litellm.organization
  * Passing Args to completion() (or any litellm endpoint - `transcription`, `embedding`, `text_completion`, etc)
    * api_key
    * api_base
    * api_version
  * Helper Functions
    * `check_valid_key()`
    * `get_valid_models()`
    * `get_valid_models(check_provider_endpoint: True)`
    * `validate_environment(model: str)`


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
Releases
  * v1.67.4-stable - Improved User Management
  * v1.67.0-stable - SCIM Integration
  * v1.66.0-stable - Realtime API Cost Tracking
  * v1.65.4-stable
  * v1.65.0-stable - Model Context Protocol
  * v1.65.0 - Team Model Add - update
  * v1.63.14-stable
  * v1.63.11-stable
  * v1.63.2-stable
  * v1.63.0 - Anthropic 'thinking' response update
  * v1.61.20-stable
  * v1.59.8-stable
  * v1.59.0
  * v1.57.8-stable
  * v1.57.7
  * v1.57.3 - New Base Docker Image
  * v1.56.4
  * v1.56.3
  * v1.56.1
  * v1.55.10
  * v1.55.8-stable


## v1.67.4-stable - Improved User Management
April 26, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
## Deploy this version​
  * Docker
  * Pip


docker run litellm
```
docker run  
-e STORE_MODEL_IN_DB=True  
-p 4000:4000  
ghcr.io/berriai/litellm:main-v1.67.4-stable  

```

pip install litellm
```
pip install litellm==1.67.4.post1  

```

## Key Highlights​
  * **Improved User Management** : This release enables search and filtering across users, keys, teams, and models.
  * **Responses API Load Balancing** : Route requests across provider regions and ensure session continuity. 
  * **UI Session Logs** : Group several requests to LiteLLM into a session. 


## Improved User Management​
  

This release makes it easier to manage users and keys on LiteLLM. You can now search and filter across users, keys, teams, and models, and control user settings more easily.
New features include:
  * Search for users by email, ID, role, or team.
  * See all of a user's models, teams, and keys in one place.
  * Change user roles and model access right from the Users Tab.


These changes help you spend less time on user setup and management on LiteLLM.
## Responses API Load Balancing​
  

This release introduces load balancing for the Responses API, allowing you to route requests across provider regions and ensure session continuity. It works as follows:
  * If a `previous_response_id` is provided, LiteLLM will route the request to the original deployment that generated the prior response — ensuring session continuity.
  * If no `previous_response_id` is provided, LiteLLM will load-balance requests across your available deployments.


Read more
## UI Session Logs​
  

This release allow you to group requests to LiteLLM proxy into a session. If you specify a litellm_session_id in your request LiteLLM will automatically group all logs in the same session. This allows you to easily track usage and request content per session. 
Read more
## New Models / Updated Models​
  * **OpenAI**
    1. Added `gpt-image-1` cost tracking Get Started
    2. Bug fix: added cost tracking for gpt-image-1 when quality is unspecified PR
  * **Azure**
    1. Fixed timestamp granularities passing to whisper in Azure Get Started
    2. Added azure/gpt-image-1 pricing Get Started, PR
    3. Added cost tracking for `azure/computer-use-preview`, `azure/gpt-4o-audio-preview-2024-12-17`, `azure/gpt-4o-mini-audio-preview-2024-12-17` PR
  * **Bedrock**
    1. Added support for all compatible Bedrock parameters when model="arn:.." (Bedrock application inference profile models) Get started, PR
    2. Fixed wrong system prompt transformation PR
  * **VertexAI / Google AI Studio**
    1. Allow setting `budget_tokens=0` for `gemini-2.5-flash` Get Started,PR
    2. Ensure returned `usage` includes thinking token usage PR
    3. Added cost tracking for `gemini-2.5-pro-preview-03-25` PR
  * **Cohere**
    1. Added support for cohere command-a-03-2025 Get Started, PR
  * **SageMaker**
    1. Added support for max_completion_tokens parameter Get Started, PR
  * **Responses API**
    1. Added support for GET and DELETE operations - `/v1/responses/{response_id}` Get Started
    2. Added session management support for non-OpenAI models PR
    3. Added routing affinity to maintain model consistency within sessions Get Started, PR


## Spend Tracking Improvements​
  * **Bug Fix** : Fixed spend tracking bug, ensuring default litellm params aren't modified in memory PR
  * **Deprecation Dates** : Added deprecation dates for Azure, VertexAI models PR


## Management Endpoints / UI​
#### Users​
  * **Filtering and Searching** : 
    * Filter users by user_id, role, team, sso_id 
    * Search users by email
  

  * **User Info Panel** : Added a new user information pane PR
    * View teams, keys, models associated with User 
    * Edit user role, model permissions 


#### Teams​
  * **Filtering and Searching** : 
    * Filter teams by Organization, Team ID PR
    * Search teams by Team Name PR
  



#### Keys​
  * **Key Management** : 
    * Support for cross-filtering and filtering by key hash PR
    * Fixed key alias reset when resetting filters PR
    * Fixed table rendering on key creation PR


#### UI Logs Page​
  * **Session Logs** : Added UI Session Logs Get Started


#### UI Authentication & Security​
  * **Required Authentication** : Authentication now required for all dashboard pages PR
  * **SSO Fixes** : Fixed SSO user login invalid token error PR
  * [BETA] **Encrypted Tokens** : Moved UI to encrypted token usage PR
  * **Token Expiry** : Support token refresh by re-routing to login page (fixes issue where expired token would show a blank page) PR


#### UI General fixes​
  * **Fixed UI Flicker** : Addressed UI flickering issues in Dashboard PR
  * **Improved Terminology** : Better loading and no-data states on Keys and Tools pages PR
  * **Azure Model Support** : Fixed editing Azure public model names and changing model names after creation PR
  * **Team Model Selector** : Bug fix for team model selection PR


## Logging / Guardrail Integrations​
  * **Datadog** :
    1. Fixed Datadog LLM observability logging Get Started, PR
  * **Prometheus / Grafana** : 
    1. Enable datasource selection on LiteLLM Grafana Template Get Started, PR
  * **AgentOps** : 
    1. Added AgentOps Integration Get Started, PR
  * **Arize** : 
    1. Added missing attributes for Arize & Phoenix Integration Get Started, PR


## General Proxy Improvements​
  * **Caching** : Fixed caching to account for `thinking` or `reasoning_effort` when calculating cache key PR
  * **Model Groups** : Fixed handling for cases where user sets model_group inside model_info PR
  * **Passthrough Endpoints** : Ensured `PassthroughStandardLoggingPayload` is logged with method, URL, request/response body PR
  * **Fix SQL Injection** : Fixed potential SQL injection vulnerability in spend_management_endpoints.py PR


## Helm​
  * Fixed serviceAccountName on migration job PR


## Full Changelog​
The complete list of changes can be found in the GitHub release notes.
**Tags:**
  * responses_api
  * ui_improvements
  * security
  * session_management


## v1.67.0-stable - SCIM Integration
April 19, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
## Key Highlights​
  * **SCIM Integration** : Enables identity providers (Okta, Azure AD, OneLogin, etc.) to automate user and team (group) provisioning, updates, and deprovisioning
  * **Team and Tag based usage tracking** : You can now see usage and spend by team and tag at 1M+ spend logs.
  * **Unified Responses API** : Support for calling Anthropic, Gemini, Groq, etc. via OpenAI's new Responses API.


Let's dive in.
## SCIM Integration​
This release adds SCIM support to LiteLLM. This allows your SSO provider (Okta, Azure AD, etc) to automatically create/delete users, teams, and memberships on LiteLLM. This means that when you remove a team on your SSO provider, your SSO provider will automatically delete the corresponding team on LiteLLM. 
Read more
## Team and Tag based usage tracking​
This release improves team and tag based usage tracking at 1m+ spend logs, making it easy to monitor your LLM API Spend in production. This covers:
  * View **daily spend** by teams + tags
  * View **usage / spend by key** , within teams
  * View **spend by multiple tags**
  * Allow **internal users** to view spend of teams they're a member of


Read more
## Unified Responses API​
This release allows you to call Azure OpenAI, Anthropic, AWS Bedrock, and Google Vertex AI models via the POST /v1/responses endpoint on LiteLLM. This means you can now use popular tools like OpenAI Codex with your own models. 
Read more
## New Models / Updated Models​
  * **OpenAI**
    1. gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3, o3-mini, o4-mini pricing - Get Started, PR
    2. o4 - correctly map o4 to openai o_series model
  * **Azure AI**
    1. Phi-4 output cost per token fix - PR
    2. Responses API support Get Started,PR
  * **Anthropic**
    1. redacted message thinking support - Get Started,PR
  * **Cohere**
    1. `/v2/chat` Passthrough endpoint support w/ cost tracking - Get Started, PR
  * **Azure**
    1. Support azure tenant_id/client_id env vars - Get Started, PR
    2. Fix response_format check for 2025+ api versions - PR
    3. Add gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3, o3-mini, o4-mini pricing
  * **VLLM**
    1. Files - Support 'file' message type for VLLM video url's - Get Started, PR
    2. Passthrough - new `/vllm/` passthrough endpoint support Get Started, PR
  * **Mistral**
    1. new `/mistral` passthrough endpoint support Get Started, PR
  * **AWS**
    1. New mapped bedrock regions - PR
  * **VertexAI / Google AI Studio**
    1. Gemini - Response format - Retain schema field ordering for google gemini and vertex by specifying propertyOrdering - Get Started, PR
    2. Gemini-2.5-flash - return reasoning content Google AI Studio, Vertex AI
    3. Gemini-2.5-flash - pricing + model information PR
    4. Passthrough - new `/vertex_ai/discovery` route - enables calling AgentBuilder API routes Get Started, PR
  * **Fireworks AI**
    1. return tool calling responses in `tool_calls` field (fireworks incorrectly returns this as a json str in content) PR
  * **Triton**
    1. Remove fixed remove bad_words / stop words from `/generate` call - Get Started, PR
  * **Other**
    1. Support for all litellm providers on Responses API (works with Codex) - Get Started, PR
    2. Fix combining multiple tool calls in streaming response - Get Started, PR


## Spend Tracking Improvements​
  * **Cost Control** - inject cache control points in prompt for cost reduction Get Started, PR
  * **Spend Tags** - spend tags in headers - support x-litellm-tags even if tag based routing not enabled Get Started, PR
  * **Gemini-2.5-flash** - support cost calculation for reasoning tokens PR


## Management Endpoints / UI​
  * **Users**
    1. Show created_at and updated_at on users page - PR
  * **Virtual Keys**
    1. Filter by key alias - https://github.com/BerriAI/litellm/pull/10085
  * **Usage Tab**
    1. Team based usage
       * New `LiteLLM_DailyTeamSpend` Table for aggregate team based usage logging - PR
       * New Team based usage dashboard + new `/team/daily/activity` API - PR
       * Return team alias on /team/daily/activity API - PR
       * allow internal user view spend for teams they belong to - PR
       * allow viewing top keys by team - PR
    2. Tag Based Usage
       * New `LiteLLM_DailyTagSpend` Table for aggregate tag based usage logging - PR
       * Restrict to only Proxy Admins - PR
       * allow viewing top keys by tag
       * Return tags passed in request (i.e. dynamic tags) on `/tag/list` API - PR
    3. Track prompt caching metrics in daily user, team, tag tables - PR
    4. Show usage by key (on all up, team, and tag usage dashboards) - PR
    5. swap old usage with new usage tab
  * **Models**
    1. Make columns resizable/hideable - PR
  * **API Playground**
    1. Allow internal user to call api playground - PR
  * **SCIM**
    1. Add LiteLLM SCIM Integration for Team and User management - Get Started, PR


## Logging / Guardrail Integrations​
  * **GCS**
    1. Fix gcs pub sub logging with env var GCS_PROJECT_ID - Get Started, PR
  * **AIM**
    1. Add litellm call id passing to Aim guardrails on pre and post-hooks calls - Get Started, PR
  * **Azure blob storage**
    1. Ensure logging works in high throughput scenarios - Get Started, PR


## General Proxy Improvements​
  * **Support setting`litellm.modify_params` via env var** PR
  * **Model Discovery** - Check provider’s `/models` endpoints when calling proxy’s `/v1/models` endpoint - Get Started, PR
  * **`/utils/token_counter`**- fix retrieving custom tokenizer for db models -Get Started, PR
  * **Prisma migrate** - handle existing columns in db table - PR


**Tags:**
  * sso
  * unified_file_id
  * cost_tracking
  * security


## v1.66.0-stable - Realtime API Cost Tracking
April 12, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
## Deploy this version​
  * Docker
  * Pip


docker run litellm
```
docker run  
-e STORE_MODEL_IN_DB=True  
-p 4000:4000  
ghcr.io/berriai/litellm:main-v1.66.0-stable  

```

pip install litellm
```
pip install litellm==1.66.0.post1  

```

v1.66.0-stable is live now, here are the key highlights of this release
## Key Highlights​
  * **Realtime API Cost Tracking** : Track cost of realtime API calls
  * **Microsoft SSO Auto-sync** : Auto-sync groups and group members from Azure Entra ID to LiteLLM
  * **xAI grok-3** : Added support for `xai/grok-3` models
  * **Security Fixes** : Fixed CVE-2025-0330 and CVE-2024-6825 vulnerabilities


Let's dive in.
## Realtime API Cost Tracking​
This release adds Realtime API logging + cost tracking. 
  * **Logging** : LiteLLM now logs the complete response from realtime calls to all logging integrations (DB, S3, Langfuse, etc.) 
  * **Cost Tracking** : You can now set 'base_model' and custom pricing for realtime models. Custom Pricing
  * **Budgets** : Your key/user/team budgets now work for realtime models as well.


Start here
## Microsoft SSO Auto-sync​
Auto-sync groups and members from Azure Entra ID to LiteLLM
This release adds support for auto-syncing groups and members on Microsoft Entra ID with LiteLLM. This means that LiteLLM proxy administrators can spend less time managing teams and members and LiteLLM handles the following: 
  * Auto-create teams that exist on Microsoft Entra ID 
  * Sync team members on Microsoft Entra ID with LiteLLM teams


Get started with this here
## New Models / Updated Models​
  * **xAI**
    1. Added reasoning_effort support for `xai/grok-3-mini-beta` Get Started
    2. Added cost tracking for `xai/grok-3` models PR
  * **Hugging Face**
    1. Added inference providers support Get Started
  * **Azure**
    1. Added azure/gpt-4o-realtime-audio cost tracking PR
  * **VertexAI**
    1. Added enterpriseWebSearch tool support Get Started
    2. Moved to only passing keys accepted by the Vertex AI response schema PR
  * **Google AI Studio**
    1. Added cost tracking for `gemini-2.5-pro` PR
    2. Fixed pricing for 'gemini/gemini-2.5-pro-preview-03-25' PR
    3. Fixed handling file_data being passed in PR
  * **Azure**
    1. Updated Azure Phi-4 pricing PR
    2. Added azure/gpt-4o-realtime-audio cost tracking PR
  * **Databricks**
    1. Removed reasoning_effort from parameters PR
    2. Fixed custom endpoint check for Databricks PR
  * **General**
    1. Added litellm.supports_reasoning() util to track if an llm supports reasoning Get Started
    2. Function Calling - Handle pydantic base model in message tool calls, handle tools = [], and support fake streaming on tool calls for meta.llama3-3-70b-instruct-v1:0 PR
    3. LiteLLM Proxy - Allow passing `thinking` param to litellm proxy via client sdk PR
    4. Fixed correctly translating 'thinking' param for litellm PR


## Spend Tracking Improvements​
  * **OpenAI, Azure**
    1. Realtime API Cost tracking with token usage metrics in spend logs Get Started
  * **Anthropic**
    1. Fixed Claude Haiku cache read pricing per token PR
    2. Added cost tracking for Claude responses with base_model PR
    3. Fixed Anthropic prompt caching cost calculation and trimmed logged message in db PR
  * **General**
    1. Added token tracking and log usage object in spend logs PR
    2. Handle custom pricing at deployment level PR


## Management Endpoints / UI​
  * **Test Key Tab**
    1. Added rendering of Reasoning content, ttft, usage metrics on test key page PR
View input, output, reasoning tokens, ttft metrics.
  * **Tag / Policy Management**
    1. Added Tag/Policy Management. Create routing rules based on request metadata. This allows you to enforce that requests with `tags="private"` only go to specific models. Get Started
  

Create and manage tags.
  * **Redesigned Login Screen**
    1. Polished login screen PR
  * **Microsoft SSO Auto-Sync**
    1. Added debug route to allow admins to debug SSO JWT fields PR
    2. Added ability to use MSFT Graph API to assign users to teams PR
    3. Connected litellm to Azure Entra ID Enterprise Application PR
    4. Added ability for admins to set `default_team_params` for when litellm SSO creates default teams PR
    5. Fixed MSFT SSO to use correct field for user email PR
    6. Added UI support for setting Default Team setting when litellm SSO auto creates teams PR
  * **UI Bug Fixes**
    1. Prevented team, key, org, model numerical values changing on scrolling PR
    2. Instantly reflect key and team updates in UI PR


## Logging / Guardrail Improvements​
  * **Prometheus**
    1. Emit Key and Team Budget metrics on a cron job schedule Get Started


## Security Fixes​
  * Fixed CVE-2025-0330 - Leakage of Langfuse API keys in team exception handling PR
  * Fixed CVE-2024-6825 - Remote code execution in post call rules PR


## Helm​
  * Added service annotations to litellm-helm chart PR
  * Added extraEnvVars to the helm deployment PR


## Demo​
Try this on the demo instance today
## Complete Git Diff​
See the complete git diff since v1.65.4-stable, here
**Tags:**
  * sso
  * unified_file_id
  * cost_tracking
  * security


## v1.65.4-stable
April 5, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
## Deploy this version​
  * Docker
  * Pip


docker run litellm
```
docker run  
-e STORE_MODEL_IN_DB=True  
-p 4000:4000  
ghcr.io/berriai/litellm:main-v1.65.4-stable  

```

pip install litellm
```
pip install litellm==1.65.4.post1  

```

v1.65.4-stable is live. Here are the improvements since v1.65.0-stable.
## Key Highlights​
  * **Preventing DB Deadlocks** : Fixes a high-traffic issue when multiple instances were writing to the DB at the same time. 
  * **New Usage Tab** : Enables viewing spend by model and customizing date range


Let's dive in. 
### Preventing DB Deadlocks​
This release fixes the DB deadlocking issue that users faced in high traffic (10K+ RPS). This is great because it enables user/key/team spend tracking works at that scale.
Read more about the new architecture here
### New Usage Tab​
The new Usage tab now brings the ability to track daily spend by model. This makes it easier to catch any spend tracking or token counting errors, when combined with the ability to view successful requests, and token usage.
To test this out, just go to Experimental > New Usage > Activity.
## New Models / Updated Models​
  1. Databricks - claude-3-7-sonnet cost tracking PR
  2. VertexAI - `gemini-2.5-pro-exp-03-25` cost tracking PR
  3. VertexAI - `gemini-2.0-flash` cost tracking PR
  4. Groq - add whisper ASR models to model cost map PR
  5. IBM - Add watsonx/ibm/granite-3-8b-instruct to model cost map PR
  6. Google AI Studio - add gemini/gemini-2.5-pro-preview-03-25 to model cost map PR


## LLM Translation​
  1. Vertex AI - Support anyOf param for OpenAI json schema translation Get Started
  2. Anthropic- response_format + thinking param support (works across Anthropic API, Bedrock, Vertex) Get Started
  3. Anthropic - if thinking token is specified and max tokens is not - ensure max token to anthropic is higher than thinking tokens (works across Anthropic API, Bedrock, Vertex) PR
  4. Bedrock - latency optimized inference support Get Started
  5. Sagemaker - handle special tokens + multibyte character code in response Get Started
  6. MCP - add support for using SSE MCP servers Get Started
  7. Anthropic - new `litellm.messages.create` interface for calling Anthropic `/v1/messages` via passthrough Get Started
  8. Anthropic - support ‘file’ content type in message param (works across Anthropic API, Bedrock, Vertex) Get Started
  9. Anthropic - map openai 'reasoning_effort' to anthropic 'thinking' param (works across Anthropic API, Bedrock, Vertex) Get Started
  10. Google AI Studio (Gemini) - [BETA] `/v1/files` upload support Get Started
  11. Azure - fix o-series tool calling Get Started
  12. Unified file id - [ALPHA] allow calling multiple providers with same file id PR
     * This is experimental, and not recommended for production use.
     * We plan to have a production-ready implementation by next week.
  13. Google AI Studio (Gemini) - return logprobs PR
  14. Anthropic - Support prompt caching for Anthropic tool calls Get Started
  15. OpenRouter - unwrap extra body on open router calls PR
  16. VertexAI - fix credential caching issue PR
  17. XAI - filter out 'name' param for XAI PR
  18. Gemini - image generation output support Get Started
  19. Databricks - support claude-3-7-sonnet w/ thinking + response_format Get Started


## Spend Tracking Improvements​
  1. Reliability fix - Check sent and received model for cost calculation PR
  2. Vertex AI - Multimodal embedding cost tracking Get Started, PR


## Management Endpoints / UI​
  1. New Usage Tab
     * Report 'total_tokens' + report success/failure calls
     * Remove double bars on scroll
     * Ensure ‘daily spend’ chart ordered from earliest to latest date
     * showing spend per model per day
     * show key alias on usage tab
     * Allow non-admins to view their activity
     * Add date picker to new usage tab
  2. Virtual Keys Tab
     * remove 'default key' on user signup
     * fix showing user models available for personal key creation
  3. Test Key Tab
     * Allow testing image generation models
  4. Models Tab
     * Fix bulk adding models 
     * support reusable credentials for passthrough endpoints
     * Allow team members to see team models
  5. Teams Tab
     * Fix json serialization error on update team metadata
  6. Request Logs Tab
     * Add reasoning_content token tracking across all providers on streaming
  7. API 
     * return key alias on /user/daily/activity Get Started
  8. SSO
     * Allow assigning SSO users to teams on MSFT SSO PR


## Logging / Guardrail Integrations​
  1. Console Logs - Add json formatting for uncaught exceptions PR
  2. Guardrails - AIM Guardrails support for virtual key based policies Get Started
  3. Logging - fix completion start time tracking PR
  4. Prometheus
     * Allow adding authentication on Prometheus /metrics endpoints PR
     * Distinguish LLM Provider Exception vs. LiteLLM Exception in metric naming PR
     * Emit operational metrics for new DB Transaction architecture PR


## Performance / Loadbalancing / Reliability improvements​
  1. Preventing Deadlocks
     * Reduce DB Deadlocks by storing spend updates in Redis and then committing to DB PR
     * Ensure no deadlocks occur when updating DailyUserSpendTransaction PR
     * High Traffic fix - ensure new DB + Redis architecture accurately tracks spend PR
     * Use Redis for PodLock Manager instead of PG (ensures no deadlocks occur) PR
     * v2 DB Deadlock Reduction Architecture – Add Max Size for In-Memory Queue + Backpressure Mechanism PR
  2. Prisma Migrations Get Started
     * connects litellm proxy to litellm's prisma migration files
     * Handle db schema updates from new `litellm-proxy-extras` sdk
  3. Redis - support password for sync sentinel clients PR
  4. Fix "Circular reference detected" error when max_parallel_requests = 0 PR
  5. Code QA - Ban hardcoded numbers PR


## Helm​
  1. fix: wrong indentation of ttlSecondsAfterFinished in chart PR


## General Proxy Improvements​
  1. Fix - only apply service_account_settings.enforced_params on service accounts PR
  2. Fix - handle metadata null on `/chat/completion` PR
  3. Fix - Move daily user transaction logging outside of 'disable_spend_logs' flag, as they’re unrelated PR


## Demo​
Try this on the demo instance today
## Complete Git Diff​
See the complete git diff since v1.65.0-stable, here
## v1.65.0-stable - Model Context Protocol
March 30, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
v1.65.0-stable is live now. Here are the key highlights of this release:
  * **MCP Support** : Support for adding and using MCP servers on the LiteLLM proxy.
  * **UI view total usage after 1M+ logs** : You can now view usage analytics after crossing 1M+ logs in DB. 


## Model Context Protocol (MCP)​
This release introduces support for centrally adding MCP servers on LiteLLM. This allows you to add MCP server endpoints and your developers can `list` and `call` MCP tools through LiteLLM.
Read more about MCP here.
Expose and use MCP servers through LiteLLM
## UI view total usage after 1M+ logs​
This release brings the ability to view total usage analytics even after exceeding 1M+ logs in your database. We've implemented a scalable architecture that stores only aggregate usage data, resulting in significantly more efficient queries and reduced database CPU utilization.
View total usage after 1M+ logs
  * How this works:
    * We now aggregate usage data into a dedicated DailyUserSpend table, significantly reducing query load and CPU usage even beyond 1M+ logs.
  * Daily Spend Breakdown API:
    * Retrieve granular daily usage data (by model, provider, and API key) with a single endpoint. Example Request:
Daily Spend Breakdown API
```
curl -L -X GET 'http://localhost:4000/user/daily/activity?start_date=2025-03-20&end_date=2025-03-27' \  
-H 'Authorization: Bearer sk-...'  

```

Daily Spend Breakdown API Response
```
{  
  "results": [  
    {  
      "date": "2025-03-27",  
      "metrics": {  
        "spend": 0.0177072,  
        "prompt_tokens": 111,  
        "completion_tokens": 1711,  
        "total_tokens": 1822,  
        "api_requests": 11  
      },  
      "breakdown": {  
        "models": {  
          "gpt-4o-mini": {  
            "spend": 1.095e-05,  
            "prompt_tokens": 37,  
            "completion_tokens": 9,  
            "total_tokens": 46,  
            "api_requests": 1  
        },  
        "providers": { "openai": { ... }, "azure_ai": { ... } },  
        "api_keys": { "3126b6eaf1...": { ... } }  
      }  
    }  
  ],  
  "metadata": {  
    "total_spend": 0.7274667,  
    "total_prompt_tokens": 280990,  
    "total_completion_tokens": 376674,  
    "total_api_requests": 14  
  }  
}  

```



## New Models / Updated Models​
  * Support for Vertex AI gemini-2.0-flash-lite & Google AI Studio gemini-2.0-flash-lite PR
  * Support for Vertex AI Fine-Tuned LLMs PR
  * Nova Canvas image generation support PR
  * OpenAI gpt-4o-transcribe support PR
  * Added new Vertex AI text embedding model PR


## LLM Translation​
  * OpenAI Web Search Tool Call Support PR
  * Vertex AI topLogprobs support PR
  * Support for sending images and video to Vertex AI multimodal embedding Doc
  * Support litellm.api_base for Vertex AI + Gemini across completion, embedding, image_generation PR
  * Bug fix for returning `response_cost` when using litellm python SDK with LiteLLM Proxy PR
  * Support for `max_completion_tokens` on Mistral API PR
  * Refactored Vertex AI passthrough routes - fixes unpredictable behaviour with auto-setting default_vertex_region on router model add PR


## Spend Tracking Improvements​
  * Log 'api_base' on spend logs PR
  * Support for Gemini audio token cost tracking PR
  * Fixed OpenAI audio input token cost tracking PR


## UI​
### Model Management​
  * Allowed team admins to add/update/delete models on UI PR
  * Added render supports_web_search on model hub PR


### Request Logs​
  * Show API base and model ID on request logs PR
  * Allow viewing keyinfo on request logs PR


### Usage Tab​
  * Added Daily User Spend Aggregate view - allows UI Usage tab to work > 1m rows PR
  * Connected UI to "LiteLLM_DailyUserSpend" spend table PR


## Logging Integrations​
  * Fixed StandardLoggingPayload for GCS Pub Sub Logging Integration PR
  * Track `litellm_model_name` on `StandardLoggingPayload` Docs


## Performance / Reliability Improvements​
  * LiteLLM Redis semantic caching implementation PR
  * Gracefully handle exceptions when DB is having an outage PR
  * Allow Pods to startup + passing /health/readiness when allow_requests_on_db_unavailable: True and DB is down PR


## General Improvements​
  * Support for exposing MCP tools on litellm proxy PR
  * Support discovering Gemini, Anthropic, xAI models by calling their /v1/model endpoint PR
  * Fixed route check for non-proxy admins on JWT auth PR
  * Added baseline Prisma database migrations PR
  * View all wildcard models on /model/info PR


## Security​
  * Bumped next from 14.2.21 to 14.2.25 in UI dashboard PR


## Complete Git Diff​
Here's the complete git diff
**Tags:**
  * mcp
  * custom_prompt_management


## v1.65.0 - Team Model Add - update
March 28, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
v1.65.0 updates the `/model/new` endpoint to prevent non-team admins from creating team models.
This means that only proxy admins or team admins can create team models.
## Additional Changes​
  * Allows team admins to call `/model/update` to update team models.
  * Allows team admins to call `/model/delete` to delete team models.
  * Introduces new `user_models_only` param to `/v2/model/info` - only return models added by this user.


These changes enable team admins to add and manage models for their team on the LiteLLM UI + API.
**Tags:**
  * management endpoints
  * team models
  * ui


## v1.63.14-stable
March 22, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
These are the changes since `v1.63.11-stable`.
This release brings:
  * LLM Translation Improvements (MCP Support and Bedrock Application Profiles)
  * Perf improvements for Usage-based Routing
  * Streaming guardrail support via websockets
  * Azure OpenAI client perf fix (from previous release)


## Docker Run LiteLLM Proxy​
```
docker run  
-e STORE_MODEL_IN_DB=True  
-p 4000:4000  
ghcr.io/berriai/litellm:main-v1.63.14-stable.patch1  

```

## Demo Instance​
Here's a Demo Instance to test changes:
  * Instance: https://demo.litellm.ai/
  * Login Credentials:
    * Username: admin
    * Password: sk-1234


## New Models / Updated Models​
  * Azure gpt-4o - fixed pricing to latest global pricing - PR
  * O1-Pro - add pricing + model information - PR
  * Azure AI - mistral 3.1 small pricing added - PR
  * Azure - gpt-4.5-preview pricing added - PR


## LLM Translation​
  1. **New LLM Features**


  * Bedrock: Support bedrock application inference profiles Docs
    * Infer aws region from bedrock application profile id - (`arn:aws:bedrock:us-east-1:...`)
  * Ollama - support calling via `/v1/completions` Get Started
  * Bedrock - support `us.deepseek.r1-v1:0` model name Docs
  * OpenRouter - `OPENROUTER_API_BASE` env var support Docs
  * Azure - add audio model parameter support - Docs
  * OpenAI - PDF File support Docs
  * OpenAI - o1-pro Responses API streaming support Docs
  * [BETA] MCP - Use MCP Tools with LiteLLM SDK Docs


  1. **Bug Fixes**


  * Voyage: prompt token on embedding tracking fix - PR
  * Sagemaker - Fix ‘Too little data for declared Content-Length’ error - PR
  * OpenAI-compatible models - fix issue when calling openai-compatible models w/ custom_llm_provider set - PR
  * VertexAI - Embedding ‘outputDimensionality’ support - PR
  * Anthropic - return consistent json response format on streaming/non-streaming - PR


## Spend Tracking Improvements​
  * `litellm_proxy/` - support reading litellm response cost header from proxy, when using client sdk 
  * Reset Budget Job - fix budget reset error on keys/teams/users PR
  * Streaming - Prevents final chunk w/ usage from being ignored (impacted bedrock streaming + cost tracking) PR


## UI​
  1. Users Page
     * Feature: Control default internal user settings PR
  2. Icons:
     * Feature: Replace external "artificialanalysis.ai" icons by local svg PR
  3. Sign In/Sign Out
     * Fix: Default login when `default_user_id` user does not exist in DB PR


## Logging Integrations​
  * Support post-call guardrails for streaming responses Get Started
  * Arize Get Started
    * fix invalid package import PR
    * migrate to using standardloggingpayload for metadata, ensures spans land successfully PR
    * fix logging to just log the LLM I/O PR
    * Dynamic API Key/Space param support Get Started
  * StandardLoggingPayload - Log litellm_model_name in payload. Allows knowing what the model sent to API provider was Get Started
  * Prompt Management - Allow building custom prompt management integration Get Started


## Performance / Reliability improvements​
  * Redis Caching - add 5s default timeout, prevents hanging redis connection from impacting llm calls PR
  * Allow disabling all spend updates / writes to DB - patch to allow disabling all spend updates to DB with a flag PR
  * Azure OpenAI - correctly re-use azure openai client, fixes perf issue from previous Stable release PR
  * Azure OpenAI - uses litellm.ssl_verify on Azure/OpenAI clients PR
  * Usage-based routing - Wildcard model support Get Started
  * Usage-based routing - Support batch writing increments to redis - reduces latency to same as ‘simple-shuffle’ PR
  * Router - show reason for model cooldown on ‘no healthy deployments available error’ PR
  * Caching - add max value limit to an item in in-memory cache (1MB) - prevents OOM errors on large image url’s being sent through proxy PR


## General Improvements​
  * Passthrough Endpoints - support returning api-base on pass-through endpoints Response Headers Docs
  * SSL - support reading ssl security level from env var - Allows user to specify lower security settings Get Started
  * Credentials - only poll Credentials table when `STORE_MODEL_IN_DB` is True PR
  * Image URL Handling - new architecture doc on image url handling Docs
  * OpenAI - bump to pip install "openai==1.68.2" PR
  * Gunicorn - security fix - bump gunicorn==23.0.0 PR


## Complete Git Diff​
Here's the complete git diff
**Tags:**
  * credential management
  * thinking content
  * responses api
  * snowflake


## v1.63.11-stable
March 15, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg)
Ishaan Jaffer
CTO, LiteLLM
These are the changes since `v1.63.2-stable`.
This release is primarily focused on:
  * [Beta] Responses API Support
  * Snowflake Cortex Support, Amazon Nova Image Generation
  * UI - Credential Management, re-use credentials when adding new models
  * UI - Test Connection to LLM Provider before adding a model


## Known Issues​
  * 🚨 Known issue on Azure OpenAI - We don't recommend upgrading if you use Azure OpenAI. This version failed our Azure OpenAI load test


## Docker Run LiteLLM Proxy​
```
docker run  
-e STORE_MODEL_IN_DB=True  
-p 4000:4000  
ghcr.io/berriai/litellm:main-v1.63.11-stable  

```

## Demo Instance​
Here's a Demo Instance to test changes:
  * Instance: https://demo.litellm.ai/
  * Login Credentials:
    * Username: admin
    * Password: sk-1234


## New Models / Updated Models​
  * Image Generation support for Amazon Nova Canvas Getting Started
  * Add pricing for Jamba new models PR
  * Add pricing for Amazon EU models PR
  * Add Bedrock Deepseek R1 model pricing PR
  * Update Gemini pricing: Gemma 3, Flash 2 thinking update, LearnLM PR
  * Mark Cohere Embedding 3 models as Multimodal PR
  * Add Azure Data Zone pricing PR
    * LiteLLM Tracks cost for `azure/eu` and `azure/us` models


## LLM Translation​
  1. **New Endpoints**


  * [Beta] POST `/responses` API. Getting Started


  1. **New LLM Providers**


  * Snowflake Cortex Getting Started


  1. **New LLM Features**


  * Support OpenRouter `reasoning_content` on streaming Getting Started


  1. **Bug Fixes**


  * OpenAI: Return `code`, `param` and `type` on bad request error More information on litellm exceptions
  * Bedrock: Fix converse chunk parsing to only return empty dict on tool use PR
  * Bedrock: Support extra_headers PR
  * Azure: Fix Function Calling Bug & Update Default API Version to `2025-02-01-preview` PR
  * Azure: Fix AI services URL PR
  * Vertex AI: Handle HTTP 201 status code in response PR
  * Perplexity: Fix incorrect streaming response PR
  * Triton: Fix streaming completions bug PR
  * Deepgram: Support bytes.IO when handling audio files for transcription PR
  * Ollama: Fix "system" role has become unacceptable PR
  * All Providers (Streaming): Fix String `data:` stripped from entire content in streamed responses PR


## Spend Tracking Improvements​
  1. Support Bedrock converse cache token tracking Getting Started
  2. Cost Tracking for Responses API Getting Started
  3. Fix Azure Whisper cost tracking Getting Started


## UI​
### Re-Use Credentials on UI​
You can now onboard LLM provider credentials on LiteLLM UI. Once these credentials are added you can re-use them when adding new models Getting Started
### Test Connections before adding models​
Before adding a model you can test the connection to the LLM provider to verify you have setup your API Base + API Key correctly
![](https://docs.litellm.ai/assets/images/litellm_test_connection-029765a2de4dcabccfe3be9a8d33dbdd.gif)
### General UI Improvements​
  1. Add Models Page
     * Allow adding Cerebras, Sambanova, Perplexity, Fireworks, Openrouter, TogetherAI Models, Text-Completion OpenAI on Admin UI
     * Allow adding EU OpenAI models
     * Fix: Instantly show edit + deletes to models
  2. Keys Page
     * Fix: Instantly show newly created keys on Admin UI (don't require refresh)
     * Fix: Allow clicking into Top Keys when showing users Top API Key
     * Fix: Allow Filter Keys by Team Alias, Key Alias and Org
     * UI Improvements: Show 100 Keys Per Page, Use full height, increase width of key alias
  3. Users Page
     * Fix: Show correct count of internal user keys on Users Page
     * Fix: Metadata not updating in Team UI
  4. Logs Page
     * UI Improvements: Keep expanded log in focus on LiteLLM UI
     * UI Improvements: Minor improvements to logs page
     * Fix: Allow internal user to query their own logs
     * Allow switching off storing Error Logs in DB Getting Started
  5. Sign In/Sign Out
     * Fix: Correctly use `PROXY_LOGOUT_URL` when set Getting Started


## Security​
  1. Support for Rotating Master Keys Getting Started
  2. Fix: Internal User Viewer Permissions, don't allow `internal_user_viewer` role to see `Test Key Page` or `Create Key Button` More information on role based access controls
  3. Emit audit logs on All user + model Create/Update/Delete endpoints Getting Started
  4. JWT
     * Support multiple JWT OIDC providers Getting Started
     * Fix JWT access with Groups not working when team is assigned All Proxy Models access
  5. Using K/V pairs in 1 AWS Secret Getting Started


## Logging Integrations​
  1. Prometheus: Track Azure LLM API latency metric Getting Started
  2. Athina: Added tags, user_feedback and model_options to additional_keys which can be sent to Athina Getting Started


## Performance / Reliability improvements​
  1. Redis + litellm router - Fix Redis cluster mode for litellm router PR


## General Improvements​
  1. OpenWebUI Integration - display `thinking` tokens


  * Guide on getting started with LiteLLM x OpenWebUI. Getting Started
  * Display `thinking` tokens on OpenWebUI (Bedrock, Anthropic, Deepseek) Getting Started

![](https://docs.litellm.ai/assets/images/litellm_thinking_openweb-5ec7dddb7e7b6a10252694c27cfc177d.gif)
## Complete Git Diff​
Here's the complete git diff
**Tags:**
  * credential management
  * thinking content
  * responses api
  * snowflake


## v1.63.2-stable
March 8, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
These are the changes since `v1.61.20-stable`.
This release is primarily focused on:
  * LLM Translation improvements (more `thinking` content improvements)
  * UI improvements (Error logs now shown on UI)


info
This release will be live on 03/09/2025
## Demo Instance​
Here's a Demo Instance to test changes:
  * Instance: https://demo.litellm.ai/
  * Login Credentials:
    * Username: admin
    * Password: sk-1234


## New Models / Updated Models​
  1. Add `supports_pdf_input` for specific Bedrock Claude models PR
  2. Add pricing for amazon `eu` models PR
  3. Fix Azure O1 mini pricing PR


## LLM Translation​
  1. Support `/openai/` passthrough for Assistant endpoints. Get Started
  2. Bedrock Claude - fix tool calling transformation on invoke route. Get Started
  3. Bedrock Claude - response_format support for claude on invoke route. Get Started
  4. Bedrock - pass `description` if set in response_format. Get Started
  5. Bedrock - Fix passing response_format: {"type": "text"}. PR
  6. OpenAI - Handle sending image_url as str to openai. Get Started
  7. Deepseek - return 'reasoning_content' missing on streaming. Get Started
  8. Caching - Support caching on reasoning content. Get Started
  9. Bedrock - handle thinking blocks in assistant message. Get Started
  10. Anthropic - Return `signature` on streaming. Get Started


  * Note: We've also migrated from `signature_delta` to `signature`. Read more


  1. Support format param for specifying image type. Get Started
  2. Anthropic - `/v1/messages` endpoint - `thinking` param support. Get Started


  * Note: this refactors the [BETA] unified `/v1/messages` endpoint, to just work for the Anthropic API. 


  1. Vertex AI - handle $id in response schema when calling vertex ai. Get Started


## Spend Tracking Improvements​
  1. Batches API - Fix cost calculation to run on retrieve_batch. Get Started
  2. Batches API - Log batch models in spend logs / standard logging payload. Get Started


## Management Endpoints / UI​
  1. Virtual Keys Page
     * Allow team/org filters to be searchable on the Create Key Page
     * Add created_by and updated_by fields to Keys table
     * Show 'user_email' on key table
     * Show 100 Keys Per Page, Use full height, increase width of key alias
  2. Logs Page
     * Show Error Logs on LiteLLM UI
     * Allow Internal Users to View their own logs
  3. Internal Users Page 
     * Allow admin to control default model access for internal users
  4. Fix session handling with cookies


## Logging / Guardrail Integrations​
  1. Fix prometheus metrics w/ custom metrics, when keys containing team_id make requests. PR


## Performance / Loadbalancing / Reliability improvements​
  1. Cooldowns - Support cooldowns on models called with client side credentials. Get Started
  2. Tag-based Routing - ensures tag-based routing across all endpoints (`/embeddings`, `/image_generation`, etc.). Get Started


## General Proxy Improvements​
  1. Raise BadRequestError when unknown model passed in request
  2. Enforce model access restrictions on Azure OpenAI proxy route
  3. Reliability fix - Handle emoji’s in text - fix orjson error
  4. Model Access Patch - don't overwrite litellm.anthropic_models when running auth checks
  5. Enable setting timezone information in docker image 


## Complete Git Diff​
Here's the complete git diff
**Tags:**
  * llm translation
  * thinking
  * reasoning_content
  * claude-3-7-sonnet


## v1.63.0 - Anthropic 'thinking' response update
March 5, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
v1.63.0 fixes Anthropic 'thinking' response on streaming to return the `signature` block. Github Issue
It also moves the response structure from `signature_delta` to `signature` to be the same as Anthropic. Anthropic Docs
## Diff​
```
"message": {  
  ...  
  "reasoning_content": "The capital of France is Paris.",  
  "thinking_blocks": [  
    {  
      "type": "thinking",  
      "thinking": "The capital of France is Paris.",  
-      "signature_delta": "EqoBCkgIARABGAIiQL2UoU0b1OHYi+..." # 👈 OLD FORMAT  
+      "signature": "EqoBCkgIARABGAIiQL2UoU0b1OHYi+..." # 👈 KEY CHANGE  
    }  
  ]  
}  

```

**Tags:**
  * llm translation
  * thinking
  * reasoning_content
  * claude-3-7-sonnet


## v1.61.20-stable
March 1, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
These are the changes since `v1.61.13-stable`.
This release is primarily focused on:
  * LLM Translation improvements (claude-3-7-sonnet + 'thinking'/'reasoning_content' support)
  * UI improvements (add model flow, user management, etc)


## Demo Instance​
Here's a Demo Instance to test changes:
  * Instance: https://demo.litellm.ai/
  * Login Credentials:
    * Username: admin
    * Password: sk-1234


## New Models / Updated Models​
  1. Anthropic 3-7 sonnet support + cost tracking (Anthropic API + Bedrock + Vertex AI + OpenRouter) 
    1. Anthropic API Start here
    2. Bedrock API Start here
    3. Vertex AI API See here
    4. OpenRouter See here
  2. Gpt-4.5-preview support + cost tracking See here
  3. Azure AI - Phi-4 cost tracking See here
  4. Claude-3.5-sonnet - vision support updated on Anthropic API See here
  5. Bedrock llama vision support See here
  6. Cerebras llama3.3-70b pricing See here


## LLM Translation​
  1. Infinity Rerank - support returning documents when return_documents=True Start here
  2. Amazon Deepseek - `<think>` param extraction into ‘reasoning_content’ Start here
  3. Amazon Titan Embeddings - filter out ‘aws_’ params from request body Start here
  4. Anthropic ‘thinking’ + ‘reasoning_content’ translation support (Anthropic API, Bedrock, Vertex AI) Start here
  5. VLLM - support ‘video_url’ Start here
  6. Call proxy via litellm SDK: Support `litellm_proxy/` for embedding, image_generation, transcription, speech, rerank Start here
  7. OpenAI Pass-through - allow using Assistants GET, DELETE on /openai pass through routes Start here
  8. Message Translation - fix openai message for assistant msg if role is missing - openai allows this
  9. O1/O3 - support ‘drop_params’ for o3-mini and o1 parallel_tool_calls param (not supported currently) See here


## Spend Tracking Improvements​
  1. Cost tracking for rerank via Bedrock See PR
  2. Anthropic pass-through - fix race condition causing cost to not be tracked See PR
  3. Anthropic pass-through: Ensure accurate token counting See PR


## Management Endpoints / UI​
  1. Models Page - Allow sorting models by ‘created at’
  2. Models Page - Edit Model Flow Improvements
  3. Models Page - Fix Adding Azure, Azure AI Studio models on UI 
  4. Internal Users Page - Allow Bulk Adding Internal Users on UI 
  5. Internal Users Page - Allow sorting users by ‘created at’ 
  6. Virtual Keys Page - Allow searching for UserIDs on the dropdown when assigning a user to a team See PR
  7. Virtual Keys Page - allow creating a user when assigning keys to users See PR
  8. Model Hub Page - fix text overflow issue See PR
  9. Admin Settings Page - Allow adding MSFT SSO on UI 
  10. Backend - don't allow creating duplicate internal users in DB


## Helm​
  1. support ttlSecondsAfterFinished on the migration job - See PR
  2. enhance migrations job with additional configurable properties - See PR


## Logging / Guardrail Integrations​
  1. Arize Phoenix support 
  2. ‘No-log’ - fix ‘no-log’ param support on embedding calls 


## Performance / Loadbalancing / Reliability improvements​
  1. Single Deployment Cooldown logic - Use allowed_fails or allowed_fail_policy if set Start here


## General Proxy Improvements​
  1. Hypercorn - fix reading / parsing request body 
  2. Windows - fix running proxy in windows 
  3. DD-Trace - fix dd-trace enablement on proxy


## Complete Git Diff​
View the complete git diff here.
**Tags:**
  * llm translation
  * rerank
  * ui
  * thinking
  * reasoning_content
  * claude-3-7-sonnet


## v1.59.8-stable
January 31, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
info
Get a 7 day free trial for LiteLLM Enterprise here.
**no call needed**
## New Models / Updated Models​
  1. New OpenAI `/image/variations` endpoint BETA support Docs
  2. Topaz API support on OpenAI `/image/variations` BETA endpoint Docs
  3. Deepseek - r1 support w/ reasoning_content (Deepseek API, Vertex AI, Bedrock) 
  4. Azure - Add azure o1 pricing See Here
  5. Anthropic - handle `-latest` tag in model for cost calculation
  6. Gemini-2.0-flash-thinking - add model pricing (it’s 0.0) See Here
  7. Bedrock - add stability sd3 model pricing See Here (s/o Marty Sullivan)
  8. Bedrock - add us.amazon.nova-lite-v1:0 to model cost map See Here
  9. TogetherAI - add new together_ai llama3.3 models See Here


## LLM Translation​
  1. LM Studio -> fix async embedding call 
  2. Gpt 4o models - fix response_format translation 
  3. Bedrock nova - expand supported document types to include .md, .csv, etc. Start Here
  4. Bedrock - docs on IAM role based access for bedrock - Start Here
  5. Bedrock - cache IAM role credentials when used 
  6. Google AI Studio (`gemini/`) - support gemini 'frequency_penalty' and 'presence_penalty'
  7. Azure O1 - fix model name check 
  8. WatsonX - ZenAPIKey support for WatsonX Docs
  9. Ollama Chat - support json schema response format Start Here
  10. Bedrock - return correct bedrock status code and error message if error during streaming
  11. Anthropic - Supported nested json schema on anthropic calls
  12. OpenAI - `metadata` param preview support 
    1. SDK - enable via `litellm.enable_preview_features = True`
    2. PROXY - enable via `litellm_settings::enable_preview_features: true`
  13. Replicate - retry completion response on status=processing 


## Spend Tracking Improvements​
  1. Bedrock - QA asserts all bedrock regional models have same `supported_` as base model 
  2. Bedrock - fix bedrock converse cost tracking w/ region name specified
  3. Spend Logs reliability fix - when `user` passed in request body is int instead of string 
  4. Ensure ‘base_model’ cost tracking works across all endpoints 
  5. Fixes for Image generation cost tracking 
  6. Anthropic - fix anthropic end user cost tracking
  7. JWT / OIDC Auth - add end user id tracking from jwt auth


## Management Endpoints / UI​
  1. allows team member to become admin post-add (ui + endpoints) 
  2. New edit/delete button for updating team membership on UI 
  3. If team admin - show all team keys 
  4. Model Hub - clarify cost of models is per 1m tokens 
  5. Invitation Links - fix invalid url generated
  6. New - SpendLogs Table Viewer - allows proxy admin to view spend logs on UI 
    1. New spend logs - allow proxy admin to ‘opt in’ to logging request/response in spend logs table - enables easier abuse detection 
    2. Show country of origin in spend logs 
    3. Add pagination + filtering by key name/team name 
  7. `/key/delete` - allow team admin to delete team keys 
  8. Internal User ‘view’ - fix spend calculation when team selected
  9. Model Analytics is now on Free 
  10. Usage page - shows days when spend = 0, and round spend on charts to 2 sig figs 
  11. Public Teams - allow admins to expose teams for new users to ‘join’ on UI - Start Here
  12. Guardrails
    1. set/edit guardrails on a virtual key 
    2. Allow setting guardrails on a team 
    3. Set guardrails on team create + edit page
  13. Support temporary budget increases on `/key/update` - new `temp_budget_increase` and `temp_budget_expiry` fields - Start Here
  14. Support writing new key alias to AWS Secret Manager - on key rotation Start Here


## Helm​
  1. add securityContext and pull policy values to migration job (s/o https://github.com/Hexoplon) 
  2. allow specifying envVars on values.yaml
  3. new helm lint test


## Logging / Guardrail Integrations​
  1. Log the used prompt when prompt management used. Start Here
  2. Support s3 logging with team alias prefixes - Start Here
  3. Prometheus Start Here
    1. fix litellm_llm_api_time_to_first_token_metric not populating for bedrock models
    2. emit remaining team budget metric on regular basis (even when call isn’t made) - allows for more stable metrics on Grafana/etc. 
    3. add key and team level budget metrics
    4. emit `litellm_overhead_latency_metric`
    5. Emit `litellm_team_budget_reset_at_metric` and `litellm_api_key_budget_remaining_hours_metric`
  4. Datadog - support logging spend tags to Datadog. Start Here
  5. Langfuse - fix logging request tags, read from standard logging payload 
  6. GCS - don’t truncate payload on logging 
  7. New GCS Pub/Sub logging support Start Here
  8. Add AIM Guardrails support Start Here


## Security​
  1. New Enterprise SLA for patching security vulnerabilities. See Here
  2. Hashicorp - support using vault namespace for TLS auth. Start Here
  3. Azure - DefaultAzureCredential support 


## Health Checks​
  1. Cleanup pricing-only model names from wildcard route list - prevent bad health checks 
  2. Allow specifying a health check model for wildcard routes - https://docs.litellm.ai/docs/proxy/health#wildcard-routes
  3. New ‘health_check_timeout ‘ param with default 1min upperbound to prevent bad model from health check to hang and cause pod restarts. Start Here
  4. Datadog - add data dog service health check + expose new `/health/services` endpoint. Start Here


## Performance / Reliability improvements​
  1. 3x increase in RPS - moving to orjson for reading request body 
  2. LLM Routing speedup - using cached get model group info 
  3. SDK speedup - using cached get model info helper - reduces CPU work to get model info 
  4. Proxy speedup - only read request body 1 time per request 
  5. Infinite loop detection scripts added to codebase 
  6. Bedrock - pure async image transformation requests 
  7. Cooldowns - single deployment model group if 100% calls fail in high traffic - prevents an o1 outage from impacting other calls 
  8. Response Headers - return 
    1. `x-litellm-timeout`
    2. `x-litellm-attempted-retries`
    3. `x-litellm-overhead-duration-ms`
    4. `x-litellm-response-duration-ms`
  9. ensure duplicate callbacks are not added to proxy
  10. Requirements.txt - bump certifi version


## General Proxy Improvements​
  1. JWT / OIDC Auth - new `enforce_rbac` param,allows proxy admin to prevent any unmapped yet authenticated jwt tokens from calling proxy. Start Here
  2. fix custom openapi schema generation for customized swagger’s 
  3. Request Headers - support reading `x-litellm-timeout` param from request headers. Enables model timeout control when using Vercel’s AI SDK + LiteLLM Proxy. Start Here
  4. JWT / OIDC Auth - new `role` based permissions for model authentication. See Here


## Complete Git Diff​
This is the diff between v1.57.8-stable and v1.59.8-stable.
Use this to see the changes in the codebase.
**Git Diff**
**Tags:**
  * admin ui
  * logging
  * db schema


## v1.59.0
January 17, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
info
Get a 7 day free trial for LiteLLM Enterprise here.
**no call needed**
## UI Improvements​
### [Opt In] Admin UI - view messages / responses​
You can now view messages and response logs on Admin UI.
How to enable it - add `store_prompts_in_spend_logs: true` to your `proxy_config.yaml`
Once this flag is enabled, your `messages` and `responses` will be stored in the `LiteLLM_Spend_Logs` table.
```
general_settings:  
 store_prompts_in_spend_logs: true  

```

## DB Schema Change​
Added `messages` and `responses` to the `LiteLLM_Spend_Logs` table.
**By default this is not logged.** If you want `messages` and `responses` to be logged, you need to opt in with this setting 
```
general_settings:  
 store_prompts_in_spend_logs: true  

```

**Tags:**
  * admin ui
  * logging
  * db schema


## v1.57.8-stable
January 11, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
`alerting`, `prometheus`, `secret management`, `management endpoints`, `ui`, `prompt management`, `finetuning`, `batch`
## New / Updated Models​
  1. Mistral large pricing - https://github.com/BerriAI/litellm/pull/7452
  2. Cohere command-r7b-12-2024 pricing - https://github.com/BerriAI/litellm/pull/7553/files
  3. Voyage - new models, prices and context window information - https://github.com/BerriAI/litellm/pull/7472
  4. Anthropic - bump Bedrock claude-3-5-haiku max_output_tokens to 8192


## General Proxy Improvements​
  1. Health check support for realtime models 
  2. Support calling Azure realtime routes via virtual keys 
  3. Support custom tokenizer on `/utils/token_counter` - useful when checking token count for self-hosted models 
  4. Request Prioritization - support on `/v1/completion` endpoint as well 


## LLM Translation Improvements​
  1. Deepgram STT support. Start Here
  2. OpenAI Moderations - `omni-moderation-latest` support. Start Here
  3. Azure O1 - fake streaming support. This ensures if a `stream=true` is passed, the response is streamed. Start Here
  4. Anthropic - non-whitespace char stop sequence handling - PR
  5. Azure OpenAI - support Entra ID username + password based auth. Start Here
  6. LM Studio - embedding route support. Start Here
  7. WatsonX - ZenAPIKeyAuth support. Start Here


## Prompt Management Improvements​
  1. Langfuse integration
  2. HumanLoop integration 
  3. Support for using load balanced models 
  4. Support for loading optional params from prompt manager 


Start Here
## Finetuning + Batch APIs Improvements​
  1. Improved unified endpoint support for Vertex AI finetuning - PR
  2. Add support for retrieving vertex api batch jobs - PR


##  _NEW_ Alerting Integration​
PagerDuty Alerting Integration. 
Handles two types of alerts:
  * High LLM API Failure Rate. Configure X fails in Y seconds to trigger an alert.
  * High Number of Hanging LLM Requests. Configure X hangs in Y seconds to trigger an alert.


Start Here
## Prometheus Improvements​
Added support for tracking latency/spend/tokens based on custom metrics. Start Here
##  _NEW_ Hashicorp Secret Manager Support​
Support for reading credentials + writing LLM API keys. Start Here
## Management Endpoints / UI Improvements​
  1. Create and view organizations + assign org admins on the Proxy UI
  2. Support deleting keys by key_alias
  3. Allow assigning teams to org on UI
  4. Disable using ui session token for 'test key' pane
  5. Show model used in 'test key' pane 
  6. Support markdown output in 'test key' pane


## Helm Improvements​
  1. Prevent istio injection for db migrations cron job
  2. allow using migrationJob.enabled variable within job


## Logging Improvements​
  1. braintrust logging: respect project_id, add more metrics - https://github.com/BerriAI/litellm/pull/7613
  2. Athina - support base url - `ATHINA_BASE_URL`
  3. Lunary - Allow passing custom parent run id to LLM Calls 


## Git Diff​
This is the diff between v1.56.3-stable and v1.57.8-stable. 
Use this to see the changes in the codebase. 
Git Diff
**Tags:**
  * langfuse
  * humanloop
  * alerting
  * prometheus
  * secret management
  * management endpoints
  * ui
  * prompt management
  * finetuning
  * batch


## v1.57.7
January 10, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
`langfuse`, `management endpoints`, `ui`, `prometheus`, `secret management`
## Langfuse Prompt Management​
Langfuse Prompt Management is being labelled as BETA. This allows us to iterate quickly on the feedback we're receiving, and making the status clearer to users. We expect to make this feature to be stable by next month (February 2025).
Changes:
  * Include the client message in the LLM API Request. (Previously only the prompt template was sent, and the client message was ignored).
  * Log the prompt template in the logged request (e.g. to s3/langfuse). 
  * Log the 'prompt_id' and 'prompt_variables' in the logged request (e.g. to s3/langfuse). 


Start Here
## Team/Organization Management + UI Improvements​
Managing teams and organizations on the UI is now easier. 
Changes:
  * Support for editing user role within team on UI. 
  * Support updating team member role to admin via api - `/team/member_update`
  * Show team admins all keys for their team. 
  * Add organizations with budgets
  * Assign teams to orgs on the UI
  * Auto-assign SSO users to teams


Start Here
## Hashicorp Vault Support​
We now support writing LiteLLM Virtual API keys to Hashicorp Vault. 
Start Here
## Custom Prometheus Metrics​
Define custom prometheus metrics, and track usage/latency/no. of requests against them
This allows for more fine-grained tracking - e.g. on prompt template passed in request metadata
Start Here
**Tags:**
  * langfuse
  * management endpoints
  * ui
  * prometheus
  * secret management


## v1.57.3 - New Base Docker Image
January 8, 2025
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
`docker image`, `security`, `vulnerability`
# 0 Critical/High Vulnerabilities
## What changed?​
  * LiteLLMBase image now uses `cgr.dev/chainguard/python:latest-dev`


## Why the change?​
To ensure there are 0 critical/high vulnerabilities on LiteLLM Docker Image
## Migration Guide​
  * If you use a custom dockerfile with litellm as a base image + `apt-get`


Instead of `apt-get` use `apk`, the base litellm image will no longer have `apt-get` installed.
**You are only impacted if you use`apt-get` in your Dockerfile**
```
# Use the provided base image  
FROM ghcr.io/berriai/litellm:main-latest  
  
# Set the working directory  
WORKDIR /app  
  
# Install dependencies - CHANGE THIS to `apk`  
RUN apt-get update && apt-get install -y dumb-init   

```

Before Change
```
RUN apt-get update && apt-get install -y dumb-init  

```

After Change
```
RUN apk update && apk add --no-cache dumb-init  

```

**Tags:**
  * docker image
  * security
  * vulnerability


## v1.56.4
December 29, 2024
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
`deepgram`, `fireworks ai`, `vision`, `admin ui`, `dependency upgrades`
## New Models​
### **Deepgram Speech to Text**​
New Speech to Text support for Deepgram models. **Start Here**
```
from litellm import transcription  
import os   
  
# set api keys   
os.environ["DEEPGRAM_API_KEY"] = ""  
audio_file = open("/path/to/audio.mp3", "rb")  
  
response = transcription(model="deepgram/nova-2", file=audio_file)  
  
print(f"response: {response}")  

```

### **Fireworks AI - Vision** support for all models​
LiteLLM supports document inlining for Fireworks AI models. This is useful for models that are not vision models, but still need to parse documents/images/etc. LiteLLM will add `#transform=inline` to the url of the image_url, if the model is not a vision model See Code
## Proxy Admin UI​
  * `Test Key` Tab displays `model` used in response


  * `Test Key` Tab renders content in `.md`, `.py` (any code/markdown format)


## Dependency Upgrades​
  * (Security fix) Upgrade to `fastapi==0.115.5` https://github.com/BerriAI/litellm/pull/7447


## Bug Fixes​
  * Add health check support for realtime models Here
  * Health check error with audio_transcription model https://github.com/BerriAI/litellm/issues/5999


**Tags:**
  * deepgram
  * fireworks ai
  * vision
  * admin ui
  * dependency upgrades


## v1.56.3
December 28, 2024
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
`guardrails`, `logging`, `virtual key management`, `new models`
info
Get a 7 day free trial for LiteLLM Enterprise here.
**no call needed**
## New Features​
### ✨ Log Guardrail Traces​
Track guardrail failure rate and if a guardrail is going rogue and failing requests. Start here
#### Traced Guardrail Success​
#### Traced Guardrail Failure​
### `/guardrails/list`​
`/guardrails/list` allows clients to view available guardrails + supported guardrail params
```
curl -X GET 'http://0.0.0.0:4000/guardrails/list'  

```

Expected response
```
{  
  "guardrails": [  
    {  
    "guardrail_name": "aporia-post-guard",  
    "guardrail_info": {  
      "params": [  
      {  
        "name": "toxicity_score",  
        "type": "float",  
        "description": "Score between 0-1 indicating content toxicity level"  
      },  
      {  
        "name": "pii_detection",  
        "type": "boolean"  
      }  
      ]  
    }  
    }  
  ]  
}  

```

### ✨ Guardrails with Mock LLM​
Send `mock_response` to test guardrails without making an LLM call. More info on `mock_response` here
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi my email is ishaan@berri.ai"}  
  ],  
  "mock_response": "This is a mock response",  
  "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
 }'  

```

### Assign Keys to Users​
You can now assign keys to users via Proxy UI
## New Models​
  * `openrouter/openai/o1`
  * `vertex_ai/mistral-large@2411`


## Fixes​
  * Fix `vertex_ai/` mistral model pricing: https://github.com/BerriAI/litellm/pull/7345
  * Missing model_group field in logs for aspeech call types https://github.com/BerriAI/litellm/pull/7392


**Tags:**
  * guardrails
  * logging
  * virtual key management
  * new models


## v1.56.1
December 27, 2024
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
`key management`, `budgets/rate limits`, `logging`, `guardrails`
info
Get a 7 day free trial for LiteLLM Enterprise here.
**no call needed**
## ✨ Budget / Rate Limit Tiers​
Define tiers with rate limits. Assign them to keys. 
Use this to control access and budgets across a lot of keys.
**Start here**
```
curl -L -X POST 'http://0.0.0.0:4000/budget/new' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "budget_id": "high-usage-tier",  
  "model_max_budget": {  
    "gpt-4o": {"rpm_limit": 1000000}  
  }  
}'  

```

## OTEL Bug Fix​
LiteLLM was double logging litellm_request span. This is now fixed.
Relevant PR
## Logging for Finetuning Endpoints​
Logs for finetuning requests are now available on all logging providers (e.g. Datadog). 
What's logged per request:
  * file_id
  * finetuning_job_id
  * any key/team metadata


**Start Here:**
  * Setup Finetuning
  * Setup Logging


## Dynamic Params for Guardrails​
You can now set custom parameters (like success threshold) for your guardrails in each request.
See guardrails spec for more details
**Tags:**
  * key management
  * budgets/rate limits
  * logging
  * guardrails


## v1.55.10
December 24, 2024
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
`batches`, `guardrails`, `team management`, `custom auth`
  

info
Get a free 7-day LiteLLM Enterprise trial here. Start here
**No call needed**
## ✨ Cost Tracking, Logging for Batches API (`/batches`)​
Track cost, usage for Batch Creation Jobs. Start here
## ✨ `/guardrails/list` endpoint​
Show available guardrails to users. Start here
## ✨ Allow teams to add models​
This enables team admins to call their own finetuned models via litellm proxy. Start here
## ✨ Common checks for custom auth​
Calling the internal common_checks function in custom auth is now enforced as an enterprise feature. This allows admins to use litellm's default budget/auth checks within their custom auth implementation. Start here
## ✨ Assigning team admins​
Team admins is graduating from beta and moving to our enterprise tier. This allows proxy admins to allow others to manage keys/models for their own teams (useful for projects in production). Start here
**Tags:**
  * batches
  * guardrails
  * team management
  * custom auth


## v1.55.8-stable
December 22, 2024
![Krrish Dholakia](https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8)
Krrish Dholakia
CEO, LiteLLM
![Ishaan Jaffer](https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc)
Ishaan Jaffer
CTO, LiteLLM
A new LiteLLM Stable release just went out. Here are 5 updates since v1.52.2-stable. 
`langfuse`, `fallbacks`, `new models`, `azure_storage`
## Langfuse Prompt Management​
This makes it easy to run experiments or change the specific models `gpt-4o` to `gpt-4o-mini` on Langfuse, instead of making changes in your applications. Start here
## Control fallback prompts client-side​
> Claude prompts are different than OpenAI
Pass in prompts specific to model when doing fallbacks. Start here
## New Providers / Models​
  * NVIDIA Triton `/infer` endpoint. Start here
  * Infinity Rerank Models Start here


## ✨ Azure Data Lake Storage Support​
Send LLM usage (spend, tokens) data to Azure Data Lake. This makes it easy to consume usage data on other services (eg. Databricks) Start here
## Docker Run LiteLLM​
```
docker run \  
-e STORE_MODEL_IN_DB=True \  
-p 4000:4000 \  
ghcr.io/berriai/litellm:litellm_stable_release_branch-v1.55.8-stable  

```

## Get Daily Updates​
LiteLLM ships new releases every day. Follow us on LinkedIn to get daily updates.
**Tags:**
  * langfuse
  * fallbacks
  * new models
  * azure_storage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
    * OpenWeb UI with LiteLLM
    * Using LiteLLM with OpenAI Codex
    * Microsoft SSO: Sync Groups, Members with LiteLLM
    * Auto-Inject Prompt Caching Checkpoints
    * [Beta] Routing based on request metadata
    * Aporia Guardrails with LiteLLM Gateway
    * LiteLLM Python SDK Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Tutorials
  * OpenWeb UI with LiteLLM


On this page
# OpenWeb UI with LiteLLM
This guide walks you through connecting OpenWeb UI to LiteLLM. Using LiteLLM with OpenWeb UI allows teams to 
  * Access 100+ LLMs on OpenWeb UI
  * Track Spend / Usage, Set Budget Limits 
  * Send Request/Response Logs to logging destinations like langfuse, s3, gcs buckets, etc.
  * Set access controls eg. Control what models OpenWebUI can access.


## Quickstart​
  * Make sure to setup LiteLLM with the LiteLLM Getting Started Guide


## 1. Start LiteLLM & OpenWebUI​
  * OpenWebUI starts running on http://localhost:3000
  * LiteLLM starts running on http://localhost:4000


## 2. Create a Virtual Key on LiteLLM​
Virtual Keys are API Keys that allow you to authenticate to LiteLLM Proxy. We will create a Virtual Key that will allow OpenWebUI to access LiteLLM.
### 2.1 LiteLLM User Management Hierarchy​
On LiteLLM, you can create Organizations, Teams, Users and Virtual Keys. For this tutorial, we will create a Team and a Virtual Key.
  * `Organization` - An Organization is a group of Teams. (US Engineering, EU Developer Tools)
  * `Team` - A Team is a group of Users. (OpenWeb UI Team, Data Science Team, etc.)
  * `User` - A User is an individual user (employee, developer, eg. `krrish@litellm.ai`)
  * `Virtual Key` - A Virtual Key is an API Key that allows you to authenticate to LiteLLM Proxy. A Virtual Key is associated with a User or Team.


Once the Team is created, you can invite Users to the Team. You can read more about LiteLLM's User Management here.
### 2.2 Create a Team on LiteLLM​
Navigate to http://localhost:4000/ui and create a new team.
![](https://docs.litellm.ai/assets/images/litellm_create_team-de6a9faa3cf43040cd7845e3d4adc533.gif)
### 2.2 Create a Virtual Key on LiteLLM​
Navigate to http://localhost:4000/ui and create a new virtual Key. 
LiteLLM allows you to specify what models are available on OpenWeb UI (by specifying the models the key will have access to).
![](https://docs.litellm.ai/assets/images/create_key_in_team_oweb-986dfe58f393f9d6887b7ad1ce7cc4f5.gif)
## 3. Connect OpenWeb UI to LiteLLM​
On OpenWeb UI, navigate to Settings -> Connections and create a new connection to LiteLLM
Enter the following details:
  * URL: `http://localhost:4000` (your litellm proxy base url)
  * Key: `your-virtual-key` (the key you created in the previous step)

![](https://docs.litellm.ai/assets/images/litellm_setup_openweb-436a66958edb2807b923d6c82b2e5491.gif)
### 3.1 Test Request​
On the top left corner, select models you should only see the models you gave the key access to in Step 2.
Once you selected a model, enter your message content and click on `Submit`
![](https://docs.litellm.ai/assets/images/basic_litellm-ab3549cf6f1d3ff7f02ca10c5c18c90c.gif)
### 3.2 Tracking Spend / Usage​
After your request is made, navigate to `Logs` on the LiteLLM UI, you can see Team, Key, Model, Usage and Cost.
## Render `thinking` content on OpenWeb UI​
OpenWebUI requires reasoning/thinking content to be rendered with `<think></think>` tags. In order to render this for specific models, you can use the `merge_reasoning_content_in_choices` litellm parameter.
Example litellm config.yaml:
```
model_list:  
 - model_name: thinking-anthropic-claude-3-7-sonnet  
  litellm_params:  
   model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0  
   thinking: {"type": "enabled", "budget_tokens": 1024}  
   max_tokens: 1080  
   merge_reasoning_content_in_choices: true  

```

### Test it on OpenWeb UI​
On the models dropdown select `thinking-anthropic-claude-3-7-sonnet`
![](https://docs.litellm.ai/assets/images/litellm_thinking_openweb-5ec7dddb7e7b6a10252694c27cfc177d.gif)
## Additional Resources​
  * Running LiteLLM and OpenWebUI on Windows Localhost: A Comprehensive Guide https://www.tanyongsheng.com/note/running-litellm-and-openwebui-on-windows-localhost-a-comprehensive-guide/


Previous
Comet Opik - Logging + Evals
Next
Using LiteLLM with OpenAI Codex
  * Quickstart
  * 1. Start LiteLLM & OpenWebUI
  * 2. Create a Virtual Key on LiteLLM
    * 2.1 LiteLLM User Management Hierarchy
    * 2.2 Create a Team on LiteLLM
    * 2.2 Create a Virtual Key on LiteLLM
  * 3. Connect OpenWeb UI to LiteLLM
    * 3.1 Test Request
    * 3.2 Tracking Spend / Usage
  * Render `thinking` content on OpenWeb UI
    * Test it on OpenWeb UI
  * Additional Resources


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
    * Router - Load Balancing
    * [BETA] Request Prioritization
    * Proxy - Load Balancing
    * Fallbacks
    * Timeouts
    * Tag Based Routing
    * Budget Routing
    * Provider specific Wildcard routing
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Routing, Loadbalancing & Fallbacks
  * Router - Load Balancing


On this page
# Router - Load Balancing
LiteLLM manages:
  * Load-balance across multiple deployments (e.g. Azure/OpenAI)
  * Prioritizing important requests to ensure they don't fail (i.e. Queueing)
  * Basic reliability logic - cooldowns, fallbacks, timeouts and retries (fixed + exponential backoff) across multiple deployments/providers.


In production, litellm supports using Redis as a way to track cooldown server and usage (managing tpm/rpm limits).
info
If you want a server to load balance across different LLM APIs, use our LiteLLM Proxy Server
## Load Balancing​
(s/o @paulpierre and sweep proxy for their contributions to this implementation) **See Code**
### Quick Start​
Loadbalance across multiple azure/bedrock/provider deployments. LiteLLM will handle retrying in different regions if a call fails.
  * SDK
  * PROXY


```
from litellm import Router  
  
model_list = [{ # list of model deployments   
  "model_name": "gpt-3.5-turbo", # model alias -> loadbalance between models with same `model_name`  
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-v-2", # actual model name  
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
  }  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-functioncalling",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
  }  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "gpt-3.5-turbo",   
    "api_key": os.getenv("OPENAI_API_KEY"),  
  }  
}, {  
  "model_name": "gpt-4",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/gpt-4",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_base": os.getenv("AZURE_API_BASE"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
  }  
}, {  
  "model_name": "gpt-4",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "gpt-4",   
    "api_key": os.getenv("OPENAI_API_KEY"),  
  }  
},  
  
]  
  
router = Router(model_list=model_list)  
  
# openai.ChatCompletion.create replacement  
# requests with model="gpt-3.5-turbo" will pick a deployment where model_name="gpt-3.5-turbo"  
response = await router.acompletion(model="gpt-3.5-turbo",   
        messages=[{"role": "user", "content": "Hey, how's it going?"}])  
  
print(response)  
  
# openai.ChatCompletion.create replacement  
# requests with model="gpt-4" will pick a deployment where model_name="gpt-4"  
response = await router.acompletion(model="gpt-4",   
        messages=[{"role": "user", "content": "Hey, how's it going?"}])  
  
print(response)  

```

info
See detailed proxy loadbalancing/fallback docs here
  1. Setup model_list with multiple deployments


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/<your-deployment-name>  
   api_base: <your-azure-endpoint>  
   api_key: <your-azure-api-key>  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/gpt-turbo-small-ca  
   api_base: https://my-endpoint-canada-berri992.openai.azure.com/  
   api_key: <your-azure-api-key>  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/gpt-turbo-large  
   api_base: https://openai-france-1234.openai.azure.com/  
   api_key: <your-azure-api-key>  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml   

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gpt-3.5-turbo",  
 "messages": [  
    {"role": "user", "content": "Hi there!"}  
  ],  
  "mock_testing_rate_limit_error": true  
}'  

```

### Available Endpoints​
  * `router.completion()` - chat completions endpoint to call 100+ LLMs
  * `router.acompletion()` - async chat completion calls
  * `router.embedding()` - embedding endpoint for Azure, OpenAI, Huggingface endpoints
  * `router.aembedding()` - async embeddings calls
  * `router.text_completion()` - completion calls in the old OpenAI `/v1/completions` endpoint format
  * `router.atext_completion()` - async text completion calls
  * `router.image_generation()` - completion calls in OpenAI `/v1/images/generations` endpoint format
  * `router.aimage_generation()` - async image generation calls


## Advanced - Routing Strategies ⭐️​
#### Routing Strategies - Weighted Pick, Rate Limit Aware, Least Busy, Latency Based, Cost Based​
Router provides 4 strategies for routing your calls across multiple deployments: 
  * Rate-Limit Aware v2 (ASYNC)
  * Latency-Based
  * (Default) Weighted Pick (Async)
  * Rate-Limit Aware
  * Least-Busy
  * Custom Routing Strategy
  * Lowest Cost Routing (Async)


**🎉 NEW** This is an async implementation of usage-based-routing.
**Filters out deployment if tpm/rpm limit exceeded** - If you pass in the deployment's tpm/rpm limits.
Routes to **deployment with lowest TPM usage** for that minute. 
In production, we use Redis to track usage (TPM/RPM) across multiple deployments. This implementation uses **async redis calls** (redis.incr and redis.mget).
For Azure, you get 6 RPM per 1000 TPM
  * sdk
  * proxy


```
from litellm import Router   
  
  
model_list = [{ # list of model deployments   
  "model_name": "gpt-3.5-turbo", # model alias   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-v-2", # actual model name  
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
    "tpm": 100000,  
    "rpm": 10000,  
  },   
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-functioncalling",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
    "tpm": 100000,  
    "rpm": 1000,  
  },  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "gpt-3.5-turbo",   
    "api_key": os.getenv("OPENAI_API_KEY"),  
    "tpm": 100000,  
    "rpm": 1000,  
  },  
}]  
router = Router(model_list=model_list,   
        redis_host=os.environ["REDIS_HOST"],   
        redis_password=os.environ["REDIS_PASSWORD"],   
        redis_port=os.environ["REDIS_PORT"],   
        routing_strategy="usage-based-routing-v2" # 👈 KEY CHANGE  
        enable_pre_call_checks=True, # enables router rate limits for concurrent calls  
        )  
  
response = await router.acompletion(model="gpt-3.5-turbo",   
        messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  
print(response)  

```

**1. Set strategy in config**
```
model_list:  
  - model_name: gpt-3.5-turbo # model alias   
   litellm_params: # params for litellm completion/embedding call   
    model: azure/chatgpt-v-2 # actual model name  
    api_key: os.environ/AZURE_API_KEY  
    api_version: os.environ/AZURE_API_VERSION  
    api_base: os.environ/AZURE_API_BASE  
   tpm: 100000  
   rpm: 10000  
  - model_name: gpt-3.5-turbo   
   litellm_params: # params for litellm completion/embedding call   
    model: gpt-3.5-turbo   
    api_key: os.getenv(OPENAI_API_KEY)  
   tpm: 100000  
   rpm: 1000  
  
router_settings:  
 routing_strategy: usage-based-routing-v2 # 👈 KEY CHANGE  
 redis_host: <your-redis-host>  
 redis_password: <your-redis-password>  
 redis_port: <your-redis-port>  
 enable_pre_call_check: true  
  
general_settings:  
 master_key: sk-1234  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  

```

**3. Test it!**
```
curl --location 'http://localhost:4000/v1/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer sk-1234' \  
--data '{  
  "model": "gpt-3.5-turbo",   
  "messages": [{"role": "user", "content": "Hey, how's it going?"}]  
}'  

```

Picks the deployment with the lowest response time.
It caches, and updates the response times for deployments based on when a request was sent and received from a deployment.
**How to test**
```
from litellm import Router   
import asyncio  
  
model_list = [{ ... }]  
  
# init router  
router = Router(model_list=model_list,  
        routing_strategy="latency-based-routing",# 👈 set routing strategy  
        enable_pre_call_check=True, # enables router rate limits for concurrent calls  
        )  
  
## CALL 1+2  
tasks = []  
response = None  
final_response = None  
for _ in range(2):  
  tasks.append(router.acompletion(model=model, messages=messages))  
response = await asyncio.gather(*tasks)  
  
if response is not None:  
  ## CALL 3   
  await asyncio.sleep(1) # let the cache update happen  
  picked_deployment = router.lowestlatency_logger.get_available_deployments(  
    model_group=model, healthy_deployments=router.healthy_deployments  
  )  
  final_response = await router.acompletion(model=model, messages=messages)  
  print(f"min deployment id: {picked_deployment}")  
  print(f"model id: {final_response._hidden_params['model_id']}")  
  assert (  
    final_response._hidden_params["model_id"]  
    == picked_deployment["model_info"]["id"]  
  )  

```

#### Set Time Window​
Set time window for how far back to consider when averaging latency for a deployment. 
**In Router**
```
router = Router(..., routing_strategy_args={"ttl": 10})  

```

**In Proxy**
```
router_settings:  
  routing_strategy_args: {"ttl": 10}  

```

#### Set Lowest Latency Buffer​
Set a buffer within which deployments are candidates for making calls to. 
E.g. 
if you have 5 deployments
```
https://litellm-prod-1.openai.azure.com/: 0.07s  
https://litellm-prod-2.openai.azure.com/: 0.1s  
https://litellm-prod-3.openai.azure.com/: 0.1s  
https://litellm-prod-4.openai.azure.com/: 0.1s  
https://litellm-prod-5.openai.azure.com/: 4.66s  

```

to prevent initially overloading `prod-1`, with all requests - we can set a buffer of 50%, to consider deployments `prod-2, prod-3, prod-4`. 
**In Router**
```
router = Router(..., routing_strategy_args={"lowest_latency_buffer": 0.5})  

```

**In Proxy**
```
router_settings:  
  routing_strategy_args: {"lowest_latency_buffer": 0.5}  

```

**Default** Picks a deployment based on the provided **Requests per minute (rpm) or Tokens per minute (tpm)**
If `rpm` or `tpm` is not provided, it randomly picks a deployment
You can also set a `weight` param, to specify which model should get picked when.
  * RPM-based shuffling
  * Weight-based shuffling


##### **LiteLLM Proxy Config.yaml**​
```
model_list:  
  - model_name: gpt-3.5-turbo  
   litellm_params:  
    model: azure/chatgpt-v-2  
    api_key: os.environ/AZURE_API_KEY  
    api_version: os.environ/AZURE_API_VERSION  
    api_base: os.environ/AZURE_API_BASE  
    rpm: 900   
  - model_name: gpt-3.5-turbo  
   litellm_params:  
    model: azure/chatgpt-functioncalling  
    api_key: os.environ/AZURE_API_KEY  
    api_version: os.environ/AZURE_API_VERSION  
    api_base: os.environ/AZURE_API_BASE  
    rpm: 10   

```

##### **Python SDK**​
```
from litellm import Router   
import asyncio  
  
model_list = [{ # list of model deployments   
  "model_name": "gpt-3.5-turbo", # model alias   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-v-2", # actual model name  
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE"),  
    "rpm": 900,     # requests per minute for this API  
  }  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-functioncalling",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE"),  
    "rpm": 10,  
  }  
},]  
  
# init router  
router = Router(model_list=model_list, routing_strategy="simple-shuffle")  
async def router_acompletion():  
  response = await router.acompletion(  
    model="gpt-3.5-turbo",   
    messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  )  
  print(response)  
  return response  
  
asyncio.run(router_acompletion())  

```

##### **LiteLLM Proxy Config.yaml**​
```
model_list:  
  - model_name: gpt-3.5-turbo  
   litellm_params:  
    model: azure/chatgpt-v-2  
    api_key: os.environ/AZURE_API_KEY  
    api_version: os.environ/AZURE_API_VERSION  
    api_base: os.environ/AZURE_API_BASE  
    weight: 9  
  - model_name: gpt-3.5-turbo  
   litellm_params:  
    model: azure/chatgpt-functioncalling  
    api_key: os.environ/AZURE_API_KEY  
    api_version: os.environ/AZURE_API_VERSION  
    api_base: os.environ/AZURE_API_BASE  
    weight: 1   

```

##### **Python SDK**​
```
from litellm import Router   
import asyncio  
  
model_list = [{  
  "model_name": "gpt-3.5-turbo", # model alias   
  "litellm_params": {   
    "model": "azure/chatgpt-v-2", # actual model name  
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE"),  
    "weight": 9, # pick this 90% of the time  
  }  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": {   
    "model": "azure/chatgpt-functioncalling",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE"),  
    "weight": 1,  
  }  
}]  
  
# init router  
router = Router(model_list=model_list, routing_strategy="simple-shuffle")  
async def router_acompletion():  
  response = await router.acompletion(  
    model="gpt-3.5-turbo",   
    messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  )  
  print(response)  
  return response  
  
asyncio.run(router_acompletion())  

```

This will route to the deployment with the lowest TPM usage for that minute. 
In production, we use Redis to track usage (TPM/RPM) across multiple deployments. 
If you pass in the deployment's tpm/rpm limits, this will also check against that, and filter out any who's limits would be exceeded. 
For Azure, your RPM = TPM/6. 
```
from litellm import Router   
  
  
model_list = [{ # list of model deployments   
  "model_name": "gpt-3.5-turbo", # model alias   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-v-2", # actual model name  
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
  },   
  "tpm": 100000,  
  "rpm": 10000,  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-functioncalling",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
  },  
  "tpm": 100000,  
  "rpm": 1000,  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "gpt-3.5-turbo",   
    "api_key": os.getenv("OPENAI_API_KEY"),  
  },  
  "tpm": 100000,  
  "rpm": 1000,  
}]  
router = Router(model_list=model_list,   
        redis_host=os.environ["REDIS_HOST"],   
        redis_password=os.environ["REDIS_PASSWORD"],   
        redis_port=os.environ["REDIS_PORT"],   
        routing_strategy="usage-based-routing"  
        enable_pre_call_check=True, # enables router rate limits for concurrent calls  
        )  
  
response = await router.acompletion(model="gpt-3.5-turbo",   
        messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  
print(response)  

```

Picks a deployment with the least number of ongoing calls, it's handling.
**How to test**
```
from litellm import Router   
import asyncio  
  
model_list = [{ # list of model deployments   
  "model_name": "gpt-3.5-turbo", # model alias   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-v-2", # actual model name  
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE"),  
  }  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-functioncalling",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE"),  
  }  
}, {  
  "model_name": "gpt-3.5-turbo",   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "gpt-3.5-turbo",   
    "api_key": os.getenv("OPENAI_API_KEY"),  
  }  
}]  
  
# init router  
router = Router(model_list=model_list, routing_strategy="least-busy")  
async def router_acompletion():  
  response = await router.acompletion(  
    model="gpt-3.5-turbo",   
    messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  )  
  print(response)  
  return response  
  
asyncio.run(router_acompletion())  

```

**Plugin a custom routing strategy to select deployments**
Step 1. Define your custom routing strategy
```
  
from litellm.router import CustomRoutingStrategyBase  
class CustomRoutingStrategy(CustomRoutingStrategyBase):  
  async def async_get_available_deployment(  
    self,  
    model: str,  
    messages: Optional[List[Dict[str, str]]] = None,  
    input: Optional[Union[str, List]] = None,  
    specific_deployment: Optional[bool] = False,  
    request_kwargs: Optional[Dict] = None,  
  ):  
    """  
    Asynchronously retrieves the available deployment based on the given parameters.  
  
    Args:  
      model (str): The name of the model.  
      messages (Optional[List[Dict[str, str]]], optional): The list of messages for a given request. Defaults to None.  
      input (Optional[Union[str, List]], optional): The input for a given embedding request. Defaults to None.  
      specific_deployment (Optional[bool], optional): Whether to retrieve a specific deployment. Defaults to False.  
      request_kwargs (Optional[Dict], optional): Additional request keyword arguments. Defaults to None.  
  
    Returns:  
      Returns an element from litellm.router.model_list  
  
    """  
    print("In CUSTOM async get available deployment")  
    model_list = router.model_list  
    print("router model list=", model_list)  
    for model in model_list:  
      if isinstance(model, dict):  
        if model["litellm_params"]["model"] == "openai/very-special-endpoint":  
          return model  
    pass  
  
  def get_available_deployment(  
    self,  
    model: str,  
    messages: Optional[List[Dict[str, str]]] = None,  
    input: Optional[Union[str, List]] = None,  
    specific_deployment: Optional[bool] = False,  
    request_kwargs: Optional[Dict] = None,  
  ):  
    """  
    Synchronously retrieves the available deployment based on the given parameters.  
  
    Args:  
      model (str): The name of the model.  
      messages (Optional[List[Dict[str, str]]], optional): The list of messages for a given request. Defaults to None.  
      input (Optional[Union[str, List]], optional): The input for a given embedding request. Defaults to None.  
      specific_deployment (Optional[bool], optional): Whether to retrieve a specific deployment. Defaults to False.  
      request_kwargs (Optional[Dict], optional): Additional request keyword arguments. Defaults to None.  
  
    Returns:  
      Returns an element from litellm.router.model_list  
  
    """  
    pass  

```

Step 2. Initialize Router with custom routing strategy
```
from litellm import Router  
  
router = Router(  
  model_list=[  
    {  
      "model_name": "azure-model",  
      "litellm_params": {  
        "model": "openai/very-special-endpoint",  
        "api_base": "https://exampleopenaiendpoint-production.up.railway.app/", # If you are Krrish, this is OpenAI Endpoint3 on our Railway endpoint :)  
        "api_key": "fake-key",  
      },  
      "model_info": {"id": "very-special-endpoint"},  
    },  
    {  
      "model_name": "azure-model",  
      "litellm_params": {  
        "model": "openai/fast-endpoint",  
        "api_base": "https://exampleopenaiendpoint-production.up.railway.app/",  
        "api_key": "fake-key",  
      },  
      "model_info": {"id": "fast-endpoint"},  
    },  
  ],  
  set_verbose=True,  
  debug_level="DEBUG",  
  timeout=1,  
) # type: ignore  
  
router.set_custom_routing_strategy(CustomRoutingStrategy()) # 👈 Set your routing strategy here  

```

Step 3. Test your routing strategy. Expect your custom routing strategy to be called when running `router.acompletion` requests
```
for _ in range(10):  
  response = await router.acompletion(  
    model="azure-model", messages=[{"role": "user", "content": "hello"}]  
  )  
  print(response)  
  _picked_model_id = response._hidden_params["model_id"]  
  print("picked model=", _picked_model_id)  

```

Picks a deployment based on the lowest cost
How this works:
  * Get all healthy deployments
  * Select all deployments that are under their provided `rpm/tpm` limits
  * For each deployment check if `litellm_param["model"]` exists in `litellm_model_cost_map`
    * if deployment does not exist in `litellm_model_cost_map` -> use deployment_cost= `$1`
  * Select deployment with lowest cost


```
from litellm import Router   
import asyncio  
  
model_list = [  
  {  
    "model_name": "gpt-3.5-turbo",  
    "litellm_params": {"model": "gpt-4"},  
    "model_info": {"id": "openai-gpt-4"},  
  },  
  {  
    "model_name": "gpt-3.5-turbo",  
    "litellm_params": {"model": "groq/llama3-8b-8192"},  
    "model_info": {"id": "groq-llama"},  
  },  
]  
  
# init router  
router = Router(model_list=model_list, routing_strategy="cost-based-routing")  
async def router_acompletion():  
  response = await router.acompletion(  
    model="gpt-3.5-turbo",   
    messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  )  
  print(response)  
  
  print(response._hidden_params["model_id"]) # expect groq-llama, since groq/llama has lowest cost  
  return response  
  
asyncio.run(router_acompletion())  
  

```

#### Using Custom Input/Output pricing​
Set `litellm_params["input_cost_per_token"]` and `litellm_params["output_cost_per_token"]` for using custom pricing when routing
```
model_list = [  
  {  
    "model_name": "gpt-3.5-turbo",  
    "litellm_params": {  
      "model": "azure/chatgpt-v-2",  
      "input_cost_per_token": 0.00003,  
      "output_cost_per_token": 0.00003,  
    },  
    "model_info": {"id": "chatgpt-v-experimental"},  
  },  
  {  
    "model_name": "gpt-3.5-turbo",  
    "litellm_params": {  
      "model": "azure/chatgpt-v-1",  
      "input_cost_per_token": 0.000000001,  
      "output_cost_per_token": 0.00000001,  
    },  
    "model_info": {"id": "chatgpt-v-1"},  
  },  
  {  
    "model_name": "gpt-3.5-turbo",  
    "litellm_params": {  
      "model": "azure/chatgpt-v-5",  
      "input_cost_per_token": 10,  
      "output_cost_per_token": 12,  
    },  
    "model_info": {"id": "chatgpt-v-5"},  
  },  
]  
# init router  
router = Router(model_list=model_list, routing_strategy="cost-based-routing")  
async def router_acompletion():  
  response = await router.acompletion(  
    model="gpt-3.5-turbo",   
    messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  )  
  print(response)  
  
  print(response._hidden_params["model_id"]) # expect chatgpt-v-1, since chatgpt-v-1 has lowest cost  
  return response  
  
asyncio.run(router_acompletion())  

```

## Basic Reliability​
### Weighted Deployments​
Set `weight` on a deployment to pick one deployment more often than others. 
This works across **simple-shuffle** routing strategy (this is the default, if no routing strategy is selected). 
  * SDK
  * PROXY


```
from litellm import Router   
  
model_list = [  
  {  
    "model_name": "o1",  
    "litellm_params": {  
      "model": "o1-preview",   
      "api_key": os.getenv("OPENAI_API_KEY"),   
      "weight": 1  
    },  
  },  
  {  
    "model_name": "o1",  
    "litellm_params": {  
      "model": "o1-preview",   
      "api_key": os.getenv("OPENAI_API_KEY"),   
      "weight": 2 # 👈 PICK THIS DEPLOYMENT 2x MORE OFTEN THAN o1-preview  
    },  
  },  
]  
  
router = Router(model_list=model_list, routing_strategy="cost-based-routing")  
  
response = await router.acompletion(  
  model="gpt-3.5-turbo",   
  messages=[{"role": "user", "content": "Hey, how's it going?"}]  
)  
print(response)  

```

```
model_list:  
 - model_name: o1  
  litellm_params:  
    model: o1  
    api_key: os.environ/OPENAI_API_KEY  
    weight: 1    
 - model_name: o1  
  litellm_params:  
    model: o1-preview  
    api_key: os.environ/OPENAI_API_KEY  
    weight: 2 # 👈 PICK THIS DEPLOYMENT 2x MORE OFTEN THAN o1-preview  

```

### Max Parallel Requests (ASYNC)​
Used in semaphore for async requests on router. Limit the max concurrent calls made to a deployment. Useful in high-traffic scenarios. 
If tpm/rpm is set, and no max parallel request limit given, we use the RPM or calculated RPM (tpm/1000/6) as the max parallel request limit. 
```
from litellm import Router   
  
model_list = [{  
  "model_name": "gpt-4",  
  "litellm_params": {  
    "model": "azure/gpt-4",  
    ...  
    "max_parallel_requests": 10 # 👈 SET PER DEPLOYMENT  
  }  
}]  
  
### OR ###   
  
router = Router(model_list=model_list, default_max_parallel_requests=20) # 👈 SET DEFAULT MAX PARALLEL REQUESTS   
  
  
# deployment max parallel requests > default max parallel requests  

```

**See Code**
### Cooldowns​
Set the limit for how many calls a model is allowed to fail in a minute, before being cooled down for a minute. 
  * SDK
  * PROXY


```
from litellm import Router  
  
model_list = [{...}]  
  
router = Router(model_list=model_list,   
        allowed_fails=1,   # cooldown model if it fails > 1 call in a minute.   
        cooldown_time=100  # cooldown the deployment for 100 seconds if it num_fails > allowed_fails  
    )  
  
user_message = "Hello, whats the weather in San Francisco??"  
messages = [{"content": user_message, "role": "user"}]  
  
# normal call   
response = router.completion(model="gpt-3.5-turbo", messages=messages)  
  
print(f"response: {response}")  

```

**Set Global Value**
```
router_settings:  
  allowed_fails: 3 # cooldown model if it fails > 1 call in a minute.   
  cooldown_time: 30 # (in seconds) how long to cooldown model if fails/min > allowed_fails  

```

Defaults:
  * allowed_fails: 3
  * cooldown_time: 5s (`DEFAULT_COOLDOWN_TIME_SECONDS` in constants.py)


**Set Per Model**
```
model_list:  
- model_name: fake-openai-endpoint  
 litellm_params:  
  model: predibase/llama-3-8b-instruct  
  api_key: os.environ/PREDIBASE_API_KEY  
  tenant_id: os.environ/PREDIBASE_TENANT_ID  
  max_new_tokens: 256  
  cooldown_time: 0 # 👈 KEY CHANGE  

```

**Expected Response**
```
No deployments available for selected model, Try again in 60 seconds. Passed model=claude-3-5-sonnet. pre-call-checks=False, allowed_model_region=n/a.  

```

#### **Disable cooldowns**​
  * SDK
  * PROXY


```
from litellm import Router   
  
  
router = Router(..., disable_cooldowns=True)  

```

```
router_settings:  
  disable_cooldowns: True  

```

### Retries​
For both async + sync functions, we support retrying failed requests. 
For RateLimitError we implement exponential backoffs 
For generic errors, we retry immediately 
Here's a quick look at how we can set `num_retries = 3`: 
```
from litellm import Router  
  
model_list = [{...}]  
  
router = Router(model_list=model_list,   
        num_retries=3)  
  
user_message = "Hello, whats the weather in San Francisco??"  
messages = [{"content": user_message, "role": "user"}]  
  
# normal call   
response = router.completion(model="gpt-3.5-turbo", messages=messages)  
  
print(f"response: {response}")  

```

We also support setting minimum time to wait before retrying a failed request. This is via the `retry_after` param. 
```
from litellm import Router  
  
model_list = [{...}]  
  
router = Router(model_list=model_list,   
        num_retries=3, retry_after=5) # waits min 5s before retrying request  
  
user_message = "Hello, whats the weather in San Francisco??"  
messages = [{"content": user_message, "role": "user"}]  
  
# normal call   
response = router.completion(model="gpt-3.5-turbo", messages=messages)  
  
print(f"response: {response}")  

```

### [Advanced]: Custom Retries, Cooldowns based on Error Type​
  * Use `RetryPolicy` if you want to set a `num_retries` based on the Exception received
  * Use `AllowedFailsPolicy` to set a custom number of `allowed_fails`/minute before cooling down a deployment


**See All Exception Types**
  * SDK
  * PROXY


Example:
```
retry_policy = RetryPolicy(  
  ContentPolicyViolationErrorRetries=3,     # run 3 retries for ContentPolicyViolationErrors  
  AuthenticationErrorRetries=0,         # run 0 retries for AuthenticationErrorRetries  
)  
  
allowed_fails_policy = AllowedFailsPolicy(  
  ContentPolicyViolationErrorAllowedFails=1000, # Allow 1000 ContentPolicyViolationError before cooling down a deployment  
  RateLimitErrorAllowedFails=100,        # Allow 100 RateLimitErrors before cooling down a deployment  
)  

```

Example Usage
```
from litellm.router import RetryPolicy, AllowedFailsPolicy  
  
retry_policy = RetryPolicy(  
  ContentPolicyViolationErrorRetries=3,     # run 3 retries for ContentPolicyViolationErrors  
  AuthenticationErrorRetries=0,         # run 0 retries for AuthenticationErrorRetries  
  BadRequestErrorRetries=1,  
  TimeoutErrorRetries=2,  
  RateLimitErrorRetries=3,  
)  
  
allowed_fails_policy = AllowedFailsPolicy(  
  ContentPolicyViolationErrorAllowedFails=1000, # Allow 1000 ContentPolicyViolationError before cooling down a deployment  
  RateLimitErrorAllowedFails=100,        # Allow 100 RateLimitErrors before cooling down a deployment  
)  
  
router = litellm.Router(  
  model_list=[  
    {  
      "model_name": "gpt-3.5-turbo", # openai model name  
      "litellm_params": { # params for litellm completion/embedding call  
        "model": "azure/chatgpt-v-2",  
        "api_key": os.getenv("AZURE_API_KEY"),  
        "api_version": os.getenv("AZURE_API_VERSION"),  
        "api_base": os.getenv("AZURE_API_BASE"),  
      },  
    },  
    {  
      "model_name": "bad-model", # openai model name  
      "litellm_params": { # params for litellm completion/embedding call  
        "model": "azure/chatgpt-v-2",  
        "api_key": "bad-key",  
        "api_version": os.getenv("AZURE_API_VERSION"),  
        "api_base": os.getenv("AZURE_API_BASE"),  
      },  
    },  
  ],  
  retry_policy=retry_policy,  
  allowed_fails_policy=allowed_fails_policy,  
)  
  
response = await router.acompletion(  
  model=model,  
  messages=messages,  
)  

```

```
router_settings:   
 retry_policy: {  
  "BadRequestErrorRetries": 3,  
  "ContentPolicyViolationErrorRetries": 4  
 }  
 allowed_fails_policy: {  
  "ContentPolicyViolationErrorAllowedFails": 1000, # Allow 1000 ContentPolicyViolationError before cooling down a deployment  
  "RateLimitErrorAllowedFails": 100 # Allow 100 RateLimitErrors before cooling down a deployment  
 }  

```

### Caching​
In production, we recommend using a Redis cache. For quickly testing things locally, we also support simple in-memory caching. 
**In-memory Cache**
```
router = Router(model_list=model_list,   
        cache_responses=True)  
  
print(response)  

```

**Redis Cache**
```
router = Router(model_list=model_list,   
        redis_host=os.getenv("REDIS_HOST"),   
        redis_password=os.getenv("REDIS_PASSWORD"),   
        redis_port=os.getenv("REDIS_PORT"),  
        cache_responses=True)  
  
print(response)  

```

**Pass in Redis URL, additional kwargs**
```
router = Router(model_list: Optional[list] = None,  
         ## CACHING ##   
         redis_url=os.getenv("REDIS_URL")",  
         cache_kwargs= {}, # additional kwargs to pass to RedisCache (see caching.py)  
         cache_responses=True)  

```

## Pre-Call Checks (Context Window, EU-Regions)​
Enable pre-call checks to filter out:
  1. deployments with context window limit < messages for a call.
  2. deployments outside of eu-region


  * SDK
  * Proxy


**1. Enable pre-call checks**
```
from litellm import Router   
# ...  
router = Router(model_list=model_list, enable_pre_call_checks=True) # 👈 Set to True  

```

**2. Set Model List**
For context window checks on azure deployments, set the base model. Pick the base model from this list, all the azure models start with `azure/`. 
For 'eu-region' filtering, Set 'region_name' of deployment. 
**Note:** We automatically infer region_name for Vertex AI, Bedrock, and IBM WatsonxAI based on your litellm params. For Azure, set `litellm.enable_preview = True`.
**See Code**
```
model_list = [  
      {  
        "model_name": "gpt-3.5-turbo", # model group name  
        "litellm_params": { # params for litellm completion/embedding call  
          "model": "azure/chatgpt-v-2",  
          "api_key": os.getenv("AZURE_API_KEY"),  
          "api_version": os.getenv("AZURE_API_VERSION"),  
          "api_base": os.getenv("AZURE_API_BASE"),  
          "region_name": "eu" # 👈 SET 'EU' REGION NAME  
          "base_model": "azure/gpt-35-turbo", # 👈 (Azure-only) SET BASE MODEL  
        },  
      },  
      {  
        "model_name": "gpt-3.5-turbo", # model group name  
        "litellm_params": { # params for litellm completion/embedding call  
          "model": "gpt-3.5-turbo-1106",  
          "api_key": os.getenv("OPENAI_API_KEY"),  
        },  
      },  
      {  
        "model_name": "gemini-pro",  
        "litellm_params: {  
          "model": "vertex_ai/gemini-pro-1.5",   
          "vertex_project": "adroit-crow-1234",  
          "vertex_location": "us-east1" # 👈 AUTOMATICALLY INFERS 'region_name'  
        }  
      }  
    ]  
  
router = Router(model_list=model_list, enable_pre_call_checks=True)   

```

**3. Test it!**
  * Context Window Check
  * EU Region Check


```
"""  
- Give a gpt-3.5-turbo model group with different context windows (4k vs. 16k)  
- Send a 5k prompt  
- Assert it works  
"""  
from litellm import Router  
import os  
  
model_list = [  
  {  
    "model_name": "gpt-3.5-turbo", # model group name  
    "litellm_params": { # params for litellm completion/embedding call  
      "model": "azure/chatgpt-v-2",  
      "api_key": os.getenv("AZURE_API_KEY"),  
      "api_version": os.getenv("AZURE_API_VERSION"),  
      "api_base": os.getenv("AZURE_API_BASE"),  
      "base_model": "azure/gpt-35-turbo",  
    },  
    "model_info": {  
      "base_model": "azure/gpt-35-turbo",   
    }  
  },  
  {  
    "model_name": "gpt-3.5-turbo", # model group name  
    "litellm_params": { # params for litellm completion/embedding call  
      "model": "gpt-3.5-turbo-1106",  
      "api_key": os.getenv("OPENAI_API_KEY"),  
    },  
  },  
]  
  
router = Router(model_list=model_list, enable_pre_call_checks=True)   
  
text = "What is the meaning of 42?" * 5000  
  
response = router.completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {"role": "system", "content": text},  
    {"role": "user", "content": "Who was Alexander?"},  
  ],  
)  
  
print(f"response: {response}")  

```

```
"""  
- Give 2 gpt-3.5-turbo deployments, in eu + non-eu regions  
- Make a call  
- Assert it picks the eu-region model  
"""  
  
from litellm import Router  
import os  
  
model_list = [  
  {  
    "model_name": "gpt-3.5-turbo", # model group name  
    "litellm_params": { # params for litellm completion/embedding call  
      "model": "azure/chatgpt-v-2",  
      "api_key": os.getenv("AZURE_API_KEY"),  
      "api_version": os.getenv("AZURE_API_VERSION"),  
      "api_base": os.getenv("AZURE_API_BASE"),  
      "region_name": "eu"  
    },  
    "model_info": {  
      "id": "1"  
    }  
  },  
  {  
    "model_name": "gpt-3.5-turbo", # model group name  
    "litellm_params": { # params for litellm completion/embedding call  
      "model": "gpt-3.5-turbo-1106",  
      "api_key": os.getenv("OPENAI_API_KEY"),  
    },  
    "model_info": {  
      "id": "2"  
    }  
  },  
]  
  
router = Router(model_list=model_list, enable_pre_call_checks=True)   
  
response = router.completion(  
  model="gpt-3.5-turbo",  
  messages=[{"role": "user", "content": "Who was Alexander?"}],  
)  
  
print(f"response: {response}")  
  
print(f"response id: {response._hidden_params['model_id']}")  

```

info
Go here for how to do this on the proxy
## Caching across model groups​
If you want to cache across 2 different model groups (e.g. azure deployments, and openai), use caching groups. 
```
import litellm, asyncio, time  
from litellm import Router   
  
# set os env  
os.environ["OPENAI_API_KEY"] = ""  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
async def test_acompletion_caching_on_router_caching_groups():   
  # tests acompletion + caching on router   
  try:  
    litellm.set_verbose = True  
    model_list = [  
      {  
        "model_name": "openai-gpt-3.5-turbo",  
        "litellm_params": {  
          "model": "gpt-3.5-turbo-0613",  
          "api_key": os.getenv("OPENAI_API_KEY"),  
        },  
      },  
      {  
        "model_name": "azure-gpt-3.5-turbo",  
        "litellm_params": {  
          "model": "azure/chatgpt-v-2",  
          "api_key": os.getenv("AZURE_API_KEY"),  
          "api_base": os.getenv("AZURE_API_BASE"),  
          "api_version": os.getenv("AZURE_API_VERSION")  
        },  
      }  
    ]  
  
    messages = [  
      {"role": "user", "content": f"write a one sentence poem {time.time()}?"}  
    ]  
    start_time = time.time()  
    router = Router(model_list=model_list,   
        cache_responses=True,   
        caching_groups=[("openai-gpt-3.5-turbo", "azure-gpt-3.5-turbo")])  
    response1 = await router.acompletion(model="openai-gpt-3.5-turbo", messages=messages, temperature=1)  
    print(f"response1: {response1}")  
    await asyncio.sleep(1) # add cache is async, async sleep for cache to get set  
    response2 = await router.acompletion(model="azure-gpt-3.5-turbo", messages=messages, temperature=1)  
    assert response1.id == response2.id  
    assert len(response1.choices[0].message.content) > 0  
    assert response1.choices[0].message.content == response2.choices[0].message.content  
  except Exception as e:  
    traceback.print_exc()  
  
asyncio.run(test_acompletion_caching_on_router_caching_groups())  

```

## Alerting 🚨​
Send alerts to slack / your webhook url for the following events
  * LLM API Exceptions
  * Slow LLM Responses


Get a slack webhook url from https://api.slack.com/messaging/webhooks
#### Usage​
Initialize an `AlertingConfig` and pass it to `litellm.Router`. The following code will trigger an alert because `api_key=bad-key` which is invalid
```
from litellm.router import AlertingConfig  
import litellm  
import os  
  
router = litellm.Router(  
  model_list=[  
    {  
      "model_name": "gpt-3.5-turbo",  
      "litellm_params": {  
        "model": "gpt-3.5-turbo",  
        "api_key": "bad_key",  
      },  
    }  
  ],  
  alerting_config= AlertingConfig(  
    alerting_threshold=10,            # threshold for slow / hanging llm responses (in seconds). Defaults to 300 seconds  
    webhook_url= os.getenv("SLACK_WEBHOOK_URL")  # webhook you want to send alerts to  
  ),  
)  
try:  
  await router.acompletion(  
    model="gpt-3.5-turbo",  
    messages=[{"role": "user", "content": "Hey, how's it going?"}],  
  )  
except:  
  pass  

```

## Track cost for Azure Deployments​
**Problem** : Azure returns `gpt-4` in the response when `azure/gpt-4-1106-preview` is used. This leads to inaccurate cost tracking
**Solution** ✅ : Set `model_info["base_model"]` on your router init so litellm uses the correct model for calculating azure cost
Step 1. Router Setup
```
from litellm import Router  
  
model_list = [  
  { # list of model deployments   
    "model_name": "gpt-4-preview", # model alias   
    "litellm_params": { # params for litellm completion/embedding call   
      "model": "azure/chatgpt-v-2", # actual model name  
      "api_key": os.getenv("AZURE_API_KEY"),  
      "api_version": os.getenv("AZURE_API_VERSION"),  
      "api_base": os.getenv("AZURE_API_BASE")  
    },  
    "model_info": {  
      "base_model": "azure/gpt-4-1106-preview" # azure/gpt-4-1106-preview will be used for cost tracking, ensure this exists in litellm model_prices_and_context_window.json  
    }  
  },   
  {  
    "model_name": "gpt-4-32k",   
    "litellm_params": { # params for litellm completion/embedding call   
      "model": "azure/chatgpt-functioncalling",   
      "api_key": os.getenv("AZURE_API_KEY"),  
      "api_version": os.getenv("AZURE_API_VERSION"),  
      "api_base": os.getenv("AZURE_API_BASE")  
    },  
    "model_info": {  
      "base_model": "azure/gpt-4-32k" # azure/gpt-4-32k will be used for cost tracking, ensure this exists in litellm model_prices_and_context_window.json  
    }  
  }  
]  
  
router = Router(model_list=model_list)  
  

```

Step 2. Access `response_cost` in the custom callback, **litellm calculates the response cost for you**
```
import litellm  
from litellm.integrations.custom_logger import CustomLogger  
  
class MyCustomHandler(CustomLogger):      
  def log_success_event(self, kwargs, response_obj, start_time, end_time):   
    print(f"On Success")  
    response_cost = kwargs.get("response_cost")  
    print("response_cost=", response_cost)  
  
customHandler = MyCustomHandler()  
litellm.callbacks = [customHandler]  
  
# router completion call  
response = router.completion(  
  model="gpt-4-32k",   
  messages=[{ "role": "user", "content": "Hi who are you"}]  
)  

```

#### Default litellm.completion/embedding params​
You can also set default params for litellm completion/embedding calls. Here's how to do that: 
```
from litellm import Router  
  
fallback_dict = {"gpt-3.5-turbo": "gpt-3.5-turbo-16k"}  
  
router = Router(model_list=model_list,   
        default_litellm_params={"context_window_fallback_dict": fallback_dict})  
  
user_message = "Hello, whats the weather in San Francisco??"  
messages = [{"content": user_message, "role": "user"}]  
  
# normal call   
response = router.completion(model="gpt-3.5-turbo", messages=messages)  
  
print(f"response: {response}")  

```

## Custom Callbacks - Track API Key, API Endpoint, Model Used​
If you need to track the api_key, api endpoint, model, custom_llm_provider used for each completion call, you can setup a custom callback
### Usage​
```
import litellm  
from litellm.integrations.custom_logger import CustomLogger  
  
class MyCustomHandler(CustomLogger):      
  def log_success_event(self, kwargs, response_obj, start_time, end_time):   
    print(f"On Success")  
    print("kwargs=", kwargs)  
    litellm_params= kwargs.get("litellm_params")  
    api_key = litellm_params.get("api_key")  
    api_base = litellm_params.get("api_base")  
    custom_llm_provider= litellm_params.get("custom_llm_provider")  
    response_cost = kwargs.get("response_cost")  
  
    # print the values  
    print("api_key=", api_key)  
    print("api_base=", api_base)  
    print("custom_llm_provider=", custom_llm_provider)  
    print("response_cost=", response_cost)  
  
  def log_failure_event(self, kwargs, response_obj, start_time, end_time):   
    print(f"On Failure")  
    print("kwargs=")  
  
customHandler = MyCustomHandler()  
  
litellm.callbacks = [customHandler]  
  
# Init Router  
router = Router(model_list=model_list, routing_strategy="simple-shuffle")  
  
# router completion call  
response = router.completion(  
  model="gpt-3.5-turbo",   
  messages=[{ "role": "user", "content": "Hi who are you"}]  
)  

```

## Deploy Router​
If you want a server to load balance across different LLM APIs, use our LiteLLM Proxy Server
## Debugging Router​
### Basic Debugging​
Set `Router(set_verbose=True)`
```
from litellm import Router  
  
router = Router(  
  model_list=model_list,  
  set_verbose=True  
)  

```

### Detailed Debugging​
Set `Router(set_verbose=True,debug_level="DEBUG")`
```
from litellm import Router  
  
router = Router(  
  model_list=model_list,  
  set_verbose=True,  
  debug_level="DEBUG" # defaults to INFO  
)  

```

### Very Detailed Debugging​
Set `litellm.set_verbose=True` and `Router(set_verbose=True,debug_level="DEBUG")`
```
from litellm import Router  
import litellm  
  
litellm.set_verbose = True  
  
router = Router(  
  model_list=model_list,  
  set_verbose=True,  
  debug_level="DEBUG" # defaults to INFO  
)  

```

## Router General Settings​
### Usage​
```
router = Router(model_list=..., router_general_settings=RouterGeneralSettings(async_only_mode=True))  

```

### Spec​
```
class RouterGeneralSettings(BaseModel):  
  async_only_mode: bool = Field(  
    default=False  
  ) # this will only initialize async clients. Good for memory utils  
  pass_through_all_models: bool = Field(  
    default=False  
  ) # if passed a model not llm_router model list, pass through the request to litellm.acompletion/embedding  

```

Previous
Routing, Loadbalancing & Fallbacks
Next
[BETA] Request Prioritization
  * Load Balancing
    * Quick Start
    * Available Endpoints
  * Advanced - Routing Strategies ⭐️
  * Basic Reliability
    * Weighted Deployments
    * Max Parallel Requests (ASYNC)
    * Cooldowns
    * Retries
    * Advanced: Custom Retries, Cooldowns based on Error Type
    * Caching
  * Pre-Call Checks (Context Window, EU-Regions)
  * Caching across model groups
  * Alerting 🚨
  * Track cost for Azure Deployments
  * Custom Callbacks - Track API Key, API Endpoint, Model Used
    * Usage
  * Deploy Router
  * Debugging Router
    * Basic Debugging
    * Detailed Debugging
    * Very Detailed Debugging
  * Router General Settings
    * Usage
    * Spec


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Support & Talk with founders


# Support & Talk with founders
Schedule Demo 👋
Community Discord 💭
Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
Our emails ✉️ ishaan@berri.ai / krrish@berri.ai
![Chat on WhatsApp](https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square) ![Chat on Discord](https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square)
Previous
[OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /v1/messages [BETA]


On this page
# /v1/messages [BETA]
Use LiteLLM to call all your LLM APIs in the Anthropic `v1/messages` format. 
## Overview​
Feature| Supported| Notes  
---|---|---  
Cost Tracking| ✅|   
Logging| ✅| works across all integrations  
End-user Tracking| ✅|   
Streaming| ✅|   
Fallbacks| ✅| between anthropic models  
Loadbalancing| ✅| between anthropic models  
Planned improvement:
  * Vertex AI Anthropic support
  * Bedrock Anthropic support


## Usage​
* * *
### LiteLLM Python SDK​
#### Non-streaming example​
Example using LiteLLM Python SDK
```
import litellm  
response = await litellm.anthropic.messages.acreate(  
  messages=[{"role": "user", "content": "Hello, can you tell me a short joke?"}],  
  api_key=api_key,  
  model="anthropic/claude-3-haiku-20240307",  
  max_tokens=100,  
)  

```

Example response:
```
{  
 "content": [  
  {  
   "text": "Hi! this is a very short joke",  
   "type": "text"  
  }  
 ],  
 "id": "msg_013Zva2CMHLNnXjNJJKqJ2EF",  
 "model": "claude-3-7-sonnet-20250219",  
 "role": "assistant",  
 "stop_reason": "end_turn",  
 "stop_sequence": null,  
 "type": "message",  
 "usage": {  
  "input_tokens": 2095,  
  "output_tokens": 503,  
  "cache_creation_input_tokens": 2095,  
  "cache_read_input_tokens": 0  
 }  
}  

```

#### Streaming example​
Example using LiteLLM Python SDK
```
import litellm  
response = await litellm.anthropic.messages.acreate(  
  messages=[{"role": "user", "content": "Hello, can you tell me a short joke?"}],  
  api_key=api_key,  
  model="anthropic/claude-3-haiku-20240307",  
  max_tokens=100,  
  stream=True,  
)  
async for chunk in response:  
  print(chunk)  

```

### LiteLLM Proxy Server​
  1. Setup config.yaml


```
model_list:  
  - model_name: anthropic-claude  
   litellm_params:  
    model: claude-3-7-sonnet-latest  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


  * Anthropic Python SDK
  * curl


Example using LiteLLM Proxy Server
```
import anthropic  
  
# point anthropic sdk to litellm proxy   
client = anthropic.Anthropic(  
  base_url="http://0.0.0.0:4000",  
  api_key="sk-1234",  
)  
  
response = client.messages.create(  
  messages=[{"role": "user", "content": "Hello, can you tell me a short joke?"}],  
  model="anthropic-claude",  
  max_tokens=100,  
)  

```

Example using LiteLLM Proxy Server
```
curl -L -X POST 'http://0.0.0.0:4000/v1/messages' \  
-H 'content-type: application/json' \  
-H 'x-api-key: $LITELLM_API_KEY' \  
-H 'anthropic-version: 2023-06-01' \  
-d '{  
 "model": "anthropic-claude",  
 "messages": [  
  {  
   "role": "user",  
   "content": "Hello, can you tell me a short joke?"  
  }  
 ],  
 "max_tokens": 100  
}'  

```

## Request Format​
* * *
Request body will be in the Anthropic messages API format. **litellm follows the Anthropic messages specification for this endpoint.**
#### Example request body​
```
{  
 "model": "claude-3-7-sonnet-20250219",  
 "max_tokens": 1024,  
 "messages": [  
  {  
   "role": "user",  
   "content": "Hello, world"  
  }  
 ]  
}  

```

#### Required Fields​
  * **model** (string):  
The model identifier (e.g., `"claude-3-7-sonnet-20250219"`).
  * **max_tokens** (integer):  
The maximum number of tokens to generate before stopping.  
_Note: The model may stop before reaching this limit; value must be greater than 1._
  * **messages** (array of objects):  
An ordered list of conversational turns.  
Each message object must include:
    * **role** (enum: `"user"` or `"assistant"`):  
Specifies the speaker of the message.
    * **content** (string or array of content blocks):  
The text or content blocks (e.g., an array containing objects with a `type` such as `"text"`) that form the message.  
_Example equivalence:_
```
{"role": "user", "content": "Hello, Claude"}  

```

is equivalent to:
```
{"role": "user", "content": [{"type": "text", "text": "Hello, Claude"}]}  

```



#### Optional Fields​
  * **metadata** (object):  
Contains additional metadata about the request (e.g., `user_id` as an opaque identifier).
  * **stop_sequences** (array of strings):  
Custom sequences that, when encountered in the generated text, cause the model to stop.
  * **stream** (boolean):  
Indicates whether to stream the response using server-sent events.
  * **system** (string or array):  
A system prompt providing context or specific instructions to the model.
  * **temperature** (number):  
Controls randomness in the model’s responses. Valid range: `0 < temperature < 1`.
  * **thinking** (object):  
Configuration for enabling extended thinking. If enabled, it includes:
    * **budget_tokens** (integer):  
Minimum of 1024 tokens (and less than `max_tokens`).
    * **type** (enum):  
E.g., `"enabled"`.
  * **tool_choice** (object):  
Instructs how the model should utilize any provided tools.
  * **tools** (array of objects):  
Definitions for tools available to the model. Each tool includes:
    * **name** (string):  
The tool’s name.
    * **description** (string):  
A detailed description of the tool.
    * **input_schema** (object):  
A JSON schema describing the expected input format for the tool.
  * **top_k** (integer):  
Limits sampling to the top K options.
  * **top_p** (number):  
Enables nucleus sampling with a cumulative probability cutoff. Valid range: `0 < top_p < 1`.


## Response Format​
* * *
Responses will be in the Anthropic messages API format.
#### Example Response​
```
{  
 "content": [  
  {  
   "text": "Hi! My name is Claude.",  
   "type": "text"  
  }  
 ],  
 "id": "msg_013Zva2CMHLNnXjNJJKqJ2EF",  
 "model": "claude-3-7-sonnet-20250219",  
 "role": "assistant",  
 "stop_reason": "end_turn",  
 "stop_sequence": null,  
 "type": "message",  
 "usage": {  
  "input_tokens": 2095,  
  "output_tokens": 503,  
  "cache_creation_input_tokens": 2095,  
  "cache_read_input_tokens": 0  
 }  
}  

```

#### Response fields​
  * **content** (array of objects):  
Contains the generated content blocks from the model. Each block includes:
    * **type** (string):  
Indicates the type of content (e.g., `"text"`, `"tool_use"`, `"thinking"`, or `"redacted_thinking"`).
    * **text** (string):  
The generated text from the model.  
_Note: Maximum length is 5,000,000 characters._
    * **citations** (array of objects or `null`):  
Optional field providing citation details. Each citation includes:
      * **cited_text** (string):  
The excerpt being cited.
      * **document_index** (integer):  
An index referencing the cited document.
      * **document_title** (string or `null`):  
The title of the cited document.
      * **start_char_index** (integer):  
The starting character index for the citation.
      * **end_char_index** (integer):  
The ending character index for the citation.
      * **type** (string):  
Typically `"char_location"`.
  * **id** (string):  
A unique identifier for the response message.  
_Note: The format and length of IDs may change over time._
  * **model** (string):  
Specifies the model that generated the response.
  * **role** (string):  
Indicates the role of the generated message. For responses, this is always `"assistant"`.
  * **stop_reason** (string):  
Explains why the model stopped generating text. Possible values include:
    * `"end_turn"`: The model reached a natural stopping point.
    * `"max_tokens"`: The generation stopped because the maximum token limit was reached.
    * `"stop_sequence"`: A custom stop sequence was encountered.
    * `"tool_use"`: The model invoked one or more tools.
  * **stop_sequence** (string or `null`):  
Contains the specific stop sequence that caused the generation to halt, if applicable; otherwise, it is `null`.
  * **type** (string):  
Denotes the type of response object, which is always `"message"`.
  * **usage** (object):  
Provides details on token usage for billing and rate limiting. This includes:
    * **input_tokens** (integer):  
Total number of input tokens processed.
    * **output_tokens** (integer):  
Total number of output tokens generated.
    * **cache_creation_input_tokens** (integer or `null`):  
Number of tokens used to create a cache entry.
    * **cache_read_input_tokens** (integer or `null`):  
Number of tokens read from the cache.


Previous
/embeddings
Next
/mcp [BETA] - Model Context Protocol
  * Overview
  * Usage
    * LiteLLM Python SDK
    * LiteLLM Proxy Server
  * Request Format
  * Response Format


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---




---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
      * /audio/transcriptions
      * /audio/speech
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /audio
  * /audio/transcriptions


On this page
# /audio/transcriptions
Use this to loadbalance across Azure + OpenAI. 
## Quick Start​
### LiteLLM Python SDK​
```
from litellm import transcription  
import os   
  
# set api keys   
os.environ["OPENAI_API_KEY"] = ""  
audio_file = open("/path/to/audio.mp3", "rb")  
  
response = transcription(model="whisper", file=audio_file)  
  
print(f"response: {response}")  

```

### LiteLLM Proxy​
### Add model to config​
  * OpenAI
  * OpenAI + Azure


```
model_list:  
- model_name: whisper  
 litellm_params:  
  model: whisper-1  
  api_key: os.environ/OPENAI_API_KEY  
 model_info:  
  mode: audio_transcription  
    
general_settings:  
 master_key: sk-1234  

```

```
model_list:  
- model_name: whisper  
 litellm_params:  
  model: whisper-1  
  api_key: os.environ/OPENAI_API_KEY  
 model_info:  
  mode: audio_transcription  
- model_name: whisper  
 litellm_params:  
  model: azure/azure-whisper  
  api_version: 2024-02-15-preview  
  api_base: os.environ/AZURE_EUROPE_API_BASE  
  api_key: os.environ/AZURE_EUROPE_API_KEY  
 model_info:  
  mode: audio_transcription  
  
general_settings:  
 master_key: sk-1234  

```

### Start proxy​
```
litellm --config /path/to/config.yaml   
  
# RUNNING on http://0.0.0.0:8000  

```

### Test​
  * Curl
  * OpenAI Python SDK


```
curl --location 'http://0.0.0.0:8000/v1/audio/transcriptions' \  
--header 'Authorization: Bearer sk-1234' \  
--form 'file=@"/Users/krrishdholakia/Downloads/gettysburg.wav"' \  
--form 'model="whisper"'  

```

```
from openai import OpenAI  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:8000"  
)  
  
  
audio_file = open("speech.mp3", "rb")  
transcript = client.audio.transcriptions.create(  
 model="whisper",  
 file=audio_file  
)  

```

## Supported Providers​
  * OpenAI
  * Azure
  * Fireworks AI
  * Groq
  * Deepgram


Previous
[BETA] Image Variations
Next
/audio/speech
  * Quick Start
    * LiteLLM Python SDK
    * LiteLLM Proxy
    * Add model to config
    * Start proxy
    * Test
  * Supported Providers


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /assistants


On this page
# /assistants
Covers Threads, Messages, Assistants. 
LiteLLM currently covers: 
  * Create Assistants 
  * Delete Assistants
  * Get Assistants
  * Create Thread
  * Get Thread
  * Add Messages
  * Get Messages
  * Run Thread


## **Supported Providers** :​
  * OpenAI
  * Azure OpenAI
  * OpenAI-Compatible APIs


## Quick Start​
Call an existing Assistant. 
  * Get the Assistant 
  * Create a Thread when a user starts a conversation.
  * Add Messages to the Thread as the user asks questions.
  * Run the Assistant on the Thread to generate a response by calling the model and the tools.


### SDK + PROXY​
  * SDK
  * PROXY


**Create an Assistant**
```
import litellm  
import os   
  
# setup env  
os.environ["OPENAI_API_KEY"] = "sk-.."  
  
assistant = litellm.create_assistants(  
      custom_llm_provider="openai",  
      model="gpt-4-turbo",  
      instructions="You are a personal math tutor. When asked a question, write and run Python code to answer the question.",  
      name="Math Tutor",  
      tools=[{"type": "code_interpreter"}],  
)  
  
### ASYNC USAGE ###   
# assistant = await litellm.acreate_assistants(  
#       custom_llm_provider="openai",  
#       model="gpt-4-turbo",  
#       instructions="You are a personal math tutor. When asked a question, write and run Python code to answer the question.",  
#       name="Math Tutor",  
#       tools=[{"type": "code_interpreter"}],  
# )  

```

**Get the Assistant**
```
from litellm import get_assistants, aget_assistants  
import os   
  
# setup env  
os.environ["OPENAI_API_KEY"] = "sk-.."  
  
assistants = get_assistants(custom_llm_provider="openai")  
  
### ASYNC USAGE ###   
# assistants = await aget_assistants(custom_llm_provider="openai")  

```

**Create a Thread**
```
from litellm import create_thread, acreate_thread  
import os   
  
os.environ["OPENAI_API_KEY"] = "sk-.."  
  
new_thread = create_thread(  
      custom_llm_provider="openai",  
      messages=[{"role": "user", "content": "Hey, how's it going?"}], # type: ignore  
    )  
  
### ASYNC USAGE ###   
# new_thread = await acreate_thread(custom_llm_provider="openai",messages=[{"role": "user", "content": "Hey, how's it going?"}])  

```

**Add Messages to the Thread**
```
from litellm import create_thread, get_thread, aget_thread, add_message, a_add_message  
import os   
  
os.environ["OPENAI_API_KEY"] = "sk-.."  
  
## CREATE A THREAD  
_new_thread = create_thread(  
      custom_llm_provider="openai",  
      messages=[{"role": "user", "content": "Hey, how's it going?"}], # type: ignore  
    )  
  
## OR retrieve existing thread  
received_thread = get_thread(  
      custom_llm_provider="openai",  
      thread_id=_new_thread.id,  
    )  
  
### ASYNC USAGE ###   
# received_thread = await aget_thread(custom_llm_provider="openai", thread_id=_new_thread.id,)  
  
## ADD MESSAGE TO THREAD  
message = {"role": "user", "content": "Hey, how's it going?"}  
added_message = add_message(  
      thread_id=_new_thread.id, custom_llm_provider="openai", **message  
    )  
  
### ASYNC USAGE ###   
# added_message = await a_add_message(thread_id=_new_thread.id, custom_llm_provider="openai", **message)  

```

**Run the Assistant on the Thread**
```
from litellm import get_assistants, create_thread, add_message, run_thread, arun_thread  
import os   
  
os.environ["OPENAI_API_KEY"] = "sk-.."  
assistants = get_assistants(custom_llm_provider="openai")  
  
## get the first assistant ###  
assistant_id = assistants.data[0].id  
  
## GET A THREAD  
_new_thread = create_thread(  
      custom_llm_provider="openai",  
      messages=[{"role": "user", "content": "Hey, how's it going?"}], # type: ignore  
    )  
  
## ADD MESSAGE  
message = {"role": "user", "content": "Hey, how's it going?"}  
added_message = add_message(  
      thread_id=_new_thread.id, custom_llm_provider="openai", **message  
    )  
  
## 🚨 RUN THREAD  
response = run_thread(  
      custom_llm_provider="openai", thread_id=thread_id, assistant_id=assistant_id  
    )  
  
### ASYNC USAGE ###   
# response = await arun_thread(custom_llm_provider="openai", thread_id=thread_id, assistant_id=assistant_id)  
  
print(f"run_thread: {run_thread}")  

```

```
assistant_settings:  
 custom_llm_provider: azure  
 litellm_params:   
  api_key: os.environ/AZURE_API_KEY  
  api_base: os.environ/AZURE_API_BASE  
  api_version: os.environ/AZURE_API_VERSION  

```

```
$ litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

**Create the Assistant**
```
curl "http://localhost:4000/v1/assistants" \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",  
  "name": "Math Tutor",  
  "tools": [{"type": "code_interpreter"}],  
  "model": "gpt-4-turbo"  
 }'  

```

**Get the Assistant**
```
curl "http://0.0.0.0:4000/v1/assistants?order=desc&limit=20" \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234"  

```

**Create a Thread**
```
curl http://0.0.0.0:4000/v1/threads \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d ''  

```

**Get a Thread**
```
curl http://0.0.0.0:4000/v1/threads/{thread_id} \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234"  

```

**Add Messages to the Thread**
```
curl http://0.0.0.0:4000/v1/threads/{thread_id}/messages \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
   "role": "user",  
   "content": "How does AI work? Explain it in simple terms."  
  }'  

```

**Run the Assistant on the Thread**
```
curl http://0.0.0.0:4000/v1/threads/thread_abc123/runs \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "assistant_id": "asst_abc123"  
 }'  

```

## Streaming​
  * SDK
  * PROXY


```
from litellm import run_thread_stream   
import os  
  
os.environ["OPENAI_API_KEY"] = "sk-.."  
  
message = {"role": "user", "content": "Hey, how's it going?"}   
  
data = {"custom_llm_provider": "openai", "thread_id": _new_thread.id, "assistant_id": assistant_id, **message}  
  
run = run_thread_stream(**data)  
with run as run:  
  assert isinstance(run, AssistantEventHandler)  
  for chunk in run:   
   print(f"chunk: {chunk}")  
  run.until_done()  

```

```
curl -X POST 'http://0.0.0.0:4000/threads/{thread_id}/runs' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-D '{  
   "assistant_id": "asst_6xVZQFFy1Kw87NbnYeNebxTf",  
   "stream": true  
}'  

```

## 👉 Proxy API Reference​
## Azure OpenAI​
**config**
```
assistant_settings:  
 custom_llm_provider: azure  
 litellm_params:   
  api_key: os.environ/AZURE_API_KEY  
  api_base: os.environ/AZURE_API_BASE  

```

**curl**
```
curl -X POST "http://localhost:4000/v1/assistants" \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",  
  "name": "Math Tutor",  
  "tools": [{"type": "code_interpreter"}],  
  "model": "<my-azure-deployment-name>"  
 }'  

```

## OpenAI-Compatible APIs​
To call openai-compatible Assistants API's (eg. Astra Assistants API), just add `openai/` to the model name: 
**config**
```
assistant_settings:  
 custom_llm_provider: openai  
 litellm_params:   
  api_key: os.environ/ASTRA_API_KEY  
  api_base: os.environ/ASTRA_API_BASE  

```

**curl**
```
curl -X POST "http://localhost:4000/v1/assistants" \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",  
  "name": "Math Tutor",  
  "tools": [{"type": "code_interpreter"}],  
  "model": "openai/<my-astra-model-name>"  
 }'  

```

Previous
/rerank
Next
Provider Files Endpoints
  * **Supported Providers** :
  * Quick Start
    * SDK + PROXY
  * Streaming
  * 👉 Proxy API Reference
  * Azure OpenAI
  * OpenAI-Compatible APIs


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# Page Not Found
We could not find what you were looking for.
Please contact the owner of the site that linked you to the original URL and let them know their link is broken.
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /batches


On this page
# /batches
Covers Batches, Files
Feature| Supported| Notes  
---|---|---  
Supported Providers| OpenAI, Azure, Vertex| -  
✨ Cost Tracking| ✅| LiteLLM Enterprise only  
Logging| ✅| Works across all logging integrations  
## Quick Start​
  * Create File for Batch Completion
  * Create Batch Request
  * List Batches
  * Retrieve the Specific Batch and File Content


  * LiteLLM PROXY Server
  * SDK


```
$ export OPENAI_API_KEY="sk-..."  
  
$ litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

**Create File for Batch Completion**
```
curl http://localhost:4000/v1/files \  
  -H "Authorization: Bearer sk-1234" \  
  -F purpose="batch" \  
  -F file="@mydata.jsonl"  

```

**Create Batch Request**
```
curl http://localhost:4000/v1/batches \  
    -H "Authorization: Bearer sk-1234" \  
    -H "Content-Type: application/json" \  
    -d '{  
      "input_file_id": "file-abc123",  
      "endpoint": "/v1/chat/completions",  
      "completion_window": "24h"  
  }'  

```

**Retrieve the Specific Batch**
```
curl http://localhost:4000/v1/batches/batch_abc123 \  
  -H "Authorization: Bearer sk-1234" \  
  -H "Content-Type: application/json" \  

```

**List Batches**
```
curl http://localhost:4000/v1/batches \  
  -H "Authorization: Bearer sk-1234" \  
  -H "Content-Type: application/json" \  

```

**Create File for Batch Completion**
```
from litellm  
import os   
  
os.environ["OPENAI_API_KEY"] = "sk-.."  
  
file_name = "openai_batch_completions.jsonl"  
_current_dir = os.path.dirname(os.path.abspath(__file__))  
file_path = os.path.join(_current_dir, file_name)  
file_obj = await litellm.acreate_file(  
  file=open(file_path, "rb"),  
  purpose="batch",  
  custom_llm_provider="openai",  
)  
print("Response from creating file=", file_obj)  

```

**Create Batch Request**
```
from litellm  
import os   
  
create_batch_response = await litellm.acreate_batch(  
  completion_window="24h",  
  endpoint="/v1/chat/completions",  
  input_file_id=batch_input_file_id,  
  custom_llm_provider="openai",  
  metadata={"key1": "value1", "key2": "value2"},  
)  
  
print("response from litellm.create_batch=", create_batch_response)  

```

**Retrieve the Specific Batch and File Content**
```
  
retrieved_batch = await litellm.aretrieve_batch(  
  batch_id=create_batch_response.id, custom_llm_provider="openai"  
)  
print("retrieved batch=", retrieved_batch)  
# just assert that we retrieved a non None batch  
  
assert retrieved_batch.id == create_batch_response.id  
  
# try to get file content for our original file  
  
file_content = await litellm.afile_content(  
  file_id=batch_input_file_id, custom_llm_provider="openai"  
)  
  
print("file content = ", file_content)  

```

**List Batches**
```
list_batches_response = litellm.list_batches(custom_llm_provider="openai", limit=2)  
print("list_batches_response=", list_batches_response)  

```

## **Supported Providers** :​
### Azure OpenAI​
### OpenAI​
### Vertex AI​
## How Cost Tracking for Batches API Works​
LiteLLM tracks batch processing costs by logging two key events:
Event Type| Description| When it's Logged  
---|---|---  
`acreate_batch`| Initial batch creation| When batch request is submitted  
`batch_success`| Final usage and cost| When batch processing completes  
Cost calculation:
  * LiteLLM polls the batch status until completion
  * Upon completion, it aggregates usage and costs from all responses in the output file
  * Total `token` and `response_cost` reflect the combined metrics across all batch responses


## Swagger API Reference​
Previous
[BETA] Unified File ID
Next
/realtime
  * Quick Start
  * **Supported Providers** :
    * Azure OpenAI
    * OpenAI
    * Vertex AI
  * How Cost Tracking for Batches API Works
  * Swagger API Reference


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
      * Input Params
      * Output
      * Usage
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /chat/completions


# Chat Completions
Details on the completion() function
## 📄️ Input Params
Common Params
## 📄️ Output
Format
## 📄️ Usage
LiteLLM returns the OpenAI compatible usage object across all providers.
Previous
Supported Endpoints
Next
Input Params
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Using Audio Models


On this page
# Using Audio Models
How to send / receive audio to a `/chat/completions` endpoint
## Audio Output from a model​
Example for creating a human-like audio response to a prompt
  * LiteLLM Python SDK
  * LiteLLM Proxy Server


```
import os   
import base64  
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
# openai call  
completion = await litellm.acompletion(  
  model="gpt-4o-audio-preview",  
  modalities=["text", "audio"],  
  audio={"voice": "alloy", "format": "wav"},  
  messages=[{"role": "user", "content": "Is a golden retriever a good family dog?"}],  
)  
  
wav_bytes = base64.b64decode(completion.choices[0].message.audio.data)  
with open("dog.wav", "wb") as f:  
  f.write(wav_bytes)  

```

  1. Define an audio model on config.yaml


```
model_list:  
 - model_name: gpt-4o-audio-preview # OpenAI gpt-4o-audio-preview  
  litellm_params:  
   model: openai/gpt-4o-audio-preview  
   api_key: os.environ/OPENAI_API_KEY   
  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Test it using the OpenAI Python SDK


```
import base64  
from openai import OpenAI  
  
client = OpenAI(  
  api_key="LITELLM_PROXY_KEY", # sk-1234  
  base_url="LITELLM_PROXY_BASE" # http://0.0.0.0:4000  
)  
  
completion = client.chat.completions.create(  
  model="gpt-4o-audio-preview",  
  modalities=["text", "audio"],  
  audio={"voice": "alloy", "format": "wav"},  
  messages=[  
    {  
      "role": "user",  
      "content": "Is a golden retriever a good family dog?"  
    }  
  ]  
)  
  
print(completion.choices[0])  
  
wav_bytes = base64.b64decode(completion.choices[0].message.audio.data)  
with open("dog.wav", "wb") as f:  
  f.write(wav_bytes)  
  

```

## Audio Input to a model​
  * LiteLLM Python SDK
  * LiteLLM Proxy Server


```
import base64  
import requests  
  
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"  
response = requests.get(url)  
response.raise_for_status()  
wav_data = response.content  
encoded_string = base64.b64encode(wav_data).decode("utf-8")  
  
completion = litellm.completion(  
  model="gpt-4o-audio-preview",  
  modalities=["text", "audio"],  
  audio={"voice": "alloy", "format": "wav"},  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "What is in this recording?"},  
        {  
          "type": "input_audio",  
          "input_audio": {"data": encoded_string, "format": "wav"},  
        },  
      ],  
    },  
  ],  
)  
  
print(completion.choices[0].message)  

```

  1. Define an audio model on config.yaml


```
model_list:  
 - model_name: gpt-4o-audio-preview # OpenAI gpt-4o-audio-preview  
  litellm_params:  
   model: openai/gpt-4o-audio-preview  
   api_key: os.environ/OPENAI_API_KEY   
  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Test it using the OpenAI Python SDK


```
import base64  
from openai import OpenAI  
  
client = OpenAI(  
  api_key="LITELLM_PROXY_KEY", # sk-1234  
  base_url="LITELLM_PROXY_BASE" # http://0.0.0.0:4000  
)  
  
  
# Fetch the audio file and convert it to a base64 encoded string  
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"  
response = requests.get(url)  
response.raise_for_status()  
wav_data = response.content  
encoded_string = base64.b64encode(wav_data).decode('utf-8')  
  
completion = client.chat.completions.create(  
  model="gpt-4o-audio-preview",  
  modalities=["text", "audio"],  
  audio={"voice": "alloy", "format": "wav"},  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {   
          "type": "text",  
          "text": "What is in this recording?"  
        },  
        {  
          "type": "input_audio",  
          "input_audio": {  
            "data": encoded_string,  
            "format": "wav"  
          }  
        }  
      ]  
    },  
  ]  
)  
  
print(completion.choices[0].message)  

```

## Checking if a model supports `audio_input` and `audio_output`​
  * LiteLLM Python SDK
  * LiteLLM Proxy Server


Use `litellm.supports_audio_output(model="")` -> returns `True` if model can generate audio output
Use `litellm.supports_audio_input(model="")` -> returns `True` if model can accept audio input
```
assert litellm.supports_audio_output(model="gpt-4o-audio-preview") == True  
assert litellm.supports_audio_input(model="gpt-4o-audio-preview") == True  
  
assert litellm.supports_audio_output(model="gpt-3.5-turbo") == False  
assert litellm.supports_audio_input(model="gpt-3.5-turbo") == False  

```

  1. Define vision models on config.yaml


```
model_list:  
 - model_name: gpt-4o-audio-preview # OpenAI gpt-4o-audio-preview  
  litellm_params:  
   model: openai/gpt-4o-audio-preview  
   api_key: os.environ/OPENAI_API_KEY  
 - model_name: llava-hf     # Custom OpenAI compatible model  
  litellm_params:  
   model: openai/llava-hf/llava-v1.6-vicuna-7b-hf  
   api_base: http://localhost:8000  
   api_key: fake-key  
  model_info:  
   supports_audio_output: True    # set supports_audio_output to True so /model/info returns this attribute as True  
   supports_audio_input: True     # set supports_audio_input to True so /model/info returns this attribute as True  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Call `/model_group/info` to check if your model supports `vision`


```
curl -X 'GET' \  
 'http://localhost:4000/model_group/info' \  
 -H 'accept: application/json' \  
 -H 'x-api-key: sk-1234'  

```

Expected Response 
```
{  
 "data": [  
  {  
   "model_group": "gpt-4o-audio-preview",  
   "providers": ["openai"],  
   "max_input_tokens": 128000,  
   "max_output_tokens": 16384,  
   "mode": "chat",  
   "supports_audio_output": true, # 👈 supports_audio_output is true  
   "supports_audio_input": true, # 👈 supports_audio_input is true  
  },  
  {  
   "model_group": "llava-hf",  
   "providers": ["openai"],  
   "max_input_tokens": null,  
   "max_output_tokens": null,  
   "mode": null,  
   "supports_audio_output": true, # 👈 supports_audio_output is true  
   "supports_audio_input": true, # 👈 supports_audio_input is true  
  }  
 ]  
}  

```

## Response Format with Audio​
Below is an example JSON data structure for a `message` you might receive from a `/chat/completions` endpoint when sending audio input to a model.
```
{  
 "index": 0,  
 "message": {  
  "role": "assistant",  
  "content": null,  
  "refusal": null,  
  "audio": {  
   "id": "audio_abc123",  
   "expires_at": 1729018505,  
   "data": "<bytes omitted>",  
   "transcript": "Yes, golden retrievers are known to be ..."  
  }  
 },  
 "finish_reason": "stop"  
}  

```

  * `audio` If the audio output modality is requested, this object contains data about the audio response from the model
    * `audio.id` Unique identifier for the audio response
    * `audio.expires_at` The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations.
    * `audio.data` Base64 encoded audio bytes generated by the model, in the format specified in the request.
    * `audio.transcript` Transcript of the audio generated by the model.


Previous
SSL Security Settings
Next
Using Web Search
  * Audio Output from a model
  * Audio Input to a model
  * Checking if a model supports `audio_input` and `audio_output`
  * Response Format with Audio


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk


On this page
# Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
**See Code**
info
  * For Proxy Server? Doc here: Caching Proxy Server
  * For OpenAI/Anthropic Prompt Caching, go here


## Initialize Cache - In Memory, Redis, s3 Bucket, Redis Semantic, Disk Cache, Qdrant Semantic​
  * redis-cache
  * s3-cache
  * redis-semantic cache
  * qdrant-semantic cache
  * in memory cache
  * disk cache


Install redis
```
pip install redis  

```

For the hosted version you can setup your own Redis DB here: https://redis.io/try-free/
```
import litellm  
from litellm import completion  
from litellm.caching.caching import Cache  
  
litellm.cache = Cache(type="redis", host=<host>, port=<port>, password=<password>)  
  
# Make completion calls  
response1 = completion(  
  model="gpt-3.5-turbo",  
  messages=[{"role": "user", "content": "Tell me a joke."}]  
)  
response2 = completion(  
  model="gpt-3.5-turbo",  
  messages=[{"role": "user", "content": "Tell me a joke."}]  
)  
  
# response1 == response2, response 1 is cached  

```

Install boto3
```
pip install boto3  

```

Set AWS environment variables
```
AWS_ACCESS_KEY_ID = "AKI*******"  
AWS_SECRET_ACCESS_KEY = "WOl*****"  

```

```
import litellm  
from litellm import completion  
from litellm.caching.caching import Cache  
  
# pass s3-bucket name  
litellm.cache = Cache(type="s3", s3_bucket_name="cache-bucket-litellm", s3_region_name="us-west-2")  
  
# Make completion calls  
response1 = completion(  
  model="gpt-3.5-turbo",   
  messages=[{"role": "user", "content": "Tell me a joke."}]  
)  
response2 = completion(  
  model="gpt-3.5-turbo",   
  messages=[{"role": "user", "content": "Tell me a joke."}]  
)  
  
# response1 == response2, response 1 is cached  

```

Install redisvl client
```
pip install redisvl==0.4.1  

```

For the hosted version you can setup your own Redis DB here: https://redis.io/try-free/
```
import litellm  
from litellm import completion  
from litellm.caching.caching import Cache  
  
random_number = random.randint(  
  1, 100000  
) # add a random number to ensure it's always adding / reading from cache  
  
print("testing semantic caching")  
litellm.cache = Cache(  
  type="redis-semantic",  
  host=os.environ["REDIS_HOST"],  
  port=os.environ["REDIS_PORT"],  
  password=os.environ["REDIS_PASSWORD"],  
  similarity_threshold=0.8, # similarity threshold for cache hits, 0 == no similarity, 1 = exact matches, 0.5 == 50% similarity  
  ttl=120,  
  redis_semantic_cache_embedding_model="text-embedding-ada-002", # this model is passed to litellm.embedding(), any litellm.embedding() model is supported here  
)  
response1 = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": f"write a one sentence poem about: {random_number}",  
    }  
  ],  
  max_tokens=20,  
)  
print(f"response1: {response1}")  
  
random_number = random.randint(1, 100000)  
  
response2 = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": f"write a one sentence poem about: {random_number}",  
    }  
  ],  
  max_tokens=20,  
)  
print(f"response2: {response1}")  
assert response1.id == response2.id  
# response1 == response2, response 1 is cached  

```

You can set up your own cloud Qdrant cluster by following this: https://qdrant.tech/documentation/quickstart-cloud/
To set up a Qdrant cluster locally follow: https://qdrant.tech/documentation/quickstart/
```
import litellm  
from litellm import completion  
from litellm.caching.caching import Cache  
  
random_number = random.randint(  
  1, 100000  
) # add a random number to ensure it's always adding / reading from cache  
  
print("testing semantic caching")  
litellm.cache = Cache(  
  type="qdrant-semantic",  
  qdrant_api_base=os.environ["QDRANT_API_BASE"],   
  qdrant_api_key=os.environ["QDRANT_API_KEY"],  
  qdrant_collection_name="your_collection_name", # any name of your collection  
  similarity_threshold=0.7, # similarity threshold for cache hits, 0 == no similarity, 1 = exact matches, 0.5 == 50% similarity  
  qdrant_quantization_config ="binary", # can be one of 'binary', 'product' or 'scalar' quantizations that is supported by qdrant  
  qdrant_semantic_cache_embedding_model="text-embedding-ada-002", # this model is passed to litellm.embedding(), any litellm.embedding() model is supported here  
)  
  
response1 = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": f"write a one sentence poem about: {random_number}",  
    }  
  ],  
  max_tokens=20,  
)  
print(f"response1: {response1}")  
  
random_number = random.randint(1, 100000)  
  
response2 = completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {  
      "role": "user",  
      "content": f"write a one sentence poem about: {random_number}",  
    }  
  ],  
  max_tokens=20,  
)  
print(f"response2: {response2}")  
assert response1.id == response2.id  
# response1 == response2, response 1 is cached  

```

### Quick Start​
```
import litellm  
from litellm import completion  
from litellm.caching.caching import Cache  
litellm.cache = Cache()  
  
# Make completion calls  
response1 = completion(  
  model="gpt-3.5-turbo",   
  messages=[{"role": "user", "content": "Tell me a joke."}],  
  caching=True  
)  
response2 = completion(  
  model="gpt-3.5-turbo",   
  messages=[{"role": "user", "content": "Tell me a joke."}],  
  caching=True  
)  
  
# response1 == response2, response 1 is cached  
  

```

### Quick Start​
Install diskcache:
```
pip install diskcache  

```

Then you can use the disk cache as follows.
```
import litellm  
from litellm import completion  
from litellm.caching.caching import Cache  
litellm.cache = Cache(type="disk")  
  
# Make completion calls  
response1 = completion(  
  model="gpt-3.5-turbo",   
  messages=[{"role": "user", "content": "Tell me a joke."}],  
  caching=True  
)  
response2 = completion(  
  model="gpt-3.5-turbo",   
  messages=[{"role": "user", "content": "Tell me a joke."}],  
  caching=True  
)  
  
# response1 == response2, response 1 is cached  
  

```

If you run the code two times, response1 will use the cache from the first run that was stored in a cache file.
## Switch Cache On / Off Per LiteLLM Call​
LiteLLM supports 4 cache-controls:
  * `no-cache`: _Optional(bool)_ When `True`, Will not return a cached response, but instead call the actual endpoint. 
  * `no-store`: _Optional(bool)_ When `True`, Will not cache the response. 
  * `ttl`: _Optional(int)_ - Will cache the response for the user-defined amount of time (in seconds).
  * `s-maxage`: _Optional(int)_ Will only accept cached responses that are within user-defined range (in seconds).


Let us know if you need more
  * No-Cache
  * No-Store
  * ttl
  * s-maxage


Example usage `no-cache` - When `True`, Will not return a cached response
```
response = litellm.completion(  
    model="gpt-3.5-turbo",  
    messages=[  
      {  
        "role": "user",  
        "content": "hello who are you"  
      }  
    ],  
    cache={"no-cache": True},  
  )  

```

Example usage `no-store` - When `True`, Will not cache the response. 
```
response = litellm.completion(  
    model="gpt-3.5-turbo",  
    messages=[  
      {  
        "role": "user",  
        "content": "hello who are you"  
      }  
    ],  
    cache={"no-store": True},  
  )  

```

Example usage `ttl` - cache the response for 10 seconds
```
response = litellm.completion(  
    model="gpt-3.5-turbo",  
    messages=[  
      {  
        "role": "user",  
        "content": "hello who are you"  
      }  
    ],  
    cache={"ttl": 10},  
  )  

```

Example usage `s-maxage` - Will only accept cached responses for 60 seconds
```
response = litellm.completion(  
    model="gpt-3.5-turbo",  
    messages=[  
      {  
        "role": "user",  
        "content": "hello who are you"  
      }  
    ],  
    cache={"s-maxage": 60},  
  )  

```

## Cache Context Manager - Enable, Disable, Update Cache​
Use the context manager for easily enabling, disabling & updating the litellm cache 
### Enabling Cache​
Quick Start Enable
```
litellm.enable_cache()  

```

Advanced Params
```
litellm.enable_cache(  
  type: Optional[Literal["local", "redis", "s3", "disk"]] = "local",  
  host: Optional[str] = None,  
  port: Optional[str] = None,  
  password: Optional[str] = None,  
  supported_call_types: Optional[  
    List[Literal["completion", "acompletion", "embedding", "aembedding", "atranscription", "transcription"]]  
  ] = ["completion", "acompletion", "embedding", "aembedding", "atranscription", "transcription"],  
  **kwargs,  
)  

```

### Disabling Cache​
Switch caching off 
```
litellm.disable_cache()  

```

### Updating Cache Params (Redis Host, Port etc)​
Update the Cache params
```
litellm.update_cache(  
  type: Optional[Literal["local", "redis", "s3", "disk"]] = "local",  
  host: Optional[str] = None,  
  port: Optional[str] = None,  
  password: Optional[str] = None,  
  supported_call_types: Optional[  
    List[Literal["completion", "acompletion", "embedding", "aembedding", "atranscription", "transcription"]]  
  ] = ["completion", "acompletion", "embedding", "aembedding", "atranscription", "transcription"],  
  **kwargs,  
)  

```

## Custom Cache Keys:​
Define function to return cache key
```
# this function takes in *args, **kwargs and returns the key you want to use for caching  
def custom_get_cache_key(*args, **kwargs):  
  # return key to use for your cache:  
  key = kwargs.get("model", "") + str(kwargs.get("messages", "")) + str(kwargs.get("temperature", "")) + str(kwargs.get("logit_bias", ""))  
  print("key for cache", key)  
  return key  
  

```

Set your function as litellm.cache.get_cache_key
```
from litellm.caching.caching import Cache  
  
cache = Cache(type="redis", host=os.environ['REDIS_HOST'], port=os.environ['REDIS_PORT'], password=os.environ['REDIS_PASSWORD'])  
  
cache.get_cache_key = custom_get_cache_key # set get_cache_key function for your cache  
  
litellm.cache = cache # set litellm.cache to your cache   
  

```

## How to write custom add/get cache functions​
### 1. Init Cache​
```
from litellm.caching.caching import Cache  
cache = Cache()  

```

### 2. Define custom add/get cache functions​
```
def add_cache(self, result, *args, **kwargs):  
 your logic  
   
def get_cache(self, *args, **kwargs):  
 your logic  

```

### 3. Point cache add/get functions to your add/get functions​
```
cache.add_cache = add_cache  
cache.get_cache = get_cache  

```

## Cache Initialization Parameters​
```
def __init__(  
  self,  
  type: Optional[Literal["local", "redis", "redis-semantic", "s3", "disk"]] = "local",  
  supported_call_types: Optional[  
    List[Literal["completion", "acompletion", "embedding", "aembedding", "atranscription", "transcription"]]  
  ] = ["completion", "acompletion", "embedding", "aembedding", "atranscription", "transcription"],  
  ttl: Optional[float] = None,  
  default_in_memory_ttl: Optional[float] = None,  
  
  # redis cache params  
  host: Optional[str] = None,  
  port: Optional[str] = None,  
  password: Optional[str] = None,  
  namespace: Optional[str] = None,  
  default_in_redis_ttl: Optional[float] = None,  
  redis_flush_size=None,  
  
  # redis semantic cache params  
  similarity_threshold: Optional[float] = None,  
  redis_semantic_cache_embedding_model: str = "text-embedding-ada-002",  
  redis_semantic_cache_index_name: Optional[str] = None,  
  
  # s3 Bucket, boto3 configuration  
  s3_bucket_name: Optional[str] = None,  
  s3_region_name: Optional[str] = None,  
  s3_api_version: Optional[str] = None,  
  s3_path: Optional[str] = None, # if you wish to save to a specific path  
  s3_use_ssl: Optional[bool] = True,  
  s3_verify: Optional[Union[bool, str]] = None,  
  s3_endpoint_url: Optional[str] = None,  
  s3_aws_access_key_id: Optional[str] = None,  
  s3_aws_secret_access_key: Optional[str] = None,  
  s3_aws_session_token: Optional[str] = None,  
  s3_config: Optional[Any] = None,  
  
  # disk cache params  
  disk_cache_dir=None,  
  
  # qdrant cache params  
  qdrant_api_base: Optional[str] = None,  
  qdrant_api_key: Optional[str] = None,  
  qdrant_collection_name: Optional[str] = None,  
  qdrant_quantization_config: Optional[str] = None,  
  qdrant_semantic_cache_embedding_model="text-embedding-ada-002",  
  
  **kwargs  
):  

```

## Logging​
Cache hits are logged in success events as `kwarg["cache_hit"]`. 
Here's an example of accessing it: 
```
import litellm  
from litellm.integrations.custom_logger import CustomLogger  
from litellm import completion, acompletion, Cache  
  
# create custom callback for success_events  
class MyCustomHandler(CustomLogger):  
 async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):   
   print(f"On Success")  
   print(f"Value of Cache hit: {kwargs['cache_hit']"})  
  
async def test_async_completion_azure_caching():  
 # set custom callback  
 customHandler_caching = MyCustomHandler()  
 litellm.callbacks = [customHandler_caching]  
  
 # init cache   
 litellm.cache = Cache(type="redis", host=os.environ['REDIS_HOST'], port=os.environ['REDIS_PORT'], password=os.environ['REDIS_PASSWORD'])  
 unique_time = time.time()  
 response1 = await litellm.acompletion(model="azure/chatgpt-v-2",  
             messages=[{  
               "role": "user",  
               "content": f"Hi 👋 - i'm async azure {unique_time}"  
             }],  
             caching=True)  
 await asyncio.sleep(1)  
 print(f"customHandler_caching.states pre-cache hit: {customHandler_caching.states}")  
 response2 = await litellm.acompletion(model="azure/chatgpt-v-2",  
             messages=[{  
               "role": "user",  
               "content": f"Hi 👋 - i'm async azure {unique_time}"  
             }],  
             caching=True)  
 await asyncio.sleep(1) # success callbacks are done in parallel  

```

Previous
Budget Manager
Next
Migration Guide - LiteLLM v1.0.0+
  * Initialize Cache - In Memory, Redis, s3 Bucket, Redis Semantic, Disk Cache, Qdrant Semantic
    * Quick Start
    * Quick Start
  * Switch Cache On / Off Per LiteLLM Call
  * Cache Context Manager - Enable, Disable, Update Cache
    * Enabling Cache
    * Disabling Cache
    * Updating Cache Params (Redis Host, Port etc)
  * Custom Cache Keys:
  * How to write custom add/get cache functions
    * 1. Init Cache
    * 2. Define custom add/get cache functions
    * 3. Point cache add/get functions to your add/get functions
  * Cache Initialization Parameters
  * Logging


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * Budget Manager


On this page
# Budget Manager
Don't want to get crazy bills because either while you're calling LLM APIs **or** while your users are calling them? use this. 
info
If you want a server to manage user keys, budgets, etc. use our LiteLLM Proxy Server
LiteLLM exposes: 
  * `litellm.max_budget`: a global variable you can use to set the max budget (in USD) across all your litellm calls. If this budget is exceeded, it will raise a BudgetExceededError 
  * `BudgetManager`: A class to help set budgets per user. BudgetManager creates a dictionary to manage the user budgets, where the key is user and the object is their current cost + model-specific costs. 
  * `LiteLLM Proxy Server`: A server to call 100+ LLMs with an openai-compatible endpoint. Manages user budgets, spend tracking, load balancing etc. 


## quick start​
```
import litellm, os   
from litellm import completion  
  
# set env variable   
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
litellm.max_budget = 0.001 # sets a max budget of $0.001  
  
messages = [{"role": "user", "content": "Hey, how's it going"}]  
completion(model="gpt-4", messages=messages)  
print(litellm._current_cost)  
completion(model="gpt-4", messages=messages)  

```

## User-based rate limiting​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
```
from litellm import BudgetManager, completion   
  
budget_manager = BudgetManager(project_name="test_project")  
  
user = "1234"  
  
# create a budget if new user user  
if not budget_manager.is_valid_user(user):  
  budget_manager.create_budget(total_budget=10, user=user)  
  
# check if a given call can be made  
if budget_manager.get_current_cost(user=user) <= budget_manager.get_total_budget(user):  
  response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hey, how's it going?"}])  
  budget_manager.update_cost(completion_obj=response, user=user)  
else:  
  response = "Sorry - no budget!"  

```

**Implementation Code**
## use with Text Input / Output​
Update cost by just passing in the text input / output and model name. 
```
from litellm import BudgetManager  
  
budget_manager = BudgetManager(project_name="test_project")  
user = "12345"  
budget_manager.create_budget(total_budget=10, user=user, duration="daily")  
  
input_text = "hello world"  
output_text = "it's a sunny day in san francisco"  
model = "gpt-3.5-turbo"  
  
budget_manager.update_cost(user=user, model=model, input_text=input_text, output_text=output_text) # 👈  
print(budget_manager.get_current_cost(user))  

```

## advanced usage​
In production, we will need to 
  * store user budgets in a database
  * reset user budgets based on a set duration 


### LiteLLM API​
The LiteLLM API provides both. It stores the user object in a hosted db, and runs a cron job daily to reset user-budgets based on the set duration (e.g. reset budget daily/weekly/monthly/etc.). 
**Usage**
```
budget_manager = BudgetManager(project_name="<my-unique-project>", client_type="hosted")  

```

**Complete Code**
```
from litellm import BudgetManager, completion   
  
budget_manager = BudgetManager(project_name="<my-unique-project>", client_type="hosted")  
  
user = "1234"  
  
# create a budget if new user user  
if not budget_manager.is_valid_user(user):  
  budget_manager.create_budget(total_budget=10, user=user, duration="monthly") # 👈 duration = 'daily'/'weekly'/'monthly'/'yearly'  
  
# check if a given call can be made  
if budget_manager.get_current_cost(user=user) <= budget_manager.get_total_budget(user):  
  response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hey, how's it going?"}])  
  budget_manager.update_cost(completion_obj=response, user=user)  
else:  
  response = "Sorry - no budget!"  

```

### Self-hosted​
To use your own db, set the BudgetManager client type to `hosted` **and** set the api_base. 
Your api is expected to expose `/get_budget` and `/set_budget` endpoints. See code for details
**Usage**
```
budget_manager = BudgetManager(project_name="<my-unique-project>", client_type="hosted", api_base="your_custom_api")  

```

**Complete Code**
```
from litellm import BudgetManager, completion   
  
budget_manager = BudgetManager(project_name="<my-unique-project>", client_type="hosted", api_base="your_custom_api")  
  
user = "1234"  
  
# create a budget if new user user  
if not budget_manager.is_valid_user(user):  
  budget_manager.create_budget(total_budget=10, user=user, duration="monthly") # 👈 duration = 'daily'/'weekly'/'monthly'/'yearly'  
  
# check if a given call can be made  
if budget_manager.get_current_cost(user=user) <= budget_manager.get_total_budget(user):  
  response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hey, how's it going?"}])  
  budget_manager.update_cost(completion_obj=response, user=user)  
else:  
  response = "Sorry - no budget!"  

```

## Budget Manager Class​
The `BudgetManager` class is used to manage budgets for different users. It provides various functions to create, update, and retrieve budget information. 
Below is a list of public functions exposed by the Budget Manager class and their input/outputs. 
### **init**​
```
def __init__(self, project_name: str, client_type: str = "local", api_base: Optional[str] = None)  

```

  * `project_name` (str): The name of the project.
  * `client_type` (str): The client type ("local" or "hosted"). Defaults to "local".
  * `api_base` (Optional[str]): The base URL of the API. Defaults to None.


### create_budget​
```
def create_budget(self, total_budget: float, user: str, duration: Literal["daily", "weekly", "monthly", "yearly"], created_at: float = time.time())  

```

Creates a budget for a user.
  * `total_budget` (float): The total budget of the user.
  * `user` (str): The user id.
  * `duration` (Literal["daily", "weekly", "monthly", "yearly"]): The budget duration.
  * `created_at` (float): The creation time. Default is the current time.


### projected_cost​
```
def projected_cost(self, model: str, messages: list, user: str)  

```

Computes the projected cost for a session.
  * `model` (str): The name of the model.
  * `messages` (list): The list of messages.
  * `user` (str): The user id.


### get_total_budget​
```
def get_total_budget(self, user: str)  

```

Returns the total budget of a user.
  * `user` (str): user id.


### update_cost​
```
def update_cost(self, completion_obj: ModelResponse, user: str)  

```

Updates the user's cost.
  * `completion_obj` (ModelResponse): The completion object received from the model.
  * `user` (str): The user id.


### get_current_cost​
```
def get_current_cost(self, user: str)  

```

Returns the current cost of a user.
  * `user` (str): The user id.


### get_model_cost​
```
def get_model_cost(self, user: str)  

```

Returns the model cost of a user.
  * `user` (str): The user id.


### is_valid_user​
```
def is_valid_user(self, user: str) -> bool  

```

Checks if a user is valid.
  * `user` (str): The user id.


### get_users​
```
def get_users(self)  

```

Returns a list of all users.
### reset_cost​
```
def reset_cost(self, user: str)  

```

Resets the cost of a user.
  * `user` (str): The user id.


### reset_on_duration​
```
def reset_on_duration(self, user: str)  

```

Resets the cost of a user based on the duration.
  * `user` (str): The user id.


### update_budget_all_users​
```
def update_budget_all_users(self)  

```

Updates the budget for all users.
### save_data​
```
def save_data(self)  

```

Stores the user dictionary.
Previous
litellm.moderation()
Next
Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
  * quick start
  * User-based rate limiting
  * use with Text Input / Output
  * advanced usage
    * LiteLLM API
    * Self-hosted
  * Budget Manager Class
    * **init**
    * create_budget
    * projected_cost
    * get_total_budget
    * update_cost
    * get_current_cost
    * get_model_cost
    * is_valid_user
    * get_users
    * reset_cost
    * reset_on_duration
    * update_budget_all_users
    * save_data


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Batching Completion()


On this page
# Batching Completion()
LiteLLM allows you to:
  * Send many completion calls to 1 model
  * Send 1 completion call to many models: Return Fastest Response
  * Send 1 completion call to many models: Return All Responses


info
Trying to do batch completion on LiteLLM Proxy ? Go here: https://docs.litellm.ai/docs/proxy/user_keys#beta-batch-completions---pass-model-as-list
## Send multiple completion calls to 1 model​
In the batch_completion method, you provide a list of `messages` where each sub-list of messages is passed to `litellm.completion()`, allowing you to process multiple prompts efficiently in a single API call.
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
### Example Code​
```
import litellm  
import os  
from litellm import batch_completion  
  
os.environ['ANTHROPIC_API_KEY'] = ""  
  
  
responses = batch_completion(  
  model="claude-2",  
  messages = [  
    [  
      {  
        "role": "user",  
        "content": "good morning? "  
      }  
    ],  
    [  
      {  
        "role": "user",  
        "content": "what's the time? "  
      }  
    ]  
  ]  
)  

```

## Send 1 completion call to many models: Return Fastest Response​
This makes parallel calls to the specified `models` and returns the first response 
Use this to reduce latency
  * SDK
  * PROXY


### Example Code​
```
import litellm  
import os  
from litellm import batch_completion_models  
  
os.environ['ANTHROPIC_API_KEY'] = ""  
os.environ['OPENAI_API_KEY'] = ""  
os.environ['COHERE_API_KEY'] = ""  
  
response = batch_completion_models(  
  models=["gpt-3.5-turbo", "claude-instant-1.2", "command-nightly"],   
  messages=[{"role": "user", "content": "Hey, how's it going"}]  
)  
print(result)  

```

how to setup proxy config
Just pass a comma-separated string of model names and the flag `fastest_response=True`.
  * curl
  * OpenAI SDK


```
  
curl -X POST 'http://localhost:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \   
-D '{  
  "model": "gpt-4o, groq-llama", # 👈 Comma-separated models  
  "messages": [  
   {  
    "role": "user",  
    "content": "What's the weather like in Boston today?"  
   }  
  ],  
  "stream": true,  
  "fastest_response": true # 👈 FLAG  
}  
  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-4o, groq-llama", # 👈 Comma-separated models  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={"fastest_response": true} # 👈 FLAG  
)  
  
print(response)  

```

* * *
### Example Setup:​
```
model_list:   
- model_name: groq-llama  
 litellm_params:  
  model: groq/llama3-8b-8192  
  api_key: os.environ/GROQ_API_KEY  
- model_name: gpt-4o  
 litellm_params:  
  model: gpt-4o  
  api_key: os.environ/OPENAI_API_KEY  

```

```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

### Output​
Returns the first response in OpenAI format. Cancels other LLM API calls. 
```
{  
 "object": "chat.completion",  
 "choices": [  
  {  
   "finish_reason": "stop",  
   "index": 0,  
   "message": {  
    "content": " I'm doing well, thanks for asking! I'm an AI assistant created by Anthropic to be helpful, harmless, and honest.",  
    "role": "assistant",  
    "logprobs": null  
   }  
  }  
 ],  
 "id": "chatcmpl-23273eed-e351-41be-a492-bafcf5cf3274",  
 "created": 1695154628.2076092,  
 "model": "command-nightly",  
 "usage": {  
  "prompt_tokens": 6,  
  "completion_tokens": 14,  
  "total_tokens": 20  
 }  
}  

```

## Send 1 completion call to many models: Return All Responses​
This makes parallel calls to the specified models and returns all responses
Use this to process requests concurrently and get responses from multiple models.
### Example Code​
```
import litellm  
import os  
from litellm import batch_completion_models_all_responses  
  
os.environ['ANTHROPIC_API_KEY'] = ""  
os.environ['OPENAI_API_KEY'] = ""  
os.environ['COHERE_API_KEY'] = ""  
  
responses = batch_completion_models_all_responses(  
  models=["gpt-3.5-turbo", "claude-instant-1.2", "command-nightly"],   
  messages=[{"role": "user", "content": "Hey, how's it going"}]  
)  
print(responses)  
  

```

### Output​
```
[<ModelResponse chat.completion id=chatcmpl-e673ec8e-4e8f-4c9e-bf26-bf9fa7ee52b9 at 0x103a62160> JSON: {  
 "object": "chat.completion",  
 "choices": [  
  {  
   "finish_reason": "stop_sequence",  
   "index": 0,  
   "message": {  
    "content": " It's going well, thank you for asking! How about you?",  
    "role": "assistant",  
    "logprobs": null  
   }  
  }  
 ],  
 "id": "chatcmpl-e673ec8e-4e8f-4c9e-bf26-bf9fa7ee52b9",  
 "created": 1695222060.917964,  
 "model": "claude-instant-1.2",  
 "usage": {  
  "prompt_tokens": 14,  
  "completion_tokens": 9,  
  "total_tokens": 23  
 }  
}, <ModelResponse chat.completion id=chatcmpl-ab6c5bd3-b5d9-4711-9697-e28d9fb8a53c at 0x103a62b60> JSON: {  
 "object": "chat.completion",  
 "choices": [  
  {  
   "finish_reason": "stop",  
   "index": 0,  
   "message": {  
    "content": " It's going well, thank you for asking! How about you?",  
    "role": "assistant",  
    "logprobs": null  
   }  
  }  
 ],  
 "id": "chatcmpl-ab6c5bd3-b5d9-4711-9697-e28d9fb8a53c",  
 "created": 1695222061.0445492,  
 "model": "command-nightly",  
 "usage": {  
  "prompt_tokens": 6,  
  "completion_tokens": 14,  
  "total_tokens": 20  
 }  
}, <OpenAIObject chat.completion id=chatcmpl-80szFnKHzCxObW0RqCMw1hWW1Icrq at 0x102dd6430> JSON: {  
 "id": "chatcmpl-80szFnKHzCxObW0RqCMw1hWW1Icrq",  
 "object": "chat.completion",  
 "created": 1695222061,  
 "model": "gpt-3.5-turbo-0613",  
 "choices": [  
  {  
   "index": 0,  
   "message": {  
    "role": "assistant",  
    "content": "Hello! I'm an AI language model, so I don't have feelings, but I'm here to assist you with any questions or tasks you might have. How can I help you today?"  
   },  
   "finish_reason": "stop"  
  }  
 ],  
 "usage": {  
  "prompt_tokens": 13,  
  "completion_tokens": 39,  
  "total_tokens": 52  
 }  
}]  
  

```

Previous
Model Alias
Next
Mock Completion() Responses - Save Testing Costs 💰
  * Send multiple completion calls to 1 model
    * Example Code
  * Send 1 completion call to many models: Return Fastest Response
    * Example Code
    * Example Setup:
    * Output
  * Send 1 completion call to many models: Return All Responses
    * Example Code
    * Output


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Using PDF Input


On this page
# Using PDF Input
How to send / receive pdf's (other document types) to a `/chat/completions` endpoint
Works for:
  * Vertex AI models (Gemini + Anthropic)
  * Bedrock Models
  * Anthropic API Models


## Quick Start​
### url​
  * SDK
  * PROXY


```
from litellm.utils import supports_pdf_input, completion  
  
# set aws credentials  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
  
# pdf url  
file_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"  
  
# model  
model = "bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0"  
  
file_content = [  
  {"type": "text", "text": "What's this file about?"},  
  {  
    "type": "file",  
    "file": {  
      "file_id": file_url,  
    }  
  },  
]  
  
  
if not supports_pdf_input(model, None):  
  print("Model does not support image input")  
  
response = completion(  
  model=model,  
  messages=[{"role": "user", "content": file_content}],  
)  
assert response is not None  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-model  
  litellm_params:  
   model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  

```

  1. Start the proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "bedrock-model",  
  "messages": [  
    {"role": "user", "content": [  
      {"type": "text", "text": "What's this file about?"},  
      {  
        "type": "file",  
        "file": {  
          "file_id": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf",  
        }  
      }  
    ]},  
  ]  
}'  

```

### base64​
  * SDK
  * PROXY


```
from litellm.utils import supports_pdf_input, completion  
  
# set aws credentials  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
  
# pdf url  
image_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"  
response = requests.get(url)  
file_data = response.content  
  
encoded_file = base64.b64encode(file_data).decode("utf-8")  
base64_url = f"data:application/pdf;base64,{encoded_file}"  
  
# model  
model = "bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0"  
  
file_content = [  
  {"type": "text", "text": "What's this file about?"},  
  {  
    "type": "file",  
    "file": {  
      "file_data": base64_url,  
    }  
  },  
]  
  
  
if not supports_pdf_input(model, None):  
  print("Model does not support image input")  
  
response = completion(  
  model=model,  
  messages=[{"role": "user", "content": file_content}],  
)  
assert response is not None  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-model  
  litellm_params:  
   model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  

```

  1. Start the proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "bedrock-model",  
  "messages": [  
    {"role": "user", "content": [  
      {"type": "text", "text": "What's this file about?"},  
      {  
        "type": "file",  
        "file": {  
          "file_data": "data:application/pdf;base64...",  
        }  
      }  
    ]},  
  ]  
}'  

```

## Checking if a model supports pdf input​
  * SDK
  * PROXY


Use `litellm.supports_pdf_input(model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0")` -> returns `True` if model can accept pdf input
```
assert litellm.supports_pdf_input(model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0") == True  

```

  1. Define bedrock models on config.yaml


```
model_list:  
 - model_name: bedrock-model # model group name  
  litellm_params:  
   model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  
  model_info: # OPTIONAL - set manually  
   supports_pdf_input: True  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Call `/model_group/info` to check if a model supports `pdf` input


```
curl -X 'GET' \  
 'http://localhost:4000/model_group/info' \  
 -H 'accept: application/json' \  
 -H 'x-api-key: sk-1234'  

```

Expected Response 
```
{  
 "data": [  
  {  
   "model_group": "bedrock-model",  
   "providers": ["bedrock"],  
   "max_input_tokens": 128000,  
   "max_output_tokens": 16384,  
   "mode": "chat",  
   ...,  
   "supports_pdf_input": true, # 👈 supports_pdf_input is true  
  }  
 ]  
}  

```

Previous
Using Web Search
Next
Using Vision Models
  * Quick Start
    * url
    * base64
  * Checking if a model supports pdf input


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Drop Unsupported Params


On this page
# Drop Unsupported Params
Drop unsupported OpenAI params by your LLM Provider.
## Quick Start​
```
import litellm   
import os   
  
# set keys   
os.environ["COHERE_API_KEY"] = "co-.."  
  
litellm.drop_params = True # 👈 KEY CHANGE  
  
response = litellm.completion(  
        model="command-r",  
        messages=[{"role": "user", "content": "Hey, how's it going?"}],  
        response_format={"key": "value"},  
      )  

```

LiteLLM maps all supported openai params by provider + model (e.g. function calling is supported by anthropic on bedrock but not titan). 
See `litellm.get_supported_openai_params("command-r")` **Code**
If a provider/model doesn't support a particular param, you can drop it. 
## OpenAI Proxy Usage​
```
litellm_settings:  
  drop_params: true  

```

## Pass drop_params in `completion(..)`​
Just drop_params when calling specific models 
  * SDK
  * PROXY


```
import litellm   
import os   
  
# set keys   
os.environ["COHERE_API_KEY"] = "co-.."  
  
response = litellm.completion(  
        model="command-r",  
        messages=[{"role": "user", "content": "Hey, how's it going?"}],  
        response_format={"key": "value"},  
        drop_params=True  
      )  

```

```
- litellm_params:  
  api_base: my-base  
  model: openai/my-model  
  drop_params: true # 👈 KEY CHANGE  
 model_name: my-model  

```

## Specify params to drop​
To drop specific params when calling a provider (E.g. 'logit_bias' for vllm)
Use `additional_drop_params`
  * SDK
  * PROXY


```
import litellm   
import os   
  
# set keys   
os.environ["COHERE_API_KEY"] = "co-.."  
  
response = litellm.completion(  
        model="command-r",  
        messages=[{"role": "user", "content": "Hey, how's it going?"}],  
        response_format={"key": "value"},  
        additional_drop_params=["response_format"]  
      )  

```

```
- litellm_params:  
  api_base: my-base  
  model: openai/my-model  
  additional_drop_params: ["response_format"] # 👈 KEY CHANGE  
 model_name: my-model  

```

**additional_drop_params** : List or null - Is a list of openai params you want to drop when making a call to the model.
## Specify allowed openai params in a request​
Tell litellm to allow specific openai params in a request. Use this if you get a `litellm.UnsupportedParamsError` and want to allow a param. LiteLLM will pass the param as is to the model.
  * LiteLLM Python SDK
  * LiteLLM Proxy


In this example we pass `allowed_openai_params=["tools"]` to allow the `tools` param.
Pass allowed_openai_params to LiteLLM Python SDK
```
await litellm.acompletion(  
  model="azure/o_series/<my-deployment-name>",  
  api_key="xxxxx",  
  api_base=api_base,  
  messages=[{"role": "user", "content": "Hello! return a json object"}],  
  tools=[{"type": "function", "function": {"name": "get_current_time", "description": "Get the current time in a given location.", "parameters": {"type": "object", "properties": {"location": {"type": "string", "description": "The city name, e.g. San Francisco"}}, "required": ["location"]}}}]  
  allowed_openai_params=["tools"],  
)  

```

When using litellm proxy you can pass `allowed_openai_params` in two ways:
  1. Dynamically pass `allowed_openai_params` in a request
  2. Set `allowed_openai_params` on the config.yaml file for a specific model


#### Dynamically pass allowed_openai_params in a request​
In this example we pass `allowed_openai_params=["tools"]` to allow the `tools` param for a request sent to the model set on the proxy.
Dynamically pass allowed_openai_params in a request
```
import openai  
from openai import AsyncAzureOpenAI  
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={   
    "allowed_openai_params": ["tools"]  
  }  
)  

```

#### Set allowed_openai_params on config.yaml​
You can also set `allowed_openai_params` on the config.yaml file for a specific model. This means that all requests to this deployment are allowed to pass in the `tools` param.
Set allowed_openai_params on config.yaml
```
model_list:  
 - model_name: azure-o1-preview  
  litellm_params:  
   model: azure/o_series/<my-deployment-name>  
   api_key: xxxxx  
   api_base: https://openai-prod-test.openai.azure.com/openai/deployments/o1/chat/completions?api-version=2025-01-01-preview  
   allowed_openai_params: ["tools"]  

```

Previous
Pre-fix Assistant Messages
Next
Prompt Formatting
  * Quick Start
  * OpenAI Proxy Usage
  * Pass drop_params in `completion(..)`
  * Specify params to drop
  * Specify allowed openai params in a request


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Function Calling


On this page
# Function Calling
## Checking if a model supports function calling​
Use `litellm.supports_function_calling(model="")` -> returns `True` if model supports Function calling, `False` if not
```
assert litellm.supports_function_calling(model="gpt-3.5-turbo") == True  
assert litellm.supports_function_calling(model="azure/gpt-4-1106-preview") == True  
assert litellm.supports_function_calling(model="palm/chat-bison") == False  
assert litellm.supports_function_calling(model="xai/grok-2-latest") == True  
assert litellm.supports_function_calling(model="ollama/llama2") == False  

```

## Checking if a model supports parallel function calling​
Use `litellm.supports_parallel_function_calling(model="")` -> returns `True` if model supports parallel function calling, `False` if not
```
assert litellm.supports_parallel_function_calling(model="gpt-4-turbo-preview") == True  
assert litellm.supports_parallel_function_calling(model="gpt-4") == False  

```

## Parallel Function calling​
Parallel function calling is the model's ability to perform multiple function calls together, allowing the effects and results of these function calls to be resolved in parallel
## Quick Start - gpt-3.5-turbo-1106​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
In this example we define a single function `get_current_weather`. 
  * Step 1: Send the model the `get_current_weather` with the user question
  * Step 2: Parse the output from the model response - Execute the `get_current_weather` with the model provided args
  * Step 3: Send the model the output from running the `get_current_weather` function


### Full Code - Parallel function calling with `gpt-3.5-turbo-1106`​
```
import litellm  
import json  
# set openai api key  
import os  
os.environ['OPENAI_API_KEY'] = "" # litellm reads OPENAI_API_KEY from .env and sends the request  
  
# Example dummy function hard coded to return the same weather  
# In production, this could be your backend API or an external API  
def get_current_weather(location, unit="fahrenheit"):  
  """Get the current weather in a given location"""  
  if "tokyo" in location.lower():  
    return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})  
  elif "san francisco" in location.lower():  
    return json.dumps({"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"})  
  elif "paris" in location.lower():  
    return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})  
  else:  
    return json.dumps({"location": location, "temperature": "unknown"})  
  
  
def test_parallel_function_call():  
  try:  
    # Step 1: send the conversation and available functions to the model  
    messages = [{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}]  
    tools = [  
      {  
        "type": "function",  
        "function": {  
          "name": "get_current_weather",  
          "description": "Get the current weather in a given location",  
          "parameters": {  
            "type": "object",  
            "properties": {  
              "location": {  
                "type": "string",  
                "description": "The city and state, e.g. San Francisco, CA",  
              },  
              "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
            },  
            "required": ["location"],  
          },  
        },  
      }  
    ]  
    response = litellm.completion(  
      model="gpt-3.5-turbo-1106",  
      messages=messages,  
      tools=tools,  
      tool_choice="auto", # auto is default, but we'll be explicit  
    )  
    print("\nFirst LLM Response:\n", response)  
    response_message = response.choices[0].message  
    tool_calls = response_message.tool_calls  
  
    print("\nLength of tool calls", len(tool_calls))  
  
    # Step 2: check if the model wanted to call a function  
    if tool_calls:  
      # Step 3: call the function  
      # Note: the JSON response may not always be valid; be sure to handle errors  
      available_functions = {  
        "get_current_weather": get_current_weather,  
      } # only one function in this example, but you can have multiple  
      messages.append(response_message) # extend conversation with assistant's reply  
  
      # Step 4: send the info for each function call and function response to the model  
      for tool_call in tool_calls:  
        function_name = tool_call.function.name  
        function_to_call = available_functions[function_name]  
        function_args = json.loads(tool_call.function.arguments)  
        function_response = function_to_call(  
          location=function_args.get("location"),  
          unit=function_args.get("unit"),  
        )  
        messages.append(  
          {  
            "tool_call_id": tool_call.id,  
            "role": "tool",  
            "name": function_name,  
            "content": function_response,  
          }  
        ) # extend conversation with function response  
      second_response = litellm.completion(  
        model="gpt-3.5-turbo-1106",  
        messages=messages,  
      ) # get a new response from the model where it can see the function response  
      print("\nSecond LLM response:\n", second_response)  
      return second_response  
  except Exception as e:  
   print(f"Error occurred: {e}")  
  
test_parallel_function_call()  

```

### Explanation - Parallel function calling​
Below is an explanation of what is happening in the code snippet above for Parallel function calling with `gpt-3.5-turbo-1106`
### Step1: litellm.completion() with `tools` set to `get_current_weather`​
```
import litellm  
import json  
# set openai api key  
import os  
os.environ['OPENAI_API_KEY'] = "" # litellm reads OPENAI_API_KEY from .env and sends the request  
# Example dummy function hard coded to return the same weather  
# In production, this could be your backend API or an external API  
def get_current_weather(location, unit="fahrenheit"):  
  """Get the current weather in a given location"""  
  if "tokyo" in location.lower():  
    return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})  
  elif "san francisco" in location.lower():  
    return json.dumps({"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"})  
  elif "paris" in location.lower():  
    return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})  
  else:  
    return json.dumps({"location": location, "temperature": "unknown"})  
  
messages = [{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}]  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
  
response = litellm.completion(  
  model="gpt-3.5-turbo-1106",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto", # auto is default, but we'll be explicit  
)  
print("\nLLM Response1:\n", response)  
response_message = response.choices[0].message  
tool_calls = response.choices[0].message.tool_calls  

```

##### Expected output​
In the output you can see the model calls the function multiple times - for San Francisco, Tokyo, Paris
```
ModelResponse(  
 id='chatcmpl-8MHBKZ9t6bXuhBvUMzoKsfmmlv7xq',   
 choices=[  
  Choices(finish_reason='tool_calls',   
  index=0,   
  message=Message(content=None, role='assistant',   
   tool_calls=[  
    ChatCompletionMessageToolCall(id='call_DN6IiLULWZw7sobV6puCji1O', function=Function(arguments='{"location": "San Francisco", "unit": "celsius"}', name='get_current_weather'), type='function'),   
  
    ChatCompletionMessageToolCall(id='call_ERm1JfYO9AFo2oEWRmWUd40c', function=Function(arguments='{"location": "Tokyo", "unit": "celsius"}', name='get_current_weather'), type='function'),   
      
    ChatCompletionMessageToolCall(id='call_2lvUVB1y4wKunSxTenR0zClP', function=Function(arguments='{"location": "Paris", "unit": "celsius"}', name='get_current_weather'), type='function')  
    ]))  
  ],   
  created=1700319953,   
  model='gpt-3.5-turbo-1106',   
  object='chat.completion',   
  system_fingerprint='fp_eeff13170a',  
  usage={'completion_tokens': 77, 'prompt_tokens': 88, 'total_tokens': 165},   
  _response_ms=1177.372  
)  

```

### Step 2 - Parse the Model Response and Execute Functions​
After sending the initial request, parse the model response to identify the function calls it wants to make. In this example, we expect three tool calls, each corresponding to a location (San Francisco, Tokyo, and Paris). 
```
# Check if the model wants to call a function  
if tool_calls:  
  # Execute the functions and prepare responses  
  available_functions = {  
    "get_current_weather": get_current_weather,  
  }  
  
  messages.append(response_message) # Extend conversation with assistant's reply  
  
  for tool_call in tool_calls:  
   print(f"\nExecuting tool call\n{tool_call}")  
   function_name = tool_call.function.name  
   function_to_call = available_functions[function_name]  
   function_args = json.loads(tool_call.function.arguments)  
   # calling the get_current_weather() function  
   function_response = function_to_call(  
     location=function_args.get("location"),  
     unit=function_args.get("unit"),  
   )  
   print(f"Result from tool call\n{function_response}\n")  
  
   # Extend conversation with function response  
   messages.append(  
     {  
       "tool_call_id": tool_call.id,  
       "role": "tool",  
       "name": function_name,  
       "content": function_response,  
     }  
   )  
  

```

### Step 3 - Second litellm.completion() call​
Once the functions are executed, send the model the information for each function call and its response. This allows the model to generate a new response considering the effects of the function calls.
```
second_response = litellm.completion(  
  model="gpt-3.5-turbo-1106",  
  messages=messages,  
)  
print("Second Response\n", second_response)  

```

#### Expected output​
```
ModelResponse(  
 id='chatcmpl-8MHBLh1ldADBP71OrifKap6YfAd4w',   
 choices=[  
  Choices(finish_reason='stop', index=0,   
  message=Message(content="The current weather in San Francisco is 72°F, in Tokyo it's 10°C, and in Paris it's 22°C.", role='assistant'))  
 ],   
 created=1700319955,   
 model='gpt-3.5-turbo-1106',   
 object='chat.completion',   
 system_fingerprint='fp_eeff13170a',   
 usage={'completion_tokens': 28, 'prompt_tokens': 169, 'total_tokens': 197},   
 _response_ms=1032.431  
)  

```

## Parallel Function Calling - Azure OpenAI​
```
# set Azure env variables  
import os  
os.environ['AZURE_API_KEY'] = "" # litellm reads AZURE_API_KEY from .env and sends the request  
os.environ['AZURE_API_BASE'] = "https://openai-gpt-4-test-v-1.openai.azure.com/"  
os.environ['AZURE_API_VERSION'] = "2023-07-01-preview"  
  
import litellm  
import json  
# Example dummy function hard coded to return the same weather  
# In production, this could be your backend API or an external API  
def get_current_weather(location, unit="fahrenheit"):  
  """Get the current weather in a given location"""  
  if "tokyo" in location.lower():  
    return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})  
  elif "san francisco" in location.lower():  
    return json.dumps({"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"})  
  elif "paris" in location.lower():  
    return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})  
  else:  
    return json.dumps({"location": location, "temperature": "unknown"})  
  
## Step 1: send the conversation and available functions to the model  
messages = [{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}]  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
  
response = litellm.completion(  
  model="azure/chatgpt-functioncalling", # model = azure/<your-azure-deployment-name>  
  messages=messages,  
  tools=tools,  
  tool_choice="auto", # auto is default, but we'll be explicit  
)  
print("\nLLM Response1:\n", response)  
response_message = response.choices[0].message  
tool_calls = response.choices[0].message.tool_calls  
print("\nTool Choice:\n", tool_calls)  
  
## Step 2 - Parse the Model Response and Execute Functions  
# Check if the model wants to call a function  
if tool_calls:  
  # Execute the functions and prepare responses  
  available_functions = {  
    "get_current_weather": get_current_weather,  
  }  
  
  messages.append(response_message) # Extend conversation with assistant's reply  
  
  for tool_call in tool_calls:  
   print(f"\nExecuting tool call\n{tool_call}")  
   function_name = tool_call.function.name  
   function_to_call = available_functions[function_name]  
   function_args = json.loads(tool_call.function.arguments)  
   # calling the get_current_weather() function  
   function_response = function_to_call(  
     location=function_args.get("location"),  
     unit=function_args.get("unit"),  
   )  
   print(f"Result from tool call\n{function_response}\n")  
  
   # Extend conversation with function response  
   messages.append(  
     {  
       "tool_call_id": tool_call.id,  
       "role": "tool",  
       "name": function_name,  
       "content": function_response,  
     }  
   )  
  
## Step 3 - Second litellm.completion() call  
second_response = litellm.completion(  
  model="azure/chatgpt-functioncalling",  
  messages=messages,  
)  
print("Second Response\n", second_response)  
print("Second Response Message\n", second_response.choices[0].message.content)  
  

```

## Deprecated - Function Calling with `completion(functions=functions)`​
```
import os, litellm  
from litellm import completion  
  
os.environ['OPENAI_API_KEY'] = ""  
  
messages = [  
  {"role": "user", "content": "What is the weather like in Boston?"}  
]  
  
# python function that will get executed  
def get_current_weather(location):  
 if location == "Boston, MA":  
  return "The weather is 12F"  
  
# JSON Schema to pass to OpenAI  
functions = [  
  {  
   "name": "get_current_weather",  
   "description": "Get the current weather in a given location",  
   "parameters": {  
    "type": "object",  
    "properties": {  
     "location": {  
      "type": "string",  
      "description": "The city and state, e.g. San Francisco, CA"  
     },  
     "unit": {  
      "type": "string",  
      "enum": ["celsius", "fahrenheit"]  
     }  
    },  
    "required": ["location"]  
   }  
  }  
 ]  
  
response = completion(model="gpt-3.5-turbo-0613", messages=messages, functions=functions)  
print(response)  

```

## litellm.function_to_dict - Convert Functions to dictionary for OpenAI function calling​
`function_to_dict` allows you to pass a function docstring and produce a dictionary usable for OpenAI function calling
### Using `function_to_dict`​
  1. Define your function `get_current_weather`
  2. Add a docstring to your function `get_current_weather`
  3. Pass the function to `litellm.utils.function_to_dict` to get the dictionary for OpenAI function calling


```
# function with docstring  
def get_current_weather(location: str, unit: str):  
    """Get the current weather in a given location  
  
    Parameters  
    ----------  
    location : str  
      The city and state, e.g. San Francisco, CA  
    unit : {'celsius', 'fahrenheit'}  
      Temperature unit  
  
    Returns  
    -------  
    str  
      a sentence indicating the weather  
    """  
    if location == "Boston, MA":  
      return "The weather is 12F"  
  
# use litellm.utils.function_to_dict to convert function to dict  
function_json = litellm.utils.function_to_dict(get_current_weather)  
print(function_json)  

```

#### Output from function_to_dict​
```
{  
  'name': 'get_current_weather',   
  'description': 'Get the current weather in a given location',   
  'parameters': {  
    'type': 'object',   
    'properties': {  
      'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'},   
      'unit': {'type': 'string', 'description': 'Temperature unit', 'enum': "['fahrenheit', 'celsius']"}  
    },   
    'required': ['location', 'unit']  
  }  
}  

```

### Using function_to_dict with Function calling​
```
import os, litellm  
from litellm import completion  
  
os.environ['OPENAI_API_KEY'] = ""  
  
messages = [  
  {"role": "user", "content": "What is the weather like in Boston?"}  
]  
  
def get_current_weather(location: str, unit: str):  
  """Get the current weather in a given location  
  
  Parameters  
  ----------  
  location : str  
    The city and state, e.g. San Francisco, CA  
  unit : str {'celsius', 'fahrenheit'}  
    Temperature unit  
  
  Returns  
  -------  
  str  
    a sentence indicating the weather  
  """  
  if location == "Boston, MA":  
    return "The weather is 12F"  
  
functions = [litellm.utils.function_to_dict(get_current_weather)]  
  
response = completion(model="gpt-3.5-turbo-0613", messages=messages, functions=functions)  
print(response)  

```

## Function calling for Models w/out function-calling support​
### Adding Function to prompt​
For Models/providers without function calling support, LiteLLM allows you to add the function to the prompt set: `litellm.add_function_to_prompt = True`
#### Usage​
```
import os, litellm  
from litellm import completion  
  
# IMPORTANT - Set this to TRUE to add the function to the prompt for Non OpenAI LLMs  
litellm.add_function_to_prompt = True # set add_function_to_prompt for Non OpenAI LLMs  
  
os.environ['ANTHROPIC_API_KEY'] = ""  
  
messages = [  
  {"role": "user", "content": "What is the weather like in Boston?"}  
]  
  
def get_current_weather(location):  
 if location == "Boston, MA":  
  return "The weather is 12F"  
  
functions = [  
  {  
   "name": "get_current_weather",  
   "description": "Get the current weather in a given location",  
   "parameters": {  
    "type": "object",  
    "properties": {  
     "location": {  
      "type": "string",  
      "description": "The city and state, e.g. San Francisco, CA"  
     },  
     "unit": {  
      "type": "string",  
      "enum": ["celsius", "fahrenheit"]  
     }  
    },  
    "required": ["location"]  
   }  
  }  
 ]  
  
response = completion(model="claude-2", messages=messages, functions=functions)  
print(response)  

```

Previous
Trimming Input Messages
Next
Model Alias
  * Checking if a model supports function calling
  * Checking if a model supports parallel function calling
  * Parallel Function calling
  * Quick Start - gpt-3.5-turbo-1106
    * Full Code - Parallel function calling with `gpt-3.5-turbo-1106`
    * Explanation - Parallel function calling
    * Step1: litellm.completion() with `tools` set to `get_current_weather`
    * Step 2 - Parse the Model Response and Execute Functions
    * Step 3 - Second litellm.completion() call
  * Parallel Function Calling - Azure OpenAI
  * Deprecated - Function Calling with `completion(functions=functions)`
  * litellm.function_to_dict - Convert Functions to dictionary for OpenAI function calling
    * Using `function_to_dict`
    * Using function_to_dict with Function calling
  * Function calling for Models w/out function-calling support
    * Adding Function to prompt


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
      * Input Params
      * Output
      * Usage
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /chat/completions
  * Input Params


On this page
# Input Params
## Common Params​
LiteLLM accepts and translates the OpenAI Chat Completion params across all providers. 
### Usage​
```
import litellm  
  
# set env variables  
os.environ["OPENAI_API_KEY"] = "your-openai-key"  
  
## SET MAX TOKENS - via completion()   
response = litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
print(response)  

```

### Translated OpenAI params​
Use this function to get an up-to-date list of supported openai params for any model + provider. 
```
from litellm import get_supported_openai_params  
  
response = get_supported_openai_params(model="anthropic.claude-3", custom_llm_provider="bedrock")  
  
print(response) # ["max_tokens", "tools", "tool_choice", "stream"]  

```

This is a list of openai params we translate across providers.
Use `litellm.get_supported_openai_params()` for an updated list of params for each model + provider 
Provider| temperature| max_completion_tokens| max_tokens| top_p| stream| stream_options| stop| n| presence_penalty| frequency_penalty| functions| function_call| logit_bias| user| response_format| seed| tools| tool_choice| logprobs| top_logprobs| extra_headers|   
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
Anthropic| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | ✅| ✅| | ✅| ✅| | | ✅|   
OpenAI| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅|   
Azure OpenAI| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | ✅|   
xAI| ✅| | ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| |   
Replicate| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | | | | |   
Anyscale| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | | | | |   
Cohere| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | |   
Huggingface| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | | |   
Openrouter| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | ✅| ✅| | | | |   
AI21| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | |   
VertexAI| ✅| ✅| ✅| | ✅| ✅| | | | | | | | | ✅| ✅| | | | | |   
Bedrock| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | ✅ (model dependent)| | | | | |   
Sagemaker| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | | |   
TogetherAI| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | ✅| | | ✅| | ✅| ✅| | | |   
AlephAlpha| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | | |   
NLP Cloud| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | | | | |   
Petals| ✅| ✅| | ✅| ✅| | | | | | | | | | | | | | | | |   
Ollama| ✅| ✅| ✅| ✅| ✅| ✅| | | ✅| | | | | ✅| | | ✅| | | | |   
Databricks| ✅| ✅| ✅| ✅| ✅| ✅| | | | | | | | | | | | | | | |   
ClarifAI| ✅| ✅| ✅| | ✅| ✅| | | | | | | | | | | | | | | |   
Github| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| ✅| | | | | ✅| ✅ (model dependent)| ✅ (model dependent)| | | |   
note
By default, LiteLLM raises an exception if the openai param being passed in isn't supported. 
To drop the param instead, set `litellm.drop_params = True` or `completion(..drop_params=True)`.
This **ONLY DROPS UNSUPPORTED OPENAI PARAMS**. 
LiteLLM assumes any non-openai param is provider specific and passes it in as a kwarg in the request body
## Input Params​
```
def completion(  
  model: str,  
  messages: List = [],  
  # Optional OpenAI params  
  timeout: Optional[Union[float, int]] = None,  
  temperature: Optional[float] = None,  
  top_p: Optional[float] = None,  
  n: Optional[int] = None,  
  stream: Optional[bool] = None,  
  stream_options: Optional[dict] = None,  
  stop=None,  
  max_completion_tokens: Optional[int] = None,  
  max_tokens: Optional[int] = None,  
  presence_penalty: Optional[float] = None,  
  frequency_penalty: Optional[float] = None,  
  logit_bias: Optional[dict] = None,  
  user: Optional[str] = None,  
  # openai v1.0+ new params  
  response_format: Optional[dict] = None,  
  seed: Optional[int] = None,  
  tools: Optional[List] = None,  
  tool_choice: Optional[str] = None,  
  parallel_tool_calls: Optional[bool] = None,  
  logprobs: Optional[bool] = None,  
  top_logprobs: Optional[int] = None,  
  deployment_id=None,  
  # soon to be deprecated params by OpenAI  
  functions: Optional[List] = None,  
  function_call: Optional[str] = None,  
  # set api_base, api_version, api_key  
  base_url: Optional[str] = None,  
  api_version: Optional[str] = None,  
  api_key: Optional[str] = None,  
  model_list: Optional[list] = None, # pass in a list of api_base,keys, etc.  
  # Optional liteLLM function params  
  **kwargs,  
  
) -> ModelResponse:  

```

### Required Fields​
  * `model`: _string_ - ID of the model to use. Refer to the model endpoint compatibility table for details on which models work with the Chat API.
  * `messages`: _array_ - A list of messages comprising the conversation so far.


#### Properties of `messages`​
 _Note_ - Each message in the array contains the following properties:
  * `role`: _string_ - The role of the message's author. Roles can be: system, user, assistant, function or tool.
  * `content`: _string or list[dict] or null_ - The contents of the message. It is required for all messages, but may be null for assistant messages with function calls.
  * `name`: _string (optional)_ - The name of the author of the message. It is required if the role is "function". The name should match the name of the function represented in the content. It can contain characters (a-z, A-Z, 0-9), and underscores, with a maximum length of 64 characters.
  * `function_call`: _object (optional)_ - The name and arguments of a function that should be called, as generated by the model.
  * `tool_call_id`: _str (optional)_ - Tool call that this message is responding to.


**See All Message Values**
## Optional Fields​
  * `temperature`: _number or null (optional)_ - The sampling temperature to be used, between 0 and 2. Higher values like 0.8 produce more random outputs, while lower values like 0.2 make outputs more focused and deterministic. 
  * `top_p`: _number or null (optional)_ - An alternative to sampling with temperature. It instructs the model to consider the results of the tokens with top_p probability. For example, 0.1 means only the tokens comprising the top 10% probability mass are considered.
  * `n`: _integer or null (optional)_ - The number of chat completion choices to generate for each input message.
  * `stream`: _boolean or null (optional)_ - If set to true, it sends partial message deltas. Tokens will be sent as they become available, with the stream terminated by a [DONE] message.
  * `stream_options` _dict or null (optional)_ - Options for streaming response. Only set this when you set `stream: true`
    * `include_usage` _boolean (optional)_ - If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value. 
  * `stop`: _string/ array/ null (optional)_ - Up to 4 sequences where the API will stop generating further tokens.
  * `max_completion_tokens`: _integer (optional)_ - An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.
  * `max_tokens`: _integer (optional)_ - The maximum number of tokens to generate in the chat completion.
  * `presence_penalty`: _number or null (optional)_ - It is used to penalize new tokens based on their existence in the text so far.
  * `response_format`: _object (optional)_ - An object specifying the format that the model must output.
    * Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
    * Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if finish_reason="length", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
  * `seed`: _integer or null (optional)_ - This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
  * `tools`: _array (optional)_ - A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.
    * `type`: _string_ - The type of the tool. Currently, only function is supported.
    * `function`: _object_ - Required.
  * `tool_choice`: _string or object (optional)_ - Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via `{"type: "function", "function": {"name": "my_function"}}` forces the model to call that function.
    * `none` is the default when no functions are present. `auto` is the default if functions are present.
  * `parallel_tool_calls`: _boolean (optional)_ - Whether to enable parallel function calling during tool use.. OpenAI default is true.
  * `frequency_penalty`: _number or null (optional)_ - It is used to penalize new tokens based on their frequency in the text so far.
  * `logit_bias`: _map (optional)_ - Used to modify the probability of specific tokens appearing in the completion.
  * `user`: _string (optional)_ - A unique identifier representing your end-user. This can help OpenAI to monitor and detect abuse.
  * `timeout`: _int (optional)_ - Timeout in seconds for completion requests (Defaults to 600 seconds)
  * `logprobs`: _bool (optional)_ - Whether to return log probabilities of the output tokens or not. If true returns the log probabilities of each output token returned in the content of message 
  * `top_logprobs`: _int (optional)_ - An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to true if this parameter is used.
  * `headers`: _dict (optional)_ - A dictionary of headers to be sent with the request.
  * `extra_headers`: _dict (optional)_ - Alternative to `headers`, used to send extra headers in LLM API request. 


#### Deprecated Params​
  * `functions`: _array_ - A list of functions that the model may use to generate JSON inputs. Each function should have the following properties:
    * `name`: _string_ - The name of the function to be called. It should contain a-z, A-Z, 0-9, underscores and dashes, with a maximum length of 64 characters.
    * `description`: _string (optional)_ - A description explaining what the function does. It helps the model to decide when and how to call the function.
    * `parameters`: _object_ - The parameters that the function accepts, described as a JSON Schema object.
  * `function_call`: _string or object (optional)_ - Controls how the model responds to function calls.


#### litellm-specific params​
  * `api_base`: _string (optional)_ - The api endpoint you want to call the model with
  * `api_version`: _string (optional)_ - (Azure-specific) the api version for the call
  * `num_retries`: _int (optional)_ - The number of times to retry the API call if an APIError, TimeoutError or ServiceUnavailableError occurs 
  * `context_window_fallback_dict`: _dict (optional)_ - A mapping of model to use if call fails due to context window error
  * `fallbacks`: _list (optional)_ - A list of model names + params to be used, in case the initial call fails
  * `metadata`: _dict (optional)_ - Any additional data you want to be logged when the call is made (sent to logging integrations, eg. promptlayer and accessible via custom callback function)


**CUSTOM MODEL COST**
  * `input_cost_per_token`: _float (optional)_ - The cost per input token for the completion call 
  * `output_cost_per_token`: _float (optional)_ - The cost per output token for the completion call 


**CUSTOM PROMPT TEMPLATE** (See prompt formatting for more info)
  * `initial_prompt_value`: _string (optional)_ - Initial string applied at the start of the input messages
  * `roles`: _dict (optional)_ - Dictionary specifying how to format the prompt based on the role + message passed in via `messages`. 
  * `final_prompt_value`: _string (optional)_ - Final string applied at the end of the input messages
  * `bos_token`: _string (optional)_ - Initial string applied at the start of a sequence
  * `eos_token`: _string (optional)_ - Initial string applied at the end of a sequence
  * `hf_model_name`: _string (optional)_ - [Sagemaker Only] The corresponding huggingface name of the model, used to pull the right chat template for the model.


Previous
/chat/completions
Next
Output
  * Common Params
    * Usage
    * Translated OpenAI params
  * Input Params
    * Required Fields
  * Optional Fields


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Structured Outputs (JSON Mode)


On this page
# Structured Outputs (JSON Mode)
## Quick Start​
  * SDK
  * PROXY


```
from litellm import completion  
import os   
  
os.environ["OPENAI_API_KEY"] = ""  
  
response = completion(  
 model="gpt-4o-mini",  
 response_format={ "type": "json_object" },  
 messages=[  
  {"role": "system", "content": "You are a helpful assistant designed to output JSON."},  
  {"role": "user", "content": "Who won the world series in 2020?"}  
 ]  
)  
print(response.choices[0].message.content)  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "gpt-4o-mini",  
  "response_format": { "type": "json_object" },  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful assistant designed to output JSON."  
   },  
   {  
    "role": "user",  
    "content": "Who won the world series in 2020?"  
   }  
  ]  
 }'  

```

## Check Model Support​
### 1. Check if model supports `response_format`​
Call `litellm.get_supported_openai_params` to check if a model/provider supports `response_format`. 
```
from litellm import get_supported_openai_params  
  
params = get_supported_openai_params(model="anthropic.claude-3", custom_llm_provider="bedrock")  
  
assert "response_format" in params  

```

### 2. Check if model supports `json_schema`​
This is used to check if you can pass 
  * `response_format={ "type": "json_schema", "json_schema": … , "strict": true }`
  * `response_format=<Pydantic Model>`


```
from litellm import supports_response_schema  
  
assert supports_response_schema(model="gemini-1.5-pro-preview-0215", custom_llm_provider="bedrock")  

```

Check out model_prices_and_context_window.json for a full list of models and their support for `response_schema`.
## Pass in 'json_schema'​
To use Structured Outputs, simply specify
```
response_format: { "type": "json_schema", "json_schema": … , "strict": true }  

```

Works for:
  * OpenAI models 
  * Azure OpenAI models
  * xAI models (Grok-2 or later)
  * Google AI Studio - Gemini models
  * Vertex AI models (Gemini + Anthropic)
  * Bedrock Models
  * Anthropic API Models
  * Groq Models
  * Ollama Models
  * Databricks Models


  * SDK
  * PROXY


```
import os  
from litellm import completion   
from pydantic import BaseModel  
  
# add to env var   
os.environ["OPENAI_API_KEY"] = ""  
  
messages = [{"role": "user", "content": "List 5 important events in the XIX century"}]  
  
class CalendarEvent(BaseModel):  
 name: str  
 date: str  
 participants: list[str]  
  
class EventsList(BaseModel):  
  events: list[CalendarEvent]  
  
resp = completion(  
  model="gpt-4o-2024-08-06",  
  messages=messages,  
  response_format=EventsList  
)  
  
print("Received={}".format(resp))  

```

  1. Add openai model to config.yaml


```
model_list:  
 - model_name: "gpt-4o"  
  litellm_params:  
   model: "gpt-4o-2024-08-06"  

```

  1. Start proxy with config.yaml


```
litellm --config /path/to/config.yaml  

```

  1. Call with OpenAI SDK / Curl!


Just replace the 'base_url' in the openai sdk, to call the proxy with 'json_schema' for openai models
**OpenAI SDK**
```
from pydantic import BaseModel  
from openai import OpenAI  
  
client = OpenAI(  
  api_key="anything", # 👈 PROXY KEY (can be anything, if master_key not set)  
  base_url="http://0.0.0.0:4000" # 👈 PROXY BASE URL  
)  
  
class Step(BaseModel):  
  explanation: str  
  output: str  
  
class MathReasoning(BaseModel):  
  steps: list[Step]  
  final_answer: str  
  
completion = client.beta.chat.completions.parse(  
  model="gpt-4o",  
  messages=[  
    {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},  
    {"role": "user", "content": "how can I solve 8x + 7 = -23"}  
  ],  
  response_format=MathReasoning,  
)  
  
math_reasoning = completion.choices[0].message.parsed  

```

**Curl**
```
curl -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "gpt-4o",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   {  
    "role": "user",  
    "content": "how can I solve 8x + 7 = -23"  
   }  
  ],  
  "response_format": {  
   "type": "json_schema",  
   "json_schema": {  
    "name": "math_reasoning",  
    "schema": {  
     "type": "object",  
     "properties": {  
      "steps": {  
       "type": "array",  
       "items": {  
        "type": "object",  
        "properties": {  
         "explanation": { "type": "string" },  
         "output": { "type": "string" }  
        },  
        "required": ["explanation", "output"],  
        "additionalProperties": false  
       }  
      },  
      "final_answer": { "type": "string" }  
     },  
     "required": ["steps", "final_answer"],  
     "additionalProperties": false  
    },  
    "strict": true  
   }  
  }  
 }'  

```

## Validate JSON Schema​
Not all vertex models support passing the json_schema to them (e.g. `gemini-1.5-flash`). To solve this, LiteLLM supports client-side validation of the json schema. 
```
litellm.enable_json_schema_validation=True  

```

If `litellm.enable_json_schema_validation=True` is set, LiteLLM will validate the json response using `jsonvalidator`. 
**See Code**
  * SDK
  * PROXY


```
# !gcloud auth application-default login - run this to add vertex credentials to your env  
import litellm, os  
from litellm import completion   
from pydantic import BaseModel   
  
  
messages=[  
    {"role": "system", "content": "Extract the event information."},  
    {"role": "user", "content": "Alice and Bob are going to a science fair on Friday."},  
  ]  
  
litellm.enable_json_schema_validation = True  
litellm.set_verbose = True # see the raw request made by litellm  
  
class CalendarEvent(BaseModel):  
 name: str  
 date: str  
 participants: list[str]  
  
resp = completion(  
  model="gemini/gemini-1.5-pro",  
  messages=messages,  
  response_format=CalendarEvent,  
)  
  
print("Received={}".format(resp))  

```

  1. Create config.yaml


```
model_list:  
 - model_name: "gemini-1.5-flash"  
  litellm_params:  
   model: "gemini/gemini-1.5-flash"  
   api_key: os.environ/GEMINI_API_KEY  
  
litellm_settings:  
 enable_json_schema_validation: True  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -d '{  
  "model": "gemini-1.5-flash",  
  "messages": [  
    {"role": "system", "content": "Extract the event information."},  
    {"role": "user", "content": "Alice and Bob are going to a science fair on Friday."},  
  ],  
  "response_format": {   
    "type": "json_object",  
    "response_schema": {   
      "type": "json_schema",  
      "json_schema": {  
       "name": "math_reasoning",  
       "schema": {  
        "type": "object",  
        "properties": {  
         "steps": {  
          "type": "array",  
          "items": {  
           "type": "object",  
           "properties": {  
            "explanation": { "type": "string" },  
            "output": { "type": "string" }  
           },  
           "required": ["explanation", "output"],  
           "additionalProperties": false  
          }  
         },  
         "final_answer": { "type": "string" }  
        },  
        "required": ["steps", "final_answer"],  
        "additionalProperties": false  
       },  
       "strict": true  
      },  
    }  
  },  
 }'  

```

Previous
Using Vision Models
Next
'Thinking' / 'Reasoning Content'
  * Quick Start
  * Check Model Support
    * 1. Check if model supports `response_format`
    * 2. Check if model supports `json_schema`
  * Pass in 'json_schema'
  * Validate JSON Schema


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Mock Completion() Responses - Save Testing Costs 💰


On this page
# Mock Completion() Responses - Save Testing Costs 💰
For testing purposes, you can use `completion()` with `mock_response` to mock calling the completion endpoint. 
This will return a response object with a default response (works for streaming as well), without calling the LLM APIs. 
## quick start​
```
from litellm import completion   
  
model = "gpt-3.5-turbo"  
messages = [{"role":"user", "content":"This is a test request"}]  
  
completion(model=model, messages=messages, mock_response="It's simple to use and easy to get started")  

```

## streaming​
```
from litellm import completion   
model = "gpt-3.5-turbo"  
messages = [{"role": "user", "content": "Hey, I'm a mock request"}]  
response = completion(model=model, messages=messages, stream=True, mock_response="It's simple to use and easy to get started")  
for chunk in response:   
  print(chunk) # {'choices': [{'delta': {'role': 'assistant', 'content': 'Thi'}, 'finish_reason': None}]}  
  complete_response += chunk["choices"][0]["delta"]["content"]  

```

## (Non-streaming) Mock Response Object​
```
{  
 "choices": [  
  {  
   "finish_reason": "stop",  
   "index": 0,  
   "message": {  
    "content": "This is a mock request",  
    "role": "assistant",  
    "logprobs": null  
   }  
  }  
 ],  
 "created": 1694459929.4496052,  
 "model": "MockResponse",  
 "usage": {  
  "prompt_tokens": null,  
  "completion_tokens": null,  
  "total_tokens": null  
 }  
}  

```

## Building a pytest function using `completion` with `mock_response`​
```
from litellm import completion  
import pytest  
  
def test_completion_openai():  
  try:  
    response = completion(  
      model="gpt-3.5-turbo",  
      messages=[{"role":"user", "content":"Why is LiteLLM amazing?"}],  
      mock_response="LiteLLM is awesome"  
    )  
    # Add any assertions here to check the response  
    print(response)  
    assert(response['choices'][0]['message']['content'] == "LiteLLM is awesome")  
  except Exception as e:  
    pytest.fail(f"Error occurred: {e}")  

```

Previous
Batching Completion()
Next
Reliability - Retries, Fallbacks
  * quick start
  * streaming
  * (Non-streaming) Mock Response Object
  * Building a pytest function using `completion` with `mock_response`


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Model Alias


On this page
# Model Alias
The model name you show an end-user might be different from the one you pass to LiteLLM - e.g. Displaying `GPT-3.5` while calling `gpt-3.5-turbo-16k` on the backend. 
LiteLLM simplifies this by letting you pass in a model alias mapping. 
# expected format
```
litellm.model_alias_map = {  
  # a dictionary containing a mapping of the alias string to the actual litellm model name string  
  "model_alias": "litellm_model_name"  
}  

```

# usage
### Relevant Code​
```
model_alias_map = {  
  "GPT-3.5": "gpt-3.5-turbo-16k",  
  "llama2": "replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf"  
}  
  
litellm.model_alias_map = model_alias_map  

```

### Complete Code​
```
import litellm   
from litellm import completion   
  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "openai key"  
os.environ["REPLICATE_API_KEY"] = "cohere key"  
  
## set model alias map  
model_alias_map = {  
  "GPT-3.5": "gpt-3.5-turbo-16k",  
  "llama2": "replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf"  
}  
  
litellm.model_alias_map = model_alias_map  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
# call "gpt-3.5-turbo-16k"  
response = completion(model="GPT-3.5", messages=messages)  
  
# call replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca1...  
response = completion("llama2", messages)  

```

Previous
Function Calling
Next
Batching Completion()
  * Relevant Code
  * Complete Code


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Trimming Input Messages


On this page
# Trimming Input Messages
**Use litellm.trim_messages() to ensure messages does not exceed a model's token limit or specified`max_tokens`**
## Usage​
```
from litellm import completion  
from litellm.utils import trim_messages  
  
response = completion(  
  model=model,   
  messages=trim_messages(messages, model) # trim_messages ensures tokens(messages) < max_tokens(model)  
)   

```

## Usage - set max_tokens​
```
from litellm import completion  
from litellm.utils import trim_messages  
  
response = completion(  
  model=model,   
  messages=trim_messages(messages, model, max_tokens=10), # trim_messages ensures tokens(messages) < max_tokens  
)   

```

## Parameters​
The function uses the following parameters:
  * `messages`:[Required] This should be a list of input messages 
  * `model`:[Optional] This is the LiteLLM model being used. This parameter is optional, as you can alternatively specify the `max_tokens` parameter.
  * `max_tokens`:[Optional] This is an int, manually set upper limit on messages
  * `trim_ratio`:[Optional] This represents the target ratio of tokens to use following trimming. It's default value is 0.75, which implies that messages will be trimmed to utilise about 75%


Previous
Streaming + Async
Next
Function Calling
  * Usage
  * Usage - set max_tokens
  * Parameters


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Predicted Outputs


On this page
# Predicted Outputs
Property| Details  
---|---  
Description| Use this when most of the output of the LLM is known ahead of time. For instance, if you are asking the model to rewrite some text or code with only minor changes, you can reduce your latency significantly by using Predicted Outputs, passing in the existing content as your prediction.  
Supported providers| `openai`  
Link to OpenAI doc on Predicted Outputs| Predicted Outputs ↗  
Supported from LiteLLM Version| `v1.51.4`  
## Using Predicted Outputs​
  * LiteLLM Python SDK
  * LiteLLM Proxy Server


In this example we want to refactor a piece of C# code, and convert the Username property to Email instead:
```
import litellm  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
code = """  
/// <summary>  
/// Represents a user with a first name, last name, and username.  
/// </summary>  
public class User  
{  
  /// <summary>  
  /// Gets or sets the user's first name.  
  /// </summary>  
  public string FirstName { get; set; }  
  
  /// <summary>  
  /// Gets or sets the user's last name.  
  /// </summary>  
  public string LastName { get; set; }  
  
  /// <summary>  
  /// Gets or sets the user's username.  
  /// </summary>  
  public string Username { get; set; }  
}  
"""  
  
completion = litellm.completion(  
  model="gpt-4o-mini",  
  messages=[  
    {  
      "role": "user",  
      "content": "Replace the Username property with an Email property. Respond only with code, and with no markdown formatting.",  
    },  
    {"role": "user", "content": code},  
  ],  
  prediction={"type": "content", "content": code},  
)  
  
print(completion)  

```

  1. Define models on config.yaml


```
model_list:  
 - model_name: gpt-4o-mini # OpenAI gpt-4o-mini  
  litellm_params:  
   model: openai/gpt-4o-mini  
   api_key: os.environ/OPENAI_API_KEY   
  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Test it using the OpenAI Python SDK


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="LITELLM_PROXY_KEY", # sk-1234  
  base_url="LITELLM_PROXY_BASE" # http://0.0.0.0:4000  
)  
  
completion = client.chat.completions.create(  
  model="gpt-4o-mini",  
  messages=[  
    {  
      "role": "user",  
      "content": "Replace the Username property with an Email property. Respond only with code, and with no markdown formatting.",  
    },  
    {"role": "user", "content": code},  
  ],  
  prediction={"type": "content", "content": code},  
)  
  
print(completion)  

```

Previous
Prompt Caching
Next
Pre-fix Assistant Messages
  * Using Predicted Outputs


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Pre-fix Assistant Messages


On this page
# Pre-fix Assistant Messages
Supported by:
  * Deepseek
  * Mistral
  * Anthropic


```
{  
 "role": "assistant",   
 "content": "..",   
 ...  
 "prefix": true # 👈 KEY CHANGE  
}  

```

## Quick Start​
  * SDK
  * PROXY


```
from litellm import completion  
import os   
  
os.environ["DEEPSEEK_API_KEY"] = ""  
  
response = completion(  
 model="deepseek/deepseek-chat",  
 messages=[  
  {"role": "user", "content": "Who won the world cup in 2022?"},  
  {"role": "assistant", "content": "Argentina", "prefix": True}  
 ]  
)  
print(response.choices[0].message.content)  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "deepseek/deepseek-chat",  
  "messages": [  
   {  
    "role": "user",  
    "content": "Who won the world cup in 2022?"  
   },  
   {  
    "role": "assistant",   
    "content": "Argentina", "prefix": true  
   }  
  ]  
}'  

```

**Expected Response**
```
{  
  "id": "3b66124d79a708e10c603496b363574c",  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": " won the FIFA World Cup in 2022.",  
        "role": "assistant",  
        "tool_calls": null,  
        "function_call": null  
      }  
    }  
  ],  
  "created": 1723323084,  
  "model": "deepseek/deepseek-chat",  
  "object": "chat.completion",  
  "system_fingerprint": "fp_7e0991cad4",  
  "usage": {  
    "completion_tokens": 12,  
    "prompt_tokens": 16,  
    "total_tokens": 28,  
  },  
  "service_tier": null  
}  

```

## Check Model Support​
Call `litellm.get_model_info` to check if a model/provider supports `prefix`. 
  * SDK
  * PROXY


```
from litellm import get_model_info  
  
params = get_model_info(model="deepseek/deepseek-chat")  
  
assert params["supports_assistant_prefill"] is True  

```

Call the `/model/info` endpoint to get a list of models + their supported params.
```
curl -X GET 'http://0.0.0.0:4000/v1/model/info' \  
-H 'Authorization: Bearer $LITELLM_KEY' \  

```

Previous
Predicted Outputs
Next
Drop Unsupported Params
  * Quick Start
  * Check Model Support


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Prompt Caching


On this page
# Prompt Caching
Supported Providers:
  * OpenAI (`openai/`)
  * Anthropic API (`anthropic/`)
  * Bedrock (`bedrock/`, `bedrock/invoke/`, `bedrock/converse`) (All models bedrock supports prompt caching on)
  * Deepseek API (`deepseek/`)


For the supported providers, LiteLLM follows the OpenAI prompt caching usage object format:
```
"usage": {  
 "prompt_tokens": 2006,  
 "completion_tokens": 300,  
 "total_tokens": 2306,  
 "prompt_tokens_details": {  
  "cached_tokens": 1920  
 },  
 "completion_tokens_details": {  
  "reasoning_tokens": 0  
 }  
 # ANTHROPIC_ONLY #  
 "cache_creation_input_tokens": 0  
}  

```

  * `prompt_tokens`: These are the non-cached prompt tokens (same as Anthropic, equivalent to Deepseek `prompt_cache_miss_tokens`).
  * `completion_tokens`: These are the output tokens generated by the model.
  * `total_tokens`: Sum of prompt_tokens + completion_tokens.
  * `prompt_tokens_details`: Object containing cached_tokens.
    * `cached_tokens`: Tokens that were a cache-hit for that call.
  * `completion_tokens_details`: Object containing reasoning_tokens.
  * **ANTHROPIC_ONLY** : `cache_creation_input_tokens` are the number of tokens that were written to cache. (Anthropic charges for this).


## Quick Start​
Note: OpenAI caching is only available for prompts containing 1024 tokens or more
  * SDK
  * PROXY


```
from litellm import completion   
import os  
  
os.environ["OPENAI_API_KEY"] = ""  
  
for _ in range(2):  
  response = completion(  
    model="gpt-4o",  
    messages=[  
      # System Message  
      {  
        "role": "system",  
        "content": [  
          {  
            "type": "text",  
            "text": "Here is the full text of a complex legal agreement"  
            * 400,  
          }  
        ],  
      },  
      # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.  
      {  
        "role": "user",  
        "content": [  
          {  
            "type": "text",  
            "text": "What are the key terms and conditions in this agreement?",  
          }  
        ],  
      },  
      {  
        "role": "assistant",  
        "content": "Certainly! the key terms and conditions are the following: the contract is 1 year long for $10/mo",  
      },  
      # The final turn is marked with cache-control, for continuing in followups.  
      {  
        "role": "user",  
        "content": [  
          {  
            "type": "text",  
            "text": "What are the key terms and conditions in this agreement?",  
          }  
        ],  
      },  
    ],  
    temperature=0.2,  
    max_tokens=10,  
  )  
  
print("response=", response)  
print("response.usage=", response.usage)  
  
assert "prompt_tokens_details" in response.usage  
assert response.usage.prompt_tokens_details.cached_tokens > 0  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: gpt-4o  
   litellm_params:  
    model: openai/gpt-4o  
    api_key: os.environ/OPENAI_API_KEY  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
from openai import OpenAI   
import os  
  
client = OpenAI(  
  api_key="LITELLM_PROXY_KEY", # sk-1234  
  base_url="LITELLM_PROXY_BASE" # http://0.0.0.0:4000  
)  
  
for _ in range(2):  
  response = client.chat.completions.create(  
    model="gpt-4o",  
    messages=[  
      # System Message  
      {  
        "role": "system",  
        "content": [  
          {  
            "type": "text",  
            "text": "Here is the full text of a complex legal agreement"  
            * 400,  
          }  
        ],  
      },  
      # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.  
      {  
        "role": "user",  
        "content": [  
          {  
            "type": "text",  
            "text": "What are the key terms and conditions in this agreement?",  
          }  
        ],  
      },  
      {  
        "role": "assistant",  
        "content": "Certainly! the key terms and conditions are the following: the contract is 1 year long for $10/mo",  
      },  
      # The final turn is marked with cache-control, for continuing in followups.  
      {  
        "role": "user",  
        "content": [  
          {  
            "type": "text",  
            "text": "What are the key terms and conditions in this agreement?",  
          }  
        ],  
      },  
    ],  
    temperature=0.2,  
    max_tokens=10,  
  )  
  
print("response=", response)  
print("response.usage=", response.usage)  
  
assert "prompt_tokens_details" in response.usage  
assert response.usage.prompt_tokens_details.cached_tokens > 0  

```

### Anthropic Example​
Anthropic charges for cache writes. 
Specify the content to cache with `"cache_control": {"type": "ephemeral"}`.
If you pass that in for any other llm provider, it will be ignored. 
  * SDK
  * PROXY


```
from litellm import completion   
import litellm   
import os   
  
litellm.set_verbose = True # 👈 SEE RAW REQUEST  
os.environ["ANTHROPIC_API_KEY"] = ""   
  
response = completion(  
  model="anthropic/claude-3-5-sonnet-20240620",  
  messages=[  
    {  
      "role": "system",  
      "content": [  
        {  
          "type": "text",  
          "text": "You are an AI assistant tasked with analyzing legal documents.",  
        },  
        {  
          "type": "text",  
          "text": "Here is the full text of a complex legal agreement" * 400,  
          "cache_control": {"type": "ephemeral"},  
        },  
      ],  
    },  
    {  
      "role": "user",  
      "content": "what are the key terms and conditions in this agreement?",  
    },  
  ]  
)  
  
print(response.usage)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: claude-3-5-sonnet-20240620  
   litellm_params:  
    model: anthropic/claude-3-5-sonnet-20240620  
    api_key: os.environ/ANTHROPIC_API_KEY  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
from openai import OpenAI   
import os  
  
client = OpenAI(  
  api_key="LITELLM_PROXY_KEY", # sk-1234  
  base_url="LITELLM_PROXY_BASE" # http://0.0.0.0:4000  
)  
  
response = client.chat.completions.create(  
  model="claude-3-5-sonnet-20240620",  
  messages=[  
    {  
      "role": "system",  
      "content": [  
        {  
          "type": "text",  
          "text": "You are an AI assistant tasked with analyzing legal documents.",  
        },  
        {  
          "type": "text",  
          "text": "Here is the full text of a complex legal agreement" * 400,  
          "cache_control": {"type": "ephemeral"},  
        },  
      ],  
    },  
    {  
      "role": "user",  
      "content": "what are the key terms and conditions in this agreement?",  
    },  
  ]  
)  
  
print(response.usage)  

```

### Deepeek Example​
Works the same as OpenAI. 
```
from litellm import completion   
import litellm  
import os   
  
os.environ["DEEPSEEK_API_KEY"] = ""   
  
litellm.set_verbose = True # 👈 SEE RAW REQUEST  
  
model_name = "deepseek/deepseek-chat"  
messages_1 = [  
  {  
    "role": "system",  
    "content": "You are a history expert. The user will provide a series of questions, and your answers should be concise and start with `Answer:`",  
  },  
  {  
    "role": "user",  
    "content": "In what year did Qin Shi Huang unify the six states?",  
  },  
  {"role": "assistant", "content": "Answer: 221 BC"},  
  {"role": "user", "content": "Who was the founder of the Han Dynasty?"},  
  {"role": "assistant", "content": "Answer: Liu Bang"},  
  {"role": "user", "content": "Who was the last emperor of the Tang Dynasty?"},  
  {"role": "assistant", "content": "Answer: Li Zhu"},  
  {  
    "role": "user",  
    "content": "Who was the founding emperor of the Ming Dynasty?",  
  },  
  {"role": "assistant", "content": "Answer: Zhu Yuanzhang"},  
  {  
    "role": "user",  
    "content": "Who was the founding emperor of the Qing Dynasty?",  
  },  
]  
  
message_2 = [  
  {  
    "role": "system",  
    "content": "You are a history expert. The user will provide a series of questions, and your answers should be concise and start with `Answer:`",  
  },  
  {  
    "role": "user",  
    "content": "In what year did Qin Shi Huang unify the six states?",  
  },  
  {"role": "assistant", "content": "Answer: 221 BC"},  
  {"role": "user", "content": "Who was the founder of the Han Dynasty?"},  
  {"role": "assistant", "content": "Answer: Liu Bang"},  
  {"role": "user", "content": "Who was the last emperor of the Tang Dynasty?"},  
  {"role": "assistant", "content": "Answer: Li Zhu"},  
  {  
    "role": "user",  
    "content": "Who was the founding emperor of the Ming Dynasty?",  
  },  
  {"role": "assistant", "content": "Answer: Zhu Yuanzhang"},  
  {"role": "user", "content": "When did the Shang Dynasty fall?"},  
]  
  
response_1 = litellm.completion(model=model_name, messages=messages_1)  
response_2 = litellm.completion(model=model_name, messages=message_2)  
  
# Add any assertions here to check the response  
print(response_2.usage)  

```

## Calculate Cost​
Cost cache-hit prompt tokens can differ from cache-miss prompt tokens.
Use the `completion_cost()` function for calculating cost (handles prompt caching cost calculation as well). **See more helper functions**
```
cost = completion_cost(completion_response=response, model=model)  

```

### Usage​
  * SDK
  * PROXY


```
from litellm import completion, completion_cost  
import litellm   
import os   
  
litellm.set_verbose = True # 👈 SEE RAW REQUEST  
os.environ["ANTHROPIC_API_KEY"] = ""   
model = "anthropic/claude-3-5-sonnet-20240620"  
response = completion(  
  model=model,  
  messages=[  
    {  
      "role": "system",  
      "content": [  
        {  
          "type": "text",  
          "text": "You are an AI assistant tasked with analyzing legal documents.",  
        },  
        {  
          "type": "text",  
          "text": "Here is the full text of a complex legal agreement" * 400,  
          "cache_control": {"type": "ephemeral"},  
        },  
      ],  
    },  
    {  
      "role": "user",  
      "content": "what are the key terms and conditions in this agreement?",  
    },  
  ]  
)  
  
print(response.usage)  
  
cost = completion_cost(completion_response=response, model=model)   
  
formatted_string = f"${float(cost):.10f}"  
print(formatted_string)  

```

LiteLLM returns the calculated cost in the response headers - `x-litellm-response-cost`
```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="LITELLM_PROXY_KEY", # sk-1234..  
  base_url="LITELLM_PROXY_BASE" # http://0.0.0.0:4000  
)  
response = client.chat.completions.with_raw_response.create(  
  messages=[{  
    "role": "user",  
    "content": "Say this is a test",  
  }],  
  model="gpt-3.5-turbo",  
)  
print(response.headers.get('x-litellm-response-cost'))  
  
completion = response.parse() # get the object that `chat.completions.create()` would have returned  
print(completion)  

```

## Check Model Support​
Check if a model supports prompt caching with `supports_prompt_caching()`
  * SDK
  * PROXY


```
from litellm.utils import supports_prompt_caching  
  
supports_pc: bool = supports_prompt_caching(model="anthropic/claude-3-5-sonnet-20240620")  
  
assert supports_pc  

```

Use the `/model/info` endpoint to check if a model on the proxy supports prompt caching 
  1. Setup config.yaml 


```
model_list:  
  - model_name: claude-3-5-sonnet-20240620  
   litellm_params:  
    model: anthropic/claude-3-5-sonnet-20240620  
    api_key: os.environ/ANTHROPIC_API_KEY  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -L -X GET 'http://0.0.0.0:4000/v1/model/info' \  
-H 'Authorization: Bearer sk-1234' \  

```

**Expected Response**
```
{  
  "data": [  
    {  
      "model_name": "claude-3-5-sonnet-20240620",  
      "litellm_params": {  
        "model": "anthropic/claude-3-5-sonnet-20240620"  
      },  
      "model_info": {  
        "key": "claude-3-5-sonnet-20240620",  
        ...  
        "supports_prompt_caching": true # 👈 LOOK FOR THIS!  
      }  
    }  
  ]  
}  

```

This checks our maintained model info/cost map
Previous
'Thinking' / 'Reasoning Content'
Next
Predicted Outputs
  * Quick Start
    * Anthropic Example
    * Deepeek Example
  * Calculate Cost
    * Usage
  * Check Model Support


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Prompt Formatting


On this page
# Prompt Formatting
LiteLLM automatically translates the OpenAI ChatCompletions prompt format, to other models. You can control this by setting a custom prompt template for a model as well. 
## Huggingface Models​
LiteLLM supports Huggingface Chat Templates, and will automatically check if your huggingface model has a registered chat template (e.g. Mistral-7b).
For popular models (e.g. meta-llama/llama2), we have their templates saved as part of the package. 
**Stored Templates**
Model Name| Works for Models| Completion Call  
---|---|---  
mistralai/Mistral-7B-Instruct-v0.1| mistralai/Mistral-7B-Instruct-v0.1| `completion(model='huggingface/mistralai/Mistral-7B-Instruct-v0.1', messages=messages, api_base="your_api_endpoint")`  
meta-llama/Llama-2-7b-chat| All meta-llama llama2 chat models| `completion(model='huggingface/meta-llama/Llama-2-7b', messages=messages, api_base="your_api_endpoint")`  
tiiuae/falcon-7b-instruct| All falcon instruct models| `completion(model='huggingface/tiiuae/falcon-7b-instruct', messages=messages, api_base="your_api_endpoint")`  
mosaicml/mpt-7b-chat| All mpt chat models| `completion(model='huggingface/mosaicml/mpt-7b-chat', messages=messages, api_base="your_api_endpoint")`  
codellama/CodeLlama-34b-Instruct-hf| All codellama instruct models| `completion(model='huggingface/codellama/CodeLlama-34b-Instruct-hf', messages=messages, api_base="your_api_endpoint")`  
WizardLM/WizardCoder-Python-34B-V1.0| All wizardcoder models| `completion(model='huggingface/WizardLM/WizardCoder-Python-34B-V1.0', messages=messages, api_base="your_api_endpoint")`  
Phind/Phind-CodeLlama-34B-v2| All phind-codellama models| `completion(model='huggingface/Phind/Phind-CodeLlama-34B-v2', messages=messages, api_base="your_api_endpoint")`  
**Jump to code**
## Format Prompt Yourself​
You can also format the prompt yourself. Here's how: 
```
import litellm  
# Create your own custom prompt template   
litellm.register_prompt_template(  
    model="togethercomputer/LLaMA-2-7B-32K",  
    initial_prompt_value="You are a good assistant" # [OPTIONAL]  
    roles={  
      "system": {  
        "pre_message": "[INST] <<SYS>>\n", # [OPTIONAL]  
        "post_message": "\n<</SYS>>\n [/INST]\n" # [OPTIONAL]  
      },  
      "user": {   
        "pre_message": "[INST] ", # [OPTIONAL]  
        "post_message": " [/INST]" # [OPTIONAL]  
      },   
      "assistant": {  
        "pre_message": "\n" # [OPTIONAL]  
        "post_message": "\n" # [OPTIONAL]  
      }  
    }  
    final_prompt_value="Now answer as best you can:" # [OPTIONAL]  
)  
  
def test_huggingface_custom_model():  
  model = "huggingface/togethercomputer/LLaMA-2-7B-32K"  
  response = completion(model=model, messages=messages, api_base="https://my-huggingface-endpoint")  
  print(response['choices'][0]['message']['content'])  
  return response  
  
test_huggingface_custom_model()  

```

This is currently supported for Huggingface, TogetherAI, Ollama, and Petals. 
Other providers either have fixed prompt templates (e.g. Anthropic), or format it themselves (e.g. Replicate). If there's a provider we're missing coverage for, let us know! 
## All Providers​
Here's the code for how we format all providers. Let us know how we can improve this further
Provider| Model Name| Code  
---|---|---  
Anthropic| `claude-instant-1`, `claude-instant-1.2`, `claude-2`| Code  
OpenAI Text Completion| `text-davinci-003`, `text-curie-001`, `text-babbage-001`, `text-ada-001`, `babbage-002`, `davinci-002`,| Code  
Replicate| all model names starting with `replicate/`| Code  
Cohere| `command-nightly`, `command`, `command-light`, `command-medium-beta`, `command-xlarge-beta`, `command-r-plus`| Code  
Huggingface| all model names starting with `huggingface/`| Code  
OpenRouter| all model names starting with `openrouter/`| Code  
AI21| `j2-mid`, `j2-light`, `j2-ultra`| Code  
VertexAI| `text-bison`, `text-bison@001`, `chat-bison`, `chat-bison@001`, `chat-bison-32k`, `code-bison`, `code-bison@001`, `code-gecko@001`, `code-gecko@latest`, `codechat-bison`, `codechat-bison@001`, `codechat-bison-32k`| Code  
Bedrock| all model names starting with `bedrock/`| Code  
Sagemaker| `sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b`| Code  
TogetherAI| all model names starting with `together_ai/`| Code  
AlephAlpha| all model names starting with `aleph_alpha/`| Code  
Palm| all model names starting with `palm/`| Code  
NLP Cloud| all model names starting with `palm/`| Code  
Petals| all model names starting with `petals/`| Code  
Previous
Drop Unsupported Params
Next
Streaming + Async
  * Huggingface Models
  * Format Prompt Yourself
  * All Providers


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Provider-specific Params


On this page
# Provider-specific Params
Providers might offer params not supported by OpenAI (e.g. top_k). LiteLLM treats any non-openai param, as a provider-specific param, and passes it to the provider in the request body, as a kwarg. **See Reserved Params**
You can pass those in 2 ways: 
  * via completion(): We'll pass the non-openai param, straight to the provider as part of the request body.
    * e.g. `completion(model="claude-instant-1", top_k=3)`
  * via provider-specific config variable (e.g. `litellm.OpenAIConfig()`). 


## SDK Usage​
  * OpenAI
  * OpenAI Text Completion
  * Azure OpenAI
  * Anthropic
  * Huggingface
  * TogetherAI
  * Ollama
  * Replicate
  * Petals
  * Palm
  * AI21
  * Cohere


```
import litellm, os  
  
# set env variables  
os.environ["OPENAI_API_KEY"] = "your-openai-key"  
  
## SET MAX TOKENS - via completion()   
response_1 = litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.OpenAIConfig(max_tokens=10)  
  
response_2 = litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os  
  
# set env variables  
os.environ["OPENAI_API_KEY"] = "your-openai-key"  
  
  
## SET MAX TOKENS - via completion()   
response_1 = litellm.completion(  
      model="text-davinci-003",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.OpenAITextCompletionConfig(max_tokens=10)  
response_2 = litellm.completion(  
      model="text-davinci-003",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os  
  
# set env variables  
os.environ["AZURE_API_BASE"] = "your-azure-api-base"  
os.environ["AZURE_API_TYPE"] = "azure" # [OPTIONAL]   
os.environ["AZURE_API_VERSION"] = "2023-07-01-preview" # [OPTIONAL]  
  
## SET MAX TOKENS - via completion()   
response_1 = litellm.completion(  
      model="azure/chatgpt-v-2",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.AzureOpenAIConfig(max_tokens=10)  
response_2 = litellm.completion(  
      model="azure/chatgpt-v-2",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
# set env variables  
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"  
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="claude-instant-1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.AnthropicConfig(max_tokens_to_sample=200)  
response_2 = litellm.completion(  
      model="claude-instant-1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
# set env variables  
os.environ["HUGGINGFACE_API_KEY"] = "your-huggingface-key" #[OPTIONAL]  
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="huggingface/mistralai/Mistral-7B-Instruct-v0.1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      api_base="https://your-huggingface-api-endpoint",  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.HuggingfaceConfig(max_new_tokens=200)  
response_2 = litellm.completion(  
      model="huggingface/mistralai/Mistral-7B-Instruct-v0.1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      api_base="https://your-huggingface-api-endpoint"  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
# set env variables  
os.environ["TOGETHERAI_API_KEY"] = "your-togetherai-key"   
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="together_ai/togethercomputer/llama-2-70b-chat",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.TogetherAIConfig(max_tokens_to_sample=200)  
response_2 = litellm.completion(  
      model="together_ai/togethercomputer/llama-2-70b-chat",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="ollama/llama2",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.OllamConfig(num_predict=200)  
response_2 = litellm.completion(  
      model="ollama/llama2",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
# set env variables  
os.environ["REPLICATE_API_KEY"] = "your-replicate-key"   
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="replicate/meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.ReplicateConfig(max_new_tokens=200)  
response_2 = litellm.completion(  
      model="replicate/meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm  
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="petals/petals-team/StableBeluga2",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      api_base="https://chat.petals.dev/api/v1/generate",  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.PetalsConfig(max_new_tokens=10)  
response_2 = litellm.completion(  
      model="petals/petals-team/StableBeluga2",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      api_base="https://chat.petals.dev/api/v1/generate",  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
# set env variables  
os.environ["PALM_API_KEY"] = "your-palm-key"   
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="palm/chat-bison",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.PalmConfig(maxOutputTokens=10)  
response_2 = litellm.completion(  
      model="palm/chat-bison",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
# set env variables  
os.environ["AI21_API_KEY"] = "your-ai21-key"   
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="j2-mid",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.AI21Config(maxOutputTokens=10)  
response_2 = litellm.completion(  
      model="j2-mid",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

```
import litellm, os   
  
# set env variables  
os.environ["COHERE_API_KEY"] = "your-cohere-key"    
  
## SET MAX TOKENS - via completion()  
response_1 = litellm.completion(  
      model="command-nightly",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      max_tokens=10  
    )  
  
response_1_text = response_1.choices[0].message.content  
  
## SET MAX TOKENS - via config  
litellm.CohereConfig(max_tokens=200)  
response_2 = litellm.completion(  
      model="command-nightly",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
    )  
  
response_2_text = response_2.choices[0].message.content  
  
## TEST OUTPUT  
assert len(response_2_text) > len(response_1_text)  

```

**Check out the tutorial!**
## Proxy Usage​
**via Config**
```
model_list:  
  - model_name: llama-3-8b-instruct  
   litellm_params:  
    model: predibase/llama-3-8b-instruct  
    api_key: os.environ/PREDIBASE_API_KEY  
    tenant_id: os.environ/PREDIBASE_TENANT_ID  
    max_tokens: 256  
    adapter_base: <my-special_base> # 👈 PROVIDER-SPECIFIC PARAM  

```

**via Request**
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
 "model": "llama-3-8b-instruct",  
 "messages": [  
  {  
   "role": "user",  
   "content": "What'\''s the weather like in Boston today?"  
  }  
 ],  
 "adapater_id": "my-special-adapter-id" # 👈 PROVIDER-SPECIFIC PARAM  
 }'  

```

Previous
Exception Mapping
Next
Calling Finetuned Models
  * SDK Usage
  * Proxy Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Streaming + Async


On this page
# Streaming + Async
Feature| LiteLLM SDK| LiteLLM Proxy  
---|---|---  
Streaming| ✅ start here| ✅ start here  
Async| ✅ start here| ✅ start here  
Async Streaming| ✅ start here| ✅ start here  
## Streaming Responses​
LiteLLM supports streaming the model response back by passing `stream=True` as an argument to the completion function
### Usage​
```
from litellm import completion  
messages = [{"role": "user", "content": "Hey, how's it going?"}]  
response = completion(model="gpt-3.5-turbo", messages=messages, stream=True)  
for part in response:  
  print(part.choices[0].delta.content or "")  

```

### Helper function​
LiteLLM also exposes a helper function to rebuild the complete streaming response from the list of chunks. 
```
from litellm import completion  
messages = [{"role": "user", "content": "Hey, how's it going?"}]  
response = completion(model="gpt-3.5-turbo", messages=messages, stream=True)  
  
for chunk in response:   
  chunks.append(chunk)  
  
print(litellm.stream_chunk_builder(chunks, messages=messages))  

```

## Async Completion​
Asynchronous Completion with LiteLLM. LiteLLM provides an asynchronous version of the completion function called `acompletion`
### Usage​
```
from litellm import acompletion  
import asyncio  
  
async def test_get_response():  
  user_message = "Hello, how are you?"  
  messages = [{"content": user_message, "role": "user"}]  
  response = await acompletion(model="gpt-3.5-turbo", messages=messages)  
  return response  
  
response = asyncio.run(test_get_response())  
print(response)  
  

```

## Async Streaming​
We've implemented an `__anext__()` function in the streaming object returned. This enables async iteration over the streaming object. 
### Usage​
Here's an example of using it with openai.
```
from litellm import acompletion  
import asyncio, os, traceback  
  
async def completion_call():  
  try:  
    print("test acompletion + streaming")  
    response = await acompletion(  
      model="gpt-3.5-turbo",   
      messages=[{"content": "Hello, how are you?", "role": "user"}],   
      stream=True  
    )  
    print(f"response: {response}")  
    async for chunk in response:  
      print(chunk)  
  except:  
    print(f"error occurred: {traceback.format_exc()}")  
    pass  
  
asyncio.run(completion_call())  

```

## Error Handling - Infinite Loops​
Sometimes a model might enter an infinite loop, and keep repeating the same chunks - e.g. issue
Break out of it with: 
```
litellm.REPEATED_STREAMING_CHUNK_LIMIT = 100 # # catch if model starts looping the same chunk while streaming. Uses high default to prevent false positives.  

```

LiteLLM provides error handling for this, by checking if a chunk is repeated 'n' times (Default is 100). If it exceeds that limit, it will raise a `litellm.InternalServerError`, to allow retry logic to happen. 
  * SDK
  * PROXY


```
import litellm   
import os   
  
litellm.set_verbose = False  
loop_amount = litellm.REPEATED_STREAMING_CHUNK_LIMIT + 1  
chunks = [  
  litellm.ModelResponse(**{  
  "id": "chatcmpl-123",  
  "object": "chat.completion.chunk",  
  "created": 1694268190,  
  "model": "gpt-3.5-turbo-0125",  
  "system_fingerprint": "fp_44709d6fcb",  
  "choices": [  
    {"index": 0, "delta": {"content": "How are you?"}, "finish_reason": "stop"}  
  ],  
}, stream=True)  
] * loop_amount  
completion_stream = litellm.ModelResponseListIterator(model_responses=chunks)  
  
response = litellm.CustomStreamWrapper(  
  completion_stream=completion_stream,  
  model="gpt-3.5-turbo",  
  custom_llm_provider="cached_response",  
  logging_obj=litellm.Logging(  
    model="gpt-3.5-turbo",  
    messages=[{"role": "user", "content": "Hey"}],  
    stream=True,  
    call_type="completion",  
    start_time=time.time(),  
    litellm_call_id="12345",  
    function_id="1245",  
  ),  
)  
  
for chunk in response:  
  continue # expect to raise InternalServerError   

```

Define this on your config.yaml on the proxy. 
```
litellm_settings:  
  REPEATED_STREAMING_CHUNK_LIMIT: 100 # this overrides the litellm default  

```

The proxy uses the litellm SDK. To validate this works, try the 'SDK' code snippet. 
Previous
Prompt Formatting
Next
Trimming Input Messages
  * Streaming Responses
    * Usage
    * Helper function
  * Async Completion
    * Usage
  * Async Streaming
    * Usage
  * Error Handling - Infinite Loops


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
      * Input Params
      * Output
      * Usage
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /chat/completions
  * Usage


On this page
# Usage
LiteLLM returns the OpenAI compatible usage object across all providers.
```
"usage": {  
  "prompt_tokens": int,  
  "completion_tokens": int,  
  "total_tokens": int  
 }  

```

## Quick Start​
```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  
  
print(response.usage)  

```

## Streaming Usage​
if `stream_options={"include_usage": True}` is set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value.
```
from litellm import completion   
  
completion = completion(  
 model="gpt-4o",  
 messages=[  
  {"role": "system", "content": "You are a helpful assistant."},  
  {"role": "user", "content": "Hello!"}  
 ],  
 stream=True,  
 stream_options={"include_usage": True}  
)  
  
for chunk in completion:  
 print(chunk.choices[0].delta)  
  

```

Previous
Output
Next
/responses [Beta]
  * Quick Start
  * Streaming Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * Completion Token Usage & Cost


On this page
# Completion Token Usage & Cost
By default LiteLLM returns token usage in all completion requests (See here)
LiteLLM returns `response_cost` in all calls. 
```
from litellm import completion   
  
response = litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{"role": "user", "content": "Hey, how's it going?"}],  
      mock_response="Hello world",  
    )  
  
print(response._hidden_params["response_cost"])  

```

LiteLLM also exposes some helper functions:
  * `encode`: This encodes the text passed in, using the model-specific tokenizer. **Jump to code**
  * `decode` : This decodes the tokens passed in, using the model-specific tokenizer. **Jump to code**
  * `token_counter` : This returns the number of tokens for a given input - it uses the tokenizer based on the model, and defaults to tiktoken if no model-specific tokenizer is available. **Jump to code**
  * `create_pretrained_tokenizer` and `create_tokenizer`: LiteLLM provides default tokenizer support for OpenAI, Cohere, Anthropic, Llama2, and Llama3 models. If you are using a different model, you can create a custom tokenizer and pass it as `custom_tokenizer` to the `encode`, `decode`, and `token_counter` methods. **Jump to code**
  * `cost_per_token` : This returns the cost (in USD) for prompt (input) and completion (output) tokens. Uses the live list from `api.litellm.ai`. **Jump to code**
  * `completion_cost` : This returns the overall cost (in USD) for a given LLM API Call. It combines `token_counter` and `cost_per_token` to return the cost for that query (counting both cost of input and output). **Jump to code**
  * `get_max_tokens` : This returns the maximum number of tokens allowed for the given model. **Jump to code**
  * `model_cost` : This returns a dictionary for all models, with their max_tokens, input_cost_per_token and output_cost_per_token. It uses the `api.litellm.ai` call shown below. **Jump to code**
  * `register_model` : This registers new / overrides existing models (and their pricing details) in the model cost dictionary. **Jump to code**
  * `api.litellm.ai` : Live token + price count across all supported models. **Jump to code**


📣 This is a community maintained list. Contributions are welcome! ❤️
## Example Usage​
### 1. `encode`​
Encoding has model-specific tokenizers for anthropic, cohere, llama2 and openai. If an unsupported model is passed in, it'll default to using tiktoken (openai's tokenizer).
```
from litellm import encode, decode  
  
sample_text = "Hellö World, this is my input string!"  
# openai encoding + decoding  
openai_tokens = encode(model="gpt-3.5-turbo", text=sample_text)  
print(openai_tokens)  

```

### 2. `decode`​
Decoding is supported for anthropic, cohere, llama2 and openai.
```
from litellm import encode, decode  
  
sample_text = "Hellö World, this is my input string!"  
# openai encoding + decoding  
openai_tokens = encode(model="gpt-3.5-turbo", text=sample_text)  
openai_text = decode(model="gpt-3.5-turbo", tokens=openai_tokens)  
print(openai_text)  

```

### 3. `token_counter`​
```
from litellm import token_counter  
  
messages = [{"user": "role", "content": "Hey, how's it going"}]  
print(token_counter(model="gpt-3.5-turbo", messages=messages))  

```

### 4. `create_pretrained_tokenizer` and `create_tokenizer`​
```
from litellm import create_pretrained_tokenizer, create_tokenizer  
  
# get tokenizer from huggingface repo  
custom_tokenizer_1 = create_pretrained_tokenizer("Xenova/llama-3-tokenizer")  
  
# use tokenizer from json file  
with open("tokenizer.json") as f:  
  json_data = json.load(f)  
  
json_str = json.dumps(json_data)  
  
custom_tokenizer_2 = create_tokenizer(json_str)  

```

### 5. `cost_per_token`​
```
from litellm import cost_per_token  
  
prompt_tokens = 5  
completion_tokens = 10  
prompt_tokens_cost_usd_dollar, completion_tokens_cost_usd_dollar = cost_per_token(model="gpt-3.5-turbo", prompt_tokens=prompt_tokens, completion_tokens=completion_tokens))  
  
print(prompt_tokens_cost_usd_dollar, completion_tokens_cost_usd_dollar)  

```

### 6. `completion_cost`​
  * Input: Accepts a `litellm.completion()` response **OR** prompt + completion strings
  * Output: Returns a `float` of cost for the `completion` call 


**litellm.completion()**
```
from litellm import completion, completion_cost  
  
response = completion(  
      model="bedrock/anthropic.claude-v2",  
      messages=messages,  
      request_timeout=200,  
    )  
# pass your response from completion to completion_cost  
cost = completion_cost(completion_response=response)  
formatted_string = f"${float(cost):.10f}"  
print(formatted_string)  

```

**prompt + completion string**
```
from litellm import completion_cost  
cost = completion_cost(model="bedrock/anthropic.claude-v2", prompt="Hey!", completion="How's it going?")  
formatted_string = f"${float(cost):.10f}"  
print(formatted_string)  

```

### 7. `get_max_tokens`​
Input: Accepts a model name - e.g., gpt-3.5-turbo (to get a complete list, call litellm.model_list). Output: Returns the maximum number of tokens allowed for the given model
```
from litellm import get_max_tokens   
  
model = "gpt-3.5-turbo"  
  
print(get_max_tokens(model)) # Output: 4097  

```

### 8. `model_cost`​
  * Output: Returns a dict object containing the max_tokens, input_cost_per_token, output_cost_per_token for all models on community-maintained list


```
from litellm import model_cost   
  
print(model_cost) # {'gpt-3.5-turbo': {'max_tokens': 4000, 'input_cost_per_token': 1.5e-06, 'output_cost_per_token': 2e-06}, ...}  

```

### 9. `register_model`​
  * Input: Provide EITHER a model cost dictionary or a url to a hosted json blob
  * Output: Returns updated model_cost dictionary + updates litellm.model_cost with model details. 


**Dictionary**
```
from litellm import register_model  
  
litellm.register_model({  
    "gpt-4": {  
    "max_tokens": 8192,   
    "input_cost_per_token": 0.00002,   
    "output_cost_per_token": 0.00006,   
    "litellm_provider": "openai",   
    "mode": "chat"  
  },  
})  

```

**URL for json blob**
```
import litellm  
  
litellm.register_model(model_cost=  
"https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json")  

```

**Don't pull hosted model_cost_map**  
If you have firewalls, and want to just use the local copy of the model cost map, you can do so like this:
```
export LITELLM_LOCAL_MODEL_COST_MAP="True"  

```

Note: this means you will need to upgrade to get updated pricing, and newer models.
Previous
Setting API Keys, Base, Version
Next
Custom Pricing - SageMaker, Azure, etc
  * Example Usage
    * 1. `encode`
    * 2. `decode`
    * 3. `token_counter`
    * 4. `create_pretrained_tokenizer` and `create_tokenizer`
    * 5. `cost_per_token`
    * 6. `completion_cost`
    * 7. `get_max_tokens`
    * 8. `model_cost`
    * 9. `register_model`


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Using Vision Models


On this page
# Using Vision Models
## Quick Start​
Example passing images to a model 
  * LiteLLMPython SDK
  * LiteLLM Proxy Server


```
import os   
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
# openai call  
response = completion(  
  model = "gpt-4-vision-preview",   
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

  1. Define vision models on config.yaml


```
model_list:  
 - model_name: gpt-4-vision-preview # OpenAI gpt-4-vision-preview  
  litellm_params:  
   model: openai/gpt-4-vision-preview  
   api_key: os.environ/OPENAI_API_KEY  
 - model_name: llava-hf     # Custom OpenAI compatible model  
  litellm_params:  
   model: openai/llava-hf/llava-v1.6-vicuna-7b-hf  
   api_base: http://localhost:8000  
   api_key: fake-key  
  model_info:  
   supports_vision: True    # set supports_vision to True so /model/info returns this attribute as True  
  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Test it using the OpenAI Python SDK


```
import os   
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-1234", # your litellm proxy api key  
)  
  
response = client.chat.completions.create(  
  model = "gpt-4-vision-preview", # use model="llava-hf" to test your custom OpenAI endpoint  
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

## Checking if a model supports `vision`​
  * LiteLLM Python SDK
  * LiteLLM Proxy Server


Use `litellm.supports_vision(model="")` -> returns `True` if model supports `vision` and `False` if not
```
assert litellm.supports_vision(model="openai/gpt-4-vision-preview") == True  
assert litellm.supports_vision(model="vertex_ai/gemini-1.0-pro-vision") == True  
assert litellm.supports_vision(model="openai/gpt-3.5-turbo") == False  
assert litellm.supports_vision(model="xai/grok-2-vision-latest") == True  
assert litellm.supports_vision(model="xai/grok-2-latest") == False  

```

  1. Define vision models on config.yaml


```
model_list:  
 - model_name: gpt-4-vision-preview # OpenAI gpt-4-vision-preview  
  litellm_params:  
   model: openai/gpt-4-vision-preview  
   api_key: os.environ/OPENAI_API_KEY  
 - model_name: llava-hf     # Custom OpenAI compatible model  
  litellm_params:  
   model: openai/llava-hf/llava-v1.6-vicuna-7b-hf  
   api_base: http://localhost:8000  
   api_key: fake-key  
  model_info:  
   supports_vision: True    # set supports_vision to True so /model/info returns this attribute as True  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Call `/model_group/info` to check if your model supports `vision`


```
curl -X 'GET' \  
 'http://localhost:4000/model_group/info' \  
 -H 'accept: application/json' \  
 -H 'x-api-key: sk-1234'  

```

Expected Response 
```
{  
 "data": [  
  {  
   "model_group": "gpt-4-vision-preview",  
   "providers": ["openai"],  
   "max_input_tokens": 128000,  
   "max_output_tokens": 4096,  
   "mode": "chat",  
   "supports_vision": true, # 👈 supports_vision is true  
   "supports_function_calling": false  
  },  
  {  
   "model_group": "llava-hf",  
   "providers": ["openai"],  
   "max_input_tokens": null,  
   "max_output_tokens": null,  
   "mode": null,  
   "supports_vision": true, # 👈 supports_vision is true  
   "supports_function_calling": false  
  }  
 ]  
}  

```

## Explicitly specify image type​
If you have images without a mime-type, or if litellm is incorrectly inferring the mime type of your image (e.g. calling `gs://` url's with vertex ai), you can set this explicitly via the `format` param. 
```
"image_url": {  
 "url": "gs://my-gs-image",  
 "format": "image/jpeg"  
}  

```

LiteLLM will use this for any API endpoint, which supports specifying mime-type (e.g. anthropic/bedrock/vertex ai). 
For others (e.g. openai), it will be ignored. 
  * SDK
  * PROXY


```
import os   
from litellm import completion  
  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
# openai call  
response = completion(  
  model = "claude-3-7-sonnet-latest",   
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                 "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",  
                 "format": "image/jpeg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

  1. Define vision models on config.yaml


```
model_list:  
 - model_name: gpt-4-vision-preview # OpenAI gpt-4-vision-preview  
  litellm_params:  
   model: openai/gpt-4-vision-preview  
   api_key: os.environ/OPENAI_API_KEY  
 - model_name: llava-hf     # Custom OpenAI compatible model  
  litellm_params:  
   model: openai/llava-hf/llava-v1.6-vicuna-7b-hf  
   api_base: http://localhost:8000  
   api_key: fake-key  
  model_info:  
   supports_vision: True    # set supports_vision to True so /model/info returns this attribute as True  
  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Test it using the OpenAI Python SDK


```
import os   
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-1234", # your litellm proxy api key  
)  
  
response = client.chat.completions.create(  
  model = "gpt-4-vision-preview", # use model="llava-hf" to test your custom OpenAI endpoint  
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",  
                "format": "image/jpeg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

## Spec​
```
"image_url": str  
  
OR   
  
"image_url": {  
 "url": "url OR base64 encoded str",  
 "detail": "openai-only param",   
 "format": "specify mime-type of image"  
}  

```

Previous
Using PDF Input
Next
Structured Outputs (JSON Mode)
  * Quick Start
  * Checking if a model supports `vision`
  * Explicitly specify image type
  * Spec


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# Page Not Found
We could not find what you were looking for.
Please contact the owner of the site that linked you to the original URL and let them know their link is broken.
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
    * Data Privacy and Security
    * Data Retention Policy
    * Migration Policy
    * ❤️ 🚅 Projects built on LiteLLM
    * PII Masking - LiteLLM Gateway (Deprecated Version)
    * Code Quality
    * Rules
    * [DEPRECATED] Team-based Routing
    * [DEPRECATED] Region-based Routing
    * [OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
  * Support & Talk with founders


  *   * Extras
  * Data Retention Policy


On this page
# Data Retention Policy
## LiteLLM Cloud​
### Purpose​
This policy outlines the requirements and controls/procedures LiteLLM Cloud has implemented to manage the retention and deletion of customer data.
### Policy​
For Customers
  1. Active Accounts


  * Customer data is retained for as long as the customer’s account is in active status. This includes data such as prompts, generated content, logs, and usage metrics.


  1. Voluntary Account Closure


  * Data enters an “expired” state when the account is voluntarily closed.
  * Expired account data will be retained for 30 days (adjust as needed).
  * After this period, the account and all related data will be permanently removed from LiteLLM Cloud systems.
  * Customers who wish to voluntarily close their account should download or back up their data (manually or via available APIs) before initiating the closure process.


  1. Involuntary Suspension


  * If a customer account is involuntarily suspended (e.g., due to non-payment or violation of Terms of Service), there is a 14-day (adjust as needed) grace period during which the account will be inaccessible but can be reopened if the customer resolves the issues leading to suspension.
  * After the grace period, if the account remains unresolved, it will be closed and the data will enter the “expired” state.
  * Once data is in the “expired” state, it will be permanently removed 30 days (adjust as needed) thereafter, unless legal requirements dictate otherwise.


  1. Manual Backup of Suspended Accounts


  * If a customer wishes to manually back up data contained in a suspended account, they must bring the account back to good standing (by resolving payment or policy violations) to regain interface/API access.
  * Data from a suspended account will not be accessible while the account is in suspension status.
  * After 14 days of suspension (adjust as needed), if no resolution is reached, the account is closed and data follows the standard “expired” data removal timeline stated above.


  1. Custom Retention Policies


  * Enterprise customers can configure custom data retention periods based on their specific compliance and business requirements.
  * Available customization options include:
    * Adjusting the retention period for active data (0-365 days)
  * Custom retention policies must be configured through the LiteLLM Cloud dashboard or via API


### Protection of Records​
  * LiteLLM Cloud takes measures to ensure that all records under its control are protected against loss, destruction, falsification, and unauthorized access or disclosure. These measures are aligned with relevant legislative, regulatory, contractual, and business obligations.
  * When working with a third-party CSP, LiteLLM Cloud requests comprehensive information regarding the CSP’s security mechanisms to protect data, including records stored or processed on behalf of LiteLLM Cloud.
  * Cloud service providers engaged by LiteLLM Cloud must disclose their safeguarding practices for records they gather and store on LiteLLM Cloud’s behalf.


Previous
Data Privacy and Security
Next
Migration Policy
  * LiteLLM Cloud
    * Purpose
    * Policy
    * Protection of Records


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
    * Contributing Code
    * Adding Providers
    * Contributing to Documentation
    * Contributing - UI
  * Extras
  * Support & Talk with founders


  *   * Contributing
  * Contributing - UI


On this page
# Contributing - UI
Here's how to run the LiteLLM UI locally for making changes: 
## 1. Clone the repo​
```
git clone https://github.com/BerriAI/litellm.git  

```

## 2. Start the UI + Proxy​
**2.1 Start the proxy on port 4000**
Tell the proxy where the UI is located
```
export PROXY_BASE_URL="http://localhost:3000/"  

```

```
cd litellm/litellm/proxy  
python3 proxy_cli.py --config /path/to/config.yaml --port 4000  

```

**2.2 Start the UI**
Set the mode as development (this will assume the proxy is running on localhost:4000)
```
export NODE_ENV="development"   

```

```
cd litellm/ui/litellm-dashboard  
  
npm run dev  
  
# starts on http://0.0.0.0:3000/ui  

```

## 3. Go to local UI​
```
http://0.0.0.0:3000/ui  

```

Previous
Contributing to Documentation
Next
Data Privacy and Security
  * 1. Clone the repo
  * 2. Start the UI + Proxy
  * 3. Go to local UI


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Using Web Search


On this page
# Using Web Search
Use web search with litellm
Feature| Details  
---|---  
Supported Endpoints| - `/chat/completions`   
- `/responses`  
Supported Providers| `openai`  
LiteLLM Cost Tracking| ✅ Supported  
LiteLLM Version| `v1.63.15-nightly` or higher  
## `/chat/completions` (litellm.completion)​
### Quick Start​
  * SDK
  * PROXY


```
from litellm import completion  
  
response = completion(  
  model="openai/gpt-4o-search-preview",  
  messages=[  
    {  
      "role": "user",  
      "content": "What was a positive news story from today?",  
    }  
  ],  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-4o-search-preview  
  litellm_params:  
   model: openai/gpt-4o-search-preview  
   api_key: os.environ/OPENAI_API_KEY  

```

  1. Start the proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
from openai import OpenAI  
  
# Point to your proxy server  
client = OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="gpt-4o-search-preview",  
  messages=[  
    {  
      "role": "user",  
      "content": "What was a positive news story from today?"  
    }  
  ]  
)  

```

### Search context size​
  * SDK
  * PROXY


```
from litellm import completion  
  
# Customize search context size  
response = completion(  
  model="openai/gpt-4o-search-preview",  
  messages=[  
    {  
      "role": "user",  
      "content": "What was a positive news story from today?",  
    }  
  ],  
  web_search_options={  
    "search_context_size": "low" # Options: "low", "medium" (default), "high"  
  }  
)  

```

```
from openai import OpenAI  
  
# Point to your proxy server  
client = OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
# Customize search context size  
response = client.chat.completions.create(  
  model="gpt-4o-search-preview",  
  messages=[  
    {  
      "role": "user",  
      "content": "What was a positive news story from today?"  
    }  
  ],  
  web_search_options={  
    "search_context_size": "low" # Options: "low", "medium" (default), "high"  
  }  
)  

```

## `/responses` (litellm.responses)​
### Quick Start​
  * SDK
  * PROXY


```
from litellm import responses  
  
response = responses(  
  model="openai/gpt-4o",  
  input=[  
    {  
      "role": "user",  
      "content": "What was a positive news story from today?"  
    }  
  ],  
  tools=[{  
    "type": "web_search_preview" # enables web search with default medium context size  
  }]  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-4o  
  litellm_params:  
   model: openai/gpt-4o  
   api_key: os.environ/OPENAI_API_KEY  

```

  1. Start the proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
from openai import OpenAI  
  
# Point to your proxy server  
client = OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.responses.create(  
  model="gpt-4o",  
  tools=[{  
    "type": "web_search_preview"  
  }],  
  input="What was a positive news story from today?",  
)  
  
print(response.output_text)  

```

### Search context size​
  * SDK
  * PROXY


```
from litellm import responses  
  
# Customize search context size  
response = responses(  
  model="openai/gpt-4o",  
  input=[  
    {  
      "role": "user",  
      "content": "What was a positive news story from today?"  
    }  
  ],  
  tools=[{  
    "type": "web_search_preview",  
    "search_context_size": "low" # Options: "low", "medium" (default), "high"  
  }]  
)  

```

```
from openai import OpenAI  
  
# Point to your proxy server  
client = OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
# Customize search context size  
response = client.responses.create(  
  model="gpt-4o",  
  tools=[{  
    "type": "web_search_preview",  
    "search_context_size": "low" # Options: "low", "medium" (default), "high"  
  }],  
  input="What was a positive news story from today?",  
)  
  
print(response.output_text)  

```

## Checking if a model supports web search​
  * SDK
  * PROXY


Use `litellm.supports_web_search(model="openai/gpt-4o-search-preview")` -> returns `True` if model can perform web searches
```
assert litellm.supports_web_search(model="openai/gpt-4o-search-preview") == True  

```

  1. Define OpenAI models in config.yaml


```
model_list:  
 - model_name: gpt-4o-search-preview  
  litellm_params:  
   model: openai/gpt-4o-search-preview  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   supports_web_search: True  

```

  1. Run proxy server


```
litellm --config config.yaml  

```

  1. Call `/model_group/info` to check if a model supports web search


```
curl -X 'GET' \  
 'http://localhost:4000/model_group/info' \  
 -H 'accept: application/json' \  
 -H 'x-api-key: sk-1234'  

```

Expected Response 
```
{  
 "data": [  
  {  
   "model_group": "gpt-4o-search-preview",  
   "providers": ["openai"],  
   "max_tokens": 128000,  
   "supports_web_search": true, # 👈 supports_web_search is true  
  }  
 ]  
}  

```

Previous
Using Audio Models
Next
Using PDF Input
  * `/chat/completions` (litellm.completion)
    * Quick Start
    * Search context size
  * `/responses` (litellm.responses)
    * Quick Start
    * Search context size
  * Checking if a model supports web search


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Local Debugging


On this page
# Local Debugging
There's 2 ways to do local debugging - `litellm._turn_on_debug()` and by passing in a custom function `completion(...logger_fn=<your_local_function>)`. Warning: Make sure to not use `_turn_on_debug()` in production. It logs API keys, which might end up in log files.
## Set Verbose​
This is good for getting print statements for everything litellm is doing.
```
import litellm  
from litellm import completion  
  
litellm._turn_on_debug() # 👈 this is the 1-line change you need to make  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "openai key"  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
# openai call  
response = completion(model="gpt-3.5-turbo", messages=messages)  
  
# cohere call  
response = completion("command-nightly", messages)  

```

## JSON Logs​
If you need to store the logs as JSON, just set the `litellm.json_logs = True`.
We currently just log the raw POST request from litellm as a JSON - [**See Code**]. 
Share feedback here
## Logger Function​
But sometimes all you care about is seeing exactly what's getting sent to your api call and what's being returned - e.g. if the api call is failing, why is that happening? what are the exact params being set? 
In that case, LiteLLM allows you to pass in a custom logging function to see / modify the model call Input/Outputs. 
**Note** : We expect you to accept a dict object. 
Your custom function 
```
def my_custom_logging_fn(model_call_dict):  
  print(f"model call details: {model_call_dict}")  

```

### Complete Example​
```
from litellm import completion  
  
def my_custom_logging_fn(model_call_dict):  
  print(f"model call details: {model_call_dict}")  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "openai key"  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
# openai call  
response = completion(model="gpt-3.5-turbo", messages=messages, logger_fn=my_custom_logging_fn)  
  
# cohere call  
response = completion("command-nightly", messages, logger_fn=my_custom_logging_fn)  

```

## Still Seeing Issues?​
Text us @ +17708783106 or Join the Discord. 
We promise to help you in `lite`ning speed ❤️
Previous
Phoenix OSS
Next
Raw Request/Response Logging
  * Set Verbose
  * JSON Logs
  * Logger Function
    * Complete Example
  * Still Seeing Issues?


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * litellm.aembedding()


On this page
# litellm.aembedding()
LiteLLM provides an asynchronous version of the `embedding` function called `aembedding`
### Usage​
```
from litellm import aembedding  
import asyncio  
  
async def test_get_response():  
  response = await aembedding('text-embedding-ada-002', input=["good morning from litellm"])  
  return response  
  
response = asyncio.run(test_get_response())  
print(response)  

```

Previous
Custom Pricing - SageMaker, Azure, etc
Next
litellm.moderation()
  * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /embeddings


On this page
# /embeddings
## Quick Start​
```
from litellm import embedding  
import os  
os.environ['OPENAI_API_KEY'] = ""  
response = embedding(model='text-embedding-ada-002', input=["good morning from litellm"])  

```

## Proxy Usage​
**NOTE** For `vertex_ai`,
```
export GOOGLE_APPLICATION_CREDENTIALS="absolute/path/to/service_account.json"  

```

### Add model to config​
```
model_list:  
- model_name: textembedding-gecko  
 litellm_params:  
  model: vertex_ai/textembedding-gecko  
  
general_settings:  
 master_key: sk-1234  

```

### Start proxy​
```
litellm --config /path/to/config.yaml   
  
# RUNNING on http://0.0.0.0:4000  

```

### Test​
  * Curl
  * OpenAI (python)
  * Langchain Embeddings


```
curl --location 'http://0.0.0.0:4000/embeddings' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{"input": ["Academia.edu uses"], "model": "textembedding-gecko", "encoding_format": "base64"}'  

```

```
from openai import OpenAI  
client = OpenAI(  
 api_key="sk-1234",  
 base_url="http://0.0.0.0:4000"  
)  
  
client.embeddings.create(  
 model="textembedding-gecko",  
 input="The food was delicious and the waiter...",  
 encoding_format="float"  
)  

```

```
from langchain_openai import OpenAIEmbeddings  
  
embeddings = OpenAIEmbeddings(model="textembedding-gecko", openai_api_base="http://0.0.0.0:4000", openai_api_key="sk-1234")  
  
text = "This is a test document."  
  
query_result = embeddings.embed_query(text)  
  
print(f"VERTEX AI EMBEDDINGS")  
print(query_result[:5])  

```

## Image Embeddings​
For models that support image embeddings, you can pass in a base64 encoded image string to the `input` param.
  * SDK
  * PROXY


```
from litellm import embedding  
import os  
  
# set your api key  
os.environ["COHERE_API_KEY"] = ""  
  
response = embedding(model="cohere/embed-english-v3.0", input=["<base64 encoded image>"])  

```

  1. Setup config.yaml 


```
model_list:  
 - model_name: cohere-embed  
  litellm_params:  
   model: cohere/embed-english-v3.0  
   api_key: os.environ/COHERE_API_KEY  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml   
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it!


```
curl -X POST 'http://0.0.0.0:4000/v1/embeddings' \  
-H 'Authorization: Bearer sk-54d77cd67b9febbb' \  
-H 'Content-Type: application/json' \  
-d '{  
 "model": "cohere/embed-english-v3.0",  
 "input": ["<base64 encoded image>"]  
}'  

```

## Input Params for `litellm.embedding()`​
info
Any non-openai params, will be treated as provider-specific params, and sent in the request body as kwargs to the provider.
**See Reserved Params**
**See Example**
### Required Fields​
  * `model`: _string_ - ID of the model to use. `model='text-embedding-ada-002'`
  * `input`: _string or array_ - Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for text-embedding-ada-002), cannot be an empty string, and any array must be 2048 dimensions or less. 


```
input=["good morning from litellm"]  

```

### Optional LiteLLM Fields​
  * `user`: _string (optional)_ A unique identifier representing your end-user, 
  * `dimensions`: _integer (Optional)_ The number of dimensions the resulting output embeddings should have. Only supported in OpenAI/Azure text-embedding-3 and later models.
  * `encoding_format`: _string (Optional)_ The format to return the embeddings in. Can be either `"float"` or `"base64"`. Defaults to `encoding_format="float"`
  * `timeout`: _integer (Optional)_ - The maximum time, in seconds, to wait for the API to respond. Defaults to 600 seconds (10 minutes).
  * `api_base`: _string (optional)_ - The api endpoint you want to call the model with
  * `api_version`: _string (optional)_ - (Azure-specific) the api version for the call
  * `api_key`: _string (optional)_ - The API key to authenticate and authorize requests. If not provided, the default API key is used.
  * `api_type`: _string (optional)_ - The type of API to use.


### Output from `litellm.embedding()`​
```
{  
 "object": "list",  
 "data": [  
  {  
   "object": "embedding",  
   "index": 0,  
   "embedding": [  
    -0.0022326677571982145,  
    0.010749882087111473,  
    ...  
    ...  
    ...  
    
   ]  
  }  
 ],  
 "model": "text-embedding-ada-002-v2",  
 "usage": {  
  "prompt_tokens": 10,  
  "total_tokens": 10  
 }  
}  

```

## OpenAI Embedding Models​
### Usage​
```
from litellm import embedding  
import os  
os.environ['OPENAI_API_KEY'] = ""  
response = embedding(  
  model="text-embedding-3-small",  
  input=["good morning from litellm", "this is another item"],  
  metadata={"anything": "good day"},  
  dimensions=5 # Only supported in text-embedding-3 and later models.  
)  

```

Model Name| Function Call| Required OS Variables  
---|---|---  
text-embedding-3-small| `embedding('text-embedding-3-small', input)`| `os.environ['OPENAI_API_KEY']`  
text-embedding-3-large| `embedding('text-embedding-3-large', input)`| `os.environ['OPENAI_API_KEY']`  
text-embedding-ada-002| `embedding('text-embedding-ada-002', input)`| `os.environ['OPENAI_API_KEY']`  
## Azure OpenAI Embedding Models​
### API keys​
This can be set as env variables or passed as **params to litellm.embedding()**
```
import os  
os.environ['AZURE_API_KEY'] =   
os.environ['AZURE_API_BASE'] =   
os.environ['AZURE_API_VERSION'] =   

```

### Usage​
```
from litellm import embedding  
response = embedding(  
  model="azure/<your deployment name>",  
  input=["good morning from litellm"],  
  api_key=api_key,  
  api_base=api_base,  
  api_version=api_version,  
)  
print(response)  

```

Model Name| Function Call  
---|---  
text-embedding-ada-002| `embedding(model="azure/<your deployment name>", input=input)`  
h/t to Mikko for this integration
## OpenAI Compatible Embedding Models​
Use this for calling `/embedding` endpoints on OpenAI Compatible Servers, example https://github.com/xorbitsai/inference
**Note add`openai/` prefix to model so litellm knows to route to OpenAI**
### Usage​
```
from litellm import embedding  
response = embedding(  
 model = "openai/<your-llm-name>",   # add `openai/` prefix to model so litellm knows to route to OpenAI  
 api_base="http://0.0.0.0:4000/"    # set API Base of your Custom OpenAI Endpoint  
 input=["good morning from litellm"]  
)  

```

## Bedrock Embedding​
### API keys​
This can be set as env variables or passed as **params to litellm.embedding()**
```
import os  
os.environ["AWS_ACCESS_KEY_ID"] = "" # Access key  
os.environ["AWS_SECRET_ACCESS_KEY"] = "" # Secret access key  
os.environ["AWS_REGION_NAME"] = "" # us-east-1, us-east-2, us-west-1, us-west-2  

```

### Usage​
```
from litellm import embedding  
response = embedding(  
  model="amazon.titan-embed-text-v1",  
  input=["good morning from litellm"],  
)  
print(response)  

```

Model Name| Function Call  
---|---  
Titan Embeddings - G1| `embedding(model="amazon.titan-embed-text-v1", input=input)`  
Cohere Embeddings - English| `embedding(model="cohere.embed-english-v3", input=input)`  
Cohere Embeddings - Multilingual| `embedding(model="cohere.embed-multilingual-v3", input=input)`  
## Cohere Embedding Models​
https://docs.cohere.com/reference/embed
### Usage​
```
from litellm import embedding  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
# cohere call  
response = embedding(  
  model="embed-english-v3.0",   
  input=["good morning from litellm", "this is another item"],   
  input_type="search_document" # optional param for v3 llms  
)  

```

Model Name| Function Call  
---|---  
embed-english-v3.0| `embedding(model="embed-english-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-english-light-v3.0| `embedding(model="embed-english-light-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-multilingual-v3.0| `embedding(model="embed-multilingual-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-multilingual-light-v3.0| `embedding(model="embed-multilingual-light-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-english-v2.0| `embedding(model="embed-english-v2.0", input=["good morning from litellm", "this is another item"])`  
embed-english-light-v2.0| `embedding(model="embed-english-light-v2.0", input=["good morning from litellm", "this is another item"])`  
embed-multilingual-v2.0| `embedding(model="embed-multilingual-v2.0", input=["good morning from litellm", "this is another item"])`  
## NVIDIA NIM Embedding Models​
### API keys​
This can be set as env variables or passed as **params to litellm.embedding()**
```
import os  
os.environ["NVIDIA_NIM_API_KEY"] = "" # api key  
os.environ["NVIDIA_NIM_API_BASE"] = "" # nim endpoint url  

```

### Usage​
```
from litellm import embedding  
import os  
os.environ['NVIDIA_NIM_API_KEY'] = ""  
response = embedding(  
  model='nvidia_nim/<model_name>',   
  input=["good morning from litellm"]  
)  

```

All models listed here are supported:
Model Name| Function Call  
---|---  
NV-Embed-QA| `embedding(model="nvidia_nim/NV-Embed-QA", input)`  
nvidia/nv-embed-v1| `embedding(model="nvidia_nim/nvidia/nv-embed-v1", input)`  
nvidia/nv-embedqa-mistral-7b-v2| `embedding(model="nvidia_nim/nvidia/nv-embedqa-mistral-7b-v2", input)`  
nvidia/nv-embedqa-e5-v5| `embedding(model="nvidia_nim/nvidia/nv-embedqa-e5-v5", input)`  
nvidia/embed-qa-4| `embedding(model="nvidia_nim/nvidia/embed-qa-4", input)`  
nvidia/llama-3.2-nv-embedqa-1b-v1| `embedding(model="nvidia_nim/nvidia/llama-3.2-nv-embedqa-1b-v1", input)`  
nvidia/llama-3.2-nv-embedqa-1b-v2| `embedding(model="nvidia_nim/nvidia/llama-3.2-nv-embedqa-1b-v2", input)`  
snowflake/arctic-embed-l| `embedding(model="nvidia_nim/snowflake/arctic-embed-l", input)`  
baai/bge-m3| `embedding(model="nvidia_nim/baai/bge-m3", input)`  
## HuggingFace Embedding Models​
LiteLLM supports all Feature-Extraction + Sentence Similarity Embedding models: https://huggingface.co/models?pipeline_tag=feature-extraction
### Usage​
```
from litellm import embedding  
import os  
os.environ['HUGGINGFACE_API_KEY'] = ""  
response = embedding(  
  model='huggingface/microsoft/codebert-base',   
  input=["good morning from litellm"]  
)  

```

### Usage - Set input_type​
LiteLLM infers input type (feature-extraction or sentence-similarity) by making a GET request to the api base. 
Override this, by setting the `input_type` yourself.
```
from litellm import embedding  
import os  
os.environ['HUGGINGFACE_API_KEY'] = ""  
response = embedding(  
  model='huggingface/microsoft/codebert-base',   
  input=["good morning from litellm", "you are a good bot"],  
  api_base = "https://p69xlsj6rpno5drq.us-east-1.aws.endpoints.huggingface.cloud",   
  input_type="sentence-similarity"  
)  

```

### Usage - Custom API Base​
```
from litellm import embedding  
import os  
os.environ['HUGGINGFACE_API_KEY'] = ""  
response = embedding(  
  model='huggingface/microsoft/codebert-base',   
  input=["good morning from litellm"],  
  api_base = "https://p69xlsj6rpno5drq.us-east-1.aws.endpoints.huggingface.cloud"  
)  

```

Model Name| Function Call| Required OS Variables  
---|---|---  
microsoft/codebert-base| `embedding('huggingface/microsoft/codebert-base', input=input)`| `os.environ['HUGGINGFACE_API_KEY']`  
BAAI/bge-large-zh| `embedding('huggingface/BAAI/bge-large-zh', input=input)`| `os.environ['HUGGINGFACE_API_KEY']`  
any-hf-embedding-model| `embedding('huggingface/hf-embedding-model', input=input)`| `os.environ['HUGGINGFACE_API_KEY']`  
## Mistral AI Embedding Models​
All models listed here https://docs.mistral.ai/platform/endpoints are supported
### Usage​
```
from litellm import embedding  
import os  
  
os.environ['MISTRAL_API_KEY'] = ""  
response = embedding(  
  model="mistral/mistral-embed",  
  input=["good morning from litellm"],  
)  
print(response)  

```

Model Name| Function Call  
---|---  
mistral-embed| `embedding(model="mistral/mistral-embed", input)`  
## Gemini AI Embedding Models​
### API keys​
This can be set as env variables or passed as **params to litellm.embedding()**
```
import os  
os.environ["GEMINI_API_KEY"] = ""  

```

### Usage - Embedding​
```
from litellm import embedding  
response = embedding(  
 model="gemini/text-embedding-004",  
 input=["good morning from litellm"],  
)  
print(response)  

```

All models listed here are supported:
Model Name| Function Call  
---|---  
text-embedding-004| `embedding(model="gemini/text-embedding-004", input)`  
## Vertex AI Embedding Models​
### Usage - Embedding​
```
import litellm  
from litellm import embedding  
litellm.vertex_project = "hardy-device-38811" # Your Project ID  
litellm.vertex_location = "us-central1" # proj location  
  
response = embedding(  
  model="vertex_ai/textembedding-gecko",  
  input=["good morning from litellm"],  
)  
print(response)  

```

### Supported Models​
All models listed here are supported
Model Name| Function Call  
---|---  
textembedding-gecko| `embedding(model="vertex_ai/textembedding-gecko", input)`  
textembedding-gecko-multilingual| `embedding(model="vertex_ai/textembedding-gecko-multilingual", input)`  
textembedding-gecko-multilingual@001| `embedding(model="vertex_ai/textembedding-gecko-multilingual@001", input)`  
textembedding-gecko@001| `embedding(model="vertex_ai/textembedding-gecko@001", input)`  
textembedding-gecko@003| `embedding(model="vertex_ai/textembedding-gecko@003", input)`  
text-embedding-preview-0409| `embedding(model="vertex_ai/text-embedding-preview-0409", input)`  
text-multilingual-embedding-preview-0409| `embedding(model="vertex_ai/text-multilingual-embedding-preview-0409", input)`  
## Voyage AI Embedding Models​
### Usage - Embedding​
```
from litellm import embedding  
import os  
  
os.environ['VOYAGE_API_KEY'] = ""  
response = embedding(  
  model="voyage/voyage-01",  
  input=["good morning from litellm"],  
)  
print(response)  

```

## Supported Models​
All models listed here https://docs.voyageai.com/embeddings/#models-and-specifics are supported
Model Name| Function Call  
---|---  
voyage-01| `embedding(model="voyage/voyage-01", input)`  
voyage-lite-01| `embedding(model="voyage/voyage-lite-01", input)`  
voyage-lite-01-instruct| `embedding(model="voyage/voyage-lite-01-instruct", input)`  
## Provider-specific Params​
info
Any non-openai params, will be treated as provider-specific params, and sent in the request body as kwargs to the provider.
**See Reserved Params**
### **Example**​
Cohere v3 Models have a required parameter: `input_type`, it can be one of the following four values:
  * `input_type="search_document"`: (default) Use this for texts (documents) you want to store in your vector database
  * `input_type="search_query"`: Use this for search queries to find the most relevant documents in your vector database
  * `input_type="classification"`: Use this if you use the embeddings as an input for a classification system
  * `input_type="clustering"`: Use this if you use the embeddings for text clustering


https://txt.cohere.com/introducing-embed-v3/
  * SDK
  * PROXY


```
from litellm import embedding  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
# cohere call  
response = embedding(  
  model="embed-english-v3.0",   
  input=["good morning from litellm", "this is another item"],   
  input_type="search_document" # 👈 PROVIDER-SPECIFIC PARAM  
)  

```

**via config**
```
model_list:  
 - model_name: "cohere-embed"  
  litellm_params:  
   model: embed-english-v3.0  
   input_type: search_document # 👈 PROVIDER-SPECIFIC PARAM  

```

**via request**
```
curl -X POST 'http://0.0.0.0:4000/v1/embeddings' \  
-H 'Authorization: Bearer sk-54d77cd67b9febbb' \  
-H 'Content-Type: application/json' \  
-d '{  
 "model": "cohere-embed",  
 "input": ["Are you authorized to work in United States of America?"],  
 "input_type": "search_document" # 👈 PROVIDER-SPECIFIC PARAM  
}'  

```

Previous
/completions
Next
/v1/messages [BETA]
  * Quick Start
  * Proxy Usage
    * Add model to config
    * Start proxy
    * Test
  * Image Embeddings
  * Input Params for `litellm.embedding()`
    * Required Fields
    * Optional LiteLLM Fields
    * Output from `litellm.embedding()`
  * OpenAI Embedding Models
    * Usage
  * Azure OpenAI Embedding Models
    * API keys
    * Usage
  * OpenAI Compatible Embedding Models
    * Usage
  * Bedrock Embedding
    * API keys
    * Usage
  * Cohere Embedding Models
    * Usage
  * NVIDIA NIM Embedding Models
    * API keys
    * Usage
  * HuggingFace Embedding Models
    * Usage
    * Usage - Set input_type
    * Usage - Custom API Base
  * Mistral AI Embedding Models
    * Usage
  * Gemini AI Embedding Models
    * API keys
    * Usage - Embedding
  * Vertex AI Embedding Models
    * Usage - Embedding
    * Supported Models
  * Voyage AI Embedding Models
    * Usage - Embedding
  * Supported Models
  * Provider-specific Params
    * **Example**


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * litellm.moderation()


On this page
# litellm.moderation()
LiteLLM supports the moderation endpoint for OpenAI
## Usage​
```
import os  
from litellm import moderation  
os.environ['OPENAI_API_KEY'] = ""  
response = moderation(input="i'm ishaan cto of litellm")    

```

Previous
litellm.aembedding()
Next
Budget Manager
  * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
    * Data Privacy and Security
    * Data Retention Policy
    * Migration Policy
    * ❤️ 🚅 Projects built on LiteLLM
    * PII Masking - LiteLLM Gateway (Deprecated Version)
    * Code Quality
    * Rules
    * [DEPRECATED] Team-based Routing
    * [DEPRECATED] Region-based Routing
    * [OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
  * Support & Talk with founders


  *   * Extras
  * Code Quality


# Code Quality
🚅 LiteLLM follows the Google Python Style Guide.
We run: 
  * Ruff for formatting and linting checks
  * Mypy + Pyright for typing 1, 2
  * Black for formatting
  * isort for import sorting


If you have suggestions on how to improve the code quality feel free to open an issue or a PR.
Previous
PII Masking - LiteLLM Gateway (Deprecated Version)
Next
Rules
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
      * Provider Files Endpoints
      * [BETA] Unified File ID
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /files
  * Provider Files Endpoints


On this page
# Provider Files Endpoints
Files are used to upload documents that can be used with features like Assistants, Fine-tuning, and Batch API.
Use this to call the provider's `/files` endpoints directly, in the OpenAI format. 
## Quick Start​
  * Upload a File
  * List Files
  * Retrieve File Information
  * Delete File
  * Get File Content


  * LiteLLM PROXY Server
  * SDK


  1. Setup config.yaml


```
# for /files endpoints  
files_settings:  
 - custom_llm_provider: azure  
  api_base: https://exampleopenaiendpoint-production.up.railway.app  
  api_key: fake-key  
  api_version: "2023-03-15-preview"  
 - custom_llm_provider: openai  
  api_key: os.environ/OPENAI_API_KEY  

```

  1. Start LiteLLM PROXY Server


```
litellm --config /path/to/config.yaml  
  
## RUNNING on http://0.0.0.0:4000  

```

  1. Use OpenAI's /files endpoints


Upload a File
```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-...",  
  base_url="http://0.0.0.0:4000/v1"  
)  
  
client.files.create(  
  file=wav_data,  
  purpose="user_data",  
  extra_body={"custom_llm_provider": "openai"}  
)  

```

List Files
```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-...",  
  base_url="http://0.0.0.0:4000/v1"  
)  
  
files = client.files.list(extra_body={"custom_llm_provider": "openai"})  
print("files=", files)  

```

Retrieve File Information
```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-...",  
  base_url="http://0.0.0.0:4000/v1"  
)  
  
file = client.files.retrieve(file_id="file-abc123", extra_body={"custom_llm_provider": "openai"})  
print("file=", file)  

```

Delete File
```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-...",  
  base_url="http://0.0.0.0:4000/v1"  
)  
  
response = client.files.delete(file_id="file-abc123", extra_body={"custom_llm_provider": "openai"})  
print("delete response=", response)  

```

Get File Content
```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-...",  
  base_url="http://0.0.0.0:4000/v1"  
)  
  
content = client.files.content(file_id="file-abc123", extra_body={"custom_llm_provider": "openai"})  
print("content=", content)  

```

**Upload a File**
```
from litellm  
import os   
  
os.environ["OPENAI_API_KEY"] = "sk-.."  
  
file_obj = await litellm.acreate_file(  
  file=open("mydata.jsonl", "rb"),  
  purpose="fine-tune",  
  custom_llm_provider="openai",  
)  
print("Response from creating file=", file_obj)  

```

**List Files**
```
files = await litellm.alist_files(  
  custom_llm_provider="openai",  
  limit=10  
)  
print("files=", files)  

```

**Retrieve File Information**
```
file = await litellm.aretrieve_file(  
  file_id="file-abc123",  
  custom_llm_provider="openai"  
)  
print("file=", file)  

```

**Delete File**
```
response = await litellm.adelete_file(  
  file_id="file-abc123",  
  custom_llm_provider="openai"  
)  
print("delete response=", response)  

```

**Get File Content**
```
content = await litellm.afile_content(  
  file_id="file-abc123",  
  custom_llm_provider="openai"  
)  
print("file content=", content)  

```

## **Supported Providers** :​
### OpenAI​
### Azure OpenAI​
### Vertex AI​
## Swagger API Reference​
Previous
/assistants
Next
[BETA] Unified File ID
  * Quick Start
  * **Supported Providers** :
    * OpenAI
    * Azure OpenAI
    * Vertex AI
  * Swagger API Reference


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
    * Contributing Code
    * Adding Providers
    * Contributing to Documentation
    * Contributing - UI
  * Extras
  * Support & Talk with founders


  *   * Contributing
  * Contributing to Documentation


On this page
# Contributing to Documentation
This website is built using Docusaurus 2, a modern static website generator.
Clone litellm 
```
git clone https://github.com/BerriAI/litellm.git  

```

### Local setup for locally running docs​
#### Installation​
```
npm install --global yarn  

```

### Local Development​
```
cd docs/my-website  

```

Let's Install requirement
```
yarn  

```

Run website
```
yarn start  

```

Open docs here: http://localhost:3000/
```
  
This command builds your Markdown files into HTML and starts a development server to browse your documentation. Open up [http://127.0.0.1:8000/](http://127.0.0.1:8000/) in your web browser to see your documentation. You can make changes to your Markdown files and your docs will automatically rebuild.  
  
[Full tutorial here](https://docs.readthedocs.io/en/stable/intro/getting-started-with-mkdocs.html)  
  
### Making changes to Docs  
- All the docs are placed under the `docs` directory  
- If you are adding a new `.md` file or editing the hierarchy edit `mkdocs.yml` in the root of the project  
- After testing your changes, make a change to the `main` branch of [github.com/BerriAI/litellm](https://github.com/BerriAI/litellm)  

```

Previous
Add Rerank Provider
Next
Contributing - UI
  * Local setup for locally running docs
  * Local Development


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /fine_tuning


On this page
# /fine_tuning
info
This is an Enterprise only endpoint Get Started with Enterprise here
Feature| Supported| Notes  
---|---|---  
Supported Providers| OpenAI, Azure OpenAI, Vertex AI| -  
Cost Tracking| 🟡| Let us know if you need this  
Logging| ✅| Works across all logging integrations  
Add `finetune_settings` and `files_settings` to your litellm config.yaml to use the fine-tuning endpoints.
## Example config.yaml for `finetune_settings` and `files_settings`​
```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
# For /fine_tuning/jobs endpoints  
finetune_settings:  
 - custom_llm_provider: azure  
  api_base: https://exampleopenaiendpoint-production.up.railway.app  
  api_key: os.environ/AZURE_API_KEY  
  api_version: "2023-03-15-preview"  
 - custom_llm_provider: openai  
  api_key: os.environ/OPENAI_API_KEY  
 - custom_llm_provider: "vertex_ai"  
  vertex_project: "adroit-crow-413218"  
  vertex_location: "us-central1"  
  vertex_credentials: "/Users/ishaanjaffer/Downloads/adroit-crow-413218-a956eef1a2a8.json"  
  
# for /files endpoints  
files_settings:  
 - custom_llm_provider: azure  
  api_base: https://exampleopenaiendpoint-production.up.railway.app  
  api_key: fake-key  
  api_version: "2023-03-15-preview"  
 - custom_llm_provider: openai  
  api_key: os.environ/OPENAI_API_KEY  

```

## Create File for fine-tuning​
  * OpenAI Python SDK
  * curl


```
client = AsyncOpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000") # base_url is your litellm proxy url  
  
file_name = "openai_batch_completions.jsonl"  
response = await client.files.create(  
  extra_body={"custom_llm_provider": "azure"}, # tell litellm proxy which provider to use  
  file=open(file_name, "rb"),  
  purpose="fine-tune",  
)  

```

```
curl http://localhost:4000/v1/files \  
  -H "Authorization: Bearer sk-1234" \  
  -F purpose="batch" \  
  -F custom_llm_provider="azure"\  
  -F file="@mydata.jsonl"  

```

## Create fine-tuning job​
  * Azure OpenAI


  * OpenAI Python SDK
  * curl


```
ft_job = await client.fine_tuning.jobs.create(  
  model="gpt-35-turbo-1106",          # Azure OpenAI model you want to fine-tune  
  training_file="file-abc123",         # file_id from create file response  
  extra_body={"custom_llm_provider": "azure"}, # tell litellm proxy which provider to use  
)  

```

```
curl http://localhost:4000/v1/fine_tuning/jobs \  
  -H "Content-Type: application/json" \  
  -H "Authorization: Bearer sk-1234" \  
  -d '{  
  "custom_llm_provider": "azure",  
  "model": "gpt-35-turbo-1106",  
  "training_file": "file-abc123"  
  }'  

```

### Request Body​
  * Supported Params
  * Example Request Body


  * `model`
**Type:** string  
**Required:** Yes  
The name of the model to fine-tune
  * `custom_llm_provider`
**Type:** `Literal["azure", "openai", "vertex_ai"]`
**Required:** Yes The name of the model to fine-tune. You can select one of the **supported providers**
  * `training_file`
**Type:** string  
**Required:** Yes  
The ID of an uploaded file that contains training data.
    * See **upload file** for how to upload a file.
    * Your dataset must be formatted as a JSONL file.
  * `hyperparameters`
**Type:** object  
**Required:** No  
The hyperparameters used for the fine-tuning job.
> #### Supported `hyperparameters`​
> #### batch_size​
> **Type:** string or integer  
>  **Required:** No  
>  Number of examples in each batch. A larger batch size means that model parameters are updated less frequently, but with lower variance.
> #### learning_rate_multiplier​
> **Type:** string or number  
>  **Required:** No  
>  Scaling factor for the learning rate. A smaller learning rate may be useful to avoid overfitting.
> #### n_epochs​
> **Type:** string or integer  
>  **Required:** No  
>  The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.
  * `suffix` **Type:** string or null  
**Required:** No  
**Default:** null  
A string of up to 18 characters that will be added to your fine-tuned model name. Example: A `suffix` of "custom-model-name" would produce a model name like `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`.
  * `validation_file` **Type:** string or null  
**Required:** No  
The ID of an uploaded file that contains validation data.
    * If provided, this data is used to generate validation metrics periodically during fine-tuning.


  * `integrations` **Type:** array or null  
**Required:** No  
A list of integrations to enable for your fine-tuning job.
  * `seed` **Type:** integer or null  
**Required:** No  
The seed controls the reproducibility of the job. Passing in the same seed and job parameters should produce the same results, but may differ in rare cases. If a seed is not specified, one will be generated for you.


```
{  
 "model": "gpt-4o-mini",  
 "training_file": "file-abcde12345",  
 "hyperparameters": {  
  "batch_size": 4,  
  "learning_rate_multiplier": 0.1,  
  "n_epochs": 3  
 },  
 "suffix": "custom-model-v1",  
 "validation_file": "file-fghij67890",  
 "seed": 42  
}  

```

## Cancel fine-tuning job​
  * OpenAI Python SDK
  * curl


```
# cancel specific fine tuning job  
cancel_ft_job = await client.fine_tuning.jobs.cancel(  
  fine_tuning_job_id="123",             # fine tuning job id  
  extra_body={"custom_llm_provider": "azure"},    # tell litellm proxy which provider to use  
)  
  
print("response from cancel ft job={}".format(cancel_ft_job))  

```

```
curl -X POST http://localhost:4000/v1/fine_tuning/jobs/ftjob-abc123/cancel \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{"custom_llm_provider": "azure"}'  

```

## List fine-tuning jobs​
  * OpenAI Python SDK
  * curl


```
list_ft_jobs = await client.fine_tuning.jobs.list(  
  extra_query={"custom_llm_provider": "azure"}  # tell litellm proxy which provider to use  
)  
  
print("list of ft jobs={}".format(list_ft_jobs))  

```

```
curl -X GET 'http://localhost:4000/v1/fine_tuning/jobs?custom_llm_provider=azure' \  
   -H "Content-Type: application/json" \  
   -H "Authorization: Bearer sk-1234"  

```

## 👉 Proxy API Reference​
Previous
/realtime
Next
/moderations
  * Example config.yaml for `finetune_settings` and `files_settings`
  * Create File for fine-tuning
  * Create fine-tuning job
    * Request Body
  * Cancel fine-tuning job
  * List fine-tuning jobs
  * 👉 Proxy API Reference


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * Calling Finetuned Models


On this page
# Calling Finetuned Models
## OpenAI​
Model Name| Function Call  
---|---  
fine tuned `gpt-4-0613`| `response = completion(model="ft:gpt-4-0613", messages=messages)`  
fine tuned `gpt-4o-2024-05-13`| `response = completion(model="ft:gpt-4o-2024-05-13", messages=messages)`  
fine tuned `gpt-3.5-turbo-0125`| `response = completion(model="ft:gpt-3.5-turbo-0125", messages=messages)`  
fine tuned `gpt-3.5-turbo-1106`| `response = completion(model="ft:gpt-3.5-turbo-1106", messages=messages)`  
fine tuned `gpt-3.5-turbo-0613`| `response = completion(model="ft:gpt-3.5-turbo-0613", messages=messages)`  
## Vertex AI​
Fine tuned models on vertex have a numerical model/endpoint id. 
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["VERTEXAI_PROJECT"] = "hardy-device-38811"  
os.environ["VERTEXAI_LOCATION"] = "us-central1"  
  
response = completion(  
 model="vertex_ai/<your-finetuned-model>", # e.g. vertex_ai/4965075652664360960  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 base_model="vertex_ai/gemini-1.5-pro" # the base model - used for routing  
)  

```

  1. Add Vertex Credentials to your env 


```
!gcloud auth application-default login  

```

  1. Setup config.yaml 


```
- model_name: finetuned-gemini  
 litellm_params:  
  model: vertex_ai/<ENDPOINT_ID>  
  vertex_project: <PROJECT_ID>  
  vertex_location: <LOCATION>  
 model_info:  
  base_model: vertex_ai/gemini-1.5-pro # IMPORTANT  

```

  1. Test it! 


```
curl --location 'https://0.0.0.0:4000/v1/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: <LITELLM_KEY>' \  
--data '{"model": "finetuned-gemini" ,"messages":[{"role": "user", "content":[{"type": "text", "text": "hi"}]}]}'  

```

Previous
Provider-specific Params
Next
SSL Security Settings
  * OpenAI
  * Vertex AI


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
    * Exception Mapping
    * Provider-specific Params
    * Calling Finetuned Models
    * SSL Security Settings
    * Using Audio Models
    * Using Web Search
    * Using PDF Input
    * Using Vision Models
    * Structured Outputs (JSON Mode)
    * 'Thinking' / 'Reasoning Content'
    * Prompt Caching
    * Predicted Outputs
    * Pre-fix Assistant Messages
    * Drop Unsupported Params
    * Prompt Formatting
    * Streaming + Async
    * Trimming Input Messages
    * Function Calling
    * Model Alias
    * Batching Completion()
    * Mock Completion() Responses - Save Testing Costs 💰
    * Reliability - Retries, Fallbacks
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Guides
  * SSL Security Settings


# SSL Security Settings
If you're in an environment using an older TTS bundle, with an older encryption, follow this guide. 
LiteLLM uses HTTPX for network requests, unless otherwise specified. 
  1. Disable SSL verification


  * SDK
  * PROXY
  * Environment Variables


```
import litellm  
litellm.ssl_verify = False  

```

```
litellm_settings:  
 ssl_verify: false  

```

```
export SSL_VERIFY="False"  

```

  1. Lower security settings


  * SDK
  * PROXY
  * Environment Variables


```
import litellm  
litellm.ssl_security_level = 1  
litellm.ssl_certificate = "/path/to/certificate.pem"  

```

```
litellm_settings:  
 ssl_security_level: 1  
 ssl_certificate: "/path/to/certificate.pem"  

```

```
export SSL_SECURITY_LEVEL="1"  
export SSL_CERTIFICATE="/path/to/certificate.pem"  

```

Previous
Calling Finetuned Models
Next
Using Audio Models
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
      * Image Generations
      * [BETA] Image Variations
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /images
  * Image Generations


On this page
# Image Generations
## Quick Start​
### LiteLLM Python SDK​
```
from litellm import image_generation  
import os   
  
# set api keys   
os.environ["OPENAI_API_KEY"] = ""  
  
response = image_generation(prompt="A cute baby sea otter", model="dall-e-3")  
  
print(f"response: {response}")  

```

### LiteLLM Proxy​
### Setup config.yaml​
```
model_list:  
 - model_name: gpt-image-1 ### RECEIVED MODEL NAME ###  
  litellm_params: # all params accepted by litellm.image_generation()  
   model: azure/gpt-image-1 ### MODEL NAME sent to `litellm.image_generation()` ###  
   api_base: https://my-endpoint-europe-berri-992.openai.azure.com/  
   api_key: "os.environ/AZURE_API_KEY_EU" # does os.getenv("AZURE_API_KEY_EU")  
  

```

### Start proxy​
```
litellm --config /path/to/config.yaml   
  
# RUNNING on http://0.0.0.0:4000  

```

### Test​
  * Curl
  * OpenAI


```
curl -X POST 'http://0.0.0.0:4000/v1/images/generations' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "model": "gpt-image-1",  
  "prompt": "A cute baby sea otter",  
  "n": 1,  
  "size": "1024x1024"  
}'  

```

```
from openai import OpenAI  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
  
image = client.images.generate(  
  prompt="A cute baby sea otter",  
  model="dall-e-3",  
)  
  
print(image)  

```

## Input Params for `litellm.image_generation()`​
info
Any non-openai params, will be treated as provider-specific params, and sent in the request body as kwargs to the provider.
**See Reserved Params**
### Required Fields​
  * `prompt`: _string_ - A text description of the desired image(s). 


### Optional LiteLLM Fields​
```
model: Optional[str] = None,  
n: Optional[int] = None,  
quality: Optional[str] = None,  
response_format: Optional[str] = None,  
size: Optional[str] = None,  
style: Optional[str] = None,  
user: Optional[str] = None,  
timeout=600, # default to 10 minutes  
api_key: Optional[str] = None,  
api_base: Optional[str] = None,  
api_version: Optional[str] = None,  
litellm_logging_obj=None,  
custom_llm_provider=None,  

```

  * `model`: _string (optional)_ The model to use for image generation. Defaults to openai/gpt-image-1
  * `n`: _int (optional)_ The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported.
  * `quality`: _string (optional)_ The quality of the image that will be generated.
    * `auto` (default value) will automatically select the best quality for the given model.
    * `high`, `medium` and `low` are supported for `gpt-image-1`.
    * `hd` and `standard` are supported for `dall-e-3`.
    * `standard` is the only option for `dall-e-2`.
  * `response_format`: _string (optional)_ The format in which the generated images are returned. Must be one of url or b64_json.
  * `size`: _string (optional)_ The size of the generated images. Must be one of `1024x1024`, `1536x1024` (landscape), `1024x1536` (portrait), or `auto` (default value) for `gpt-image-1`, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.
  * `timeout`: _integer_ - The maximum time, in seconds, to wait for the API to respond. Defaults to 600 seconds (10 minutes).
  * `user`: _string (optional)_ A unique identifier representing your end-user, 
  * `api_base`: _string (optional)_ - The api endpoint you want to call the model with
  * `api_version`: _string (optional)_ - (Azure-specific) the api version for the call; required for dall-e-3 on Azure
  * `api_key`: _string (optional)_ - The API key to authenticate and authorize requests. If not provided, the default API key is used.
  * `api_type`: _string (optional)_ - The type of API to use.


### Output from `litellm.image_generation()`​
```
  
{  
  "created": 1703658209,  
  "data": [{  
    'b64_json': None,   
    'revised_prompt': 'Adorable baby sea otter with a coat of thick brown fur, playfully swimming in blue ocean waters. Its curious, bright eyes gleam as it is surfaced above water, tiny paws held close to its chest, as it playfully spins in the gentle waves under the soft rays of a setting sun.',   
    'url': 'https://oaidalleapiprodscus.blob.core.windows.net/private/org-ikDc4ex8NB5ZzfTf8m5WYVB7/user-JpwZsbIXubBZvan3Y3GchiiB/img-dpa3g5LmkTrotY6M93dMYrdE.png?st=2023-12-27T05%3A23%3A29Z&se=2023-12-27T07%3A23%3A29Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-26T13%3A22%3A56Z&ske=2023-12-27T13%3A22%3A56Z&sks=b&skv=2021-08-06&sig=hUuQjYLS%2BvtsDdffEAp2gwewjC8b3ilggvkd9hgY6Uw%3D'  
  }],  
  "usage": {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}  
}  

```

## OpenAI Image Generation Models​
### Usage​
```
from litellm import image_generation  
import os  
os.environ['OPENAI_API_KEY'] = ""  
response = image_generation(model='gpt-image-1', prompt="cute baby otter")  

```

Model Name| Function Call| Required OS Variables  
---|---|---  
gpt-image-1| `image_generation(model='gpt-image-1', prompt="cute baby otter")`| `os.environ['OPENAI_API_KEY']`  
dall-e-3| `image_generation(model='dall-e-3', prompt="cute baby otter")`| `os.environ['OPENAI_API_KEY']`  
dall-e-2| `image_generation(model='dall-e-2', prompt="cute baby otter")`| `os.environ['OPENAI_API_KEY']`  
## Azure OpenAI Image Generation Models​
### API keys​
This can be set as env variables or passed as **params to litellm.image_generation()**
```
import os  
os.environ['AZURE_API_KEY'] =   
os.environ['AZURE_API_BASE'] =   
os.environ['AZURE_API_VERSION'] =   

```

### Usage​
```
from litellm import embedding  
response = embedding(  
  model="azure/<your deployment name>",  
  prompt="cute baby otter",  
  api_key=api_key,  
  api_base=api_base,  
  api_version=api_version,  
)  
print(response)  

```

Model Name| Function Call  
---|---  
gpt-image-1| `image_generation(model="azure/<your deployment name>", prompt="cute baby otter")`  
dall-e-3| `image_generation(model="azure/<your deployment name>", prompt="cute baby otter")`  
dall-e-2| `image_generation(model="azure/<your deployment name>", prompt="cute baby otter")`  
## OpenAI Compatible Image Generation Models​
Use this for calling `/image_generation` endpoints on OpenAI Compatible Servers, example https://github.com/xorbitsai/inference
**Note add`openai/` prefix to model so litellm knows to route to OpenAI**
### Usage​
```
from litellm import image_generation  
response = image_generation(  
 model = "openai/<your-llm-name>",   # add `openai/` prefix to model so litellm knows to route to OpenAI  
 api_base="http://0.0.0.0:8000/"    # set API Base of your Custom OpenAI Endpoint  
 prompt="cute baby otter"  
)  

```

## Bedrock - Stable Diffusion​
Use this for stable diffusion on bedrock
### Usage​
```
import os  
from litellm import image_generation  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = image_generation(  
      prompt="A cute baby sea otter",  
      model="bedrock/stability.stable-diffusion-xl-v0",  
    )  
print(f"response: {response}")  

```

## VertexAI - Image Generation Models​
### Usage​
Use this for image generation models on VertexAI
```
response = litellm.image_generation(  
  prompt="An olympic size swimming pool",  
  model="vertex_ai/imagegeneration@006",  
  vertex_ai_project="adroit-crow-413218",  
  vertex_ai_location="us-central1",  
)  
print(f"response: {response}")  

```

Previous
/mcp [BETA] - Model Context Protocol
Next
[BETA] Image Variations
  * Quick Start
    * LiteLLM Python SDK
    * LiteLLM Proxy
    * Setup config.yaml
    * Start proxy
    * Test
  * Input Params for `litellm.image_generation()`
    * Required Fields
    * Optional LiteLLM Fields
    * Output from `litellm.image_generation()`
  * OpenAI Image Generation Models
    * Usage
  * Azure OpenAI Image Generation Models
    * API keys
    * Usage
  * OpenAI Compatible Image Generation Models
    * Usage
  * Bedrock - Stable Diffusion
    * Usage
  * VertexAI - Image Generation Models
    * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
      * Using ChatLiteLLM() - Langchain
      * Instructor - Function Calling
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * LangChain, LlamaIndex, Instructor Integration
  * Using ChatLiteLLM() - Langchain


On this page
# Using ChatLiteLLM() - Langchain
## Pre-Requisites​
```
!pip install litellm langchain  

```

## Quick Start​
  * OpenAI
  * Anthropic
  * Replicate
  * Cohere


```
import os  
from langchain_community.chat_models import ChatLiteLLM  
from langchain_core.prompts import (  
  ChatPromptTemplate,  
  SystemMessagePromptTemplate,  
  AIMessagePromptTemplate,  
  HumanMessagePromptTemplate,  
)  
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  
  
os.environ['OPENAI_API_KEY'] = ""  
chat = ChatLiteLLM(model="gpt-3.5-turbo")  
messages = [  
  HumanMessage(  
    content="what model are you"  
  )  
]  
chat.invoke(messages)  

```

```
import os  
from langchain_community.chat_models import ChatLiteLLM  
from langchain_core.prompts import (  
  ChatPromptTemplate,  
  SystemMessagePromptTemplate,  
  AIMessagePromptTemplate,  
  HumanMessagePromptTemplate,  
)  
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  
  
os.environ['ANTHROPIC_API_KEY'] = ""  
chat = ChatLiteLLM(model="claude-2", temperature=0.3)  
messages = [  
  HumanMessage(  
    content="what model are you"  
  )  
]  
chat.invoke(messages)  

```

```
import os  
from langchain_community.chat_models import ChatLiteLLM  
from langchain_core.prompts.chat import (  
  ChatPromptTemplate,  
  SystemMessagePromptTemplate,  
  AIMessagePromptTemplate,  
  HumanMessagePromptTemplate,  
)  
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  
  
os.environ['REPLICATE_API_TOKEN'] = ""  
chat = ChatLiteLLM(model="replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1")  
messages = [  
  HumanMessage(  
    content="what model are you?"  
  )  
]  
chat.invoke(messages)  

```

```
import os  
from langchain_community.chat_models import ChatLiteLLM  
from langchain_core.prompts import (  
  ChatPromptTemplate,  
  SystemMessagePromptTemplate,  
  AIMessagePromptTemplate,  
  HumanMessagePromptTemplate,  
)  
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  
  
os.environ['COHERE_API_KEY'] = ""  
chat = ChatLiteLLM(model="command-nightly")  
messages = [  
  HumanMessage(  
    content="what model are you?"  
  )  
]  
chat.invoke(messages)  

```

## Use Langchain ChatLiteLLM with MLflow​
MLflow provides open-source observability solution for ChatLiteLLM.
To enable the integration, simply call `mlflow.litellm.autolog()` before in your code. No other setup is necessary.
```
import mlflow  
  
mlflow.litellm.autolog()  

```

Once the auto-tracing is enabled, you can invoke `ChatLiteLLM` and see recorded traces in MLflow.
```
import os  
from langchain.chat_models import ChatLiteLLM  
  
os.environ['OPENAI_API_KEY']="sk-..."  
  
chat = ChatLiteLLM(model="gpt-4o-mini")  
chat.invoke("Hi!")  

```

## Use Langchain ChatLiteLLM with Lunary​
```
import os  
from langchain.chat_models import ChatLiteLLM  
from langchain.schema import HumanMessage  
import litellm  
  
os.environ["LUNARY_PUBLIC_KEY"] = "" # from https://app.lunary.ai/settings  
os.environ['OPENAI_API_KEY']="sk-..."  
  
litellm.success_callback = ["lunary"]   
litellm.failure_callback = ["lunary"]   
  
chat = ChatLiteLLM(  
 model="gpt-4o"  
 messages = [  
  HumanMessage(  
    content="what model are you"  
  )  
]  
chat(messages)  

```

Get more details here
## Use LangChain ChatLiteLLM + Langfuse​
Checkout this section here for more details on how to integrate Langfuse with ChatLiteLLM.
Previous
Migration Guide - LiteLLM v1.0.0+
Next
Instructor - Function Calling
  * Pre-Requisites
  * Quick Start
  * Use Langchain ChatLiteLLM with MLflow
  * Use Langchain ChatLiteLLM with Lunary
  * Use LangChain ChatLiteLLM + Langfuse


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
    * Benchmarks
    * LiteLLM Proxy - 1K RPS Load test on locust
    * LiteLLM SDK vs OpenAI
    * Multi-Instance TPM/RPM (litellm.Router)
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Load Testing
  * LiteLLM Proxy - 1K RPS Load test on locust


On this page
# LiteLLM Proxy - 1K RPS Load test on locust
Tutorial on how to get to 1K+ RPS with LiteLLM Proxy on locust
## Pre-Testing Checklist​
  * Ensure you're using the **latest`-stable` version** of litellm
    * Github releases
    * litellm docker containers
    * litellm database docker container
  * Ensure you're following **ALL** best practices for production
  * Locust - Ensure you're Locust instance can create 1K+ requests per second
    * 👉 You can use our **maintained locust instance here**
    * If you're self hosting locust
      * here's the spec used for our locust machine
      * here is the locustfile.py used for our tests
  * Use this **machine specification for running litellm proxy**
  * **Enterprise LiteLLM** - Use `prometheus` as a callback in your `proxy_config.yaml` to get metrics on your load test Set `litellm_settings.callbacks` to monitor success/failures/all types of errors
```
litellm_settings:  
  callbacks: ["prometheus"] # Enterprise LiteLLM Only - use prometheus to get metrics on your load test  

```



**Use this config for testing:**
**Note:** we're currently migrating to aiohttp which has 10x higher throughput. We recommend using the `aiohttp_openai/` provider for load testing.
```
model_list:  
 - model_name: "fake-openai-endpoint"  
  litellm_params:  
   model: aiohttp_openai/any  
   api_base: https://your-fake-openai-endpoint.com/chat/completions  
   api_key: "test"  

```

## Load Test - Fake OpenAI Endpoint​
### Expected Performance​
Metric| Value  
---|---  
Requests per Second| 1174+  
Median Response Time| `96ms`  
Average Response Time| `142.18ms`  
### Run Test​
  1. Add `fake-openai-endpoint` to your proxy config.yaml and start your litellm proxy litellm provides a hosted `fake-openai-endpoint` you can load test against


```
model_list:  
 - model_name: fake-openai-endpoint  
  litellm_params:  
   model: aiohttp_openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["prometheus"] # Enterprise LiteLLM Only - use prometheus to get metrics on your load test  

```

  1. `pip install locust`
  2. Create a file called `locustfile.py` on your local machine. Copy the contents from the litellm load test located here
  3. Start locust Run `locust` in the same directory as your `locustfile.py` from step 2
```
locust -f locustfile.py --processes 4  

```

  4. Run Load test on locust
Head to the locust UI on http://0.0.0.0:8089
Set **Users=1000, Ramp Up Users=1000** , Host=Base URL of your LiteLLM Proxy
  5. Expected results 


## Load test - Endpoints with Rate Limits​
Run a load test on 2 LLM deployments each with 10K RPM Quota. Expect to see ~20K RPM
### Expected Performance​
  * We expect to see 20,000+ successful responses in 1 minute
  * The remaining requests **fail because the endpoint exceeds it's 10K RPM quota limit - from the LLM API provider**

Metric| Value  
---|---  
Successful Responses in 1 minute| 20,000+  
Requests per Second| ~1170+  
Median Response Time| `70ms`  
Average Response Time| `640.18ms`  
### Run Test​
  1. Add 2 `gemini-vision` deployments on your config.yaml. Each deployment can handle 10K RPM. (We setup a fake endpoint with a rate limit of 1000 RPM on the `/v1/projects/bad-adroit-crow` route below )


info
All requests with `model="gemini-vision"` will be load balanced equally across the 2 deployments.
```
model_list:  
 - model_name: gemini-vision  
  litellm_params:  
   model: vertex_ai/gemini-1.0-pro-vision-001  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/v1/projects/bad-adroit-crow-413218/locations/us-central1/publishers/google/models/gemini-1.0-pro-vision-001  
   vertex_project: "adroit-crow-413218"  
   vertex_location: "us-central1"  
   vertex_credentials: /etc/secrets/adroit_crow.json  
 - model_name: gemini-vision  
  litellm_params:  
   model: vertex_ai/gemini-1.0-pro-vision-001  
   api_base: https://exampleopenaiendpoint-production-c715.up.railway.app/v1/projects/bad-adroit-crow-413218/locations/us-central1/publishers/google/models/gemini-1.0-pro-vision-001  
   vertex_project: "adroit-crow-413218"  
   vertex_location: "us-central1"  
   vertex_credentials: /etc/secrets/adroit_crow.json  
  
litellm_settings:  
 callbacks: ["prometheus"] # Enterprise LiteLLM Only - use prometheus to get metrics on your load test  

```

  1. `pip install locust`
  2. Create a file called `locustfile.py` on your local machine. Copy the contents from the litellm load test located here
  3. Start locust Run `locust` in the same directory as your `locustfile.py` from step 2
```
locust -f locustfile.py --processes 4 -t 60  

```

  4. Run Load test on locust
Head to the locust UI on http://0.0.0.0:8089 and use the following settings
  5. Expected results
     * Successful responses in 1 minute = 19,800 = (69415 - 49615)
     * Requests per second = 1170
     * Median response time = 70ms
     * Average response time = 640ms


## Prometheus Metrics for debugging load tests​
Use the following prometheus metrics to debug your load tests / failures
Metric Name| Description  
---|---  
`litellm_deployment_failure_responses`| Total number of failed LLM API calls for a specific LLM deployment. Labels: `"requested_model", "litellm_model_name", "model_id", "api_base", "api_provider", "hashed_api_key", "api_key_alias", "team", "team_alias", "exception_status", "exception_class"`  
`litellm_deployment_cooled_down`| Number of times a deployment has been cooled down by LiteLLM load balancing logic. Labels: `"litellm_model_name", "model_id", "api_base", "api_provider", "exception_status"`  
## Machine Specifications for Running Locust​
Metric| Value  
---|---  
`locust --processes 4`| 4  
`vCPUs` on Load Testing Machine| 2.0 vCPUs  
`Memory` on Load Testing Machine| 450 MB  
`Replicas` of Load Testing Machine| 1  
## Machine Specifications for Running LiteLLM Proxy​
👉 **Number of Replicas of LiteLLM Proxy=4** for getting 1K+ RPS
Service| Spec| CPUs| Memory| Architecture| Version  
---|---|---|---|---|---  
Server| `t2.large`.| `2vCPUs`| `8GB`| `x86`|   
## Locust file used for testing​
```
import os  
import uuid  
from locust import HttpUser, task, between  
  
class MyUser(HttpUser):  
  wait_time = between(0.5, 1) # Random wait time between requests  
  
  @task(100)  
  def litellm_completion(self):  
    # no cache hits with this  
    payload = {  
      "model": "fake-openai-endpoint",  
      "messages": [{"role": "user", "content": f"{uuid.uuid4()} This is a test there will be no cache hits and we'll fill up the context" * 150 }],  
      "user": "my-new-end-user-1"  
    }  
    response = self.client.post("chat/completions", json=payload)  
    if response.status_code != 200:  
      # log the errors in error.txt  
      with open("error.txt", "a") as error_log:  
        error_log.write(response.text + "\n")  
    
  
  
  def on_start(self):  
    self.api_key = os.getenv('API_KEY', 'sk-1234')  
    self.client.headers.update({'Authorization': f'Bearer {self.api_key}'})  

```

Previous
Benchmarks
Next
LiteLLM SDK vs OpenAI
  * Pre-Testing Checklist
  * Load Test - Fake OpenAI Endpoint
    * Expected Performance
    * Run Test
  * Load test - Endpoints with Rate Limits
    * Expected Performance
    * Run Test
  * Prometheus Metrics for debugging load tests
  * Machine Specifications for Running Locust
  * Machine Specifications for Running LiteLLM Proxy
  * Locust file used for testing


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
      * Image Generations
      * [BETA] Image Variations
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /images
  * [BETA] Image Variations


On this page
# [BETA] Image Variations
OpenAI's `/image/variations` endpoint is now supported.
## Quick Start​
```
from litellm import image_variation  
import os   
  
# set env vars   
os.environ["OPENAI_API_KEY"] = ""  
os.environ["TOPAZ_API_KEY"] = ""  
  
# openai call  
response = image_variation(  
  model="dall-e-2", image=image_url  
)  
  
# topaz call  
response = image_variation(  
  model="topaz/Standard V2", image=image_url  
)  
  
print(response)  

```

## Supported Providers​
  * OpenAI
  * Topaz


Previous
Image Generations
Next
/audio/transcriptions
  * Quick Start
  * Supported Providers


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
    * Benchmarks
    * LiteLLM Proxy - 1K RPS Load test on locust
    * LiteLLM SDK vs OpenAI
    * Multi-Instance TPM/RPM (litellm.Router)
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Load Testing
  * Multi-Instance TPM/RPM (litellm.Router)


On this page
# Multi-Instance TPM/RPM (litellm.Router)
Test if your defined tpm/rpm limits are respected across multiple instances of the Router object. 
In our test:
  * Max RPM per deployment is = 100 requests per minute
  * Max Throughput / min on router = 200 requests per minute (2 deployments)
  * Load we'll send through router = 600 requests per minute


info
If you don't want to call a real LLM API endpoint, you can setup a fake openai server. See code
### Code​
Let's hit the router with 600 requests per minute. 
Copy this script 👇. Save it as `test_loadtest_router.py` AND run it with `python3 test_loadtest_router.py`
```
from litellm import Router   
import litellm  
litellm.suppress_debug_info = True  
litellm.set_verbose = False  
import logging  
logging.basicConfig(level=logging.CRITICAL)  
import os, random, uuid, time, asyncio  
  
# Model list for OpenAI and Anthropic models  
model_list = [  
  {  
    "model_name": "fake-openai-endpoint",  
    "litellm_params": {  
      "model": "gpt-3.5-turbo",  
      "api_key": "my-fake-key",  
      "api_base": "http://0.0.0.0:8080",  
      "rpm": 100  
    },  
  },  
  {  
    "model_name": "fake-openai-endpoint",  
    "litellm_params": {  
      "model": "gpt-3.5-turbo",  
      "api_key": "my-fake-key",  
      "api_base": "http://0.0.0.0:8081",  
      "rpm": 100  
    },  
  },  
]  
  
router_1 = Router(model_list=model_list, num_retries=0, enable_pre_call_checks=True, routing_strategy="usage-based-routing-v2", redis_host=os.getenv("REDIS_HOST"), redis_port=os.getenv("REDIS_PORT"), redis_password=os.getenv("REDIS_PASSWORD"))  
router_2 = Router(model_list=model_list, num_retries=0, routing_strategy="usage-based-routing-v2", enable_pre_call_checks=True, redis_host=os.getenv("REDIS_HOST"), redis_port=os.getenv("REDIS_PORT"), redis_password=os.getenv("REDIS_PASSWORD"))  
  
  
  
async def router_completion_non_streaming():  
 try:  
  client: Router = random.sample([router_1, router_2], 1)[0] # randomly pick b/w clients  
  # print(f"client={client}")  
  response = await client.acompletion(  
       model="fake-openai-endpoint", # [CHANGE THIS] (if you call it something else on your proxy)  
       messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],  
     )  
  return response  
 except Exception as e:  
  # print(e)  
  return None  
   
async def loadtest_fn():  
  start = time.time()  
  n = 600 # Number of concurrent tasks  
  tasks = [router_completion_non_streaming() for _ in range(n)]  
  chat_completions = await asyncio.gather(*tasks)  
  successful_completions = [c for c in chat_completions if c is not None]  
  print(n, time.time() - start, len(successful_completions))  
  
def get_utc_datetime():  
  import datetime as dt  
  from datetime import datetime  
  
  if hasattr(dt, "UTC"):  
    return datetime.now(dt.UTC) # type: ignore  
  else:  
    return datetime.utcnow() # type: ignore  
  
  
# Run the event loop to execute the async function  
async def parent_fn():  
 for _ in range(10):  
  dt = get_utc_datetime()  
  current_minute = dt.strftime("%H-%M")  
  print(f"triggered new batch - {current_minute}")  
  await loadtest_fn()  
  await asyncio.sleep(10)  
  
asyncio.run(parent_fn())  

```

## Multi-Instance TPM/RPM Load Test (Proxy)​
Test if your defined tpm/rpm limits are respected across multiple instances. 
The quickest way to do this is by testing the proxy. The proxy uses the router under the hood, so if you're using either of them, this test should work for you. 
In our test:
  * Max RPM per deployment is = 100 requests per minute
  * Max Throughput / min on proxy = 200 requests per minute (2 deployments)
  * Load we'll send to proxy = 600 requests per minute


So we'll send 600 requests per minute, but expect only 200 requests per minute to succeed.
info
If you don't want to call a real LLM API endpoint, you can setup a fake openai server. See code
### 1. Setup config​
```
model_list:  
- litellm_params:  
  api_base: http://0.0.0.0:8080  
  api_key: my-fake-key  
  model: openai/my-fake-model  
  rpm: 100  
 model_name: fake-openai-endpoint  
- litellm_params:  
  api_base: http://0.0.0.0:8081  
  api_key: my-fake-key  
  model: openai/my-fake-model-2  
  rpm: 100  
 model_name: fake-openai-endpoint  
router_settings:  
 num_retries: 0  
 enable_pre_call_checks: true  
 redis_host: os.environ/REDIS_HOST ## 👈 IMPORTANT! Setup the proxy w/ redis  
 redis_password: os.environ/REDIS_PASSWORD  
 redis_port: os.environ/REDIS_PORT  
 routing_strategy: usage-based-routing-v2  

```

### 2. Start proxy 2 instances​
**Instance 1**
```
litellm --config /path/to/config.yaml --port 4000  
  
## RUNNING on http://0.0.0.0:4000  

```

**Instance 2**
```
litellm --config /path/to/config.yaml --port 4001  
  
## RUNNING on http://0.0.0.0:4001  

```

### 3. Run Test​
Let's hit the proxy with 600 requests per minute. 
Copy this script 👇. Save it as `test_loadtest_proxy.py` AND run it with `python3 test_loadtest_proxy.py`
```
from openai import AsyncOpenAI, AsyncAzureOpenAI  
import random, uuid  
import time, asyncio, litellm  
# import logging  
# logging.basicConfig(level=logging.DEBUG)  
#### LITELLM PROXY ####   
litellm_client = AsyncOpenAI(  
  api_key="sk-1234", # [CHANGE THIS]  
  base_url="http://0.0.0.0:4000"  
)  
litellm_client_2 = AsyncOpenAI(  
  api_key="sk-1234", # [CHANGE THIS]  
  base_url="http://0.0.0.0:4001"  
)  
  
async def proxy_completion_non_streaming():  
 try:  
  client = random.sample([litellm_client, litellm_client_2], 1)[0] # randomly pick b/w clients  
  # print(f"client={client}")  
  response = await client.chat.completions.create(  
       model="fake-openai-endpoint", # [CHANGE THIS] (if you call it something else on your proxy)  
       messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],  
     )  
  return response  
 except Exception as e:  
  # print(e)  
  return None  
   
async def loadtest_fn():  
  start = time.time()  
  n = 600 # Number of concurrent tasks  
  tasks = [proxy_completion_non_streaming() for _ in range(n)]  
  chat_completions = await asyncio.gather(*tasks)  
  successful_completions = [c for c in chat_completions if c is not None]  
  print(n, time.time() - start, len(successful_completions))  
  
def get_utc_datetime():  
  import datetime as dt  
  from datetime import datetime  
  
  if hasattr(dt, "UTC"):  
    return datetime.now(dt.UTC) # type: ignore  
  else:  
    return datetime.utcnow() # type: ignore  
  
  
# Run the event loop to execute the async function  
async def parent_fn():  
 for _ in range(10):  
  dt = get_utc_datetime()  
  current_minute = dt.strftime("%H-%M")  
  print(f"triggered new batch - {current_minute}")  
  await loadtest_fn()  
  await asyncio.sleep(10)  
  
asyncio.run(parent_fn())  
  

```

### Extra - Setup Fake OpenAI Server​
Let's setup a fake openai server with a RPM limit of 100.
Let's call our file `fake_openai_server.py`. 
```
# import sys, os  
# sys.path.insert(  
#   0, os.path.abspath("../")  
# ) # Adds the parent directory to the system path  
from fastapi import FastAPI, Request, status, HTTPException, Depends  
from fastapi.responses import StreamingResponse  
from fastapi.security import OAuth2PasswordBearer  
from fastapi.middleware.cors import CORSMiddleware  
from fastapi.responses import JSONResponse  
from fastapi import FastAPI, Request, HTTPException, UploadFile, File  
import httpx, os, json  
from openai import AsyncOpenAI  
from typing import Optional  
from slowapi import Limiter  
from slowapi.util import get_remote_address  
from slowapi.errors import RateLimitExceeded  
from fastapi import FastAPI, Request, HTTPException  
from fastapi.responses import PlainTextResponse  
  
  
class ProxyException(Exception):  
  # NOTE: DO NOT MODIFY THIS  
  # This is used to map exactly to OPENAI Exceptions  
  def __init__(  
    self,  
    message: str,  
    type: str,  
    param: Optional[str],  
    code: Optional[int],  
  ):  
    self.message = message  
    self.type = type  
    self.param = param  
    self.code = code  
  
  def to_dict(self) -> dict:  
    """Converts the ProxyException instance to a dictionary."""  
    return {  
      "message": self.message,  
      "type": self.type,  
      "param": self.param,  
      "code": self.code,  
    }  
  
  
limiter = Limiter(key_func=get_remote_address)  
app = FastAPI()  
app.state.limiter = limiter  
  
@app.exception_handler(RateLimitExceeded)  
async def _rate_limit_exceeded_handler(request: Request, exc: RateLimitExceeded):  
  return JSONResponse(status_code=429,  
            content={"detail": "Rate Limited!"})  
  
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)  
  
app.add_middleware(  
  CORSMiddleware,  
  allow_origins=["*"],  
  allow_credentials=True,  
  allow_methods=["*"],  
  allow_headers=["*"],  
)  
  
# for completion  
@app.post("/chat/completions")  
@app.post("/v1/chat/completions")  
@limiter.limit("100/minute")  
async def completion(request: Request):  
  # raise HTTPException(status_code=429, detail="Rate Limited!")  
  return {  
    "id": "chatcmpl-123",  
    "object": "chat.completion",  
    "created": 1677652288,  
    "model": None,  
    "system_fingerprint": "fp_44709d6fcb",  
    "choices": [{  
      "index": 0,  
      "message": {  
      "role": "assistant",  
      "content": "\n\nHello there, how may I assist you today?",  
      },  
      "logprobs": None,  
      "finish_reason": "stop"  
    }],  
    "usage": {  
      "prompt_tokens": 9,  
      "completion_tokens": 12,  
      "total_tokens": 21  
    }  
  }  
  
if __name__ == "__main__":  
  import socket  
  import uvicorn  
  port = 8080  
  while True:  
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  
    result = sock.connect_ex(('0.0.0.0', port))  
    if result != 0:  
      print(f"Port {port} is available, starting server...")  
      break  
    else:  
      port += 1  
  
  uvicorn.run(app, host="0.0.0.0", port=port)  

```

```
python3 fake_openai_server.py  

```

Previous
LiteLLM SDK vs OpenAI
Next
🖇️ AgentOps - LLM Observability Platform
  * Code
  * Multi-Instance TPM/RPM Load Test (Proxy)
    * 1. Setup config
    * 2. Start proxy 2 instances
    * 3. Run Test
    * Extra - Setup Fake OpenAI Server


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
    * Data Privacy and Security
    * Data Retention Policy
    * Migration Policy
    * ❤️ 🚅 Projects built on LiteLLM
    * PII Masking - LiteLLM Gateway (Deprecated Version)
    * Code Quality
    * Rules
    * [DEPRECATED] Team-based Routing
    * [DEPRECATED] Region-based Routing
    * [OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
  * Support & Talk with founders


  *   * Extras
  * Migration Policy


On this page
# Migration Policy
## New Beta Feature Introduction​
  * If we introduce a new feature that may move to the Enterprise Tier it will be clearly labeled as **Beta**. With the following example disclaimer **Example Disclaimer**


info
Beta Feature - This feature might move to LiteLLM Enterprise
## Policy if a Beta Feature moves to Enterprise​
If we decide to move a beta feature to the paid Enterprise version we will:
  * Provide **at least 30 days** notice to all users of the beta feature
  * Provide **a free 3 month License to prevent any disruptions to production**
  * Provide a **dedicated slack, discord, microsoft teams support channel** to help your team during this transition


Previous
Data Retention Policy
Next
❤️ 🚅 Projects built on LiteLLM
  * New Beta Feature Introduction
  * Policy if a Beta Feature moves to Enterprise


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /mcp [BETA] - Model Context Protocol


On this page
# /mcp [BETA] - Model Context Protocol
## Expose MCP tools on LiteLLM Proxy Server​
This allows you to define tools that can be called by any MCP compatible client. Define your `mcp_servers` with LiteLLM and all your clients can list and call available tools.
LiteLLM MCP Architecture: Use MCP tools with all LiteLLM supported models
#### How it works​
LiteLLM exposes the following MCP endpoints:
  * `/mcp/tools/list` - List all available tools
  * `/mcp/tools/call` - Call a specific tool with the provided arguments


When MCP clients connect to LiteLLM they can follow this workflow:
  1. Connect to the LiteLLM MCP server
  2. List all available tools on LiteLLM
  3. Client makes LLM API request with tool call(s)
  4. LLM API returns which tools to call and with what arguments
  5. MCP client makes MCP tool calls to LiteLLM
  6. LiteLLM makes the tool calls to the appropriate MCP server
  7. LiteLLM returns the tool call results to the MCP client


#### Usage​
#### 1. Define your tools on under `mcp_servers` in your config.yaml file.​
LiteLLM allows you to define your tools on the `mcp_servers` section in your config.yaml file. All tools listed here will be available to MCP clients (when they connect to LiteLLM and call `list_tools`).
config.yaml
```
model_list:  
 - model_name: gpt-4o  
  litellm_params:  
   model: openai/gpt-4o  
   api_key: sk-xxxxxxx  
  
mcp_servers:  
 {  
  "zapier_mcp": {  
   "url": "https://actions.zapier.com/mcp/sk-akxxxxx/sse"  
  },  
  "fetch": {  
   "url": "http://localhost:8000/sse"  
  }  
 }  

```

#### 2. Start LiteLLM Gateway​
  * Docker Run
  * litellm pip


Docker Run
```
docker run -d \  
 -p 4000:4000 \  
 -e OPENAI_API_KEY=$OPENAI_API_KEY \  
 --name my-app \  
 -v $(pwd)/my_config.yaml:/app/config.yaml \  
 my-app:latest \  
 --config /app/config.yaml \  
 --port 4000 \  
 --detailed_debug \  

```

litellm pip
```
litellm --config config.yaml --detailed_debug  

```

#### 3. Make an LLM API request​
In this example we will do the following:
  1. Use MCP client to list MCP tools on LiteLLM Proxy
  2. Use `transform_mcp_tool_to_openai_tool` to convert MCP tools to OpenAI tools
  3. Provide the MCP tools to `gpt-4o`
  4. Handle tool call from `gpt-4o`
  5. Convert OpenAI tool call to MCP tool call
  6. Execute tool call on MCP server


MCP Client List Tools
```
import asyncio  
from openai import AsyncOpenAI  
from openai.types.chat import ChatCompletionUserMessageParam  
from mcp import ClientSession  
from mcp.client.sse import sse_client  
from litellm.experimental_mcp_client.tools import (  
  transform_mcp_tool_to_openai_tool,  
  transform_openai_tool_call_request_to_mcp_tool_call_request,  
)  
  
  
async def main():  
  # Initialize clients  
    
  # point OpenAI client to LiteLLM Proxy  
  client = AsyncOpenAI(api_key="sk-1234", base_url="http://localhost:4000")  
  
  # Point MCP client to LiteLLM Proxy  
  async with sse_client("http://localhost:4000/mcp/") as (read, write):  
    async with ClientSession(read, write) as session:  
      await session.initialize()  
  
      # 1. List MCP tools on LiteLLM Proxy  
      mcp_tools = await session.list_tools()  
      print("List of MCP tools for MCP server:", mcp_tools.tools)  
  
      # Create message  
      messages = [  
        ChatCompletionUserMessageParam(  
          content="Send an email about LiteLLM supporting MCP", role="user"  
        )  
      ]  
  
      # 2. Use `transform_mcp_tool_to_openai_tool` to convert MCP tools to OpenAI tools  
      # Since OpenAI only supports tools in the OpenAI format, we need to convert the MCP tools to the OpenAI format.  
      openai_tools = [  
        transform_mcp_tool_to_openai_tool(tool) for tool in mcp_tools.tools  
      ]  
  
      # 3. Provide the MCP tools to `gpt-4o`  
      response = await client.chat.completions.create(  
        model="gpt-4o",  
        messages=messages,  
        tools=openai_tools,  
        tool_choice="auto",  
      )  
  
      # 4. Handle tool call from `gpt-4o`  
      if response.choices[0].message.tool_calls:  
        tool_call = response.choices[0].message.tool_calls[0]  
        if tool_call:  
  
          # 5. Convert OpenAI tool call to MCP tool call  
          # Since MCP servers expect tools in the MCP format, we need to convert the OpenAI tool call to the MCP format.  
          # This is done using litellm.experimental_mcp_client.tools.transform_openai_tool_call_request_to_mcp_tool_call_request  
          mcp_call = (  
            transform_openai_tool_call_request_to_mcp_tool_call_request(  
              openai_tool=tool_call.model_dump()  
            )  
          )  
  
          # 6. Execute tool call on MCP server  
          result = await session.call_tool(  
            name=mcp_call.name, arguments=mcp_call.arguments  
          )  
  
          print("Result:", result)  
  
  
# Run it  
asyncio.run(main())  

```

## LiteLLM Python SDK MCP Bridge​
LiteLLM Python SDK acts as a MCP bridge to utilize MCP tools with all LiteLLM supported models. LiteLLM offers the following features for using MCP
  * **List** Available MCP Tools: OpenAI clients can view all available MCP tools
    * `litellm.experimental_mcp_client.load_mcp_tools` to list all available MCP tools
  * **Call** MCP Tools: OpenAI clients can call MCP tools
    * `litellm.experimental_mcp_client.call_openai_tool` to call an OpenAI tool on an MCP server


### 1. List Available MCP Tools​
In this example we'll use `litellm.experimental_mcp_client.load_mcp_tools` to list all available MCP tools on any MCP server. This method can be used in two ways:
  * `format="mcp"` - (default) Return MCP tools 
    * Returns: `mcp.types.Tool`
  * `format="openai"` - Return MCP tools converted to OpenAI API compatible tools. Allows using with OpenAI endpoints.
    * Returns: `openai.types.chat.ChatCompletionToolParam`


  * LiteLLM Python SDK
  * OpenAI SDK + LiteLLM Proxy


MCP Client List Tools
```
# Create server parameters for stdio connection  
from mcp import ClientSession, StdioServerParameters  
from mcp.client.stdio import stdio_client  
import os  
import litellm  
from litellm import experimental_mcp_client  
  
  
server_params = StdioServerParameters(  
  command="python3",  
  # Make sure to update to the full absolute path to your mcp_server.py file  
  args=["./mcp_server.py"],  
)  
  
async with stdio_client(server_params) as (read, write):  
  async with ClientSession(read, write) as session:  
    # Initialize the connection  
    await session.initialize()  
  
    # Get tools  
    tools = await experimental_mcp_client.load_mcp_tools(session=session, format="openai")  
    print("MCP TOOLS: ", tools)  
  
    messages = [{"role": "user", "content": "what's (3 + 5)"}]  
    llm_response = await litellm.acompletion(  
      model="gpt-4o",  
      api_key=os.getenv("OPENAI_API_KEY"),  
      messages=messages,  
      tools=tools,  
    )  
    print("LLM RESPONSE: ", json.dumps(llm_response, indent=4, default=str))  

```

In this example we'll walk through how you can use the OpenAI SDK pointed to the LiteLLM proxy to call MCP tools. The key difference here is we use the OpenAI SDK to make the LLM API request
MCP Client List Tools
```
# Create server parameters for stdio connection  
from mcp import ClientSession, StdioServerParameters  
from mcp.client.stdio import stdio_client  
import os  
from openai import OpenAI  
from litellm import experimental_mcp_client  
  
server_params = StdioServerParameters(  
  command="python3",  
  # Make sure to update to the full absolute path to your mcp_server.py file  
  args=["./mcp_server.py"],  
)  
  
async with stdio_client(server_params) as (read, write):  
  async with ClientSession(read, write) as session:  
    # Initialize the connection  
    await session.initialize()  
  
    # Get tools using litellm mcp client  
    tools = await experimental_mcp_client.load_mcp_tools(session=session, format="openai")  
    print("MCP TOOLS: ", tools)  
  
    # Use OpenAI SDK pointed to LiteLLM proxy  
    client = OpenAI(  
      api_key="your-api-key", # Your LiteLLM proxy API key  
      base_url="http://localhost:4000" # Your LiteLLM proxy URL  
    )  
  
    messages = [{"role": "user", "content": "what's (3 + 5)"}]  
    llm_response = client.chat.completions.create(  
      model="gpt-4",  
      messages=messages,  
      tools=tools  
    )  
    print("LLM RESPONSE: ", llm_response)  

```

### 2. List and Call MCP Tools​
In this example we'll use 
  * `litellm.experimental_mcp_client.load_mcp_tools` to list all available MCP tools on any MCP server
  * `litellm.experimental_mcp_client.call_openai_tool` to call an OpenAI tool on an MCP server


The first llm response returns a list of OpenAI tools. We take the first tool call from the LLM response and pass it to `litellm.experimental_mcp_client.call_openai_tool` to call the tool on the MCP server.
#### How `litellm.experimental_mcp_client.call_openai_tool` works​
  * Accepts an OpenAI Tool Call from the LLM response
  * Converts the OpenAI Tool Call to an MCP Tool
  * Calls the MCP Tool on the MCP server
  * Returns the result of the MCP Tool call


  * LiteLLM Python SDK
  * OpenAI SDK + LiteLLM Proxy


MCP Client List and Call Tools
```
# Create server parameters for stdio connection  
from mcp import ClientSession, StdioServerParameters  
from mcp.client.stdio import stdio_client  
import os  
import litellm  
from litellm import experimental_mcp_client  
  
  
server_params = StdioServerParameters(  
  command="python3",  
  # Make sure to update to the full absolute path to your mcp_server.py file  
  args=["./mcp_server.py"],  
)  
  
async with stdio_client(server_params) as (read, write):  
  async with ClientSession(read, write) as session:  
    # Initialize the connection  
    await session.initialize()  
  
    # Get tools  
    tools = await experimental_mcp_client.load_mcp_tools(session=session, format="openai")  
    print("MCP TOOLS: ", tools)  
  
    messages = [{"role": "user", "content": "what's (3 + 5)"}]  
    llm_response = await litellm.acompletion(  
      model="gpt-4o",  
      api_key=os.getenv("OPENAI_API_KEY"),  
      messages=messages,  
      tools=tools,  
    )  
    print("LLM RESPONSE: ", json.dumps(llm_response, indent=4, default=str))  
  
    openai_tool = llm_response["choices"][0]["message"]["tool_calls"][0]  
    # Call the tool using MCP client  
    call_result = await experimental_mcp_client.call_openai_tool(  
      session=session,  
      openai_tool=openai_tool,  
    )  
    print("MCP TOOL CALL RESULT: ", call_result)  
  
    # send the tool result to the LLM  
    messages.append(llm_response["choices"][0]["message"])  
    messages.append(  
      {  
        "role": "tool",  
        "content": str(call_result.content[0].text),  
        "tool_call_id": openai_tool["id"],  
      }  
    )  
    print("final messages with tool result: ", messages)  
    llm_response = await litellm.acompletion(  
      model="gpt-4o",  
      api_key=os.getenv("OPENAI_API_KEY"),  
      messages=messages,  
      tools=tools,  
    )  
    print(  
      "FINAL LLM RESPONSE: ", json.dumps(llm_response, indent=4, default=str)  
    )  

```

In this example we'll walk through how you can use the OpenAI SDK pointed to the LiteLLM proxy to call MCP tools. The key difference here is we use the OpenAI SDK to make the LLM API request
MCP Client with OpenAI SDK
```
# Create server parameters for stdio connection  
from mcp import ClientSession, StdioServerParameters  
from mcp.client.stdio import stdio_client  
import os  
from openai import OpenAI  
from litellm import experimental_mcp_client  
  
server_params = StdioServerParameters(  
  command="python3",  
  # Make sure to update to the full absolute path to your mcp_server.py file  
  args=["./mcp_server.py"],  
)  
  
async with stdio_client(server_params) as (read, write):  
  async with ClientSession(read, write) as session:  
    # Initialize the connection  
    await session.initialize()  
  
    # Get tools using litellm mcp client  
    tools = await experimental_mcp_client.load_mcp_tools(session=session, format="openai")  
    print("MCP TOOLS: ", tools)  
  
    # Use OpenAI SDK pointed to LiteLLM proxy  
    client = OpenAI(  
      api_key="your-api-key", # Your LiteLLM proxy API key  
      base_url="http://localhost:8000" # Your LiteLLM proxy URL  
    )  
  
    messages = [{"role": "user", "content": "what's (3 + 5)"}]  
    llm_response = client.chat.completions.create(  
      model="gpt-4",  
      messages=messages,  
      tools=tools  
    )  
    print("LLM RESPONSE: ", llm_response)  
  
    # Get the first tool call  
    tool_call = llm_response.choices[0].message.tool_calls[0]  
      
    # Call the tool using MCP client  
    call_result = await experimental_mcp_client.call_openai_tool(  
      session=session,  
      openai_tool=tool_call.model_dump(),  
    )  
    print("MCP TOOL CALL RESULT: ", call_result)  
  
    # Send the tool result back to the LLM  
    messages.append(llm_response.choices[0].message.model_dump())  
    messages.append({  
      "role": "tool",  
      "content": str(call_result.content[0].text),  
      "tool_call_id": tool_call.id,  
    })  
  
    final_response = client.chat.completions.create(  
      model="gpt-4",  
      messages=messages,  
      tools=tools  
    )  
    print("FINAL RESPONSE: ", final_response)  

```

Previous
/v1/messages [BETA]
Next
Image Generations
  * Expose MCP tools on LiteLLM Proxy Server
  * LiteLLM Python SDK MCP Bridge
    * 1. List Available MCP Tools
    * 2. List and Call MCP Tools


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
    * Setting API Keys, Base, Version
    * Completion Token Usage & Cost
    * Custom Pricing - SageMaker, Azure, etc
    * litellm.aembedding()
    * litellm.moderation()
    * Budget Manager
    * Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
    * Migration Guide - LiteLLM v1.0.0+
    * LangChain, LlamaIndex, Instructor Integration
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Python SDK
  * Migration Guide - LiteLLM v1.0.0+


On this page
# Migration Guide - LiteLLM v1.0.0+
When we have breaking changes (i.e. going from 1.x.x to 2.x.x), we will document those changes here.
## `1.0.0`​
**Last Release before breaking change** : 0.14.0
**What changed?**
  * Requires `openai>=1.0.0`
  * `openai.InvalidRequestError` → `openai.BadRequestError`
  * `openai.ServiceUnavailableError` → `openai.APIStatusError`
  * _NEW_ litellm client, allow users to pass api_key
    * `litellm.Litellm(api_key="sk-123")`
  * response objects now inherit from `BaseModel` (prev. `OpenAIObject`)
  * _NEW_ default exception - `APIConnectionError` (prev. `APIError`)
  * litellm.get_max_tokens() now returns an int not a dict
```
max_tokens = litellm.get_max_tokens("gpt-3.5-turbo") # returns an int not a dict   
assert max_tokens==4097  

```

  * Streaming - OpenAI Chunks now return `None` for empty stream chunks. This is how to process stream chunks with content
```
response = litellm.completion(model="gpt-3.5-turbo", messages=messages, stream=True)  
for part in response:  
  print(part.choices[0].delta.content or "")  

```



**How can we communicate changes better?** Tell us
  * Discord
  * Email (krrish@berri.ai/ishaan@berri.ai)
  * Text us (+17708783106)


Previous
Caching - In-Memory, Redis, s3, Redis Semantic Cache, Disk
Next
Using ChatLiteLLM() - Langchain
  * `1.0.0`


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
    * Benchmarks
    * LiteLLM Proxy - 1K RPS Load test on locust
    * LiteLLM SDK vs OpenAI
    * Multi-Instance TPM/RPM (litellm.Router)
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Load Testing
  * LiteLLM SDK vs OpenAI


# LiteLLM SDK vs OpenAI
Here is a script to load test LiteLLM vs OpenAI 
```
from openai import AsyncOpenAI, AsyncAzureOpenAI  
import random, uuid  
import time, asyncio, litellm  
# import logging  
# logging.basicConfig(level=logging.DEBUG)  
#### LITELLM PROXY ####   
litellm_client = AsyncOpenAI(  
  api_key="sk-1234", # [CHANGE THIS]  
  base_url="http://0.0.0.0:4000"  
)  
  
#### AZURE OPENAI CLIENT ####   
client = AsyncAzureOpenAI(  
  api_key="my-api-key", # [CHANGE THIS]  
  azure_endpoint="my-api-base", # [CHANGE THIS]  
  api_version="2023-07-01-preview"   
)  
  
  
#### LITELLM ROUTER ####   
model_list = [  
 {  
  "model_name": "azure-canada",  
  "litellm_params": {  
   "model": "azure/my-azure-deployment-name", # [CHANGE THIS]  
   "api_key": "my-api-key", # [CHANGE THIS]  
   "api_base": "my-api-base", # [CHANGE THIS]  
   "api_version": "2023-07-01-preview"  
  }  
 }  
]  
  
router = litellm.Router(model_list=model_list)  
  
async def openai_completion():  
 try:  
  response = await client.chat.completions.create(  
       model="gpt-35-turbo",  
       messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],  
       stream=True  
     )  
  return response  
 except Exception as e:  
  print(e)  
  return None  
   
  
async def router_completion():  
 try:  
  response = await router.acompletion(  
       model="azure-canada", # [CHANGE THIS]  
       messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],  
       stream=True  
     )  
  return response  
 except Exception as e:  
  print(e)  
  return None  
  
async def proxy_completion_non_streaming():  
 try:  
  response = await litellm_client.chat.completions.create(  
       model="sagemaker-models", # [CHANGE THIS] (if you call it something else on your proxy)  
       messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],  
     )  
  return response  
 except Exception as e:  
  print(e)  
  return None  
  
async def loadtest_fn():  
  start = time.time()  
  n = 500 # Number of concurrent tasks  
  tasks = [proxy_completion_non_streaming() for _ in range(n)]  
  chat_completions = await asyncio.gather(*tasks)  
  successful_completions = [c for c in chat_completions if c is not None]  
  print(n, time.time() - start, len(successful_completions))  
  
# Run the event loop to execute the async function  
asyncio.run(loadtest_fn())  
  

```

Previous
LiteLLM Proxy - 1K RPS Load test on locust
Next
Multi-Instance TPM/RPM (litellm.Router)
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * /moderations


On this page
# /moderations
### Usage​
  * LiteLLM Python SDK
  * LiteLLM Proxy Server


```
from litellm import moderation  
  
response = moderation(  
  input="hello from litellm",  
  model="text-moderation-stable"  
)  

```

For `/moderations` endpoint, there is **no need to specify`model` in the request or on the litellm config.yaml**
Start litellm proxy server 
```
litellm  

```

  * OpenAI Python SDK
  * Curl Request


```
from openai import OpenAI  
  
# set base_url to your proxy server  
# set api_key to send to proxy server  
client = OpenAI(api_key="<proxy-api-key>", base_url="http://0.0.0.0:4000")  
  
response = client.moderations.create(  
  input="hello from litellm",  
  model="text-moderation-stable" # optional, defaults to `omni-moderation-latest`  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/moderations' \  
  --header 'Content-Type: application/json' \  
  --header 'Authorization: Bearer sk-1234' \  
  --data '{"input": "Sample text goes here", "model": "text-moderation-stable"}'  

```

## Input Params​
LiteLLM accepts and translates the OpenAI Moderation params across all supported providers.
### Required Fields​
  * `input`: _string or array_ - Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models.
    * If string: A string of text to classify for moderation
    * If array of strings: An array of strings to classify for moderation
    * If array of objects: An array of multi-modal inputs to the moderation model, where each object can be:
      * An object describing an image to classify with:
        * `type`: _string, required_ - Always `image_url`
        * `image_url`: _object, required_ - Contains either an image URL or a data URL for a base64 encoded image
      * An object describing text to classify with:
        * `type`: _string, required_ - Always `text`
        * `text`: _string, required_ - A string of text to classify


### Optional Fields​
  * `model`: _string (optional)_ - The moderation model to use. Defaults to `omni-moderation-latest`.


## Output Format​
Here's the exact json output and type you can expect from all moderation calls:
**LiteLLM follows OpenAI's output format**
```
{  
 "id": "modr-AB8CjOTu2jiq12hp1AQPfeqFWaORR",  
 "model": "text-moderation-007",  
 "results": [  
  {  
   "flagged": true,  
   "categories": {  
    "sexual": false,  
    "hate": false,  
    "harassment": true,  
    "self-harm": false,  
    "sexual/minors": false,  
    "hate/threatening": false,  
    "violence/graphic": false,  
    "self-harm/intent": false,  
    "self-harm/instructions": false,  
    "harassment/threatening": true,  
    "violence": true  
   },  
   "category_scores": {  
    "sexual": 0.000011726012417057063,  
    "hate": 0.22706663608551025,  
    "harassment": 0.5215635299682617,  
    "self-harm": 2.227119921371923e-6,  
    "sexual/minors": 7.107352217872176e-8,  
    "hate/threatening": 0.023547329008579254,  
    "violence/graphic": 0.00003391829886822961,  
    "self-harm/intent": 1.646940972932498e-6,  
    "self-harm/instructions": 1.1198755256458526e-9,  
    "harassment/threatening": 0.5694745779037476,  
    "violence": 0.9971134662628174  
   }  
  }  
 ]  
}  
  

```

## **Supported Providers**​
Provider  
---  
OpenAI  
Previous
/fine_tuning
Next
Routing, Loadbalancing & Fallbacks
  * Usage
  * Input Params
    * Required Fields
    * Optional Fields
  * Output Format
  * **Supported Providers**


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Argilla


On this page
# Argilla
Argilla is a collaborative annotation tool for AI engineers and domain experts who need to build high-quality datasets for their projects.
## Getting Started​
To log the data to Argilla, first you need to deploy the Argilla server. If you have not deployed the Argilla server, please follow the instructions here.
Next, you will need to configure and create the Argilla dataset.
```
import argilla as rg  
  
client = rg.Argilla(api_url="<api_url>", api_key="<api_key>")  
  
settings = rg.Settings(  
  guidelines="These are some guidelines.",  
  fields=[  
    rg.ChatField(  
      name="user_input",  
    ),  
    rg.TextField(  
      name="llm_output",  
    ),  
  ],  
  questions=[  
    rg.RatingQuestion(  
      name="rating",  
      values=[1, 2, 3, 4, 5, 6, 7],  
    ),  
  ],  
)  
  
dataset = rg.Dataset(  
  name="my_first_dataset",  
  settings=settings,  
)  
  
dataset.create()  

```

For further configuration, please refer to the Argilla documentation.
## Usage​
  * SDK
  * PROXY


```
import os  
import litellm  
from litellm import completion  
  
# add env vars  
os.environ["ARGILLA_API_KEY"]="argilla.apikey"  
os.environ["ARGILLA_BASE_URL"]="http://localhost:6900"  
os.environ["ARGILLA_DATASET_NAME"]="my_first_dataset"    
os.environ["OPENAI_API_KEY"]="sk-proj-..."  
  
litellm.callbacks = ["argilla"]  
  
# add argilla transformation object  
litellm.argilla_transformation_object = {  
  "user_input": "messages", # 👈 key= argilla field, value = either message (argilla.ChatField) | response (argilla.TextField)  
  "llm_output": "response"  
}  
  
## LLM CALL ##   
response = completion(  
  model="gpt-3.5-turbo",  
  messages=[{"role": "user", "content": "Hello, how are you?"}],  
)  

```

```
litellm_settings:  
 callbacks: ["argilla"]  
 argilla_transformation_object:  
  user_input: "messages" # 👈 key= argilla field, value = either message (argilla.ChatField) | response (argilla.TextField)  
  llm_output: "response"  

```

## Example Output​
## Add sampling rate to Argilla calls​
To just log a sample of calls to argilla, add `ARGILLA_SAMPLING_RATE` to your env vars.
```
ARGILLA_SAMPLING_RATE=0.1 # log 10% of calls to argilla  

```

Previous
Logfire
Next
Arize AI
  * Getting Started
  * Usage
  * Example Output
  * Add sampling rate to Argilla calls


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Arize AI


On this page
# Arize AI
AI Observability and Evaluation Platform
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
## Pre-Requisites​
Make an account on Arize AI
## Quick Start​
Use just 2 lines of code, to instantly log your responses **across all providers** with arize
You can also use the instrumentor option instead of the callback, which you can find here.
```
litellm.callbacks = ["arize"]  

```

```
  
import litellm  
import os  
  
os.environ["ARIZE_SPACE_KEY"] = ""  
os.environ["ARIZE_API_KEY"] = ""  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set arize as a callback, litellm will send the data to arize  
litellm.callbacks = ["arize"]  
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

### Using with LiteLLM Proxy​
  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["arize"]  
  
general_settings:  
 master_key: "sk-1234" # can also be set as an environment variable  
  
environment_variables:  
  ARIZE_SPACE_KEY: "d0*****"  
  ARIZE_API_KEY: "141a****"  
  ARIZE_ENDPOINT: "https://otlp.arize.com/v1" # OPTIONAL - your custom arize GRPC api endpoint  
  ARIZE_HTTP_ENDPOINT: "https://otlp.arize.com/v1" # OPTIONAL - your custom arize HTTP api endpoint. Set either this or ARIZE_ENDPOINT or Neither (defaults to https://otlp.arize.com/v1 on grpc)  

```

  1. Start the proxy


```
litellm --config config.yaml  

```

  1. Test it!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{ "model": "gpt-4", "messages": [{"role": "user", "content": "Hi 👋 - i'm openai"}]}'  

```

## Pass Arize Space/Key per-request​
Supported parameters:
  * `arize_api_key`
  * `arize_space_key`


  * SDK
  * PROXY


```
import litellm  
import os  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set arize as a callback, litellm will send the data to arize  
litellm.callbacks = ["arize"]  
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ],  
 arize_api_key=os.getenv("ARIZE_SPACE_2_API_KEY"),  
 arize_space_key=os.getenv("ARIZE_SPACE_2_KEY"),  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["arize"]  
  
general_settings:  
 master_key: "sk-1234" # can also be set as an environment variable  

```

  1. Start the proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it!


  * CURL
  * OpenAI Python


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gpt-4",  
 "messages": [{"role": "user", "content": "Hi 👋 - i'm openai"}],  
 "arize_api_key": "ARIZE_SPACE_2_API_KEY",  
 "arize_space_key": "ARIZE_SPACE_2_KEY"  
}'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
   "arize_api_key": "ARIZE_SPACE_2_API_KEY",  
   "arize_space_key": "ARIZE_SPACE_2_KEY"  
  }  
)  
  
print(response)  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
Argilla
Next
Phoenix OSS
  * Pre-Requisites
  * Quick Start
    * Using with LiteLLM Proxy
  * Pass Arize Space/Key per-request
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Athina


On this page
# Athina
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
Athina is an evaluation framework and production monitoring platform for your LLM-powered app. Athina is designed to enhance the performance and reliability of AI applications through real-time monitoring, granular analytics, and plug-and-play evaluations.
## Getting Started​
Use Athina to log requests across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)
liteLLM provides `callbacks`, making it easy for you to log data depending on the status of your responses.
## Using Callbacks​
First, sign up to get an API_KEY on the Athina dashboard.
Use just 1 line of code, to instantly log your responses **across all providers** with Athina:
```
litellm.success_callback = ["athina"]  

```

### Complete code​
```
from litellm import completion  
  
## set env variables  
os.environ["ATHINA_API_KEY"] = "your-athina-api-key"  
os.environ["OPENAI_API_KEY"]= ""  
  
# set callback  
litellm.success_callback = ["athina"]  
  
#openai call  
response = completion(  
 model="gpt-3.5-turbo",   
 messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}]  
)   

```

## Additional information in metadata​
You can send some additional information to Athina by using the `metadata` field in completion. This can be useful for sending metadata about the request, such as the customer_id, prompt_slug, or any other information you want to track.
```
#openai call with additional metadata  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ],  
 metadata={  
  "environment": "staging",  
  "prompt_slug": "my_prompt_slug/v1"  
 }  
)  

```

Following are the allowed fields in metadata, their types, and their descriptions:
  * `environment: Optional[str]` - Environment your app is running in (ex: production, staging, etc). This is useful for segmenting inference calls by environment.
  * `prompt_slug: Optional[str]` - Identifier for the prompt used for inference. This is useful for segmenting inference calls by prompt.
  * `customer_id: Optional[str]` - This is your customer ID. This is useful for segmenting inference calls by customer.
  * `customer_user_id: Optional[str]` - This is the end user ID. This is useful for segmenting inference calls by the end user.
  * `session_id: Optional[str]` - is the session or conversation ID. This is used for grouping different inferences into a conversation or chain. [Read more].(https://docs.athina.ai/logging/grouping_inferences)
  * `external_reference_id: Optional[str]` - This is useful if you want to associate your own internal identifier with the inference logged to Athina.
  * `context: Optional[Union[dict, str]]` - This is the context used as information for the prompt. For RAG applications, this is the "retrieved" data. You may log context as a string or as an object (dictionary).
  * `expected_response: Optional[str]` - This is the reference response to compare against for evaluation purposes. This is useful for segmenting inference calls by expected response.
  * `user_query: Optional[str]` - This is the user's query. For conversational applications, this is the user's last message.
  * `tags: Optional[list]` - This is a list of tags. This is useful for segmenting inference calls by tags.
  * `user_feedback: Optional[str]` - The end user’s feedback.
  * `model_options: Optional[dict]` - This is a dictionary of model options. This is useful for getting insights into how model behavior affects your end users.
  * `custom_attributes: Optional[dict]` - This is a dictionary of custom attributes. This is useful for additional information about the inference.


## Using a self hosted deployment of Athina​
If you are using a self hosted deployment of Athina, you will need to set the `ATHINA_BASE_URL` environment variable to point to your self hosted deployment.
```
...  
os.environ["ATHINA_BASE_URL"]= "http://localhost:9000"  
...  

```

## Support & Talk with Athina Team​
  * Schedule Demo 👋
  * Website 💻
  * Docs 📖
  * Demo Video 📺
  * Our emails ✉️ shiv@athina.ai, akshat@athina.ai, vivek@athina.ai


Previous
Slack - Logging LLM Input/Output, Exceptions
Next
Greenscale - Track LLM Spend and Responsible Usage
  * Getting Started
  * Using Callbacks
    * Complete code
  * Additional information in metadata
  * Using a self hosted deployment of Athina
  * Support & Talk with Athina Team


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Braintrust - Evals + Logging


On this page
# Braintrust - Evals + Logging
Braintrust manages evaluations, logging, prompt playground, to data management for AI products.
## Quick Start​
```
# pip install langfuse   
import litellm  
import os  
  
# set env   
os.environ["BRAINTRUST_API_KEY"] = ""   
os.environ['OPENAI_API_KEY']=""  
  
# set braintrust as a callback, litellm will send the data to braintrust  
litellm.callbacks = ["braintrust"]   
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

## OpenAI Proxy Usage​
  1. Add keys to env 


```
BRAINTRUST_API_KEY=""   

```

  1. Add braintrust to callbacks 


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  
  
  
litellm_settings:  
 callbacks: ["braintrust"]  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "model": "groq-llama3",  
  "messages": [  
    { "role": "system", "content": "Use your tools smartly"},  
    { "role": "user", "content": "What time is it now? Use your tool"}  
  ]  
}'  

```

## Advanced - pass Project ID or name​
  * SDK
  * PROXY


```
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ],   
 metadata={  
  "project_id": "1234",  
  # passing project_name will try to find a project with that name, or create one if it doesn't exist  
  # if both project_id and project_name are passed, project_id will be used  
  # "project_name": "my-special-project"   
 }  
)  

```

**Curl**
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "model": "groq-llama3",  
  "messages": [  
    { "role": "system", "content": "Use your tools smartly"},  
    { "role": "user", "content": "What time is it now? Use your tool"}  
  ],  
  "metadata": {  
    "project_id": "my-special-project"  
  }  
}'  

```

**OpenAI SDK**
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={ # pass in any provider-specific param, if not supported by openai, https://docs.litellm.ai/docs/completion/input#provider-specific-params  
    "metadata": { # 👈 use for logging additional params (e.g. to langfuse)  
      "project_id": "my-special-project"  
    }  
  }  
)  
  
print(response)  

```

For more examples, **Click Here**
## Full API Spec​
Here's everything you can pass in metadata for a braintrust request 
`braintrust_*` - any metadata field starting with `braintrust_` will be passed as metadata to the logging request 
`project_id` - set the project id for a braintrust call. Default is `litellm`.
Previous
Scrub Logged Data
Next
Sentry - Log LLM Exceptions
  * Quick Start
  * OpenAI Proxy Usage
  * Advanced - pass Project ID or name
  * Full API Spec


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Google Cloud Storage Buckets


On this page
# Google Cloud Storage Buckets
Log LLM Logs to Google Cloud Storage Buckets
info
✨ This is an Enterprise only feature Get Started with Enterprise here
### Usage​
  1. Add `gcs_bucket` to LiteLLM Config.yaml


```
model_list:  
- litellm_params:  
  api_base: https://openai-function-calling-workers.tasslexyz.workers.dev/  
  api_key: my-fake-key  
  model: openai/my-fake-model  
 model_name: fake-openai-endpoint  
  
litellm_settings:  
 callbacks: ["gcs_bucket"] # 👈 KEY CHANGE # 👈 KEY CHANGE  

```

  1. Set required env variables


```
GCS_BUCKET_NAME="<your-gcs-bucket-name>"  
GCS_PATH_SERVICE_ACCOUNT="/Users/ishaanjaffer/Downloads/adroit-crow-413218-a956eef1a2a8.json" # Add path to service account.json  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

## Expected Logs on GCS Buckets​
### Fields Logged on GCS Buckets​
**The standard logging object is logged on GCS Bucket**
## Getting `service_account.json` from Google Cloud Console​
  1. Go to Google Cloud Console
  2. Search for IAM & Admin
  3. Click on Service Accounts
  4. Select a Service Account
  5. Click on 'Keys' -> Add Key -> Create New Key -> JSON
  6. Save the JSON file and add the path to `GCS_PATH_SERVICE_ACCOUNT`


## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
🔁 MLflow - OSS LLM Observability and Evaluation
Next
Langsmith - Logging LLM Input/Output
  * Usage
  * Expected Logs on GCS Buckets
    * Fields Logged on GCS Buckets
  * Getting `service_account.json` from Google Cloud Console
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Helicone - OSS LLM Observability Platform


On this page
# Helicone - OSS LLM Observability Platform
tip
This is community maintained. Please make an issue if you run into a bug: https://github.com/BerriAI/litellm
Helicone is an open source observability platform that proxies your LLM requests and provides key insights into your usage, spend, latency and more.
## Using Helicone with LiteLLM​
LiteLLM provides `success_callbacks` and `failure_callbacks`, allowing you to easily log data to Helicone based on the status of your responses.
### Supported LLM Providers​
Helicone can log requests across various LLM providers, including:
  * OpenAI
  * Azure
  * Anthropic
  * Gemini
  * Groq
  * Cohere
  * Replicate
  * And more


### Integration Methods​
There are two main approaches to integrate Helicone with LiteLLM:
  1. Using callbacks
  2. Using Helicone as a proxy


Let's explore each method in detail.
### Approach 1: Use Callbacks​
Use just 1 line of code to instantly log your responses **across all providers** with Helicone:
```
litellm.success_callback = ["helicone"]  

```

Complete Code
```
import os  
from litellm import completion  
  
## Set env variables  
os.environ["HELICONE_API_KEY"] = "your-helicone-key"  
os.environ["OPENAI_API_KEY"] = "your-openai-key"  
  
# Set callbacks  
litellm.success_callback = ["helicone"]  
  
# OpenAI call  
response = completion(  
  model="gpt-4o",  
  messages=[{"role": "user", "content": "Hi 👋 - I'm OpenAI"}],  
)  
  
print(response)  

```

### Approach 2: Use Helicone as a proxy​
Helicone's proxy provides advanced functionality like caching, rate limiting, LLM security through PromptArmor and more.
To use Helicone as a proxy for your LLM requests:
  1. Set Helicone as your base URL via: litellm.api_base
  2. Pass in Helicone request headers via: litellm.metadata


Complete Code:
```
import os  
import litellm  
from litellm import completion  
  
litellm.api_base = "https://oai.hconeai.com/v1"  
litellm.headers = {  
  "Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}", # Authenticate to send requests to Helicone API  
}  
  
response = litellm.completion(  
  model="gpt-3.5-turbo",  
  messages=[{"role": "user", "content": "How does a court case get to the Supreme Court?"}]  
)  
  
print(response)  

```

### Advanced Usage​
You can add custom metadata and properties to your requests using Helicone headers. Here are some examples:
```
litellm.metadata = {  
  "Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}", # Authenticate to send requests to Helicone API  
  "Helicone-User-Id": "user-abc", # Specify the user making the request  
  "Helicone-Property-App": "web", # Custom property to add additional information  
  "Helicone-Property-Custom": "any-value", # Add any custom property  
  "Helicone-Prompt-Id": "prompt-supreme-court", # Assign an ID to associate this prompt with future versions  
  "Helicone-Cache-Enabled": "true", # Enable caching of responses  
  "Cache-Control": "max-age=3600", # Set cache limit to 1 hour  
  "Helicone-RateLimit-Policy": "10;w=60;s=user", # Set rate limit policy  
  "Helicone-Retry-Enabled": "true", # Enable retry mechanism  
  "helicone-retry-num": "3", # Set number of retries  
  "helicone-retry-factor": "2", # Set exponential backoff factor  
  "Helicone-Model-Override": "gpt-3.5-turbo-0613", # Override the model used for cost calculation  
  "Helicone-Session-Id": "session-abc-123", # Set session ID for tracking  
  "Helicone-Session-Path": "parent-trace/child-trace", # Set session path for hierarchical tracking  
  "Helicone-Omit-Response": "false", # Include response in logging (default behavior)  
  "Helicone-Omit-Request": "false", # Include request in logging (default behavior)  
  "Helicone-LLM-Security-Enabled": "true", # Enable LLM security features  
  "Helicone-Moderations-Enabled": "true", # Enable content moderation  
  "Helicone-Fallbacks": '["gpt-3.5-turbo", "gpt-4"]', # Set fallback models  
}  

```

### Caching and Rate Limiting​
Enable caching and set up rate limiting policies:
```
litellm.metadata = {  
  "Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}", # Authenticate to send requests to Helicone API  
  "Helicone-Cache-Enabled": "true", # Enable caching of responses  
  "Cache-Control": "max-age=3600", # Set cache limit to 1 hour  
  "Helicone-RateLimit-Policy": "100;w=3600;s=user", # Set rate limit policy  
}  

```

### Session Tracking and Tracing​
Track multi-step and agentic LLM interactions using session IDs and paths:
```
litellm.metadata = {  
  "Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}", # Authenticate to send requests to Helicone API  
  "Helicone-Session-Id": "session-abc-123", # The session ID you want to track  
  "Helicone-Session-Path": "parent-trace/child-trace", # The path of the session  
}  

```

  * `Helicone-Session-Id`: Use this to specify the unique identifier for the session you want to track. This allows you to group related requests together.
  * `Helicone-Session-Path`: This header defines the path of the session, allowing you to represent parent and child traces. For example, "parent/child" represents a child trace of a parent trace.


By using these two headers, you can effectively group and visualize multi-step LLM interactions, gaining insights into complex AI workflows.
### Retry and Fallback Mechanisms​
Set up retry mechanisms and fallback options:
```
litellm.metadata = {  
  "Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}", # Authenticate to send requests to Helicone API  
  "Helicone-Retry-Enabled": "true", # Enable retry mechanism  
  "helicone-retry-num": "3", # Set number of retries  
  "helicone-retry-factor": "2", # Set exponential backoff factor  
  "Helicone-Fallbacks": '["gpt-3.5-turbo", "gpt-4"]', # Set fallback models  
}  

```

> **Supported Headers** - For a full list of supported Helicone headers and their descriptions, please refer to the Helicone documentation. By utilizing these headers and metadata options, you can gain deeper insights into your LLM usage, optimize performance, and better manage your AI workflows with Helicone and LiteLLM.
Previous
Lago - Usage Based Billing
Next
OpenMeter - Usage-Based Billing
  * Using Helicone with LiteLLM
    * Supported LLM Providers
    * Integration Methods
    * Approach 1: Use Callbacks
    * Approach 2: Use Helicone as a proxy
    * Advanced Usage
    * Caching and Rate Limiting
    * Session Tracking and Tracing
    * Retry and Fallback Mechanisms


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Greenscale - Track LLM Spend and Responsible Usage


On this page
# Greenscale - Track LLM Spend and Responsible Usage
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
Greenscale is a production monitoring platform for your LLM-powered app that provides you granular key insights into your GenAI spending and responsible usage. Greenscale only captures metadata to minimize the exposure risk of personally identifiable information (PII).
## Getting Started​
Use Greenscale to log requests across all LLM Providers
liteLLM provides `callbacks`, making it easy for you to log data depending on the status of your responses.
## Using Callbacks​
First, email `hello@greenscale.ai` to get an API_KEY.
Use just 1 line of code, to instantly log your responses **across all providers** with Greenscale:
```
litellm.success_callback = ["greenscale"]  

```

### Complete code​
```
from litellm import completion  
  
## set env variables  
os.environ['GREENSCALE_API_KEY'] = 'your-greenscale-api-key'  
os.environ['GREENSCALE_ENDPOINT'] = 'greenscale-endpoint'  
os.environ["OPENAI_API_KEY"]= ""  
  
# set callback  
litellm.success_callback = ["greenscale"]  
  
#openai call  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}]  
 metadata={  
  "greenscale_project": "acme-project",  
  "greenscale_application": "acme-application"  
 }  
)  

```

## Additional information in metadata​
You can send any additional information to Greenscale by using the `metadata` field in completion and `greenscale_` prefix. This can be useful for sending metadata about the request, such as the project and application name, customer_id, environment, or any other information you want to track usage. `greenscale_project` and `greenscale_application` are required fields.
```
#openai call with additional metadata  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ],  
 metadata={  
  "greenscale_project": "acme-project",  
  "greenscale_application": "acme-application",  
  "greenscale_customer_id": "customer-123"  
 }  
)  

```

## Support & Talk with Greenscale Team​
  * Schedule Demo 👋
  * Website 💻
  * Our email ✉️ `hello@greenscale.ai`


Previous
Athina
Next
Supabase Tutorial
  * Getting Started
  * Using Callbacks
    * Complete code
  * Additional information in metadata
  * Support & Talk with Greenscale Team


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Humanloop


On this page
# Humanloop
Humanloop enables product teams to build robust AI features with LLMs, using best-in-class tooling for Evaluation, Prompt Management, and Observability.
## Getting Started​
Use Humanloop to manage prompts across all LiteLLM Providers.
  * SDK
  * PROXY


```
import os   
import litellm  
  
os.environ["HUMANLOOP_API_KEY"] = "" # [OPTIONAL] set here or in `.completion`  
  
litellm.set_verbose = True # see raw request to provider  
  
resp = litellm.completion(  
  model="humanloop/gpt-3.5-turbo",  
  prompt_id="test-chat-prompt",  
  prompt_variables={"user_message": "this is used"}, # [OPTIONAL]  
  messages=[{"role": "user", "content": "<IGNORED>"}],  
  # humanloop_api_key="..." ## alternative to setting env var  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: humanloop/gpt-3.5-turbo  
   prompt_id: "<humanloop_prompt_id>"  
   api_key: os.environ/OPENAI_API_KEY  

```

  1. Start the proxy


```
litellm --config config.yaml --detailed_debug  

```

  1. Test it! 


  * CURL
  * OpenAI Python SDK


```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
      "role": "user",  
      "content": "THIS WILL BE IGNORED"  
    }  
  ],  
  "prompt_variables": {  
    "key": "this is used"  
  }  
}'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
    "prompt_variables": { # [OPTIONAL]  
      "key": "this is used"  
    }  
  }  
)  
  
print(response)  

```

**Expected Logs:**
```
POST Request Sent from LiteLLM:  
curl -X POST \  
https://api.openai.com/v1/ \  
-d '{'model': 'gpt-3.5-turbo', 'messages': <YOUR HUMANLOOP PROMPT TEMPLATE>}'  

```

## How to set model​
## How to set model​
### Set the model on LiteLLM​
You can do `humanloop/<litellm_model_name>`
  * SDK
  * PROXY


```
litellm.completion(  
  model="humanloop/gpt-3.5-turbo", # or `humanloop/anthropic/claude-3-5-sonnet`  
  ...  
)  

```

```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: humanloop/gpt-3.5-turbo # OR humanloop/anthropic/claude-3-5-sonnet  
   prompt_id: <humanloop_prompt_id>  
   api_key: os.environ/OPENAI_API_KEY  

```

### Set the model on Humanloop​
LiteLLM will call humanloop's `https://api.humanloop.com/v5/prompts/<your-prompt-id>` endpoint, to get the prompt template.
This also returns the template model set on Humanloop.
```
{  
 "template": [  
  {  
   ... # your prompt template  
  }  
 ],  
 "model": "gpt-3.5-turbo" # your template model  
}  

```

Previous
Custom Callbacks
Next
Scrub Logged Data
  * Getting Started
  * How to set model
  * How to set model
    * Set the model on LiteLLM
    * Set the model on Humanloop


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Lago - Usage Based Billing


On this page
# Lago - Usage Based Billing
Lago offers a self-hosted and cloud, metering and usage-based billing solution.
## Quick Start​
Use just 1 lines of code, to instantly log your responses **across all providers** with Lago
Get your Lago API Key
```
litellm.callbacks = ["lago"] # logs cost + usage of successful calls to lago  

```

  * SDK
  * PROXY


```
# pip install lago   
import litellm  
import os  
  
os.environ["LAGO_API_BASE"] = "" # http://0.0.0.0:3000  
os.environ["LAGO_API_KEY"] = ""  
os.environ["LAGO_API_EVENT_CODE"] = "" # The billable metric's code - https://docs.getlago.com/guide/events/ingesting-usage#define-a-billable-metric  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set lago as a callback, litellm will send the data to lago  
litellm.success_callback = ["lago"]   
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ],  
 user="your_customer_id" # 👈 SET YOUR CUSTOMER ID HERE  
)  

```

  1. Add to Config.yaml


```
model_list:  
- litellm_params:  
  api_base: https://openai-function-calling-workers.tasslexyz.workers.dev/  
  api_key: my-fake-key  
  model: openai/my-fake-model  
 model_name: fake-openai-endpoint  
  
litellm_settings:  
 callbacks: ["lago"] # 👈 KEY CHANGE  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


  * Curl
  * OpenAI Python SDK
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
   "user": "your-customer-id" # 👈 SET YOUR CUSTOMER ID  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
], user="my_customer_id") # 👈 whatever your customer id is  
  
print(response)  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
import os   
  
os.environ["OPENAI_API_KEY"] = "anything"  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "gpt-3.5-turbo",  
  temperature=0.1,  
  extra_body={  
    "user": "my_customer_id" # 👈 whatever your customer id is  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Advanced - Lagos Logging object​
This is what LiteLLM will log to Lagos
```
{  
  "event": {  
   "transaction_id": "<generated_unique_id>",  
   "external_customer_id": <litellm_end_user_id>, # passed via `user` param in /chat/completion call - https://platform.openai.com/docs/api-reference/chat/create  
   "code": os.getenv("LAGO_API_EVENT_CODE"),   
   "properties": {  
     "input_tokens": <number>,  
     "output_tokens": <number>,  
     "model": <string>,  
     "response_cost": <number>, # 👈 LITELLM CALCULATED RESPONSE COST - https://github.com/BerriAI/litellm/blob/d43f75150a65f91f60dc2c0c9462ce3ffc713c1f/litellm/utils.py#L1473  
   }  
  }  
}  

```

Previous
Sentry - Log LLM Exceptions
Next
Helicone - OSS LLM Observability Platform
  * Quick Start
  * Advanced - Lagos Logging object


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Langsmith - Logging LLM Input/Output


On this page
# Langsmith - Logging LLM Input/Output
An all-in-one developer platform for every step of the application lifecycle https://smith.langchain.com/
info
We want to learn how we can make the callbacks better! Meet the LiteLLM founders or join our discord
## Pre-Requisites​
```
pip install litellm  

```

## Quick Start​
Use just 2 lines of code, to instantly log your responses **across all providers** with Langsmith
```
litellm.success_callback = ["langsmith"]  

```

```
import litellm  
import os  
  
os.environ["LANGSMITH_API_KEY"] = ""  
os.environ["LANGSMITH_PROJECT"] = "" # defaults to litellm-completion  
os.environ["LANGSMITH_DEFAULT_RUN_NAME"] = "" # defaults to LLMRun  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set langsmith as a callback, litellm will send the data to langsmith  
litellm.success_callback = ["langsmith"]   
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

## Advanced​
### Set Langsmith fields​
```
import litellm  
import os  
  
os.environ["LANGSMITH_API_KEY"] = ""  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set langsmith as a callback, litellm will send the data to langsmith  
litellm.success_callback = ["langsmith"]   
   
response = litellm.completion(  
  model="gpt-3.5-turbo",  
   messages=[  
    {"role": "user", "content": "Hi 👋 - i'm openai"}  
  ],  
  metadata={  
    "run_name": "litellmRUN",                  # langsmith run name  
    "project_name": "litellm-completion",            # langsmith project name  
    "run_id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",      # langsmith run id  
    "parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",  # langsmith run parent run id  
    "trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",     # langsmith run trace id  
    "session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",    # langsmith run session id  
    "tags": ["model1", "prod-2"],                # langsmith run tags  
    "metadata": {                        # langsmith run metadata  
      "key1": "value1"  
    },  
    "dotted_order": "20240429T004912090000Z497f6eca-6276-4993-bfeb-53cbbbba6f08"  
  }  
)  
print(response)  

```

### Make LiteLLM Proxy use Custom `LANGSMITH_BASE_URL`​
If you're using a custom LangSmith instance, you can set the `LANGSMITH_BASE_URL` environment variable to point to your instance. For example, you can make LiteLLM Proxy log to a local LangSmith instance with this config:
```
litellm_settings:  
 success_callback: ["langsmith"]  
  
environment_variables:  
 LANGSMITH_BASE_URL: "http://localhost:1984"  
 LANGSMITH_PROJECT: "litellm-proxy"  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
Google Cloud Storage Buckets
Next
Literal AI - Log, Evaluate, Monitor
  * Pre-Requisites
  * Quick Start
  * Advanced
    * Set Langsmith fields
    * Make LiteLLM Proxy use Custom `LANGSMITH_BASE_URL`
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Literal AI - Log, Evaluate, Monitor


On this page
# Literal AI - Log, Evaluate, Monitor
Literal AI is a collaborative observability, evaluation and analytics platform for building production-grade LLM apps.
## Pre-Requisites​
Ensure you have the `literalai` package installed:
```
pip install literalai litellm  

```

## Quick Start​
```
import litellm  
import os  
  
os.environ["LITERAL_API_KEY"] = ""  
os.environ['OPENAI_API_KEY']= ""  
os.environ['LITERAL_BATCH_SIZE'] = "1" # You won't see logs appear until the batch is full and sent  
  
litellm.success_callback = ["literalai"] # Log Input/Output to LiteralAI  
litellm.failure_callback = ["literalai"] # Log Errors to LiteralAI  
  
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

## Multi Step Traces​
This integration is compatible with the Literal AI SDK decorators, enabling conversation and agent tracing
```
import litellm  
from literalai import LiteralClient  
import os  
  
os.environ["LITERAL_API_KEY"] = ""  
os.environ['OPENAI_API_KEY']= ""  
os.environ['LITERAL_BATCH_SIZE'] = "1" # You won't see logs appear until the batch is full and sent  
  
litellm.input_callback = ["literalai"] # Support other Literal AI decorators and prompt templates  
litellm.success_callback = ["literalai"] # Log Input/Output to LiteralAI  
litellm.failure_callback = ["literalai"] # Log Errors to LiteralAI  
  
literalai_client = LiteralClient()  
  
@literalai_client.run  
def my_agent(question: str):  
  # agent logic here  
  response = litellm.completion(  
    model="gpt-3.5-turbo",  
    messages=[  
      {"role": "user", "content": question}  
    ],  
    metadata={"literalai_parent_id": literalai_client.get_current_step().id}  
  )  
  return response  
  
my_agent("Hello world")  
  
# Waiting to send all logs before exiting, not needed in a production server  
literalai_client.flush()  

```

Learn more about Literal AI logging capabilities.
## Bind a Generation to its Prompt Template​
This integration works out of the box with prompts managed on Literal AI. This means that a specific LLM generation will be bound to its template.
Learn more about Prompt Management on Literal AI.
## OpenAI Proxy Usage​
If you are using the Lite LLM proxy, you can use the Literal AI OpenAI instrumentation to log your calls.
```
from literalai import LiteralClient  
from openai import OpenAI  
  
client = OpenAI(  
  api_key="anything",      # litellm proxy virtual key  
  base_url="http://0.0.0.0:4000" # litellm proxy base_url  
)  
  
literalai_client = LiteralClient(api_key="")  
  
# Instrument the OpenAI client  
literalai_client.instrument_openai()  
  
settings = {  
  "model": "gpt-3.5-turbo", # model you want to send litellm proxy  
  "temperature": 0,  
  # ... more settings  
}  
  
response = client.chat.completions.create(  
    messages=[  
      {  
        "content": "You are a helpful bot, you always reply in Spanish",  
        "role": "system"  
      },  
      {  
        "content": message.content,  
        "role": "user"  
      }  
    ],  
    **settings  
  )  
  

```

Previous
Langsmith - Logging LLM Input/Output
Next
OpenTelemetry - Tracing LLMs with any observability tool
  * Pre-Requisites
  * Quick Start
  * Multi Step Traces
  * Bind a Generation to its Prompt Template
  * OpenAI Proxy Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * 🪢 Langfuse - Logging LLM Input/Output


On this page
# 🪢 Langfuse - Logging LLM Input/Output
## What is Langfuse?​
Langfuse (GitHub) is an open-source LLM engineering platform for model tracing, prompt management, and application evaluation. Langfuse helps teams to collaboratively debug, analyze, and iterate on their LLM applications. 
Example trace in Langfuse using multiple models via LiteLLM:
## Usage with LiteLLM Proxy (LLM Gateway)​
👉 **Follow this link to start sending logs to langfuse with LiteLLM Proxy server**
## Usage with LiteLLM Python SDK​
### Pre-Requisites​
Ensure you have run `pip install langfuse` for this integration
```
pip install langfuse>=2.0.0 litellm  

```

### Quick Start​
Use just 2 lines of code, to instantly log your responses **across all providers** with Langfuse:
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
Get your Langfuse API Keys from https://cloud.langfuse.com/
```
litellm.success_callback = ["langfuse"]  
litellm.failure_callback = ["langfuse"] # logs errors to langfuse  

```

```
# pip install langfuse   
import litellm  
import os  
  
# from https://cloud.langfuse.com/  
os.environ["LANGFUSE_PUBLIC_KEY"] = ""  
os.environ["LANGFUSE_SECRET_KEY"] = ""  
# Optional, defaults to https://cloud.langfuse.com  
os.environ["LANGFUSE_HOST"] # optional  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set langfuse as a callback, litellm will send the data to langfuse  
litellm.success_callback = ["langfuse"]   
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

### Advanced​
#### Set Custom Generation Names, pass Metadata​
Pass `generation_name` in `metadata`
```
import litellm  
from litellm import completion  
import os  
  
# from https://cloud.langfuse.com/  
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-..."  
os.environ["LANGFUSE_SECRET_KEY"] = "sk-..."  
  
  
# OpenAI and Cohere keys   
# You can use any of the litellm supported providers: https://docs.litellm.ai/docs/providers  
os.environ['OPENAI_API_KEY']="sk-..."  
  
# set langfuse as a callback, litellm will send the data to langfuse  
litellm.success_callback = ["langfuse"]   
   
# openai call  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ],  
 metadata = {  
  "generation_name": "litellm-ishaan-gen", # set langfuse generation name  
  # custom metadata fields  
  "project": "litellm-proxy"   
 }  
)  
   
print(response)  
  

```

#### Set Custom Trace ID, Trace User ID, Trace Metadata, Trace Version, Trace Release and Tags​
Pass `trace_id`, `trace_user_id`, `trace_metadata`, `trace_version`, `trace_release`, `tags` in `metadata`
```
import litellm  
from litellm import completion  
import os  
  
# from https://cloud.langfuse.com/  
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-..."  
os.environ["LANGFUSE_SECRET_KEY"] = "sk-..."  
  
os.environ['OPENAI_API_KEY']="sk-..."  
  
# set langfuse as a callback, litellm will send the data to langfuse  
litellm.success_callback = ["langfuse"]   
  
# set custom langfuse trace params and generation params  
response = completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ],  
 metadata={  
   "generation_name": "ishaan-test-generation", # set langfuse Generation Name  
   "generation_id": "gen-id22",         # set langfuse Generation ID   
   "parent_observation_id": "obs-id9"      # set langfuse Parent Observation ID  
   "version": "test-generation-version"     # set langfuse Generation Version  
   "trace_user_id": "user-id2",         # set langfuse Trace User ID  
   "session_id": "session-1",          # set langfuse Session ID  
   "tags": ["tag1", "tag2"],           # set langfuse Tags  
   "trace_name": "new-trace-name"        # set langfuse Trace Name  
   "trace_id": "trace-id22",           # set langfuse Trace ID  
   "trace_metadata": {"key": "value"},      # set langfuse Trace Metadata  
   "trace_version": "test-trace-version",    # set langfuse Trace Version (if not set, defaults to Generation Version)  
   "trace_release": "test-trace-release",    # set langfuse Trace Release  
   ### OR ###   
   "existing_trace_id": "trace-id22",      # if generation is continuation of past trace. This prevents default behaviour of setting a trace name  
   ### OR enforce that certain fields are trace overwritten in the trace during the continuation ###  
   "existing_trace_id": "trace-id22",  
   "trace_metadata": {"key": "updated_trace_value"},      # The new value to use for the langfuse Trace Metadata  
   "update_trace_keys": ["input", "output", "trace_metadata"], # Updates the trace input & output to be this generations input & output also updates the Trace Metadata to match the passed in value  
   "debug_langfuse": True,                   # Will log the exact metadata sent to litellm for the trace/generation as `metadata_passed_to_litellm`   
 },  
)  
  
print(response)  
  

```

You can also pass `metadata` as part of the request header with a `langfuse_*` prefix:
```
curl --location --request POST 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'langfuse_trace_id: trace-id2' \  
  --header 'langfuse_trace_user_id: user-id2' \  
  --header 'langfuse_trace_metadata: {"key":"value"}' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
}'  

```

#### Trace & Generation Parameters​
##### Trace Specific Parameters​
  * `trace_id` - Identifier for the trace, must use `existing_trace_id` instead of `trace_id` if this is an existing trace, auto-generated by default
  * `trace_name` - Name of the trace, auto-generated by default
  * `session_id` - Session identifier for the trace, defaults to `None`
  * `trace_version` - Version for the trace, defaults to value for `version`
  * `trace_release` - Release for the trace, defaults to `None`
  * `trace_metadata` - Metadata for the trace, defaults to `None`
  * `trace_user_id` - User identifier for the trace, defaults to completion argument `user`
  * `tags` - Tags for the trace, defaults to `None`


##### Updatable Parameters on Continuation​
The following parameters can be updated on a continuation of a trace by passing in the following values into the `update_trace_keys` in the metadata of the completion.
  * `input` - Will set the traces input to be the input of this latest generation
  * `output` - Will set the traces output to be the output of this generation
  * `trace_version` - Will set the trace version to be the provided value (To use the latest generations version instead, use `version`)
  * `trace_release` - Will set the trace release to be the provided value
  * `trace_metadata` - Will set the trace metadata to the provided value
  * `trace_user_id` - Will set the trace user id to the provided value


#### Generation Specific Parameters​
  * `generation_id` - Identifier for the generation, auto-generated by default
  * `generation_name` - Identifier for the generation, auto-generated by default
  * `parent_observation_id` - Identifier for the parent observation, defaults to `None`
  * `prompt` - Langfuse prompt object used for the generation, defaults to `None`


Any other key value pairs passed into the metadata not listed in the above spec for a `litellm` completion will be added as a metadata key value pair for the generation.
#### Disable Logging - Specific Calls​
To disable logging for specific calls use the `no-log` flag. 
`completion(messages = ..., model = ..., **{"no-log": True})`
### Use LangChain ChatLiteLLM + Langfuse​
Pass `trace_user_id`, `session_id` in model_kwargs
```
import os  
from langchain.chat_models import ChatLiteLLM  
from langchain.schema import HumanMessage  
import litellm  
  
# from https://cloud.langfuse.com/  
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-..."  
os.environ["LANGFUSE_SECRET_KEY"] = "sk-..."  
  
os.environ['OPENAI_API_KEY']="sk-..."  
  
# set langfuse as a callback, litellm will send the data to langfuse  
litellm.success_callback = ["langfuse"]   
  
chat = ChatLiteLLM(  
 model="gpt-3.5-turbo"  
 model_kwargs={  
   "metadata": {  
    "trace_user_id": "user-id2", # set langfuse Trace User ID  
    "session_id": "session-1" , # set langfuse Session ID  
    "tags": ["tag1", "tag2"]   
   }  
  }  
 )  
messages = [  
  HumanMessage(  
    content="what model are you"  
  )  
]  
chat(messages)  

```

### Redacting Messages, Response Content from Langfuse Logging​
#### Redact Messages and Responses from all Langfuse Logging​
Set `litellm.turn_off_message_logging=True` This will prevent the messages and responses from being logged to langfuse, but request metadata will still be logged.
#### Redact Messages and Responses from specific Langfuse Logging​
In the metadata typically passed for text completion or embedding calls you can set specific keys to mask the messages and responses for this call.
Setting `mask_input` to `True` will mask the input from being logged for this call 
Setting `mask_output` to `True` will make the output from being logged for this call.
Be aware that if you are continuing an existing trace, and you set `update_trace_keys` to include either `input` or `output` and you set the corresponding `mask_input` or `mask_output`, then that trace will have its existing input and/or output replaced with a redacted message.
## Troubleshooting & Errors​
### Data not getting logged to Langfuse ?​
  * Ensure you're on the latest version of langfuse `pip install langfuse -U`. The latest version allows litellm to log JSON input/outputs to langfuse
  * Follow this checklist if you don't see any traces in langfuse.


## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
🖇️ AgentOps - LLM Observability Platform
Next
🌙 Lunary - GenAI Observability
  * What is Langfuse?
  * Usage with LiteLLM Proxy (LLM Gateway)
  * Usage with LiteLLM Python SDK
    * Pre-Requisites
    * Quick Start
    * Advanced
    * Use LangChain ChatLiteLLM + Langfuse
    * Redacting Messages, Response Content from Langfuse Logging
  * Troubleshooting & Errors
    * Data not getting logged to Langfuse ?
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Logfire


On this page
# Logfire
Logfire is open Source Observability & Analytics for LLM Apps Detailed production traces and a granular view on quality, cost and latency
info
We want to learn how we can make the callbacks better! Meet the LiteLLM founders or join our discord
## Pre-Requisites​
Ensure you have installed the following packages to use this integration
```
pip install litellm  
  
pip install opentelemetry-api==1.25.0  
pip install opentelemetry-sdk==1.25.0  
pip install opentelemetry-exporter-otlp==1.25.0  

```

## Quick Start​
Get your Logfire token from Logfire
```
litellm.callbacks = ["logfire"]  

```

```
# pip install logfire  
import litellm  
import os  
  
# from https://logfire.pydantic.dev/  
os.environ["LOGFIRE_TOKEN"] = ""  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set logfire as a callback, litellm will send the data to logfire  
litellm.success_callback = ["logfire"]  
  
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
OpenTelemetry - Tracing LLMs with any observability tool
Next
Argilla
  * Pre-Requisites
  * Quick Start
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * 🌙 Lunary - GenAI Observability


On this page
# 🌙 Lunary - GenAI Observability
Lunary is an open-source platform providing observability, prompt management, and analytics to help team manage and improve LLM chatbots.
You can reach out to us anytime by email or directly schedule a Demo.
## Usage with LiteLLM Python SDK​
### Pre-Requisites​
```
pip install litellm lunary  

```

### Quick Start​
First, get your Lunary public key on the Lunary dashboard.
Use just 2 lines of code, to instantly log your responses **across all providers** with Lunary:
```
litellm.success_callback = ["lunary"]  
litellm.failure_callback = ["lunary"]  

```

Complete code:
```
from litellm import completion  
  
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key" # from https://app.lunary.ai/)  
os.environ["OPENAI_API_KEY"] = ""  
  
litellm.success_callback = ["lunary"]  
litellm.failure_callback = ["lunary"]  
  
response = completion(  
 model="gpt-4o",  
 messages=[{"role": "user", "content": "Hi there 👋"}],  
 user="ishaan_litellm"  
)  

```

### Usage with LangChain ChatLiteLLM​
```
import os  
from langchain.chat_models import ChatLiteLLM  
from langchain.schema import HumanMessage  
import litellm  
  
os.environ["LUNARY_PUBLIC_KEY"] = "" # from https://app.lunary.ai/settings  
os.environ['OPENAI_API_KEY']="sk-..."  
  
litellm.success_callback = ["lunary"]   
litellm.failure_callback = ["lunary"]   
  
chat = ChatLiteLLM(  
 model="gpt-4o"  
 messages = [  
  HumanMessage(  
    content="what model are you"  
  )  
]  
chat(messages)  

```

### Usage with Prompt Templates​
You can use Lunary to manage prompt templates and use them across all your LLM providers with LiteLLM.
```
from litellm import completion  
from lunary  
  
template = lunary.render_template("template-slug", {  
 "name": "John", # Inject variables  
})  
  
litellm.success_callback = ["lunary"]  
  
result = completion(**template)  

```

### Usage with custom chains​
You can wrap your LLM calls inside custom chains, so that you can visualize them as traces.
```
import litellm  
from litellm import completion  
import lunary  
  
litellm.success_callback = ["lunary"]  
litellm.failure_callback = ["lunary"]  
  
@lunary.chain("My custom chain name")  
def my_chain(chain_input):  
 chain_run_id = lunary.run_manager.current_run_id  
 response = completion(  
  model="gpt-4o",   
  messages=[{"role": "user", "content": "Say 1"}],  
  metadata={"parent_run_id": chain_run_id},  
 )  
  
 response = completion(  
  model="gpt-4o",   
  messages=[{"role": "user", "content": "Say 2"}],  
  metadata={"parent_run_id": chain_run_id},  
 )  
 chain_output = response.choices[0].message  
 return chain_output  
  
my_chain("Chain input")  

```

## Usage with LiteLLM Proxy Server​
### Step1: Install dependencies and set your environment variables​
Install the dependencies
```
pip install litellm lunary  

```

Get you Lunary public key from from https://app.lunary.ai/settings
```
export LUNARY_PUBLIC_KEY="<your-public-key>"  

```

### Step 2: Create a `config.yaml` and set `lunary` callbacks​
```
model_list:  
 - model_name: "*"  
  litellm_params:  
   model: "*"  
litellm_settings:  
 success_callback: ["lunary"]  
 failure_callback: ["lunary"]  

```

### Step 3: Start the LiteLLM proxy​
```
litellm --config config.yaml  

```

### Step 4: Make a request​
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-d '{  
  "model": "gpt-4o",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   {  
    "role": "user",  
    "content": "how can I solve 8x + 7 = -23"  
   }  
  ]  
}'  

```

You can find more details about the different ways of making requests to the LiteLLM proxy on this page
## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
🪢 Langfuse - Logging LLM Input/Output
Next
🔁 MLflow - OSS LLM Observability and Evaluation
  * Usage with LiteLLM Python SDK
    * Pre-Requisites
    * Quick Start
    * Usage with LangChain ChatLiteLLM
    * Usage with Prompt Templates
    * Usage with custom chains
  * Usage with LiteLLM Proxy Server
    * Step1: Install dependencies and set your environment variables
    * Step 2: Create a `config.yaml` and set `lunary` callbacks
    * Step 3: Start the LiteLLM proxy
    * Step 4: Make a request
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * OpenTelemetry - Tracing LLMs with any observability tool


On this page
# OpenTelemetry - Tracing LLMs with any observability tool
OpenTelemetry is a CNCF standard for observability. It connects to any observability tool, such as Jaeger, Zipkin, Datadog, New Relic, Traceloop and others.
## Getting Started​
Install the OpenTelemetry SDK:
```
pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp  

```

Set the environment variables (different providers may require different variables):
  * Log to Traceloop Cloud
  * Log to OTEL HTTP Collector
  * Log to OTEL GRPC Collector
  * Log to Laminar


```
OTEL_EXPORTER="otlp_http"  
OTEL_ENDPOINT="https://api.traceloop.com"  
OTEL_HEADERS="Authorization=Bearer%20<your-api-key>"  

```

```
OTEL_EXPORTER="otlp_http"  
OTEL_ENDPOINT="http://0.0.0.0:4318"  

```

```
OTEL_EXPORTER="otlp_grpc"  
OTEL_ENDPOINT="http://0.0.0.0:4317"  

```

```
OTEL_EXPORTER="otlp_grpc"  
OTEL_ENDPOINT="https://api.lmnr.ai:8443"  
OTEL_HEADERS="authorization=Bearer <project-api-key>"  

```

Use just 1 line of code, to instantly log your LLM responses **across all providers** with OpenTelemetry:
```
litellm.callbacks = ["otel"]  

```

## Redacting Messages, Response Content from OpenTelemetry Logging​
### Redact Messages and Responses from all OpenTelemetry Logging​
Set `litellm.turn_off_message_logging=True` This will prevent the messages and responses from being logged to OpenTelemetry, but request metadata will still be logged.
### Redact Messages and Responses from specific OpenTelemetry Logging​
In the metadata typically passed for text completion or embedding calls you can set specific keys to mask the messages and responses for this call.
Setting `mask_input` to `True` will mask the input from being logged for this call
Setting `mask_output` to `True` will make the output from being logged for this call.
Be aware that if you are continuing an existing trace, and you set `update_trace_keys` to include either `input` or `output` and you set the corresponding `mask_input` or `mask_output`, then that trace will have its existing input and/or output replaced with a redacted message.
## Support​
For any question or issue with the integration you can reach out to the OpenLLMetry maintainers on Slack or via email.
## Troubleshooting​
### Trace LiteLLM Proxy user/key/org/team information on failed requests​
LiteLLM emits the user_api_key_metadata
  * key hash
  * key_alias
  * org_id
  * user_id
  * team_id


for successful + failed requests 
click under `litellm_request` in the trace
Previous
Literal AI - Log, Evaluate, Monitor
Next
Logfire
  * Getting Started
  * Redacting Messages, Response Content from OpenTelemetry Logging
    * Redact Messages and Responses from all OpenTelemetry Logging
    * Redact Messages and Responses from specific OpenTelemetry Logging
  * Support
  * Troubleshooting
    * Trace LiteLLM Proxy user/key/org/team information on failed requests


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * OpenMeter - Usage-Based Billing


On this page
# OpenMeter - Usage-Based Billing
OpenMeter is an Open Source Usage-Based Billing solution for AI/Cloud applications. It integrates with Stripe for easy billing.
info
We want to learn how we can make the callbacks better! Meet the LiteLLM founders or join our discord
## Quick Start​
Use just 2 lines of code, to instantly log your responses **across all providers** with OpenMeter
Get your OpenMeter API Key from https://openmeter.cloud/meters
```
litellm.callbacks = ["openmeter"] # logs cost + usage of successful calls to openmeter  

```

  * SDK
  * PROXY


```
# pip install openmeter   
import litellm  
import os  
  
# from https://openmeter.cloud  
os.environ["OPENMETER_API_ENDPOINT"] = ""  
os.environ["OPENMETER_API_KEY"] = ""  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set openmeter as a callback, litellm will send the data to openmeter  
litellm.callbacks = ["openmeter"]   
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

  1. Add to Config.yaml


```
model_list:  
- litellm_params:  
  api_base: https://openai-function-calling-workers.tasslexyz.workers.dev/  
  api_key: my-fake-key  
  model: openai/my-fake-model  
 model_name: fake-openai-endpoint  
  
litellm_settings:  
 callbacks: ["openmeter"] # 👈 KEY CHANGE  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

Previous
Helicone - OSS LLM Observability Platform
Next
Promptlayer Tutorial
  * Quick Start


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * 🔁 MLflow - OSS LLM Observability and Evaluation


On this page
# 🔁 MLflow - OSS LLM Observability and Evaluation
## What is MLflow?​
**MLflow** is an end-to-end open source MLOps platform for experiment tracking, model management, evaluation, observability (tracing), and deployment. MLflow empowers teams to collaboratively develop and refine LLM applications efficiently.
MLflow’s integration with LiteLLM supports advanced observability compatible with OpenTelemetry.
## Getting Started​
Install MLflow:
```
pip install mlflow  

```

To enable MLflow auto tracing for LiteLLM:
```
import mlflow  
  
mlflow.litellm.autolog()  
  
# Alternative, you can set the callback manually in LiteLLM  
# litellm.callbacks = ["mlflow"]  

```

Since MLflow is open-source and free, **no sign-up or API key is needed to log traces!**
```
import litellm  
import os  
  
# Set your LLM provider's API key  
os.environ["OPENAI_API_KEY"] = ""  
  
# Call LiteLLM as usual  
response = litellm.completion(  
  model="gpt-4o-mini",  
  messages=[  
   {"role": "user", "content": "Hi 👋 - i'm openai"}  
  ]  
)  

```

Open the MLflow UI and go to the `Traces` tab to view logged traces:
```
mlflow ui  

```

## Tracing Tool Calls​
MLflow integration with LiteLLM support tracking tool calls in addition to the messages.
```
import mlflow  
  
# Enable MLflow auto-tracing for LiteLLM  
mlflow.litellm.autolog()  
  
# Define the tool function.  
def get_weather(location: str) -> str:  
  if location == "Tokyo":  
    return "sunny"  
  elif location == "Paris":  
    return "rainy"  
  return "unknown"  
  
# Define function spec  
get_weather_tool = {  
  "type": "function",  
  "function": {  
    "name": "get_weather",  
    "description": "Get the current weather in a given location",  
    "parameters": {  
      "properties": {  
        "location": {  
          "description": "The city and state, e.g., San Francisco, CA",  
          "type": "string",  
        },  
      },  
      "required": ["location"],  
      "type": "object",  
    },  
  },  
}  
  
# Call LiteLLM as usual  
response = litellm.completion(  
  model="gpt-4o-mini",  
  messages=[  
   {"role": "user", "content": "What's the weather like in Paris today?"}  
  ],  
  tools=[get_weather_tool]  
)  

```

## Evaluation​
MLflow LiteLLM integration allow you to run qualitative assessment against LLM to evaluate or/and monitor your GenAI application.
Visit Evaluate LLMs Tutorial for the complete guidance on how to run evaluation suite with LiteLLM and MLflow.
## Exporting Traces to OpenTelemetry collectors​
MLflow traces are compatible with OpenTelemetry. You can export traces to any OpenTelemetry collector (e.g., Jaeger, Zipkin, Datadog, New Relic) by setting the endpoint URL in the environment variables.
```
# Set the endpoint of the OpenTelemetry Collector  
os.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = "http://localhost:4317/v1/traces"  
# Optionally, set the service name to group traces  
os.environ["OTEL_SERVICE_NAME"] = "<your-service-name>"  

```

See MLflow documentation for more details.
## Combine LiteLLM Trace with Your Application Trace​
LiteLLM is often part of larger LLM applications, such as agentic models. MLflow Tracing allows you to instrument custom Python code, which can then be combined with LiteLLM traces.
```
import litellm  
import mlflow  
from mlflow.entities import SpanType  
  
# Enable MLflow auto-tracing for LiteLLM  
mlflow.litellm.autolog()  
  
  
class CustomAgent:  
  # Use @mlflow.trace to instrument Python functions.  
  @mlflow.trace(span_type=SpanType.AGENT)  
  def run(self, query: str):  
    # do something  
  
    while i < self.max_turns:  
      response = litellm.completion(  
        model="gpt-4o-mini",  
        messages=messages,  
      )  
  
      action = self.get_action(response)  
      ...  
  
  @mlflow.trace  
  def get_action(llm_response):  
    ...  

```

This approach generates a unified trace, combining your custom Python code with LiteLLM calls.
## Support​
  * For advanced usage and integrations of tracing, visit the MLflow Tracing documentation.
  * For any question or issue with this integration, please submit an issue on our Github repository!


Previous
🌙 Lunary - GenAI Observability
Next
Google Cloud Storage Buckets
  * What is MLflow?
  * Getting Started
  * Tracing Tool Calls
  * Evaluation
  * Exporting Traces to OpenTelemetry collectors
  * Combine LiteLLM Trace with Your Application Trace
  * Support


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Comet Opik - Logging + Evals


On this page
# Comet Opik - Logging + Evals
Opik is an open source end-to-end LLM Evaluation Platform that helps developers track their LLM prompts and responses during both development and production. Users can define and run evaluations to test their LLMs apps before deployment to check for hallucinations, accuracy, context retrevial, and more!
info
We want to learn how we can make the callbacks better! Meet the LiteLLM founders or join our discord
## Pre-Requisites​
You can learn more about setting up Opik in the Opik quickstart guide. You can also learn more about self-hosting Opik in our self-hosting guide.
## Quick Start​
Use just 4 lines of code, to instantly log your responses **across all providers** with Opik
Get your Opik API Key by signing up here!
```
import litellm  
litellm.callbacks = ["opik"]  

```

Full examples:
  * SDK
  * Proxy


```
import litellm  
import os  
  
# Configure the Opik API key or call opik.configure()  
os.environ["OPIK_API_KEY"] = ""  
os.environ["OPIK_WORKSPACE"] = ""  
  
# LLM provider API Keys:  
os.environ["OPENAI_API_KEY"] = ""  
  
# set "opik" as a callback, litellm will send the data to an Opik server (such as comet.com)  
litellm.callbacks = ["opik"]  
  
# openai call  
response = litellm.completion(  
  model="gpt-3.5-turbo",  
  messages=[  
    {"role": "user", "content": "Why is tracking and evaluation of LLMs important?"}  
  ]  
)  

```

If you are using liteLLM within a function tracked using Opik's `@track` decorator, you will need provide the `current_span_data` field in the metadata attribute so that the LLM call is assigned to the correct trace:
```
from opik import track  
from opik.opik_context import get_current_span_data  
import litellm  
  
litellm.callbacks = ["opik"]  
  
@track()  
def streaming_function(input):  
  messages = [{"role": "user", "content": input}]  
  response = litellm.completion(  
    model="gpt-3.5-turbo",  
    messages=messages,  
    metadata = {  
      "opik": {  
        "current_span_data": get_current_span_data(),  
        "tags": ["streaming-test"],  
      },  
    }  
  )  
  return response  
  
response = streaming_function("Why is tracking and evaluation of LLMs important?")  
chunks = list(response)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-3.5-turbo-testing  
  litellm_params:  
   model: gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  
  
litellm_settings:  
 callbacks: ["opik"]  
  
environment_variables:  
 OPIK_API_KEY: ""  
 OPIK_WORKSPACE: ""  

```

  1. Run proxy


```
litellm --config config.yaml  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gpt-3.5-turbo-testing",  
 "messages": [  
  {  
   "role": "user",  
   "content": "What's the weather like in Boston today?"  
  }  
 ]  
}'  

```

## Opik-Specific Parameters​
These can be passed inside metadata with the `opik` key.
### Fields​
  * `project_name` - Name of the Opik project to send data to.
  * `current_span_data` - The current span data to be used for tracing.
  * `tags` - Tags to be used for tracing.


### Usage​
  * SDK
  * Proxy


```
from opik import track  
from opik.opik_context import get_current_span_data  
import litellm  
  
litellm.callbacks = ["opik"]  
  
messages = [{"role": "user", "content": input}]  
response = litellm.completion(  
  model="gpt-3.5-turbo",  
  messages=messages,  
  metadata = {  
    "opik": {  
      "current_span_data": get_current_span_data(),  
      "tags": ["streaming-test"],  
    },  
  }  
)  
return response  

```

```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gpt-3.5-turbo-testing",  
 "messages": [  
  {  
   "role": "user",  
   "content": "What's the weather like in Boston today?"  
  }  
 ],  
 "metadata": {  
  "opik": {  
   "current_span_data": "...",  
   "tags": ["streaming-test"],  
  },  
 }  
}'  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
Telemetry
Next
OpenWeb UI with LiteLLM
  * Pre-Requisites
  * Quick Start
  * Opik-Specific Parameters
    * Fields
    * Usage
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Phoenix OSS


On this page
# Phoenix OSS
Open source tracing and evaluation platform
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
## Pre-Requisites​
Make an account on Phoenix OSS OR self-host your own instance of Phoenix
## Quick Start​
Use just 2 lines of code, to instantly log your responses **across all providers** with Phoenix
You can also use the instrumentor option instead of the callback, which you can find here.
```
litellm.callbacks = ["arize_phoenix"]  

```

```
import litellm  
import os  
  
os.environ["PHOENIX_API_KEY"] = "" # Necessary only using Phoenix Cloud  
os.environ["PHOENIX_COLLECTOR_HTTP_ENDPOINT"] = "" # The URL of your Phoenix OSS instance  
# This defaults to https://app.phoenix.arize.com/v1/traces for Phoenix Cloud  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set arize as a callback, litellm will send the data to arize  
litellm.callbacks = ["phoenix"]  
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

### Using with LiteLLM Proxy​
```
model_list:  
 - model_name: gpt-4o  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["arize_phoenix"]  
  
environment_variables:  
  PHOENIX_API_KEY: "d0*****"  
  PHOENIX_COLLECTOR_ENDPOINT: "https://app.phoenix.arize.com/v1/traces" # OPTIONAL, for setting the GRPC endpoint  
  PHOENIX_COLLECTOR_HTTP_ENDPOINT: "https://app.phoenix.arize.com/v1/traces" # OPTIONAL, for setting the HTTP endpoint  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
Arize AI
Next
Local Debugging
  * Pre-Requisites
  * Quick Start
    * Using with LiteLLM Proxy
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Promptlayer Tutorial


On this page
# Promptlayer Tutorial
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
Promptlayer is a platform for prompt engineers. Log OpenAI requests. Search usage history. Track performance. Visually manage prompt templates.
## Use Promptlayer to log requests across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)​
liteLLM provides `callbacks`, making it easy for you to log data depending on the status of your responses.
### Using Callbacks​
Get your PromptLayer API Key from https://promptlayer.com/
Use just 2 lines of code, to instantly log your responses **across all providers** with promptlayer:
```
litellm.success_callback = ["promptlayer"]  
  

```

Complete code
```
from litellm import completion  
  
## set env variables  
os.environ["PROMPTLAYER_API_KEY"] = "your-promptlayer-key"  
  
os.environ["OPENAI_API_KEY"], os.environ["COHERE_API_KEY"] = "", ""  
  
# set callbacks  
litellm.success_callback = ["promptlayer"]  
  
#openai call  
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}])  
  
#cohere call  
response = completion(model="command-nightly", messages=[{"role": "user", "content": "Hi 👋 - i'm cohere"}])  

```

### Logging Metadata​
You can also log completion call metadata to Promptlayer. 
You can add metadata to a completion call through the metadata param: 
```
completion(model,messages, metadata={"model": "ai21"})  

```

**Complete Code**
```
from litellm import completion  
  
## set env variables  
os.environ["PROMPTLAYER_API_KEY"] = "your-promptlayer-key"  
  
os.environ["OPENAI_API_KEY"], os.environ["COHERE_API_KEY"] = "", ""  
  
# set callbacks  
litellm.success_callback = ["promptlayer"]  
  
#openai call - log llm provider is openai  
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}], metadata={"provider": "openai"})  
  
#cohere call - log llm provider is cohere  
response = completion(model="command-nightly", messages=[{"role": "user", "content": "Hi 👋 - i'm cohere"}], metadata={"provider": "cohere"})  

```

Credits to Nick Bradford, from Vim-GPT, for the suggestion. 
## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
OpenMeter - Usage-Based Billing
Next
Weights & Biases - Logging LLM Input/Output
  * Use Promptlayer to log requests across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)
    * Using Callbacks
    * Logging Metadata
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Scrub Logged Data


# Scrub Logged Data
Redact messages / mask PII before sending data to logging integrations (langfuse/etc.).
See our **Presidio PII Masking** for reference.
  1. Setup a custom callback 


```
from litellm.integrations.custom_logger import CustomLogger  
  
class MyCustomHandler(CustomLogger):  
  async def async_logging_hook(  
    self, kwargs: dict, result: Any, call_type: str  
  ) -> Tuple[dict, Any]:  
    """  
    For masking logged request/response. Return a modified version of the request/result.   
      
    Called before `async_log_success_event`.  
    """  
    if (  
      call_type == "completion" or call_type == "acompletion"  
    ): # /chat/completions requests  
      messages: Optional[List] = kwargs.get("messages", None)  
  
      kwargs["messages"] = [{"role": "user", "content": "MASK_THIS_ASYNC_VALUE"}]  
  
    return kwargs, responses  
  
  def logging_hook(  
    self, kwargs: dict, result: Any, call_type: str  
  ) -> Tuple[dict, Any]:  
    """  
    For masking logged request/response. Return a modified version of the request/result.  
  
    Called before `log_success_event`.  
    """  
    if (  
      call_type == "completion" or call_type == "acompletion"  
    ): # /chat/completions requests  
      messages: Optional[List] = kwargs.get("messages", None)  
  
      kwargs["messages"] = [{"role": "user", "content": "MASK_THIS_SYNC_VALUE"}]  
  
    return kwargs, responses  
  
  
customHandler = MyCustomHandler()  

```

  1. Connect custom handler to LiteLLM


```
import litellm  
  
litellm.callbacks = [customHandler]  

```

  1. Test it!


```
# pip install langfuse   
  
import os  
import litellm  
from litellm import completion   
  
os.environ["LANGFUSE_PUBLIC_KEY"] = ""  
os.environ["LANGFUSE_SECRET_KEY"] = ""  
# Optional, defaults to https://cloud.langfuse.com  
os.environ["LANGFUSE_HOST"] # optional  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
litellm.callbacks = [customHandler]  
litellm.success_callback = ["langfuse"]  
  
  
  
## sync   
response = completion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],  
               stream=True)  
for chunk in response:   
  continue  
  
  
## async  
import asyncio   
  
def async completion():  
  response = await acompletion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],  
               stream=True)  
  async for chunk in response:   
    continue  
asyncio.run(completion())  

```

Previous
Humanloop
Next
Braintrust - Evals + Logging
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Raw Request/Response Logging


On this page
# Raw Request/Response Logging
## Logging​
See the raw request/response sent by LiteLLM in your logging provider (OTEL/Langfuse/etc.).
  * SDK
  * PROXY


```
# pip install langfuse   
import litellm  
import os  
  
# log raw request/response  
litellm.log_raw_request_response = True  
  
# from https://cloud.langfuse.com/  
os.environ["LANGFUSE_PUBLIC_KEY"] = ""  
os.environ["LANGFUSE_SECRET_KEY"] = ""  
# Optional, defaults to https://cloud.langfuse.com  
os.environ["LANGFUSE_HOST"] # optional  
  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set langfuse as a callback, litellm will send the data to langfuse  
litellm.success_callback = ["langfuse"]   
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

```
litellm_settings:  
 log_raw_request_response: True  

```

**Expected Log**
## Return Raw Response Headers​
Return raw response headers from llm provider. 
Currently only supported for openai. 
  * SDK
  * PROXY


```
import litellm  
import os  
  
litellm.return_response_headers = True  
  
## set ENV variables  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  
  
print(response._hidden_params)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
   api_key: os.environ/GROQ_API_KEY  
  
litellm_settings:  
 return_response_headers: true  

```

  1. Test it!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    { "role": "system", "content": "Use your tools smartly"},  
    { "role": "user", "content": "What time is it now? Use your tool"}  
  ]  
}'  

```

**Expected Response**
Previous
Local Debugging
Next
Custom Callbacks
  * Logging
  * Return Raw Response Headers


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Sentry - Log LLM Exceptions


On this page
# Sentry - Log LLM Exceptions
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
Sentry provides error monitoring for production. LiteLLM can add breadcrumbs and send exceptions to Sentry with this integration
Track exceptions for:
  * litellm.completion() - completion()for 100+ LLMs
  * litellm.acompletion() - async completion()
  * Streaming completion() & acompletion() calls


## Usage​
### Set SENTRY_DSN & callback​
```
import litellm, os  
os.environ["SENTRY_DSN"] = "your-sentry-url"  
litellm.failure_callback=["sentry"]  

```

### Sentry callback with completion​
```
import litellm  
from litellm import completion   
  
litellm.input_callback=["sentry"] # adds sentry breadcrumbing  
litellm.failure_callback=["sentry"] # [OPTIONAL] if you want litellm to capture -> send exception to sentry  
  
import os   
os.environ["SENTRY_DSN"] = "your-sentry-url"  
os.environ["OPENAI_API_KEY"] = "your-openai-key"  
  
# set bad key to trigger error   
api_key="bad-key"  
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hey!"}], stream=True, api_key=api_key)  
  
print(response)  

```

## Redacting Messages, Response Content from Sentry Logging​
Set `litellm.turn_off_message_logging=True` This will prevent the messages and responses from being logged to sentry, but request metadata will still be logged.
Let us know if you need any additional options from Sentry.
Previous
Braintrust - Evals + Logging
Next
Lago - Usage Based Billing
  * Usage
    * Set SENTRY_DSN & callback
    * Sentry callback with completion
  * Redacting Messages, Response Content from Sentry Logging


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Weights & Biases - Logging LLM Input/Output


On this page
# Weights & Biases - Logging LLM Input/Output
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
Weights & Biases helps AI developers build better models faster https://wandb.ai
info
We want to learn how we can make the callbacks better! Meet the LiteLLM founders or join our discord
## Pre-Requisites​
Ensure you have run `pip install wandb` for this integration
```
pip install wandb litellm  

```

## Quick Start​
Use just 2 lines of code, to instantly log your responses **across all providers** with Weights & Biases
```
litellm.success_callback = ["wandb"]  

```

```
# pip install wandb   
import litellm  
import os  
  
os.environ["WANDB_API_KEY"] = ""  
# LLM API Keys  
os.environ['OPENAI_API_KEY']=""  
  
# set wandb as a callback, litellm will send the data to Weights & Biases  
litellm.success_callback = ["wandb"]   
   
# openai call  
response = litellm.completion(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "user", "content": "Hi 👋 - i'm openai"}  
 ]  
)  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
Promptlayer Tutorial
Next
Slack - Logging LLM Input/Output, Exceptions
  * Pre-Requisites
  * Quick Start
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Slack - Logging LLM Input/Output, Exceptions


On this page
# Slack - Logging LLM Input/Output, Exceptions
info
We want to learn how we can make the callbacks better! Meet the LiteLLM founders or join our discord
## Pre-Requisites​
### Step 1​
```
pip install litellm  

```

### Step 2​
Get a slack webhook url from https://api.slack.com/messaging/webhooks
## Quick Start​
### Create a custom Callback to log to slack​
We create a custom callback, to log to slack webhooks, see custom callbacks on litellm
```
def send_slack_alert(  
    kwargs,  
    completion_response,  
    start_time,  
    end_time,  
):  
  print(  
    "in custom slack callback func"  
  )  
  import requests  
  import json  
  
  # Define the Slack webhook URL  
  # get it from https://api.slack.com/messaging/webhooks  
  slack_webhook_url = os.environ['SLACK_WEBHOOK_URL']  # "https://hooks.slack.com/services/<>/<>/<>"  
  
  # Remove api_key from kwargs under litellm_params  
  if kwargs.get('litellm_params'):  
    kwargs['litellm_params'].pop('api_key', None)  
    if kwargs['litellm_params'].get('metadata'):  
      kwargs['litellm_params']['metadata'].pop('deployment', None)  
  # Remove deployment under metadata  
  if kwargs.get('metadata'):  
    kwargs['metadata'].pop('deployment', None)  
  # Prevent api_key from being logged  
  if kwargs.get('api_key'):  
    kwargs.pop('api_key', None)  
  
  # Define the text payload, send data available in litellm custom_callbacks  
  text_payload = f"""LiteLLM Logging: kwargs: {str(kwargs)}\n\n, response: {str(completion_response)}\n\n, start time{str(start_time)} end time: {str(end_time)}  
  """  
  payload = {  
    "text": text_payload  
  }  
  
  # Set the headers  
  headers = {  
    "Content-type": "application/json"  
  }  
  
  # Make the POST request  
  response = requests.post(slack_webhook_url, json=payload, headers=headers)  
  
  # Check the response status  
  if response.status_code == 200:  
    print("Message sent successfully to Slack!")  
  else:  
    print(f"Failed to send message to Slack. Status code: {response.status_code}")  
    print(response.json())  

```

### Pass callback to LiteLLM​
```
litellm.success_callback = [send_slack_alert]  

```

```
import litellm  
litellm.success_callback = [send_slack_alert] # log success  
litellm.failure_callback = [send_slack_alert] # log exceptions  
  
# this will raise an exception  
response = litellm.completion(  
  model="gpt-2",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hi 👋 - i'm openai"  
    }  
  ]  
)  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
Weights & Biases - Logging LLM Input/Output
Next
Athina
  * Pre-Requisites
    * Step 1
    * Step 2
  * Quick Start
    * Create a custom Callback to log to slack
    * Pass callback to LiteLLM
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Supabase Tutorial


On this page
# Supabase Tutorial
tip
This is community maintained, Please make an issue if you run into a bug https://github.com/BerriAI/litellm
Supabase is an open source Firebase alternative. Start your project with a Postgres database, Authentication, instant APIs, Edge Functions, Realtime subscriptions, Storage, and Vector embeddings.
## Use Supabase to log requests and see total spend across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)​
liteLLM provides `success_callbacks` and `failure_callbacks`, making it easy for you to send data to a particular provider depending on the status of your responses. 
In this case, we want to log requests to Supabase in both scenarios - when it succeeds and fails. 
### Create a supabase table​
Go to your Supabase project > go to the Supabase SQL Editor and create a new table with this configuration.
Note: You can change the table name. Just don't change the column names. 
```
create table  
 public.request_logs (  
  id bigint generated by default as identity,  
  created_at timestamp with time zone null default now(),  
  model text null default ''::text,  
  messages json null default '{}'::json,  
  response json null default '{}'::json,  
  end_user text null default ''::text,  
  status text null default ''::text,  
  error json null default '{}'::json,  
  response_time real null default '0'::real,  
  total_cost real null,  
  additional_details json null default '{}'::json,  
  litellm_call_id text unique,  
  primary key (id)  
 ) tablespace pg_default;  

```

### Use Callbacks​
Use just 2 lines of code, to instantly see costs and log your responses **across all providers** with Supabase: 
```
litellm.success_callback=["supabase"]  
litellm.failure_callback=["supabase"]  

```

Complete code
```
from litellm import completion  
  
## set env variables  
### SUPABASE  
os.environ["SUPABASE_URL"] = "your-supabase-url"   
os.environ["SUPABASE_KEY"] = "your-supabase-key"   
  
## LLM API KEY  
os.environ["OPENAI_API_KEY"] = ""  
  
# set callbacks  
litellm.success_callback=["supabase"]  
litellm.failure_callback=["supabase"]  
  
# openai call  
response = completion(  
 model="gpt-3.5-turbo",   
 messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}],  
 user="ishaan22" # identify users  
)   
  
# bad call, expect this call to fail and get logged  
response = completion(  
 model="chatgpt-test",   
 messages=[{"role": "user", "content": "Hi 👋 - i'm a bad call to test error logging"}]  
)  
   

```

### Additional Controls​
**Identify end-user**
Pass `user` to `litellm.completion` to map your llm call to an end-user 
```
response = completion(  
 model="gpt-3.5-turbo",   
 messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}],  
 user="ishaan22" # identify users  
)   

```

**Different Table name**
If you modified your table name, here's how to pass the new name.
```
litellm.modify_integration("supabase",{"table_name": "litellm_logs"})  

```

## Support & Talk to Founders​
  * Schedule Demo 👋
  * Community Discord 💭
  * Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai


Previous
Greenscale - Track LLM Spend and Responsible Usage
Next
Telemetry
  * Use Supabase to log requests and see total spend across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)
    * Create a supabase table
    * Use Callbacks
    * Additional Controls
  * Support & Talk to Founders


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
    * 🖇️ AgentOps - LLM Observability Platform
    * 🪢 Langfuse - Logging LLM Input/Output
    * 🌙 Lunary - GenAI Observability
    * 🔁 MLflow - OSS LLM Observability and Evaluation
    * Google Cloud Storage Buckets
    * Langsmith - Logging LLM Input/Output
    * Literal AI - Log, Evaluate, Monitor
    * OpenTelemetry - Tracing LLMs with any observability tool
    * Logfire
    * Argilla
    * Arize AI
    * Phoenix OSS
    * Local Debugging
    * Raw Request/Response Logging
    * Custom Callbacks
    * Humanloop
    * Scrub Logged Data
    * Braintrust - Evals + Logging
    * Sentry - Log LLM Exceptions
    * Lago - Usage Based Billing
    * Helicone - OSS LLM Observability Platform
    * OpenMeter - Usage-Based Billing
    * Promptlayer Tutorial
    * Weights & Biases - Logging LLM Input/Output
    * Slack - Logging LLM Input/Output, Exceptions
    * Athina
    * Greenscale - Track LLM Spend and Responsible Usage
    * Supabase Tutorial
    * Telemetry
    * Comet Opik - Logging + Evals
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Logging & Observability
  * Telemetry


On this page
# Telemetry
There is no Telemetry on LiteLLM - no data is stored by us
## What is logged?​
NOTHING - no data is sent to LiteLLM Servers
Previous
Supabase Tutorial
Next
Comet Opik - Logging + Evals
  * What is logged?


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
      * Why Pass-Through Endpoints?
      * Vertex AI SDK
      * Google AI Studio SDK
      * Cohere SDK
      * VLLM
      * Mistral
      * OpenAI Passthrough
      * Anthropic SDK
      * Bedrock (boto3) SDK
      * Assembly AI
      * Langfuse SDK
      * Create Pass Through Endpoints
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * Pass-through Endpoints (Anthropic SDK, etc.)
  * Cohere SDK


On this page
# Cohere SDK
Pass-through endpoints for Cohere - call provider-specific endpoint, in native format (no translation).
Feature| Supported| Notes  
---|---|---  
Cost Tracking| ✅| Supported for `/v1/chat`, and `/v2/chat`  
Logging| ✅| works across all integrations  
End-user Tracking| ❌| Tell us if you need this  
Streaming| ✅|   
Just replace `https://api.cohere.com` with `LITELLM_PROXY_BASE_URL/cohere` 🚀
#### **Example Usage**​
```
curl --request POST \  
 --url http://0.0.0.0:4000/cohere/v1/chat \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer sk-anything" \  
 --data '{  
  "chat_history": [  
   {"role": "USER", "message": "Who discovered gravity?"},  
   {"role": "CHATBOT", "message": "The man who is widely credited with discovering gravity is Sir Isaac Newton"}  
  ],  
  "message": "What year was he born?",  
  "connectors": [{"id": "web-search"}]  
 }'  

```

Supports **ALL** Cohere Endpoints (including streaming).
**See All Cohere Endpoints**
## Quick Start​
Let's call the Cohere `/rerank` endpoint
  1. Add Cohere API Key to your environment 


```
export COHERE_API_KEY=""  

```

  1. Start LiteLLM Proxy 


```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


Let's call the Cohere /rerank endpoint
```
curl --request POST \  
 --url http://0.0.0.0:4000/cohere/v1/rerank \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer sk-anything" \  
 --data '{  
  "model": "rerank-english-v3.0",  
  "query": "What is the capital of the United States?",  
  "top_n": 3,  
  "documents": ["Carson City is the capital city of the American state of Nevada.",  
         "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
         "Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district.",  
         "Capitalization or capitalisation in English grammar is the use of a capital letter at the start of a word. English usage varies from capitalization in other languages.",  
         "Capital punishment (the death penalty) has existed in the United States since beforethe United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states."]  
 }'  

```

## Examples​
Anything after `http://0.0.0.0:4000/cohere` is treated as a provider-specific route, and handled accordingly.
Key Changes: 
**Original Endpoint**| **Replace With**  
---|---  
`https://api.cohere.com`| `http://0.0.0.0:4000/cohere` (LITELLM_PROXY_BASE_URL="http://0.0.0.0:4000")  
`bearer $CO_API_KEY`| `bearer anything` (use `bearer LITELLM_VIRTUAL_KEY` if Virtual Keys are setup on proxy)  
### **Example 1: Rerank endpoint**​
#### LiteLLM Proxy Call​
```
curl --request POST \  
 --url http://0.0.0.0:4000/cohere/v1/rerank \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer sk-anything" \  
 --data '{  
  "model": "rerank-english-v3.0",  
  "query": "What is the capital of the United States?",  
  "top_n": 3,  
  "documents": ["Carson City is the capital city of the American state of Nevada.",  
         "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
         "Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district.",  
         "Capitalization or capitalisation in English grammar is the use of a capital letter at the start of a word. English usage varies from capitalization in other languages.",  
         "Capital punishment (the death penalty) has existed in the United States since beforethe United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states."]  
 }'  

```

#### Direct Cohere API Call​
```
curl --request POST \  
 --url https://api.cohere.com/v1/rerank \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer $CO_API_KEY" \  
 --data '{  
  "model": "rerank-english-v3.0",  
  "query": "What is the capital of the United States?",  
  "top_n": 3,  
  "documents": ["Carson City is the capital city of the American state of Nevada.",  
         "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
         "Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district.",  
         "Capitalization or capitalisation in English grammar is the use of a capital letter at the start of a word. English usage varies from capitalization in other languages.",  
         "Capital punishment (the death penalty) has existed in the United States since beforethe United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states."]  
 }'  

```

### **Example 2: Chat API**​
#### LiteLLM Proxy Call​
```
curl --request POST \  
 --url http://0.0.0.0:4000/cohere/v1/chat \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer sk-anything" \  
 --data '{  
  "chat_history": [  
   {"role": "USER", "message": "Who discovered gravity?"},  
   {"role": "CHATBOT", "message": "The man who is widely credited with discovering gravity is Sir Isaac Newton"}  
  ],  
  "message": "What year was he born?",  
  "connectors": [{"id": "web-search"}]  
 }'  

```

#### Direct Cohere API Call​
```
curl --request POST \  
 --url https://api.cohere.com/v1/chat \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer $CO_API_KEY" \  
 --data '{  
  "chat_history": [  
   {"role": "USER", "message": "Who discovered gravity?"},  
   {"role": "CHATBOT", "message": "The man who is widely credited with discovering gravity is Sir Isaac Newton"}  
  ],  
  "message": "What year was he born?",  
  "connectors": [{"id": "web-search"}]  
 }'  

```

### **Example 3: Embedding**​
```
curl --request POST \  
 --url https://api.cohere.com/v1/embed \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer sk-anything" \  
 --data '{  
  "model": "embed-english-v3.0",  
  "texts": ["hello", "goodbye"],  
  "input_type": "classification"  
 }'  

```

#### Direct Cohere API Call​
```
curl --request POST \  
 --url https://api.cohere.com/v1/embed \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer $CO_API_KEY" \  
 --data '{  
  "model": "embed-english-v3.0",  
  "texts": ["hello", "goodbye"],  
  "input_type": "classification"  
 }'  

```

## Advanced - Use with Virtual Keys​
Pre-requisites
  * Setup proxy with DB


Use this, to avoid giving developers the raw Cohere API key, but still letting them use Cohere endpoints.
### Usage​
  1. Setup environment


```
export DATABASE_URL=""  
export LITELLM_MASTER_KEY=""  
export COHERE_API_KEY=""  

```

```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Generate virtual key 


```
curl -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{}'  

```

Expected Response 
```
{  
  ...  
  "key": "sk-1234ewknldferwedojwojw"  
}  

```

  1. Test it! 


```
curl --request POST \  
 --url http://0.0.0.0:4000/cohere/v1/rerank \  
 --header 'accept: application/json' \  
 --header 'content-type: application/json' \  
 --header "Authorization: bearer sk-1234ewknldferwedojwojw" \  
 --data '{  
  "model": "rerank-english-v3.0",  
  "query": "What is the capital of the United States?",  
  "top_n": 3,  
  "documents": ["Carson City is the capital city of the American state of Nevada.",  
         "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
         "Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district.",  
         "Capitalization or capitalisation in English grammar is the use of a capital letter at the start of a word. English usage varies from capitalization in other languages.",  
         "Capital punishment (the death penalty) has existed in the United States since beforethe United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states."]  
 }'  

```

Previous
Google AI Studio SDK
Next
VLLM
  * Quick Start
  * Examples
    * **Example 1: Rerank endpoint**
    * **Example 2: Chat API**
    * **Example 3: Embedding**
  * Advanced - Use with Virtual Keys
    * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
      * Why Pass-Through Endpoints?
      * Vertex AI SDK
      * Google AI Studio SDK
      * Cohere SDK
      * VLLM
      * Mistral
      * OpenAI Passthrough
      * Anthropic SDK
      * Bedrock (boto3) SDK
      * Assembly AI
      * Langfuse SDK
      * Create Pass Through Endpoints
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * Pass-through Endpoints (Anthropic SDK, etc.)
  * Vertex AI SDK


On this page
# Vertex AI SDK
Pass-through endpoints for Vertex AI - call provider-specific endpoint, in native format (no translation).
Feature| Supported| Notes  
---|---|---  
Cost Tracking| ✅| supports all models on `/generateContent` endpoint  
Logging| ✅| works across all integrations  
End-user Tracking| ❌| Tell us if you need this  
Streaming| ✅|   
## Supported Endpoints​
LiteLLM supports 2 vertex ai passthrough routes:
  1. `/vertex_ai` → routes to `https://{vertex_location}-aiplatform.googleapis.com/`
  2. `/vertex_ai/discovery` → routes to `https://discoveryengine.googleapis.com`


## How to use​
Just replace `https://REGION-aiplatform.googleapis.com` with `LITELLM_PROXY_BASE_URL/vertex_ai`
LiteLLM supports 3 flows for calling Vertex AI endpoints via pass-through:
  1. **Specific Credentials** : Admin sets passthrough credentials for a specific project/region.
  2. **Default Credentials** : Admin sets default credentials.
  3. **Client-Side Credentials** : User can send client-side credentials through to Vertex AI (default behavior - if no default or mapped credentials are found, the request is passed through directly).


## Example Usage​
  * Specific Project/Region
  * Default Credentials
  * Client Credentials


```
model_list:  
 - model_name: gemini-1.0-pro  
  litellm_params:  
   model: vertex_ai/gemini-1.0-pro  
   vertex_project: adroit-crow-413218  
   vertex_region: us-central1  
   vertex_credentials: /path/to/credentials.json  
   use_in_pass_through: true # 👈 KEY CHANGE  

```

  * Set in config.yaml
  * Set in environment variables


```
default_vertex_config:   
 vertex_project: adroit-crow-413218  
 vertex_region: us-central1  
 vertex_credentials: /path/to/credentials.json  

```

```
export DEFAULT_VERTEXAI_PROJECT="adroit-crow-413218"  
export DEFAULT_VERTEXAI_LOCATION="us-central1"  
export DEFAULT_GOOGLE_APPLICATION_CREDENTIALS="/path/to/credentials.json"  

```

Try Gemini 2.0 Flash (curl)
```
MODEL_ID="gemini-2.0-flash-001"  
PROJECT_ID="YOUR_PROJECT_ID"  

```

```
curl \  
 -X POST \  
 -H "Authorization: Bearer $(gcloud auth application-default print-access-token)" \  
 -H "Content-Type: application/json" \  
 "${LITELLM_PROXY_BASE_URL}/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:streamGenerateContent" -d \  
 $'{  
  "contents": {  
   "role": "user",  
   "parts": [  
    {  
    "fileData": {  
     "mimeType": "image/png",  
     "fileUri": "gs://generativeai-downloads/images/scones.jpg"  
     }  
    },  
    {  
     "text": "Describe this picture."  
    }  
   ]  
  }  
 }'  

```

#### **Example Usage**​
  * curl
  * Vertex Node.js SDK


```
curl http://localhost:4000/vertex_ai/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:generateContent \  
 -H "Content-Type: application/json" \  
 -H "x-litellm-api-key: Bearer sk-1234" \  
 -d '{  
  "contents":[{  
   "role": "user",   
   "parts":[{"text": "How are you doing today?"}]  
  }]  
 }'  

```

```
const { VertexAI } = require('@google-cloud/vertexai');  
  
const vertexAI = new VertexAI({  
  project: 'your-project-id', // enter your vertex project id  
  location: 'us-central1', // enter your vertex region  
  apiEndpoint: "localhost:4000/vertex_ai" // <proxy-server-url>/vertex_ai # note, do not include 'https://' in the url  
});  
  
const model = vertexAI.getGenerativeModel({  
  model: 'gemini-1.0-pro'  
}, {  
  customHeaders: {  
    "x-litellm-api-key": "sk-1234" // Your litellm Virtual Key  
  }  
});  
  
async function generateContent() {  
  try {  
    const prompt = {  
      contents: [{  
        role: 'user',  
        parts: [{ text: 'How are you doing today?' }]  
      }]  
    };  
  
    const response = await model.generateContent(prompt);  
    console.log('Response:', response);  
  } catch (error) {  
    console.error('Error:', error);  
  }  
}  
  
generateContent();  

```

## Quick Start​
Let's call the Vertex AI `/generateContent` endpoint
  1. Add Vertex AI Credentials to your environment 


```
export DEFAULT_VERTEXAI_PROJECT="" # "adroit-crow-413218"  
export DEFAULT_VERTEXAI_LOCATION="" # "us-central1"  
export DEFAULT_GOOGLE_APPLICATION_CREDENTIALS="" # "/Users/Downloads/adroit-crow-413218-a956eef1a2a8.json"  

```

  1. Start LiteLLM Proxy 


```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


Let's call the Google AI Studio token counting endpoint
```
curl http://localhost:4000/vertex-ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "contents":[{  
   "role": "user",  
   "parts":[{"text": "How are you doing today?"}]  
  }]  
 }'  

```

## Supported API Endpoints​
  * Gemini API
  * Embeddings API
  * Imagen API
  * Code Completion API
  * Batch prediction API
  * Tuning API
  * CountTokens API


#### Authentication to Vertex AI​
LiteLLM Proxy Server supports two methods of authentication to Vertex AI:
  1. Pass Vertex Credentials client side to proxy server
  2. Set Vertex AI credentials on proxy server


## Usage Examples​
### Gemini API (Generate Content)​
```
curl http://localhost:4000/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/gemini-1.5-flash-001:generateContent \  
 -H "Content-Type: application/json" \  
 -H "x-litellm-api-key: Bearer sk-1234" \  
 -d '{"contents":[{"role": "user", "parts":[{"text": "hi"}]}]}'  

```

### Embeddings API​
```
curl http://localhost:4000/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/textembedding-gecko@001:predict \  
 -H "Content-Type: application/json" \  
 -H "x-litellm-api-key: Bearer sk-1234" \  
 -d '{"instances":[{"content": "gm"}]}'  

```

### Imagen API​
```
curl http://localhost:4000/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/imagen-3.0-generate-001:predict \  
 -H "Content-Type: application/json" \  
 -H "x-litellm-api-key: Bearer sk-1234" \  
 -d '{"instances":[{"prompt": "make an otter"}], "parameters": {"sampleCount": 1}}'  

```

### Count Tokens API​
```
curl http://localhost:4000/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/gemini-1.5-flash-001:countTokens \  
 -H "Content-Type: application/json" \  
 -H "x-litellm-api-key: Bearer sk-1234" \  
 -d '{"contents":[{"role": "user", "parts":[{"text": "hi"}]}]}'  

```

### Tuning API​
Create Fine Tuning Job
```
curl http://localhost:4000/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/gemini-1.5-flash-001:tuningJobs \  
   -H "Content-Type: application/json" \  
   -H "x-litellm-api-key: Bearer sk-1234" \  
   -d '{  
 "baseModel": "gemini-1.0-pro-002",  
 "supervisedTuningSpec" : {  
   "training_dataset_uri": "gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl"  
 }  
}'  

```

## Advanced​
Pre-requisites
  * Setup proxy with DB


Use this, to avoid giving developers the raw Anthropic API key, but still letting them use Anthropic endpoints.
### Use with Virtual Keys​
  1. Setup environment


```
export DATABASE_URL=""  
export LITELLM_MASTER_KEY=""  
  
# vertex ai credentials  
export DEFAULT_VERTEXAI_PROJECT="" # "adroit-crow-413218"  
export DEFAULT_VERTEXAI_LOCATION="" # "us-central1"  
export DEFAULT_GOOGLE_APPLICATION_CREDENTIALS="" # "/Users/Downloads/adroit-crow-413218-a956eef1a2a8.json"  

```

```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Generate virtual key 


```
curl -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'x-litellm-api-key: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{}'  

```

Expected Response 
```
{  
  ...  
  "key": "sk-1234ewknldferwedojwojw"  
}  

```

  1. Test it! 


```
curl http://localhost:4000/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent \  
 -H "Content-Type: application/json" \  
 -H "x-litellm-api-key: Bearer sk-1234" \  
 -d '{  
  "contents":[{  
   "role": "user",   
   "parts":[{"text": "How are you doing today?"}]  
  }]  
 }'  

```

### Send `tags` in request headers​
Use this if you wants `tags` to be tracked in the LiteLLM DB and on logging callbacks
Pass `tags` in request headers as a comma separated list. In the example below the following tags will be tracked 
```
tags: ["vertex-js-sdk", "pass-through-endpoint"]  

```

  * curl
  * Vertex Node.js SDK


```
curl http://localhost:4000/vertex_ai/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent \  
 -H "Content-Type: application/json" \  
 -H "x-litellm-api-key: Bearer sk-1234" \  
 -H "tags: vertex-js-sdk,pass-through-endpoint" \  
 -d '{  
  "contents":[{  
   "role": "user",   
   "parts":[{"text": "How are you doing today?"}]  
  }]  
 }'  

```

```
const { VertexAI } = require('@google-cloud/vertexai');  
  
const vertexAI = new VertexAI({  
  project: 'your-project-id', // enter your vertex project id  
  location: 'us-central1', // enter your vertex region  
  apiEndpoint: "localhost:4000/vertex_ai" // <proxy-server-url>/vertex_ai # note, do not include 'https://' in the url  
});  
  
const model = vertexAI.getGenerativeModel({  
  model: 'gemini-1.0-pro'  
}, {  
  customHeaders: {  
    "x-litellm-api-key": "sk-1234", // Your litellm Virtual Key  
    "tags": "vertex-js-sdk,pass-through-endpoint"  
  }  
});  
  
async function generateContent() {  
  try {  
    const prompt = {  
      contents: [{  
        role: 'user',  
        parts: [{ text: 'How are you doing today?' }]  
      }]  
    };  
  
    const response = await model.generateContent(prompt);  
    console.log('Response:', response);  
  } catch (error) {  
    console.error('Error:', error);  
  }  
}  
  
generateContent();  

```

Previous
Why Pass-Through Endpoints?
Next
Google AI Studio SDK
  * Supported Endpoints
  * How to use
  * Example Usage
  * Quick Start
  * Supported API Endpoints
  * Usage Examples
    * Gemini API (Generate Content)
    * Embeddings API
    * Imagen API
    * Count Tokens API
    * Tuning API
  * Advanced
    * Use with Virtual Keys
    * Send `tags` in request headers


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
      * Why Pass-Through Endpoints?
      * Vertex AI SDK
      * Google AI Studio SDK
      * Cohere SDK
      * VLLM
      * Mistral
      * OpenAI Passthrough
      * Anthropic SDK
      * Bedrock (boto3) SDK
      * Assembly AI
      * Langfuse SDK
      * Create Pass Through Endpoints
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * Pass-through Endpoints (Anthropic SDK, etc.)
  * OpenAI Passthrough


On this page
# OpenAI Passthrough
Pass-through endpoints for `/openai`
## Overview​
Feature| Supported| Notes  
---|---|---  
Cost Tracking| ❌| Not supported  
Logging| ✅| Works across all integrations  
Streaming| ✅| Fully supported  
### When to use this?​
  * For 90% of your use cases, you should use the native LiteLLM OpenAI Integration (`/chat/completions`, `/embeddings`, `/completions`, `/images`, `/batches`, etc.)
  * Use this passthrough to call less popular or newer OpenAI endpoints that LiteLLM doesn't fully support yet, such as `/assistants`, `/threads`, `/vector_stores`


Simply replace `https://api.openai.com` with `LITELLM_PROXY_BASE_URL/openai`
## Usage Examples​
### Assistants API​
#### Create OpenAI Client​
Make sure you do the following:
  * Point `base_url` to your `LITELLM_PROXY_BASE_URL/openai`
  * Use your `LITELLM_API_KEY` as the `api_key`


```
import openai  
  
client = openai.OpenAI(  
  base_url="http://0.0.0.0:4000/openai", # <your-proxy-url>/openai  
  api_key="sk-anything" # <your-proxy-api-key>  
)  

```

#### Create an Assistant​
```
# Create an assistant  
assistant = client.beta.assistants.create(  
  name="Math Tutor",  
  instructions="You are a math tutor. Help solve equations.",  
  model="gpt-4o",  
)  

```

#### Create a Thread​
```
# Create a thread  
thread = client.beta.threads.create()  

```

#### Add a Message to the Thread​
```
# Add a message  
message = client.beta.threads.messages.create(  
  thread_id=thread.id,  
  role="user",  
  content="Solve 3x + 11 = 14",  
)  

```

#### Run the Assistant​
```
# Create a run to get the assistant's response  
run = client.beta.threads.runs.create(  
  thread_id=thread.id,  
  assistant_id=assistant.id,  
)  
  
# Check run status  
run_status = client.beta.threads.runs.retrieve(  
  thread_id=thread.id,  
  run_id=run.id  
)  

```

#### Retrieve Messages​
```
# List messages after the run completes  
messages = client.beta.threads.messages.list(  
  thread_id=thread.id  
)  

```

#### Delete the Assistant​
```
# Delete the assistant when done  
client.beta.assistants.delete(assistant.id)  

```

Previous
Mistral
Next
Anthropic SDK
  * Overview
    * When to use this?
  * Usage Examples
    * Assistants API


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
    * Data Privacy and Security
    * Data Retention Policy
    * Migration Policy
    * ❤️ 🚅 Projects built on LiteLLM
      * 🤗 Smolagents
      * Docq.AI
      * PDL
      * OpenInterpreter
      * 🐕 Elroy
      * dbally
      * FastREPL
      * PROMPTMETHEUS
      * Codium PR Agent
      * Prompt2Model
      * SalesGPT
      * Quivr
      * Langstream
      * Otter
      * GPT Migrate
      * YiVal
      * LiteLLM Proxy
      * llmcord.py
      * pgai
    * PII Masking - LiteLLM Gateway (Deprecated Version)
    * Code Quality
    * Rules
    * [DEPRECATED] Team-based Routing
    * [DEPRECATED] Region-based Routing
    * [OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
  * Support & Talk with founders


  *   * Extras
  * ❤️ 🚅 Projects built on LiteLLM


# Projects built on LiteLLM
Learn how to deploy + call models from different providers on LiteLLM
## 📄️ 🤗 Smolagents
smolagents is a barebones library for agents. Agents write python code to call tools and orchestrate other agents.
## 📄️ Docq.AI
A private and secure ChatGPT alternative that knows your business.
## 📄️ PDL
PDL - A YAML-based approach to prompt programming
## 📄️ OpenInterpreter
Open Interpreter lets LLMs run code on your computer to complete tasks.
## 📄️ 🐕 Elroy
Elroy is a scriptable AI assistant that remembers and sets goals.
## 📄️ dbally
Efficient, consistent and secure library for querying structured data with natural language. Query any database with over 100 LLMs ❤️ 🚅.
## 📄️ FastREPL
⚡Fast Run-Eval-Polish Loop for LLM Applications
## 📄️ PROMPTMETHEUS
🔥 PROMPTMETHEUS – Prompt Engineering IDE
## 📄️ Codium PR Agent
An AI-Powered 🤖 Tool for Automated Pull Request Analysis,
## 📄️ Prompt2Model
Prompt2Model - Generate Deployable Models from Instructions
## 📄️ SalesGPT
🤖 SalesGPT - Your Context-Aware AI Sales Assistant
## 📄️ Quivr
🧠 Your Second Brain supercharged by Generative AI 🧠 Dump all your files and chat with your personal assistant on your files & more using GPT 3.5/4, Private, Anthropic, VertexAI, LLMs...
## 📄️ Langstream
Build robust LLM applications with true composability 🔗
## 📄️ Otter
🦦 Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following and in-context learning ability.
## 📄️ GPT Migrate
Easily migrate your codebase from one framework or language to another.
## 📄️ YiVal
🚀 Evaluate and Evolve.🚀 YiVal is an open source GenAI-Ops framework that allows you to manually or automatically tune and evaluate your AIGC prompts, retrieval configs and fine-tune the model params all at once with your preferred choices of test dataset generation, evaluation algorithms and improvement strategies.
## 📄️ LiteLLM Proxy
LiteLLM Proxy
## 📄️ llmcord.py
llmcord.py lets you and your friends chat with LLMs directly in your Discord server. It works with practically any LLM, remote or locally hosted.
## 📄️ pgai
pgai is a suite of tools to develop RAG, semantic search, and other AI applications more easily with PostgreSQL.
Previous
Migration Policy
Next
🤗 Smolagents
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
      * Why Pass-Through Endpoints?
      * Vertex AI SDK
      * Google AI Studio SDK
      * Cohere SDK
      * VLLM
      * Mistral
      * OpenAI Passthrough
      * Anthropic SDK
      * Bedrock (boto3) SDK
      * Assembly AI
      * Langfuse SDK
      * Create Pass Through Endpoints
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * Pass-through Endpoints (Anthropic SDK, etc.)
  * Why Pass-Through Endpoints?


On this page
# Why Pass-Through Endpoints?
These endpoints are useful for 2 scenarios:
  1. **Migrate existing projects** to litellm proxy. E.g: If you have users already in production with Anthropic's SDK, you just need to change the base url to get cost tracking/logging/budgets/etc. 


  1. **Use provider-specific endpoints** E.g: If you want to use Vertex AI's token counting endpoint


## How is your request handled?​
The request is passed through to the provider's endpoint. The response is then passed back to the client. **No translation is done.**
Previous
/audio/speech
Next
Vertex AI SDK
  * How is your request handled?


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
      * Why Pass-Through Endpoints?
      * Vertex AI SDK
      * Google AI Studio SDK
      * Cohere SDK
      * VLLM
      * Mistral
      * OpenAI Passthrough
      * Anthropic SDK
      * Bedrock (boto3) SDK
      * Assembly AI
      * Langfuse SDK
      * Create Pass Through Endpoints
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * Pass-through Endpoints (Anthropic SDK, etc.)
  * Mistral


On this page
# Mistral
Pass-through endpoints for Mistral - call provider-specific endpoint, in native format (no translation).
Feature| Supported| Notes  
---|---|---  
Cost Tracking| ❌| Not supported  
Logging| ✅| works across all integrations  
End-user Tracking| ❌| Tell us if you need this  
Streaming| ✅|   
Just replace `https://api.mistral.ai/v1` with `LITELLM_PROXY_BASE_URL/mistral` 🚀
#### **Example Usage**​
```
curl -L -X POST 'http://0.0.0.0:4000/mistral/v1/ocr' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "mistral-ocr-latest",  
  "document": {  
    "type": "image_url",  
    "image_url": "https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png"  
  }  
  
}'  

```

Supports **ALL** Mistral Endpoints (including streaming).
## Quick Start​
Let's call the Mistral `/chat/completions` endpoint
  1. Add MISTRAL_API_KEY to your environment 


```
export MISTRAL_API_KEY="sk-1234"  

```

  1. Start LiteLLM Proxy 


```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


Let's call the Mistral `/ocr` endpoint
```
curl -L -X POST 'http://0.0.0.0:4000/mistral/v1/ocr' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "mistral-ocr-latest",  
  "document": {  
    "type": "image_url",  
    "image_url": "https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png"  
  }  
  
}'  

```

## Examples​
Anything after `http://0.0.0.0:4000/mistral` is treated as a provider-specific route, and handled accordingly.
Key Changes: 
**Original Endpoint**| **Replace With**  
---|---  
`https://api.mistral.ai/v1`| `http://0.0.0.0:4000/mistral` (LITELLM_PROXY_BASE_URL="http://0.0.0.0:4000")  
`bearer $MISTRAL_API_KEY`| `bearer anything` (use `bearer LITELLM_VIRTUAL_KEY` if Virtual Keys are setup on proxy)  
### **Example 1: OCR endpoint**​
#### LiteLLM Proxy Call​
```
curl -L -X POST 'http://0.0.0.0:4000/mistral/v1/ocr' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_API_KEY' \  
-d '{  
  "model": "mistral-ocr-latest",  
  "document": {  
    "type": "image_url",  
    "image_url": "https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png"  
  }  
}'  

```

#### Direct Mistral API Call​
```
curl https://api.mistral.ai/v1/ocr \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer ${MISTRAL_API_KEY}" \  
 -d '{  
  "model": "mistral-ocr-latest",  
  "document": {  
    "type": "document_url",  
    "document_url": "https://arxiv.org/pdf/2201.04234"  
  },  
  "include_image_base64": true  
 }'  

```

### **Example 2: Chat API**​
#### LiteLLM Proxy Call​
```
curl -L -X POST 'http://0.0.0.0:4000/mistral/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_VIRTUAL_KEY' \  
-d '{  
  "messages": [  
    {  
      "role": "user",  
      "content": "I am going to Paris, what should I see?"  
    }  
  ],  
  "max_tokens": 2048,  
  "temperature": 0.8,  
  "top_p": 0.1,  
  "model": "mistral-large-latest",  
}'  

```

#### Direct Mistral API Call​
```
curl -L -X POST 'https://api.mistral.ai/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-d '{  
  "messages": [  
    {  
      "role": "user",  
      "content": "I am going to Paris, what should I see?"  
    }  
  ],  
  "max_tokens": 2048,  
  "temperature": 0.8,  
  "top_p": 0.1,  
  "model": "mistral-large-latest",  
}'  

```

## Advanced - Use with Virtual Keys​
Pre-requisites
  * Setup proxy with DB


Use this, to avoid giving developers the raw Mistral API key, but still letting them use Mistral endpoints.
### Usage​
  1. Setup environment


```
export DATABASE_URL=""  
export LITELLM_MASTER_KEY=""  
export MISTRAL_API_BASE=""  

```

```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Generate virtual key 


```
curl -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{}'  

```

Expected Response 
```
{  
  ...  
  "key": "sk-1234ewknldferwedojwojw"  
}  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/mistral/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234ewknldferwedojwojw' \  
 --data '{  
  "messages": [  
    {  
      "role": "user",  
      "content": "I am going to Paris, what should I see?"  
    }  
  ],  
  "max_tokens": 2048,  
  "temperature": 0.8,  
  "top_p": 0.1,  
  "model": "qwen2.5-7b-instruct",  
}'  

```

Previous
VLLM
Next
OpenAI Passthrough
  * Quick Start
  * Examples
    * **Example 1: OCR endpoint**
    * **Example 2: Chat API**
  * Advanced - Use with Virtual Keys
    * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
    * /chat/completions
    * /responses [Beta]
    * /completions
    * /embeddings
    * /v1/messages [BETA]
    * /mcp [BETA] - Model Context Protocol
    * /images
    * /audio
    * Pass-through Endpoints (Anthropic SDK, etc.)
      * Why Pass-Through Endpoints?
      * Vertex AI SDK
      * Google AI Studio SDK
      * Cohere SDK
      * VLLM
      * Mistral
      * OpenAI Passthrough
      * Anthropic SDK
      * Bedrock (boto3) SDK
      * Assembly AI
      * Langfuse SDK
      * Create Pass Through Endpoints
    * /rerank
    * /assistants
    * /files
    * /batches
    * /realtime
    * /fine_tuning
    * /moderations
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Endpoints
  * Pass-through Endpoints (Anthropic SDK, etc.)
  * VLLM


On this page
# VLLM
Pass-through endpoints for VLLM - call provider-specific endpoint, in native format (no translation).
Feature| Supported| Notes  
---|---|---  
Cost Tracking| ❌| Not supported  
Logging| ✅| works across all integrations  
End-user Tracking| ❌| Tell us if you need this  
Streaming| ✅|   
Just replace `https://my-vllm-server.com` with `LITELLM_PROXY_BASE_URL/vllm` 🚀
#### **Example Usage**​
```
curl -L -X GET 'http://0.0.0.0:4000/vllm/metrics' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  

```

Supports **ALL** VLLM Endpoints (including streaming).
## Quick Start​
Let's call the VLLM `/metrics` endpoint
  1. Add HOSTED VLLM API BASE to your environment 


```
export HOSTED_VLLM_API_BASE="https://my-vllm-server.com"  

```

  1. Start LiteLLM Proxy 


```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


Let's call the VLLM `/metrics` endpoint
```
curl -L -X GET 'http://0.0.0.0:4000/vllm/metrics' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  

```

## Examples​
Anything after `http://0.0.0.0:4000/vllm` is treated as a provider-specific route, and handled accordingly.
Key Changes: 
**Original Endpoint**| **Replace With**  
---|---  
`https://my-vllm-server.com`| `http://0.0.0.0:4000/vllm` (LITELLM_PROXY_BASE_URL="http://0.0.0.0:4000")  
`bearer $VLLM_API_KEY`| `bearer anything` (use `bearer LITELLM_VIRTUAL_KEY` if Virtual Keys are setup on proxy)  
### **Example 1: Metrics endpoint**​
#### LiteLLM Proxy Call​
```
curl -L -X GET 'http://0.0.0.0:4000/vllm/metrics' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_VIRTUAL_KEY' \  

```

#### Direct VLLM API Call​
```
curl -L -X GET 'https://my-vllm-server.com/metrics' \  
-H 'Content-Type: application/json' \  

```

### **Example 2: Chat API**​
#### LiteLLM Proxy Call​
```
curl -L -X POST 'http://0.0.0.0:4000/vllm/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_VIRTUAL_KEY' \  
-d '{  
  "messages": [  
    {  
      "role": "user",  
      "content": "I am going to Paris, what should I see?"  
    }  
  ],  
  "max_tokens": 2048,  
  "temperature": 0.8,  
  "top_p": 0.1,  
  "model": "qwen2.5-7b-instruct",  
}'  

```

#### Direct VLLM API Call​
```
curl -L -X POST 'https://my-vllm-server.com/chat/completions' \  
-H 'Content-Type: application/json' \  
-d '{  
  "messages": [  
    {  
      "role": "user",  
      "content": "I am going to Paris, what should I see?"  
    }  
  ],  
  "max_tokens": 2048,  
  "temperature": 0.8,  
  "top_p": 0.1,  
  "model": "qwen2.5-7b-instruct",  
}'  

```

## Advanced - Use with Virtual Keys​
Pre-requisites
  * Setup proxy with DB


Use this, to avoid giving developers the raw Cohere API key, but still letting them use Cohere endpoints.
### Usage​
  1. Setup environment


```
export DATABASE_URL=""  
export LITELLM_MASTER_KEY=""  
export HOSTED_VLLM_API_BASE=""  

```

```
litellm  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Generate virtual key 


```
curl -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{}'  

```

Expected Response 
```
{  
  ...  
  "key": "sk-1234ewknldferwedojwojw"  
}  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/vllm/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234ewknldferwedojwojw' \  
 --data '{  
  "messages": [  
    {  
      "role": "user",  
      "content": "I am going to Paris, what should I see?"  
    }  
  ],  
  "max_tokens": 2048,  
  "temperature": 0.8,  
  "top_p": 0.1,  
  "model": "qwen2.5-7b-instruct",  
}'  

```

Previous
Cohere SDK
Next
Mistral
  * Quick Start
  * Examples
    * **Example 1: Metrics endpoint**
    * **Example 2: Chat API**
  * Advanced - Use with Virtual Keys
    * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * AI21


On this page
# AI21
LiteLLM supports the following AI21 models:
  * `jamba-1.5-mini`
  * `jamba-1.5-large`
  * `j2-light`
  * `j2-mid`
  * `j2-ultra`


tip
**We support ALL AI21 models, just set`model=ai21/<any-model-on-ai21>` as a prefix when sending litellm requests**. **See all litellm supported AI21 modelshere**
### API KEYS​
```
import os   
os.environ["AI21_API_KEY"] = "your-api-key"  

```

## **LiteLLM Python SDK Usage**​
### Sample Usage​
```
from litellm import completion   
  
# set env variable   
os.environ["AI21_API_KEY"] = "your-api-key"  
  
messages = [{"role": "user", "content": "Write me a poem about the blue sky"}]  
  
completion(model="ai21/jamba-1.5-mini", messages=messages)  

```

## **LiteLLM Proxy Server Usage**​
Here's how to call a ai21 model with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: ai21/<your-model-name> # add ai21/ prefix to route as ai21 provider  
   api_key: api-key         # api key to send your model  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



## Supported OpenAI Parameters​
param| type| AI21 equivalent  
---|---|---  
`tools`| **Optional[list]**| `tools`  
`response_format`| **Optional[dict]**| `response_format`  
`max_tokens`| **Optional[int]**| `max_tokens`  
`temperature`| **Optional[float]**| `temperature`  
`top_p`| **Optional[float]**| `top_p`  
`stop`| **Optional[Union[str, list]]**| `stop`  
`n`| **Optional[int]**| `n`  
`stream`| **Optional[bool]**| `stream`  
`seed`| **Optional[int]**| `seed`  
`tool_choice`| **Optional[str]**| `tool_choice`  
`user`| **Optional[str]**| `user`  
## Supported AI21 Parameters​
param| type| AI21 equivalent  
---|---|---  
`documents`| **Optional[List[Dict]]**| `documents`  
## Passing AI21 Specific Parameters - `documents`​
LiteLLM allows you to pass all AI21 specific parameters to the `litellm.completion` function. Here is an example of how to pass the `documents` parameter to the `litellm.completion` function.
  * LiteLLM Python SDK
  * LiteLLM Proxy Server


```
response = await litellm.acompletion(  
  model="jamba-1.5-large",  
  messages=[{"role": "user", "content": "what does the document say"}],  
  documents = [  
    {  
      "content": "hello world",  
      "metadata": {  
        "source": "google",  
        "author": "ishaan"  
      }  
    }  
  ]  
)  
  

```

```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
  extra_body = {  
    "documents": [  
      {  
        "content": "hello world",  
        "metadata": {  
          "source": "google",  
          "author": "ishaan"  
        }  
      }  
    ]  
  }  
)  
  
print(response)  
  

```

tip
**We support ALL AI21 models, just set`model=ai21/<any-model-on-ai21>` as a prefix when sending litellm requests** **See all litellm supported AI21 modelshere**
## AI21 Models​
Model Name| Function Call| Required OS Variables  
---|---|---  
jamba-1.5-mini| `completion('jamba-1.5-mini', messages)`| `os.environ['AI21_API_KEY']`  
jamba-1.5-large| `completion('jamba-1.5-large', messages)`| `os.environ['AI21_API_KEY']`  
j2-light| `completion('j2-light', messages)`| `os.environ['AI21_API_KEY']`  
j2-mid| `completion('j2-mid', messages)`| `os.environ['AI21_API_KEY']`  
j2-ultra| `completion('j2-ultra', messages)`| `os.environ['AI21_API_KEY']`  
Previous
DeepInfra
Next
NLP Cloud
  * API KEYS
  * **LiteLLM Python SDK Usage**
    * Sample Usage
  * **LiteLLM Proxy Server Usage**
  * Supported OpenAI Parameters
  * Supported AI21 Parameters
  * Passing AI21 Specific Parameters - `documents`
  * AI21 Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * AI/ML API


On this page
# AI/ML API
Getting started with the AI/ML API is simple. Follow these steps to set up your integration:
### 1. Get Your API Key​
To begin, you need an API key. You can obtain yours here:  
🔑 Get Your API Key
### 2. Explore Available Models​
Looking for a different model? Browse the full list of supported models:  
📚 Full List of Models
### 3. Read the Documentation​
For detailed setup instructions and usage guidelines, check out the official documentation:  
📖 AI/ML API Docs
### 4. Need Help?​
If you have any questions, feel free to reach out. We’re happy to assist! 🚀 Discord
## Usage​
You can choose from LLama, Qwen, Flux, and 200+ other open and closed-source models on aimlapi.com/models. For example:
```
import litellm  
  
response = litellm.completion(  
  model="openai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo", # The model name must include prefix "openai" + the model name from ai/ml api  
  api_key="", # your aiml api-key   
  api_base="https://api.aimlapi.com/v2",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hey, how's it going?",  
    }  
  ],  
)  

```

## Streaming​
```
import litellm  
  
response = litellm.completion(  
  model="openai/Qwen/Qwen2-72B-Instruct", # The model name must include prefix "openai" + the model name from ai/ml api  
  api_key="", # your aiml api-key   
  api_base="https://api.aimlapi.com/v2",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hey, how's it going?",  
    }  
  ],  
  stream=True,  
)  
for chunk in response:  
  print(chunk)  

```

## Async Completion​
```
import asyncio  
  
import litellm  
  
  
async def main():  
  response = await litellm.acompletion(  
    model="openai/anthropic/claude-3-5-haiku", # The model name must include prefix "openai" + the model name from ai/ml api  
    api_key="", # your aiml api-key  
    api_base="https://api.aimlapi.com/v2",  
    messages=[  
      {  
        "role": "user",  
        "content": "Hey, how's it going?",  
      }  
    ],  
  )  
  print(response)  
  
  
if __name__ == "__main__":  
  asyncio.run(main())  

```

## Async Streaming​
```
import asyncio  
import traceback  
  
import litellm  
  
  
async def main():  
  try:  
    print("test acompletion + streaming")  
    response = await litellm.acompletion(  
      model="openai/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", # The model name must include prefix "openai" + the model name from ai/ml api  
      api_key="", # your aiml api-key  
      api_base="https://api.aimlapi.com/v2",  
      messages=[{"content": "Hey, how's it going?", "role": "user"}],  
      stream=True,  
    )  
    print(f"response: {response}")  
    async for chunk in response:  
      print(chunk)  
  except:  
    print(f"error occurred: {traceback.format_exc()}")  
    pass  
  
  
if __name__ == "__main__":  
  asyncio.run(main())  

```

## Async Embedding​
```
import asyncio  
  
import litellm  
  
  
async def main():  
  response = await litellm.aembedding(  
    model="openai/text-embedding-3-small", # The model name must include prefix "openai" + the model name from ai/ml api  
    api_key="", # your aiml api-key  
    api_base="https://api.aimlapi.com/v1", # 👈 the URL has changed from v2 to v1  
    input="Your text string",  
  )  
  print(response)  
  
  
if __name__ == "__main__":  
  asyncio.run(main())  

```

## Async Image Generation​
```
import asyncio  
  
import litellm  
  
  
async def main():  
  response = await litellm.aimage_generation(  
    model="openai/dall-e-3", # The model name must include prefix "openai" + the model name from ai/ml api  
    api_key="", # your aiml api-key  
    api_base="https://api.aimlapi.com/v1", # 👈 the URL has changed from v2 to v1  
    prompt="A cute baby sea otter",  
  )  
  print(response)  
  
  
if __name__ == "__main__":  
  asyncio.run(main())  

```

Previous
Azure AI Studio
Next
VertexAI [Anthropic, Gemini, Model Garden]
  * 1. Get Your API Key
  * 2. Explore Available Models
  * 3. Read the Documentation
  * 4. Need Help?
  * Usage
  * Streaming
  * Async Completion
  * Async Streaming
  * Async Embedding
  * Async Image Generation


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Aleph Alpha


On this page
# Aleph Alpha
LiteLLM supports all models from Aleph Alpha. 
Like AI21 and Cohere, you can use these models without a waitlist. 
### API KEYS​
```
import os  
os.environ["ALEPHALPHA_API_KEY"] = ""  

```

### Aleph Alpha Models​
https://www.aleph-alpha.com/
Model Name| Function Call| Required OS Variables  
---|---|---  
luminous-base| `completion(model='luminous-base', messages=messages)`| `os.environ['ALEPHALPHA_API_KEY']`  
luminous-base-control| `completion(model='luminous-base-control', messages=messages)`| `os.environ['ALEPHALPHA_API_KEY']`  
luminous-extended| `completion(model='luminous-extended', messages=messages)`| `os.environ['ALEPHALPHA_API_KEY']`  
luminous-extended-control| `completion(model='luminous-extended-control', messages=messages)`| `os.environ['ALEPHALPHA_API_KEY']`  
luminous-supreme| `completion(model='luminous-supreme', messages=messages)`| `os.environ['ALEPHALPHA_API_KEY']`  
luminous-supreme-control| `completion(model='luminous-supreme-control', messages=messages)`| `os.environ['ALEPHALPHA_API_KEY']`  
Previous
Jina AI
Next
Baseten
  * API KEYS
  * Aleph Alpha Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Anthropic


On this page
# Anthropic
LiteLLM supports all anthropic models.
  * `claude-3.5` (`claude-3-5-sonnet-20240620`)
  * `claude-3` (`claude-3-haiku-20240307`, `claude-3-opus-20240229`, `claude-3-sonnet-20240229`)
  * `claude-2`
  * `claude-2.1`
  * `claude-instant-1.2`

Property| Details  
---|---  
Description| Claude is a highly performant, trustworthy, and intelligent AI platform built by Anthropic. Claude excels at tasks involving language, reasoning, analysis, coding, and more.  
Provider Route on LiteLLM| `anthropic/` (add this prefix to the model name, to route any requests to Anthropic - e.g. `anthropic/claude-3-5-sonnet-20240620`)  
Provider Doc| Anthropic ↗  
API Endpoint for Provider| https://api.anthropic.com  
Supported Endpoints| `/chat/completions`  
## Supported OpenAI Parameters​
Check this in code, here
```
"stream",  
"stop",  
"temperature",  
"top_p",  
"max_tokens",  
"max_completion_tokens",  
"tools",  
"tool_choice",  
"extra_headers",  
"parallel_tool_calls",  
"response_format",  
"user"  

```

info
Anthropic API fails requests when `max_tokens` are not passed. Due to this litellm passes `max_tokens=4096` when no `max_tokens` are passed.
## API Keys​
```
import os  
  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
# os.environ["ANTHROPIC_API_BASE"] = "" # [OPTIONAL] or 'ANTHROPIC_BASE_URL'  

```

## Usage​
```
import os  
from litellm import completion  
  
# set env - [OPTIONAL] replace with your anthropic key  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
messages = [{"role": "user", "content": "Hey! how's it going?"}]  
response = completion(model="claude-3-opus-20240229", messages=messages)  
print(response)  

```

## Usage - Streaming​
Just set `stream=True` when calling completion.
```
import os  
from litellm import completion  
  
# set env  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
messages = [{"role": "user", "content": "Hey! how's it going?"}]  
response = completion(model="claude-3-opus-20240229", messages=messages, stream=True)  
for chunk in response:  
  print(chunk["choices"][0]["delta"]["content"]) # same as openai format  

```

## Usage with LiteLLM Proxy​
Here's how to call Anthropic with the LiteLLM Proxy Server
### 1. Save key in your environment​
```
export ANTHROPIC_API_KEY="your-api-key"  

```

### 2. Start the proxy​
  * config.yaml
  * config - default all Anthropic Model
  * cli


```
model_list:  
 - model_name: claude-3 ### RECEIVED MODEL NAME ###  
  litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input  
   model: claude-3-opus-20240229 ### MODEL NAME sent to `litellm.completion()` ###  
   api_key: "os.environ/ANTHROPIC_API_KEY" # does os.getenv("AZURE_API_KEY_EU")  

```

```
litellm --config /path/to/config.yaml  

```

Use this if you want to make requests to `claude-3-haiku-20240307`,`claude-3-opus-20240229`,`claude-2.1` without defining them on the config.yaml
#### Required env variables​
```
ANTHROPIC_API_KEY=sk-ant****  

```

```
model_list:  
 - model_name: "*"   
  litellm_params:  
   model: "*"  

```

```
litellm --config /path/to/config.yaml  

```

Example Request for this config.yaml
**Ensure you use`anthropic/` prefix to route the request to Anthropic API**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "anthropic/claude-3-haiku-20240307",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
$ litellm --model claude-3-opus-20240229  
  
# Server running on http://0.0.0.0:4000  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "claude-3",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="claude-3", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "claude-3",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Supported Models​
`Model Name` 👉 Human-friendly name.  
`Function Call` 👉 How to call the model in LiteLLM.
Model Name| Function Call  
---|---  
claude-3-5-sonnet| `completion('claude-3-5-sonnet-20240620', messages)`  
claude-3-haiku| `completion('claude-3-haiku-20240307', messages)`  
claude-3-opus| `completion('claude-3-opus-20240229', messages)`  
claude-3-5-sonnet-20240620| `completion('claude-3-5-sonnet-20240620', messages)`  
claude-3-sonnet| `completion('claude-3-sonnet-20240229', messages)`  
claude-2.1| `completion('claude-2.1', messages)`  
claude-2| `completion('claude-2', messages)`  
claude-instant-1.2| `completion('claude-instant-1.2', messages)`  
claude-instant-1| `completion('claude-instant-1', messages)`  
## **Prompt Caching**​
Use Anthropic Prompt Caching
Relevant Anthropic API Docs
note
Here's what a sample Raw Request from LiteLLM for Anthropic Context Caching looks like: 
```
POST Request Sent from LiteLLM:  
curl -X POST \  
https://api.anthropic.com/v1/messages \  
-H 'accept: application/json' -H 'anthropic-version: 2023-06-01' -H 'content-type: application/json' -H 'x-api-key: sk-...' -H 'anthropic-beta: prompt-caching-2024-07-31' \  
-d '{'model': 'claude-3-5-sonnet-20240620', [  
  {  
   "role": "user",  
   "content": [  
    {  
     "type": "text",  
     "text": "What are the key terms and conditions in this agreement?",  
     "cache_control": {  
      "type": "ephemeral"  
     }  
    }  
   ]  
  },  
  {  
   "role": "assistant",  
   "content": [  
    {  
     "type": "text",  
     "text": "Certainly! The key terms and conditions are the following: the contract is 1 year long for $10/mo"  
    }  
   ]  
  }  
 ],  
 "temperature": 0.2,  
 "max_tokens": 10  
}'  

```

### Caching - Large Context Caching​
This example demonstrates basic Prompt Caching usage, caching the full text of the legal agreement as a prefix while keeping the user instruction uncached.
  * LiteLLM SDK
  * LiteLLM Proxy


```
response = await litellm.acompletion(  
  model="anthropic/claude-3-5-sonnet-20240620",  
  messages=[  
    {  
      "role": "system",  
      "content": [  
        {  
          "type": "text",  
          "text": "You are an AI assistant tasked with analyzing legal documents.",  
        },  
        {  
          "type": "text",  
          "text": "Here is the full text of a complex legal agreement",  
          "cache_control": {"type": "ephemeral"},  
        },  
      ],  
    },  
    {  
      "role": "user",  
      "content": "what are the key terms and conditions in this agreement?",  
    },  
  ]  
)  
  

```

info
LiteLLM Proxy is OpenAI compatible
This is an example using the OpenAI Python SDK sending a request to LiteLLM Proxy
Assuming you have a model=`anthropic/claude-3-5-sonnet-20240620` on the litellm proxy config.yaml
```
import openai  
client = openai.AsyncOpenAI(  
  api_key="anything",      # litellm proxy api key  
  base_url="http://0.0.0.0:4000" # litellm proxy base url  
)  
  
  
response = await client.chat.completions.create(  
  model="anthropic/claude-3-5-sonnet-20240620",  
  messages=[  
    {  
      "role": "system",  
      "content": [  
        {  
          "type": "text",  
          "text": "You are an AI assistant tasked with analyzing legal documents.",  
        },  
        {  
          "type": "text",  
          "text": "Here is the full text of a complex legal agreement",  
          "cache_control": {"type": "ephemeral"},  
        },  
      ],  
    },  
    {  
      "role": "user",  
      "content": "what are the key terms and conditions in this agreement?",  
    },  
  ]  
)  
  

```

### Caching - Tools definitions​
In this example, we demonstrate caching tool definitions.
The cache_control parameter is placed on the final tool
  * LiteLLM SDK
  * LiteLLM Proxy


```
import litellm  
  
response = await litellm.acompletion(  
  model="anthropic/claude-3-5-sonnet-20240620",  
  messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  tools = [  
    {  
      "type": "function",  
      "function": {  
        "name": "get_current_weather",  
        "description": "Get the current weather in a given location",  
        "parameters": {  
          "type": "object",  
          "properties": {  
            "location": {  
              "type": "string",  
              "description": "The city and state, e.g. San Francisco, CA",  
            },  
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
          },  
          "required": ["location"],  
        },  
        "cache_control": {"type": "ephemeral"}  
      },  
    }  
  ]  
)  

```

info
LiteLLM Proxy is OpenAI compatible
This is an example using the OpenAI Python SDK sending a request to LiteLLM Proxy
Assuming you have a model=`anthropic/claude-3-5-sonnet-20240620` on the litellm proxy config.yaml
```
import openai  
client = openai.AsyncOpenAI(  
  api_key="anything",      # litellm proxy api key  
  base_url="http://0.0.0.0:4000" # litellm proxy base url  
)  
  
response = await client.chat.completions.create(  
  model="anthropic/claude-3-5-sonnet-20240620",  
  messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  tools = [  
    {  
      "type": "function",  
      "function": {  
        "name": "get_current_weather",  
        "description": "Get the current weather in a given location",  
        "parameters": {  
          "type": "object",  
          "properties": {  
            "location": {  
              "type": "string",  
              "description": "The city and state, e.g. San Francisco, CA",  
            },  
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
          },  
          "required": ["location"],  
        },  
        "cache_control": {"type": "ephemeral"}  
      },  
    }  
  ]  
)  

```

### Caching - Continuing Multi-Turn Convo​
In this example, we demonstrate how to use Prompt Caching in a multi-turn conversation.
The cache_control parameter is placed on the system message to designate it as part of the static prefix.
The conversation history (previous messages) is included in the messages array. The final turn is marked with cache-control, for continuing in followups. The second-to-last user message is marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.
  * LiteLLM SDK
  * LiteLLM Proxy


```
import litellm  
  
response = await litellm.acompletion(  
  model="anthropic/claude-3-5-sonnet-20240620",  
  messages=[  
    # System Message  
    {  
      "role": "system",  
      "content": [  
        {  
          "type": "text",  
          "text": "Here is the full text of a complex legal agreement"  
          * 400,  
          "cache_control": {"type": "ephemeral"},  
        }  
      ],  
    },  
    # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "text",  
          "text": "What are the key terms and conditions in this agreement?",  
          "cache_control": {"type": "ephemeral"},  
        }  
      ],  
    },  
    {  
      "role": "assistant",  
      "content": "Certainly! the key terms and conditions are the following: the contract is 1 year long for $10/mo",  
    },  
    # The final turn is marked with cache-control, for continuing in followups.  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "text",  
          "text": "What are the key terms and conditions in this agreement?",  
          "cache_control": {"type": "ephemeral"},  
        }  
      ],  
    },  
  ]  
)  

```

info
LiteLLM Proxy is OpenAI compatible
This is an example using the OpenAI Python SDK sending a request to LiteLLM Proxy
Assuming you have a model=`anthropic/claude-3-5-sonnet-20240620` on the litellm proxy config.yaml
```
import openai  
client = openai.AsyncOpenAI(  
  api_key="anything",      # litellm proxy api key  
  base_url="http://0.0.0.0:4000" # litellm proxy base url  
)  
  
response = await client.chat.completions.create(  
  model="anthropic/claude-3-5-sonnet-20240620",  
  messages=[  
    # System Message  
    {  
      "role": "system",  
      "content": [  
        {  
          "type": "text",  
          "text": "Here is the full text of a complex legal agreement"  
          * 400,  
          "cache_control": {"type": "ephemeral"},  
        }  
      ],  
    },  
    # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "text",  
          "text": "What are the key terms and conditions in this agreement?",  
          "cache_control": {"type": "ephemeral"},  
        }  
      ],  
    },  
    {  
      "role": "assistant",  
      "content": "Certainly! the key terms and conditions are the following: the contract is 1 year long for $10/mo",  
    },  
    # The final turn is marked with cache-control, for continuing in followups.  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "text",  
          "text": "What are the key terms and conditions in this agreement?",  
          "cache_control": {"type": "ephemeral"},  
        }  
      ],  
    },  
  ]  
)  

```

## **Function/Tool Calling**​
info
LiteLLM now uses Anthropic's 'tool' param 🎉 (v1.34.29+)
```
from litellm import completion  
  
# set env  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  
response = completion(  
  model="anthropic/claude-3-opus-20240229",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto",  
)  
# Add any assertions, here to check response args  
print(response)  
assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)  
assert isinstance(  
  response.choices[0].message.tool_calls[0].function.arguments, str  
)  
  

```

### Forcing Anthropic Tool Use​
If you want Claude to use a specific tool to answer the user’s question
You can do this by specifying the tool in the `tool_choice` field like so:
```
response = completion(  
  model="anthropic/claude-3-opus-20240229",  
  messages=messages,  
  tools=tools,  
  tool_choice={"type": "tool", "name": "get_weather"},  
)  

```

### Parallel Function Calling​
Here's how to pass the result of a function call back to an anthropic model: 
```
from litellm import completion  
import os   
  
os.environ["ANTHROPIC_API_KEY"] = "sk-ant.."  
  
  
litellm.set_verbose = True  
  
### 1ST FUNCTION CALL ###  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
messages = [  
  {  
    "role": "user",  
    "content": "What's the weather like in Boston today in Fahrenheit?",  
  }  
]  
try:  
  # test without max tokens  
  response = completion(  
    model="anthropic/claude-3-opus-20240229",  
    messages=messages,  
    tools=tools,  
    tool_choice="auto",  
  )  
  # Add any assertions, here to check response args  
  print(response)  
  assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)  
  assert isinstance(  
    response.choices[0].message.tool_calls[0].function.arguments, str  
  )  
  
  messages.append(  
    response.choices[0].message.model_dump()  
  ) # Add assistant tool invokes  
  tool_result = (  
    '{"location": "Boston", "temperature": "72", "unit": "fahrenheit"}'  
  )  
  # Add user submitted tool results in the OpenAI format  
  messages.append(  
    {  
      "tool_call_id": response.choices[0].message.tool_calls[0].id,  
      "role": "tool",  
      "name": response.choices[0].message.tool_calls[0].function.name,  
      "content": tool_result,  
    }  
  )  
  ### 2ND FUNCTION CALL ###  
  # In the second response, Claude should deduce answer from tool results  
  second_response = completion(  
    model="anthropic/claude-3-opus-20240229",  
    messages=messages,  
    tools=tools,  
    tool_choice="auto",  
  )  
  print(second_response)  
except Exception as e:  
  print(f"An error occurred - {str(e)}")  

```

s/o @Shekhar Patnaik for requesting this!
### Computer Tools​
```
from litellm import completion  
  
tools = [  
  {  
    "type": "computer_20241022",  
    "function": {  
      "name": "computer",  
      "parameters": {  
        "display_height_px": 100,  
        "display_width_px": 100,  
        "display_number": 1,  
      },  
    },  
  }  
]  
model = "claude-3-5-sonnet-20241022"  
messages = [{"role": "user", "content": "Save a picture of a cat to my desktop."}]  
  
resp = completion(  
  model=model,  
  messages=messages,  
  tools=tools,  
  # headers={"anthropic-beta": "computer-use-2024-10-22"},  
)  
  
print(resp)  

```

## Usage - Vision​
```
from litellm import completion  
  
# set env  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
def encode_image(image_path):  
  import base64  
  
  with open(image_path, "rb") as image_file:  
    return base64.b64encode(image_file.read()).decode("utf-8")  
  
  
image_path = "../proxy/cached_logo.jpg"  
# Getting the base64 string  
base64_image = encode_image(image_path)  
resp = litellm.completion(  
  model="anthropic/claude-3-opus-20240229",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "Whats in this image?"},  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "data:image/jpeg;base64," + base64_image  
          },  
        },  
      ],  
    }  
  ],  
)  
print(f"\nResponse: {resp}")  

```

## Usage - Thinking / `reasoning_content`​
LiteLLM translates OpenAI's `reasoning_effort` to Anthropic's `thinking` parameter. Code
reasoning_effort| thinking  
---|---  
"low"| "budget_tokens": 1024  
"medium"| "budget_tokens": 2048  
"high"| "budget_tokens": 4096  
  * SDK
  * PROXY


```
from litellm import completion  
  
resp = completion(  
  model="anthropic/claude-3-7-sonnet-20250219",  
  messages=[{"role": "user", "content": "What is the capital of France?"}],  
  reasoning_effort="low",  
)  
  

```

  1. Setup config.yaml


```
- model_name: claude-3-7-sonnet-20250219  
 litellm_params:  
  model: anthropic/claude-3-7-sonnet-20250219  
  api_key: os.environ/ANTHROPIC_API_KEY  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "claude-3-7-sonnet-20250219",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "reasoning_effort": "low"  
 }'  

```

**Expected Response**
```
ModelResponse(  
  id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',  
  created=1740470510,  
  model='claude-3-7-sonnet-20250219',  
  object='chat.completion',  
  system_fingerprint=None,  
  choices=[  
    Choices(  
      finish_reason='stop',  
      index=0,  
      message=Message(  
        content="The capital of France is Paris.",  
        role='assistant',  
        tool_calls=None,  
        function_call=None,  
        provider_specific_fields={  
          'citations': None,  
          'thinking_blocks': [  
            {  
              'type': 'thinking',  
              'thinking': 'The capital of France is Paris. This is a very straightforward factual question.',  
              'signature': 'EuYBCkQYAiJAy6...'  
            }  
          ]  
        }  
      ),  
      thinking_blocks=[  
        {  
          'type': 'thinking',  
          'thinking': 'The capital of France is Paris. This is a very straightforward factual question.',  
          'signature': 'EuYBCkQYAiJAy6AGB...'  
        }  
      ],  
      reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'  
    )  
  ],  
  usage=Usage(  
    completion_tokens=68,  
    prompt_tokens=42,  
    total_tokens=110,  
    completion_tokens_details=None,  
    prompt_tokens_details=PromptTokensDetailsWrapper(  
      audio_tokens=None,  
      cached_tokens=0,  
      text_tokens=None,  
      image_tokens=None  
    ),  
    cache_creation_input_tokens=0,  
    cache_read_input_tokens=0  
  )  
)  

```

### Pass `thinking` to Anthropic models​
You can also pass the `thinking` parameter to Anthropic models.
You can also pass the `thinking` parameter to Anthropic models.
  * SDK
  * PROXY


```
response = litellm.completion(  
 model="anthropic/claude-3-7-sonnet-20250219",  
 messages=[{"role": "user", "content": "What is the capital of France?"}],  
 thinking={"type": "enabled", "budget_tokens": 1024},  
)  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "anthropic/claude-3-7-sonnet-20250219",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "thinking": {"type": "enabled", "budget_tokens": 1024}  
 }'  

```

## **Passing Extra Headers to Anthropic API**​
Pass `extra_headers: dict` to `litellm.completion`
```
from litellm import completion  
messages = [{"role": "user", "content": "What is Anthropic?"}]  
response = completion(  
  model="claude-3-5-sonnet-20240620",   
  messages=messages,   
  extra_headers={"anthropic-beta": "max-tokens-3-5-sonnet-2024-07-15"}  
)  

```

## Usage - "Assistant Pre-fill"​
You can "put words in Claude's mouth" by including an `assistant` role message as the last item in the `messages` array.
> [!IMPORTANT] The returned completion will _not_ include your "pre-fill" text, since it is part of the prompt itself. Make sure to prefix Claude's completion with your pre-fill.
```
import os  
from litellm import completion  
  
# set env - [OPTIONAL] replace with your anthropic key  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
messages = [  
  {"role": "user", "content": "How do you say 'Hello' in German? Return your answer as a JSON object, like this:\n\n{ \"Hello\": \"Hallo\" }"},  
  {"role": "assistant", "content": "{"},  
]  
response = completion(model="claude-2.1", messages=messages)  
print(response)  

```

#### Example prompt sent to Claude​
```
  
Human: How do you say 'Hello' in German? Return your answer as a JSON object, like this:  
  
{ "Hello": "Hallo" }  
  
Assistant: {  

```

## Usage - "System" messages​
If you're using Anthropic's Claude 2.1, `system` role messages are properly formatted for you.
```
import os  
from litellm import completion  
  
# set env - [OPTIONAL] replace with your anthropic key  
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"  
  
messages = [  
  {"role": "system", "content": "You are a snarky assistant."},  
  {"role": "user", "content": "How do I boil water?"},  
]  
response = completion(model="claude-2.1", messages=messages)  

```

#### Example prompt sent to Claude​
```
You are a snarky assistant.  
  
Human: How do I boil water?  
  
Assistant:  

```

## Usage - PDF​
Pass base64 encoded PDF files to Anthropic models using the `image_url` field.
  * SDK
  * PROXY


### **using base64**​
```
from litellm import completion, supports_pdf_input  
import base64  
import requests  
  
# URL of the file  
url = "https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf"  
  
# Download the file  
response = requests.get(url)  
file_data = response.content  
  
encoded_file = base64.b64encode(file_data).decode("utf-8")  
  
## check if model supports pdf input - (2024/11/11) only claude-3-5-haiku-20241022 supports it  
supports_pdf_input("anthropic/claude-3-5-haiku-20241022") # True  
  
response = completion(  
  model="anthropic/claude-3-5-haiku-20241022",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "You are a very professional document summarization specialist. Please summarize the given document."},  
        {  
          "type": "file",  
          "file": {  
            "file_data": f"data:application/pdf;base64,{encoded_file}", # 👈 PDF  
          }  
        },  
      ],  
    }  
  ],  
  max_tokens=300,  
)  
  
print(response.choices[0])  

```

  1. Add model to config 


```
- model_name: claude-3-5-haiku-20241022  
 litellm_params:  
  model: anthropic/claude-3-5-haiku-20241022  
  api_key: os.environ/ANTHROPIC_API_KEY  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "claude-3-5-haiku-20241022",  
  "messages": [  
   {  
    "role": "user",  
    "content": [  
     {  
      "type": "text",  
      "text": "You are a very professional document summarization specialist. Please summarize the given document"  
     },  
     {  
        "type": "file",  
        "file": {  
          "file_data": f"data:application/pdf;base64,{encoded_file}", # 👈 PDF  
        }  
      }  
     }  
    ]  
   }  
  ],  
  "max_tokens": 300  
 }'  
  

```

## [BETA] Citations API​
Pass `citations: {"enabled": true}` to Anthropic, to get citations on your document responses. 
Note: This interface is in BETA. If you have feedback on how citations should be returned, please tell us here
  * SDK
  * PROXY


```
from litellm import completion  
  
resp = completion(  
  model="claude-3-5-sonnet-20241022",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "document",  
          "source": {  
            "type": "text",  
            "media_type": "text/plain",  
            "data": "The grass is green. The sky is blue.",  
          },  
          "title": "My Document",  
          "context": "This is a trustworthy document.",  
          "citations": {"enabled": True},  
        },  
        {  
          "type": "text",  
          "text": "What color is the grass and sky?",  
        },  
      ],  
    }  
  ],  
)  
  
citations = resp.choices[0].message.provider_specific_fields["citations"]  
  
assert citations is not None  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: anthropic-claude  
   litellm_params:  
    model: anthropic/claude-3-5-sonnet-20241022  
    api_key: os.environ/ANTHROPIC_API_KEY  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "anthropic-claude",  
 "messages": [  
  {  
    "role": "user",  
    "content": [  
      {  
        "type": "document",  
        "source": {  
          "type": "text",  
          "media_type": "text/plain",  
          "data": "The grass is green. The sky is blue.",  
        },  
        "title": "My Document",  
        "context": "This is a trustworthy document.",  
        "citations": {"enabled": True},  
      },  
      {  
        "type": "text",  
        "text": "What color is the grass and sky?",  
      },  
    ],  
  }  
 ]  
}'  

```

## Usage - passing 'user_id' to Anthropic​
LiteLLM translates the OpenAI `user` param to Anthropic's `metadata[user_id]` param.
  * SDK
  * PROXY


```
response = completion(  
  model="claude-3-5-sonnet-20240620",  
  messages=messages,  
  user="user_123",  
)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: claude-3-5-sonnet-20240620  
   litellm_params:  
    model: anthropic/claude-3-5-sonnet-20240620  
    api_key: os.environ/ANTHROPIC_API_KEY  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "claude-3-5-sonnet-20240620",  
  "messages": [{"role": "user", "content": "What is Anthropic?"}],  
  "user": "user_123"  
 }'  

```

Previous
[BETA] Google AI Studio (Gemini) Files API
Next
AWS Sagemaker
  * Supported OpenAI Parameters
  * API Keys
  * Usage
  * Usage - Streaming
  * Usage with LiteLLM Proxy
    * 1. Save key in your environment
    * 2. Start the proxy
    * 3. Test it
  * Supported Models
  * **Prompt Caching**
    * Caching - Large Context Caching
    * Caching - Tools definitions
    * Caching - Continuing Multi-Turn Convo
  * **Function/Tool Calling**
    * Forcing Anthropic Tool Use
    * Parallel Function Calling
    * Computer Tools
  * Usage - Vision
  * Usage - Thinking / `reasoning_content`
    * Pass `thinking` to Anthropic models
  * **Passing Extra Headers to Anthropic API**
  * Usage - "Assistant Pre-fill"
  * Usage - "System" messages
  * Usage - PDF
    * **using base64**
  * BETA Citations API
  * Usage - passing 'user_id' to Anthropic


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * AWS Sagemaker


On this page
# AWS Sagemaker
LiteLLM supports All Sagemaker Huggingface Jumpstart Models
tip
**We support ALL Sagemaker models, just set`model=sagemaker/<any-model-on-sagemaker>` as a prefix when sending litellm requests**
### API KEYS​
```
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  

```

### Usage​
```
import os   
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
      model="sagemaker/<your-endpoint-name>",   
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      temperature=0.2,  
      max_tokens=80  
    )  

```

### Usage - Streaming​
Sagemaker currently does not support streaming - LiteLLM fakes streaming by returning chunks of the response string
```
import os   
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
      model="sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b",   
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      temperature=0.2,  
      max_tokens=80,  
      stream=True,  
    )  
for chunk in response:  
  print(chunk)  

```

## **LiteLLM Proxy Usage**​
Here's how to call Sagemaker with the LiteLLM Proxy Server
### 1. Setup config.yaml​
```
model_list:  
 - model_name: jumpstart-model  
  litellm_params:  
   model: sagemaker/jumpstart-dft-hf-textgeneration1-mp-20240815-185614  
   aws_access_key_id: os.environ/CUSTOM_AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/CUSTOM_AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/CUSTOM_AWS_REGION_NAME  

```

All possible auth params: 
```
aws_access_key_id: Optional[str],  
aws_secret_access_key: Optional[str],  
aws_session_token: Optional[str],  
aws_region_name: Optional[str],  
aws_session_name: Optional[str],  
aws_profile_name: Optional[str],  
aws_role_name: Optional[str],  
aws_web_identity_token: Optional[str],  

```

### 2. Start the proxy​
```
litellm --config /path/to/config.yaml  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "jumpstart-model",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(model="jumpstart-model", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "jumpstart-model",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Set temperature, top p, etc.​
  * SDK
  * PROXY


```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
 model="sagemaker/jumpstart-dft-hf-textgeneration1-mp-20240815-185614",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 temperature=0.7,  
 top_p=1  
)  

```

**Set on yaml**
```
model_list:  
 - model_name: jumpstart-model  
  litellm_params:  
   model: sagemaker/jumpstart-dft-hf-textgeneration1-mp-20240815-185614  
   temperature: <your-temp>  
   top_p: <your-top-p>  

```

**Set on request**
```
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="jumpstart-model", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],  
temperature=0.7,  
top_p=1  
)  
  
print(response)  
  

```

## **Allow setting temperature=0** for Sagemaker​
By default when `temperature=0` is sent in requests to LiteLLM, LiteLLM rounds up to `temperature=0.1` since Sagemaker fails most requests when `temperature=0`
If you want to send `temperature=0` for your model here's how to set it up (Since Sagemaker can host any kind of model, some models allow zero temperature)
  * SDK
  * PROXY


```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
 model="sagemaker/jumpstart-dft-hf-textgeneration1-mp-20240815-185614",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 temperature=0,  
 aws_sagemaker_allow_zero_temp=True,  
)  

```

**Set`aws_sagemaker_allow_zero_temp` on yaml**
```
model_list:  
 - model_name: jumpstart-model  
  litellm_params:  
   model: sagemaker/jumpstart-dft-hf-textgeneration1-mp-20240815-185614  
   aws_sagemaker_allow_zero_temp: true  

```

**Set`temperature=0` on request**
```
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="jumpstart-model", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],  
temperature=0,  
)  
  
print(response)  
  

```

## Pass provider-specific params​
If you pass a non-openai param to litellm, we'll assume it's provider-specific and send it as a kwarg in the request body. See more
  * SDK
  * PROXY


```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
 model="sagemaker/jumpstart-dft-hf-textgeneration1-mp-20240815-185614",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 top_k=1 # 👈 PROVIDER-SPECIFIC PARAM  
)  

```

**Set on yaml**
```
model_list:  
 - model_name: jumpstart-model  
  litellm_params:  
   model: sagemaker/jumpstart-dft-hf-textgeneration1-mp-20240815-185614  
   top_k: 1 # 👈 PROVIDER-SPECIFIC PARAM  

```

**Set on request**
```
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="jumpstart-model", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],  
temperature=0.7,  
extra_body={  
  top_k=1 # 👈 PROVIDER-SPECIFIC PARAM  
}  
)  
  
print(response)  
  

```

### Passing Inference Component Name​
If you have multiple models on an endpoint, you'll need to specify the individual model names, do this via `model_id`. 
```
import os   
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
      model="sagemaker/<your-endpoint-name>",   
      model_id="<your-model-name",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      temperature=0.2,  
      max_tokens=80  
    )  

```

### Passing credentials as parameters - Completion()​
Pass AWS credentials as parameters to litellm.completion
```
import os   
from litellm import completion  
  
response = completion(  
      model="sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      aws_access_key_id="",  
      aws_secret_access_key="",  
      aws_region_name="",  
)  

```

### Applying Prompt Templates​
To apply the correct prompt template for your sagemaker deployment, pass in it's hf model name as well. 
```
import os   
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
      model="sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b",   
      messages=messages,  
      temperature=0.2,  
      max_tokens=80,  
      hf_model_name="meta-llama/Llama-2-7b",  
    )  

```

You can also pass in your own custom prompt template
## Sagemaker Messages API​
Use route `sagemaker_chat/*` to route to Sagemaker Messages API
```
model: sagemaker_chat/<your-endpoint-name>  

```

  * SDK
  * PROXY


```
import os  
import litellm  
from litellm import completion  
  
litellm.set_verbose = True # 👈 SEE RAW REQUEST  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
      model="sagemaker_chat/<your-endpoint-name>",   
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      temperature=0.2,  
      max_tokens=80  
    )  

```

#### 1. Setup config.yaml​
```
model_list:  
 - model_name: "sagemaker-model"  
  litellm_params:  
   model: "sagemaker_chat/jumpstart-dft-hf-textgeneration1-mp-20240815-185614"  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  

```

#### 2. Start the proxy​
```
litellm --config /path/to/config.yaml  

```

#### 3. Test it​
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "sagemaker-model",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

**👉 See OpenAI SDK/Langchain/Llamaindex/etc. examples**
## Completion Models​
tip
**We support ALL Sagemaker models, just set`model=sagemaker/<any-model-on-sagemaker>` as a prefix when sending litellm requests**
Here's an example of using a sagemaker model with LiteLLM 
Model Name| Function Call  
---|---  
Your Custom Huggingface Model| `completion(model='sagemaker/<your-deployment-name>', messages=messages)`  
Meta Llama 2 7B| `completion(model='sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b', messages=messages)`  
Meta Llama 2 7B (Chat/Fine-tuned)| `completion(model='sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b-f', messages=messages)`  
Meta Llama 2 13B| `completion(model='sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b', messages=messages)`  
Meta Llama 2 13B (Chat/Fine-tuned)| `completion(model='sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b-f', messages=messages)`  
Meta Llama 2 70B| `completion(model='sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b', messages=messages)`  
Meta Llama 2 70B (Chat/Fine-tuned)| `completion(model='sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b-b-f', messages=messages)`  
## Embedding Models​
LiteLLM supports all Sagemaker Jumpstart Huggingface Embedding models. Here's how to call it: 
```
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = litellm.embedding(model="sagemaker/<your-deployment-name>", input=["good morning from litellm", "this is another item"])  
print(f"response: {response}")  

```

Previous
Anthropic
Next
AWS Bedrock
  * API KEYS
  * Usage
  * Usage - Streaming
  * **LiteLLM Proxy Usage**
    * 1. Setup config.yaml
    * 2. Start the proxy
    * 3. Test it
  * Set temperature, top p, etc.
  * **Allow setting temperature=0** for Sagemaker
  * Pass provider-specific params
    * Passing Inference Component Name
    * Passing credentials as parameters - Completion()
    * Applying Prompt Templates
  * Sagemaker Messages API
  * Completion Models
  * Embedding Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Azure OpenAI


On this page
# Azure OpenAI
## Overview​
Property| Details  
---|---  
Description| Azure OpenAI Service provides REST API access to OpenAI's powerful language models including o1, o1-mini, GPT-4o, GPT-4o mini, GPT-4 Turbo with Vision, GPT-4, GPT-3.5-Turbo, and Embeddings model series  
Provider Route on LiteLLM| `azure/`, `azure/o_series/`  
Supported Operations| `/chat/completions`, `/completions`, `/embeddings`, `/audio/speech`, `/audio/transcriptions`, `/fine_tuning`, `/batches`, `/files`, `/images`  
Link to Provider Doc| Azure OpenAI ↗  
## API Keys, Params​
api_key, api_base, api_version etc can be passed directly to `litellm.completion` - see here or set as `litellm.api_key` params see here
```
import os  
os.environ["AZURE_API_KEY"] = "" # "my-azure-api-key"  
os.environ["AZURE_API_BASE"] = "" # "https://example-endpoint.openai.azure.com"  
os.environ["AZURE_API_VERSION"] = "" # "2023-05-15"  
  
# optional  
os.environ["AZURE_AD_TOKEN"] = ""  
os.environ["AZURE_API_TYPE"] = ""  

```

## **Usage - LiteLLM Python SDK**​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
### Completion - using .env variables​
```
from litellm import completion  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
response = completion(  
  model = "azure/<your_deployment_name>",   
  messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

### Completion - using api_key, api_base, api_version​
```
import litellm  
  
# azure call  
response = litellm.completion(  
  model = "azure/<your deployment name>",       # model = azure/<your deployment name>   
  api_base = "",                   # azure api base  
  api_version = "",                  # azure api version  
  api_key = "",                    # azure api key  
  messages = [{"role": "user", "content": "good morning"}],  
)  

```

### Completion - using azure_ad_token, api_base, api_version​
```
import litellm  
  
# azure call  
response = litellm.completion(  
  model = "azure/<your deployment name>",       # model = azure/<your deployment name>   
  api_base = "",                   # azure api base  
  api_version = "",                  # azure api version  
  azure_ad_token="",                 # azure_ad_token   
  messages = [{"role": "user", "content": "good morning"}],  
)  

```

## **Usage - LiteLLM Proxy Server**​
Here's how to call Azure OpenAI models with the LiteLLM Proxy Server
### 1. Save key in your environment​
```
export AZURE_API_KEY=""  

```

### 2. Start the proxy​
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
   api_version: "2023-05-15"  
   api_key: os.environ/AZURE_API_KEY # The `os.environ/` prefix tells litellm to read this from the env.  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "gpt-3.5-turbo",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Azure OpenAI Chat Completion Models​
tip
**We support ALL Azure models, just set`model=azure/<your deployment name>` as a prefix when sending litellm requests**
Model Name| Function Call  
---|---  
o1-mini| `response = completion(model="azure/<your deployment name>", messages=messages)`  
o1-preview| `response = completion(model="azure/<your deployment name>", messages=messages)`  
gpt-4o-mini| `completion('azure/<your deployment name>', messages)`  
gpt-4o| `completion('azure/<your deployment name>', messages)`  
gpt-4| `completion('azure/<your deployment name>', messages)`  
gpt-4-0314| `completion('azure/<your deployment name>', messages)`  
gpt-4-0613| `completion('azure/<your deployment name>', messages)`  
gpt-4-32k| `completion('azure/<your deployment name>', messages)`  
gpt-4-32k-0314| `completion('azure/<your deployment name>', messages)`  
gpt-4-32k-0613| `completion('azure/<your deployment name>', messages)`  
gpt-4-1106-preview| `completion('azure/<your deployment name>', messages)`  
gpt-4-0125-preview| `completion('azure/<your deployment name>', messages)`  
gpt-3.5-turbo| `completion('azure/<your deployment name>', messages)`  
gpt-3.5-turbo-0301| `completion('azure/<your deployment name>', messages)`  
gpt-3.5-turbo-0613| `completion('azure/<your deployment name>', messages)`  
gpt-3.5-turbo-16k| `completion('azure/<your deployment name>', messages)`  
gpt-3.5-turbo-16k-0613| `completion('azure/<your deployment name>', messages)`  
## Azure OpenAI Vision Models​
Model Name| Function Call  
---|---  
gpt-4-vision| `completion(model="azure/<your deployment name>", messages=messages)`  
gpt-4o| `completion('azure/<your deployment name>', messages)`  
#### Usage​
```
import os   
from litellm import completion  
  
os.environ["AZURE_API_KEY"] = "your-api-key"  
  
# azure call  
response = completion(  
  model = "azure/<your deployment name>",   
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

#### Usage - with Azure Vision enhancements​
Note: **Azure requires the`base_url` to be set with `/extensions`**
Example 
```
base_url=https://gpt-4-vision-resource.openai.azure.com/openai/deployments/gpt-4-vision/extensions  
# base_url="{azure_endpoint}/openai/deployments/{azure_deployment}/extensions"  

```

**Usage**
```
import os   
from litellm import completion  
  
os.environ["AZURE_API_KEY"] = "your-api-key"  
  
# azure call  
response = completion(  
      model="azure/gpt-4-vision",  
      timeout=5,  
      messages=[  
        {  
          "role": "user",  
          "content": [  
            {"type": "text", "text": "Whats in this image?"},  
            {  
              "type": "image_url",  
              "image_url": {  
                "url": "https://avatars.githubusercontent.com/u/29436595?v=4"  
              },  
            },  
          ],  
        }  
      ],  
      base_url="https://gpt-4-vision-resource.openai.azure.com/openai/deployments/gpt-4-vision/extensions",  
      api_key=os.getenv("AZURE_VISION_API_KEY"),  
      enhancements={"ocr": {"enabled": True}, "grounding": {"enabled": True}},  
      dataSources=[  
        {  
          "type": "AzureComputerVision",  
          "parameters": {  
            "endpoint": "https://gpt-4-vision-enhancement.cognitiveservices.azure.com/",  
            "key": os.environ["AZURE_VISION_ENHANCE_KEY"],  
          },  
        }  
      ],  
)  

```

## O-Series Models​
Azure OpenAI O-Series models are supported on LiteLLM. 
LiteLLM routes any deployment name with `o1` or `o3` in the model name, to the O-Series transformation logic.
To set this explicitly, set `model` to `azure/o_series/<your-deployment-name>`.
**Automatic Routing**
  * SDK
  * PROXY


```
import litellm  
  
litellm.completion(model="azure/my-o3-deployment", messages=[{"role": "user", "content": "Hello, world!"}]) # 👈 Note: 'o3' in the deployment name  

```

```
model_list:  
 - model_name: o3-mini  
  litellm_params:  
   model: azure/o3-model  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  

```

**Explicit Routing**
  * SDK
  * PROXY


```
import litellm  
  
litellm.completion(model="azure/o_series/my-random-deployment-name", messages=[{"role": "user", "content": "Hello, world!"}]) # 👈 Note: 'o_series/' in the deployment name  

```

```
model_list:  
 - model_name: o3-mini  
  litellm_params:  
   model: azure/o_series/my-random-deployment-name  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  

```

## Azure Audio Model​
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
response = completion(  
  model="azure/azure-openai-4o-audio",  
  messages=[  
   {  
    "role": "user",  
    "content": "I want to try out speech to speech"  
   }  
  ],  
  modalities=["text","audio"],  
  audio={"voice": "alloy", "format": "wav"}  
)  
  
print(response)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: azure-openai-4o-audio  
  litellm_params:  
   model: azure/azure-openai-4o-audio  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: os.environ/AZURE_API_VERSION  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it!


```
curl http://localhost:4000/v1/chat/completions \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "azure-openai-4o-audio",  
  "messages": [{"role": "user", "content": "I want to try out speech to speech"}],  
  "modalities": ["text","audio"],  
  "audio": {"voice": "alloy", "format": "wav"}  
 }'  

```

## Azure Instruct Models​
Use `model="azure_text/<your-deployment>"`
Model Name| Function Call  
---|---  
gpt-3.5-turbo-instruct| `response = completion(model="azure_text/<your deployment name>", messages=messages)`  
gpt-3.5-turbo-instruct-0914| `response = completion(model="azure_text/<your deployment name>", messages=messages)`  
```
import litellm  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
response = litellm.completion(  
  model="azure_text/<your-deployment-name",  
  messages=[{"role": "user", "content": "What is the weather like in Boston?"}]  
)  
  
print(response)  

```

## Azure Text to Speech (tts)​
**LiteLLM PROXY**
```
 - model_name: azure/tts-1  
  litellm_params:  
   model: azure/tts-1  
   api_base: "os.environ/AZURE_API_BASE_TTS"  
   api_key: "os.environ/AZURE_API_KEY_TTS"  
   api_version: "os.environ/AZURE_API_VERSION"   

```

**LiteLLM SDK**
```
from litellm import completion  
  
## set ENV variables  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
os.environ["AZURE_API_VERSION"] = ""  
  
# azure call  
speech_file_path = Path(__file__).parent / "speech.mp3"  
response = speech(  
    model="azure/<your-deployment-name",  
    voice="alloy",  
    input="the quick brown fox jumped over the lazy dogs",  
  )  
response.stream_to_file(speech_file_path)  

```

## **Authentication**​
### Entra ID - use `azure_ad_token`​
This is a walkthrough on how to use Azure Active Directory Tokens - Microsoft Entra ID to make `litellm.completion()` calls 
Step 1 - Download Azure CLI Installation instructions: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli
```
brew update && brew install azure-cli  

```

Step 2 - Sign in using `az`
```
az login --output table  

```

Step 3 - Generate azure ad token
```
az account get-access-token --resource https://cognitiveservices.azure.com  

```

In this step you should see an `accessToken` generated
```
{  
 "accessToken": "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IjlHbW55RlBraGMzaE91UjIybXZTdmduTG83WSIsImtpZCI6IjlHbW55RlBraGMzaE91UjIybXZTdmduTG83WSJ9",  
 "expiresOn": "2023-11-14 15:50:46.000000",  
 "expires_on": 1700005846,  
 "subscription": "db38de1f-4bb3..",  
 "tenant": "bdfd79b3-8401-47..",  
 "tokenType": "Bearer"  
}  

```

Step 4 - Make litellm.completion call with Azure AD token
Set `azure_ad_token` = `accessToken` from step 3 or set `os.environ['AZURE_AD_TOKEN']`
  * SDK
  * PROXY config.yaml


```
response = litellm.completion(  
  model = "azure/<your deployment name>",       # model = azure/<your deployment name>   
  api_base = "",                   # azure api base  
  api_version = "",                  # azure api version  
  azure_ad_token="",                 # your accessToken from step 3   
  messages = [{"role": "user", "content": "good morning"}],  
)  
  

```

```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
   api_version: "2023-05-15"  
   azure_ad_token: os.environ/AZURE_AD_TOKEN  

```

### Entra ID - use tenant_id, client_id, client_secret​
Here is an example of setting up `tenant_id`, `client_id`, `client_secret` in your litellm proxy `config.yaml`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
   api_version: "2023-05-15"  
   tenant_id: os.environ/AZURE_TENANT_ID  
   client_id: os.environ/AZURE_CLIENT_ID  
   client_secret: os.environ/AZURE_CLIENT_SECRET  

```

Test it 
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

Example video of using `tenant_id`, `client_id`, `client_secret` with LiteLLM Proxy Server
### Entra ID - use client_id, username, password​
Here is an example of setting up `client_id`, `azure_username`, `azure_password` in your litellm proxy `config.yaml`
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
   api_version: "2023-05-15"  
   client_id: os.environ/AZURE_CLIENT_ID  
   azure_username: os.environ/AZURE_USERNAME  
   azure_password: os.environ/AZURE_PASSWORD  

```

Test it 
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

### Azure AD Token Refresh - `DefaultAzureCredential`​
Use this if you want to use Azure `DefaultAzureCredential` for Authentication on your requests
  * SDK
  * PROXY config.yaml


```
from litellm import completion  
from azure.identity import DefaultAzureCredential, get_bearer_token_provider  
  
token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")  
  
  
response = completion(  
  model = "azure/<your deployment name>",       # model = azure/<your deployment name>   
  api_base = "",                   # azure api base  
  api_version = "",                  # azure api version  
  azure_ad_token_provider=token_provider  
  messages = [{"role": "user", "content": "good morning"}],  
)  

```

  1. Add relevant env vars


```
export AZURE_TENANT_ID=""  
export AZURE_CLIENT_ID=""  
export AZURE_CLIENT_SECRET=""  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/your-deployment-name  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
  
litellm_settings:  
  enable_azure_ad_token_refresh: true # 👈 KEY CHANGE  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

## **Azure Batches API**​
Property| Details  
---|---  
Description| Azure OpenAI Batches API  
`custom_llm_provider` on LiteLLM| `azure/`  
Supported Operations| `/v1/batches`, `/v1/files`  
Azure OpenAI Batches API| Azure OpenAI Batches API ↗  
Cost Tracking, Logging Support| ✅ LiteLLM will log, track cost for Batch API Requests  
### Quick Start​
Just add the azure env vars to your environment. 
```
export AZURE_API_KEY=""  
export AZURE_API_BASE=""  

```

  * LiteLLM PROXY Server
  * LiteLLM SDK


**1. Upload a File**
  * OpenAI Python SDK
  * Curl


```
from openai import OpenAI  
  
# Initialize the client  
client = OpenAI(  
  base_url="http://localhost:4000",  
  api_key="your-api-key"  
)  
  
batch_input_file = client.files.create(  
  file=open("mydata.jsonl", "rb"),  
  purpose="batch",  
  extra_body={"custom_llm_provider": "azure"}  
)  
file_id = batch_input_file.id  

```

```
curl http://localhost:4000/v1/files \  
  -H "Authorization: Bearer sk-1234" \  
  -F purpose="batch" \  
  -F file="@mydata.jsonl"  

```

**Example File Format**
```
{"custom_id": "task-0", "method": "POST", "url": "/chat/completions", "body": {"model": "REPLACE-WITH-MODEL-DEPLOYMENT-NAME", "messages": [{"role": "system", "content": "You are an AI assistant that helps people find information."}, {"role": "user", "content": "When was Microsoft founded?"}]}}  
{"custom_id": "task-1", "method": "POST", "url": "/chat/completions", "body": {"model": "REPLACE-WITH-MODEL-DEPLOYMENT-NAME", "messages": [{"role": "system", "content": "You are an AI assistant that helps people find information."}, {"role": "user", "content": "When was the first XBOX released?"}]}}  
{"custom_id": "task-2", "method": "POST", "url": "/chat/completions", "body": {"model": "REPLACE-WITH-MODEL-DEPLOYMENT-NAME", "messages": [{"role": "system", "content": "You are an AI assistant that helps people find information."}, {"role": "user", "content": "What is Altair Basic?"}]}}  

```

**2. Create a Batch Request**
  * OpenAI Python SDK
  * Curl


```
batch = client.batches.create( # re use client from above  
  input_file_id=file_id,  
  endpoint="/v1/chat/completions",  
  completion_window="24h",  
  metadata={"description": "My batch job"},  
  extra_body={"custom_llm_provider": "azure"}  
)  

```

```
curl http://localhost:4000/v1/batches \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "input_file_id": "file-abc123",  
  "endpoint": "/v1/chat/completions",  
  "completion_window": "24h"  
 }'  

```

**3. Retrieve a Batch**
  * OpenAI Python SDK
  * Curl


```
retrieved_batch = client.batches.retrieve(  
  batch.id,  
  extra_body={"custom_llm_provider": "azure"}  
)  

```

```
curl http://localhost:4000/v1/batches/batch_abc123 \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json" \  

```

**4. Cancel a Batch**
  * OpenAI Python SDK
  * Curl


```
cancelled_batch = client.batches.cancel(  
  batch.id,  
  extra_body={"custom_llm_provider": "azure"}  
)  

```

```
curl http://localhost:4000/v1/batches/batch_abc123/cancel \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json" \  
 -X POST  

```

**5. List Batches**
  * OpenAI Python SDK
  * Curl


```
client.batches.list(extra_body={"custom_llm_provider": "azure"})  

```

```
curl http://localhost:4000/v1/batches?limit=2 \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json"  

```

**1. Create File for Batch Completion**
```
from litellm  
import os   
  
os.environ["AZURE_API_KEY"] = ""  
os.environ["AZURE_API_BASE"] = ""  
  
file_name = "azure_batch_completions.jsonl"  
_current_dir = os.path.dirname(os.path.abspath(__file__))  
file_path = os.path.join(_current_dir, file_name)  
file_obj = await litellm.acreate_file(  
  file=open(file_path, "rb"),  
  purpose="batch",  
  custom_llm_provider="azure",  
)  
print("Response from creating file=", file_obj)  

```

**2. Create Batch Request**
```
create_batch_response = await litellm.acreate_batch(  
  completion_window="24h",  
  endpoint="/v1/chat/completions",  
  input_file_id=batch_input_file_id,  
  custom_llm_provider="azure",  
  metadata={"key1": "value1", "key2": "value2"},  
)  
  
print("response from litellm.create_batch=", create_batch_response)  

```

**3. Retrieve Batch and File Content**
```
retrieved_batch = await litellm.aretrieve_batch(  
  batch_id=create_batch_response.id,   
  custom_llm_provider="azure"  
)  
print("retrieved batch=", retrieved_batch)  
  
# Get file content  
file_content = await litellm.afile_content(  
  file_id=batch_input_file_id,   
  custom_llm_provider="azure"  
)  
print("file content = ", file_content)  

```

**4. List Batches**
```
list_batches_response = litellm.list_batches(  
  custom_llm_provider="azure",   
  limit=2  
)  
print("list_batches_response=", list_batches_response)  

```

### Health Check Azure Batch models​
### [BETA] Loadbalance Multiple Azure Deployments​
In your config.yaml, set `enable_loadbalancing_on_batch_endpoints: true`
```
model_list:  
 - model_name: "batch-gpt-4o-mini"  
  litellm_params:  
   model: "azure/gpt-4o-mini"  
   api_key: os.environ/AZURE_API_KEY  
   api_base: os.environ/AZURE_API_BASE  
  model_info:  
   mode: batch  
  
litellm_settings:  
 enable_loadbalancing_on_batch_endpoints: true # 👈 KEY CHANGE  

```

Note: This works on `{PROXY_BASE_URL}/v1/files` and `{PROXY_BASE_URL}/v1/batches`. Note: Response is in the OpenAI-format. 
  1. Upload a file 


Just set `model: batch-gpt-4o-mini` in your .jsonl.
```
curl http://localhost:4000/v1/files \  
  -H "Authorization: Bearer sk-1234" \  
  -F purpose="batch" \  
  -F file="@mydata.jsonl"  

```

**Example File**
Note: `model` should be your azure deployment name.
```
{"custom_id": "task-0", "method": "POST", "url": "/chat/completions", "body": {"model": "batch-gpt-4o-mini", "messages": [{"role": "system", "content": "You are an AI assistant that helps people find information."}, {"role": "user", "content": "When was Microsoft founded?"}]}}  
{"custom_id": "task-1", "method": "POST", "url": "/chat/completions", "body": {"model": "batch-gpt-4o-mini", "messages": [{"role": "system", "content": "You are an AI assistant that helps people find information."}, {"role": "user", "content": "When was the first XBOX released?"}]}}  
{"custom_id": "task-2", "method": "POST", "url": "/chat/completions", "body": {"model": "batch-gpt-4o-mini", "messages": [{"role": "system", "content": "You are an AI assistant that helps people find information."}, {"role": "user", "content": "What is Altair Basic?"}]}}  

```

Expected Response (OpenAI-compatible)
```
{"id":"file-f0be81f654454113a922da60acb0eea6",...}  

```

  1. Create a batch 


```
curl http://0.0.0.0:4000/v1/batches \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "input_file_id": "file-f0be81f654454113a922da60acb0eea6",  
  "endpoint": "/v1/chat/completions",  
  "completion_window": "24h",  
  "model: "batch-gpt-4o-mini"  
 }'  

```

Expected Response: 
```
{"id":"batch_94e43f0a-d805-477d-adf9-bbb9c50910ed",...}  

```

  1. Retrieve a batch 


```
curl http://0.0.0.0:4000/v1/batches/batch_94e43f0a-d805-477d-adf9-bbb9c50910ed \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json" \  

```

Expected Response: 
```
{"id":"batch_94e43f0a-d805-477d-adf9-bbb9c50910ed",...}  

```

  1. List batch


```
curl http://0.0.0.0:4000/v1/batches?limit=2 \  
 -H "Authorization: Bearer $LITELLM_API_KEY" \  
 -H "Content-Type: application/json"  

```

Expected Response:
```
{"data":[{"id":"batch_R3V...}  

```

## **Azure Responses API**​
Property| Details  
---|---  
Description| Azure OpenAI Responses API  
`custom_llm_provider` on LiteLLM| `azure/`  
Supported Operations| `/v1/responses`  
Azure OpenAI Responses API| Azure OpenAI Responses API ↗  
Cost Tracking, Logging Support| ✅ LiteLLM will log, track cost for Responses API Requests  
Supported OpenAI Params| ✅ All OpenAI params are supported, See here  
## Usage​
## Create a model response​
  * LiteLLM SDK
  * OpenAI SDK with LiteLLM Proxy


#### Non-streaming​
Azure Responses API
```
import litellm  
  
# Non-streaming response  
response = litellm.responses(  
  model="azure/o1-pro",  
  input="Tell me a three sentence bedtime story about a unicorn.",  
  max_output_tokens=100,  
  api_key=os.getenv("AZURE_RESPONSES_OPENAI_API_KEY"),  
  api_base="https://litellm8397336933.openai.azure.com/",  
  api_version="2023-03-15-preview",  
)  
  
print(response)  

```

#### Streaming​
Azure Responses API
```
import litellm  
  
# Streaming response  
response = litellm.responses(  
  model="azure/o1-pro",  
  input="Tell me a three sentence bedtime story about a unicorn.",  
  stream=True,  
  api_key=os.getenv("AZURE_RESPONSES_OPENAI_API_KEY"),  
  api_base="https://litellm8397336933.openai.azure.com/",  
  api_version="2023-03-15-preview",  
)  
  
for event in response:  
  print(event)  

```

First, add this to your litellm proxy config.yaml:
Azure Responses API
```
model_list:  
 - model_name: o1-pro  
  litellm_params:  
   model: azure/o1-pro  
   api_key: os.environ/AZURE_RESPONSES_OPENAI_API_KEY  
   api_base: https://litellm8397336933.openai.azure.com/  
   api_version: 2023-03-15-preview  

```

Start your LiteLLM proxy:
```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

Then use the OpenAI SDK pointed to your proxy:
#### Non-streaming​
```
from openai import OpenAI  
  
# Initialize client with your proxy URL  
client = OpenAI(  
  base_url="http://localhost:4000", # Your proxy URL  
  api_key="your-api-key"       # Your proxy API key  
)  
  
# Non-streaming response  
response = client.responses.create(  
  model="o1-pro",  
  input="Tell me a three sentence bedtime story about a unicorn."  
)  
  
print(response)  

```

#### Streaming​
```
from openai import OpenAI  
  
# Initialize client with your proxy URL  
client = OpenAI(  
  base_url="http://localhost:4000", # Your proxy URL  
  api_key="your-api-key"       # Your proxy API key  
)  
  
# Streaming response  
response = client.responses.create(  
  model="o1-pro",  
  input="Tell me a three sentence bedtime story about a unicorn.",  
  stream=True  
)  
  
for event in response:  
  print(event)  

```

## Advanced​
### Azure API Load-Balancing​
Use this if you're trying to load-balance across multiple Azure/OpenAI deployments. 
`Router` prevents failed requests, by picking the deployment which is below rate-limit and has the least amount of tokens used. 
In production, Router connects to a Redis Cache to track usage across multiple deployments.
#### Quick Start​
```
pip install litellm  

```

```
from litellm import Router  
  
model_list = [{ # list of model deployments   
  "model_name": "gpt-3.5-turbo", # openai model name   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-v-2",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
  },  
  "tpm": 240000,  
  "rpm": 1800  
}, {  
  "model_name": "gpt-3.5-turbo", # openai model name   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "azure/chatgpt-functioncalling",   
    "api_key": os.getenv("AZURE_API_KEY"),  
    "api_version": os.getenv("AZURE_API_VERSION"),  
    "api_base": os.getenv("AZURE_API_BASE")  
  },  
  "tpm": 240000,  
  "rpm": 1800  
}, {  
  "model_name": "gpt-3.5-turbo", # openai model name   
  "litellm_params": { # params for litellm completion/embedding call   
    "model": "gpt-3.5-turbo",   
    "api_key": os.getenv("OPENAI_API_KEY"),  
  },  
  "tpm": 1000000,  
  "rpm": 9000  
}]  
  
router = Router(model_list=model_list)  
  
# openai.chat.completions.create replacement  
response = router.completion(model="gpt-3.5-turbo",   
        messages=[{"role": "user", "content": "Hey, how's it going?"}]  
  
print(response)  

```

#### Redis Queue​
```
router = Router(model_list=model_list,   
        redis_host=os.getenv("REDIS_HOST"),   
        redis_password=os.getenv("REDIS_PASSWORD"),   
        redis_port=os.getenv("REDIS_PORT"))  
  
print(response)  

```

### Tool Calling / Function Calling​
See a detailed walthrough of parallel function calling with litellm here
  * SDK
  * PROXY


```
# set Azure env variables  
import os  
import litellm  
import json  
  
os.environ['AZURE_API_KEY'] = "" # litellm reads AZURE_API_KEY from .env and sends the request  
os.environ['AZURE_API_BASE'] = "https://openai-gpt-4-test-v-1.openai.azure.com/"  
os.environ['AZURE_API_VERSION'] = "2023-07-01-preview"  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
  
response = litellm.completion(  
  model="azure/chatgpt-functioncalling", # model = azure/<your-azure-deployment-name>  
  messages=[{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}],  
  tools=tools,  
  tool_choice="auto", # auto is default, but we'll be explicit  
)  
print("\nLLM Response1:\n", response)  
response_message = response.choices[0].message  
tool_calls = response.choices[0].message.tool_calls  
print("\nTool Choice:\n", tool_calls)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: azure-gpt-3.5  
  litellm_params:  
   model: azure/chatgpt-functioncalling  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: "2023-07-01-preview"  

```

  1. Start proxy


```
litellm --config config.yaml  

```

  1. Test it


```
curl -L -X POST 'http://localhost:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "azure-gpt-3.5",  
  "messages": [  
    {  
      "role": "user",  
      "content": "Hey, how'\''s it going? Thinking long and hard before replying - what is the meaning of the world and life itself"  
    }  
  ]  
}'  

```

### Spend Tracking for Azure OpenAI Models (PROXY)
Set base model for cost tracking azure image-gen call
#### Image Generation​
```
model_list:   
 - model_name: dall-e-3  
  litellm_params:  
    model: azure/dall-e-3-test  
    api_version: 2023-06-01-preview  
    api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
    api_key: os.environ/AZURE_API_KEY  
    base_model: dall-e-3 # 👈 set dall-e-3 as base model  
  model_info:  
    mode: image_generation  

```

#### Chat Completions / Embeddings​
**Problem** : Azure returns `gpt-4` in the response when `azure/gpt-4-1106-preview` is used. This leads to inaccurate cost tracking
**Solution** ✅ : Set `base_model` on your config so litellm uses the correct model for calculating azure cost
Get the base model name from here
Example config with `base_model`
```
model_list:  
 - model_name: azure-gpt-3.5  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: "2023-07-01-preview"  
  model_info:  
   base_model: azure/gpt-4-1106-preview  

```

Previous
OpenAI-Compatible Endpoints
Next
Azure AI Studio
  * Overview
  * API Keys, Params
  * **Usage - LiteLLM Python SDK**
    * Completion - using .env variables
    * Completion - using api_key, api_base, api_version
    * Completion - using azure_ad_token, api_base, api_version
  * **Usage - LiteLLM Proxy Server**
    * 1. Save key in your environment
    * 2. Start the proxy
    * 3. Test it
  * Azure OpenAI Chat Completion Models
  * Azure OpenAI Vision Models
  * O-Series Models
  * Azure Audio Model
  * Azure Instruct Models
  * Azure Text to Speech (tts)
  * **Authentication**
    * Entra ID - use `azure_ad_token`
    * Entra ID - use tenant_id, client_id, client_secret
    * Entra ID - use client_id, username, password
    * Azure AD Token Refresh - `DefaultAzureCredential`
  * **Azure Batches API**
    * Quick Start
    * Health Check Azure Batch models
    * BETA Loadbalance Multiple Azure Deployments
  * **Azure Responses API**
  * Usage
  * Create a model response
  * Advanced
    * Azure API Load-Balancing
    * Tool Calling / Function Calling


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Anyscale


On this page
# Anyscale
https://app.endpoints.anyscale.com/
## API Key​
```
# env variable  
os.environ['ANYSCALE_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['ANYSCALE_API_KEY'] = ""  
response = completion(  
  model="anyscale/mistralai/Mistral-7B-Instruct-v0.1",   
  messages=messages  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['ANYSCALE_API_KEY'] = ""  
response = completion(  
  model="anyscale/mistralai/Mistral-7B-Instruct-v0.1",   
  messages=messages,  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Supported Models​
All models listed here https://app.endpoints.anyscale.com/ are supported. We actively maintain the list of models, pricing, token window, etc. here.
Model Name| Function Call  
---|---  
llama2-7b-chat| `completion(model="anyscale/meta-llama/Llama-2-7b-chat-hf", messages)`  
llama-2-13b-chat| `completion(model="anyscale/meta-llama/Llama-2-13b-chat-hf", messages)`  
llama-2-70b-chat| `completion(model="anyscale/meta-llama/Llama-2-70b-chat-hf", messages)`  
mistral-7b-instruct| `completion(model="anyscale/mistralai/Mistral-7B-Instruct-v0.1", messages)`  
CodeLlama-34b-Instruct| `completion(model="anyscale/codellama/CodeLlama-34b-Instruct-hf", messages)`  
Previous
Cohere
Next
Hugging Face
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Azure AI Studio


On this page
# Azure AI Studio
LiteLLM supports all models on Azure AI Studio
## Usage​
  * SDK
  * PROXY


### ENV VAR​
```
import os   
os.environ["AZURE_AI_API_KEY"] = ""  
os.environ["AZURE_AI_API_BASE"] = ""  

```

### Example Call​
```
from litellm import completion  
import os  
## set ENV variables  
os.environ["AZURE_AI_API_KEY"] = "azure ai key"  
os.environ["AZURE_AI_API_BASE"] = "azure ai base url" # e.g.: https://Mistral-large-dfgfj-serverless.eastus2.inference.ai.azure.com/  
  
# predibase llama-3 call  
response = completion(  
  model="azure_ai/command-r-plus",   
  messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

  1. Add models to your config.yaml
```
model_list:  
 - model_name: command-r-plus  
  litellm_params:  
   model: azure_ai/command-r-plus  
   api_key: os.environ/AZURE_AI_API_KEY  
   api_base: os.environ/AZURE_AI_API_BASE  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml --debug  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="command-r-plus",  
  messages = [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
 ]  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "command-r-plus",  
  "messages": [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
   ],  
}'  

```



## Passing additional params - max_tokens, temperature​
See all litellm.completion supported params here
```
# !pip install litellm  
from litellm import completion  
import os  
## set ENV variables  
os.environ["AZURE_AI_API_KEY"] = "azure ai api key"  
os.environ["AZURE_AI_API_BASE"] = "azure ai api base"  
  
# command r plus call  
response = completion(  
  model="azure_ai/command-r-plus",   
  messages = [{ "content": "Hello, how are you?","role": "user"}],  
  max_tokens=20,  
  temperature=0.5  
)  

```

**proxy**
```
 model_list:  
  - model_name: command-r-plus  
   litellm_params:  
    model: azure_ai/command-r-plus  
    api_key: os.environ/AZURE_AI_API_KEY  
    api_base: os.environ/AZURE_AI_API_BASE  
    max_tokens: 20  
    temperature: 0.5  

```

  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="mistral",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "mistral",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



## Function Calling​
  * SDK
  * PROXY


```
from litellm import completion  
  
# set env  
os.environ["AZURE_AI_API_KEY"] = "your-api-key"  
os.environ["AZURE_AI_API_BASE"] = "your-api-base"  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  
response = completion(  
  model="azure_ai/mistral-large-latest",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto",  
)  
# Add any assertions, here to check response args  
print(response)  
assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)  
assert isinstance(  
  response.choices[0].message.tool_calls[0].function.arguments, str  
)  
  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer $YOUR_API_KEY" \  
-d '{  
 "model": "mistral",  
 "messages": [  
  {  
   "role": "user",  
   "content": "What'\''s the weather like in Boston today?"  
  }  
 ],  
 "tools": [  
  {  
   "type": "function",  
   "function": {  
    "name": "get_current_weather",  
    "description": "Get the current weather in a given location",  
    "parameters": {  
     "type": "object",  
     "properties": {  
      "location": {  
       "type": "string",  
       "description": "The city and state, e.g. San Francisco, CA"  
      },  
      "unit": {  
       "type": "string",  
       "enum": ["celsius", "fahrenheit"]  
      }  
     },  
     "required": ["location"]  
    }  
   }  
  }  
 ],  
 "tool_choice": "auto"  
}'  
  

```

## Supported Models​
LiteLLM supports **ALL** azure ai models. Here's a few examples:
Model Name| Function Call  
---|---  
Cohere command-r-plus| `completion(model="azure_ai/command-r-plus", messages)`  
Cohere command-r| `completion(model="azure_ai/command-r", messages)`  
mistral-large-latest| `completion(model="azure_ai/mistral-large-latest", messages)`  
AI21-Jamba-Instruct| `completion(model="azure_ai/ai21-jamba-instruct", messages)`  
## Rerank Endpoint​
### Usage​
  * LiteLLM SDK Usage
  * LiteLLM Proxy Usage


```
from litellm import rerank  
import os  
  
os.environ["AZURE_AI_API_KEY"] = "sk-.."  
os.environ["AZURE_AI_API_BASE"] = "https://.."  
  
query = "What is the capital of the United States?"  
documents = [  
  "Carson City is the capital city of the American state of Nevada.",  
  "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
  "Washington, D.C. is the capital of the United States.",  
  "Capital punishment has existed in the United States since before it was a country.",  
]  
  
response = rerank(  
  model="azure_ai/rerank-english-v3.0",  
  query=query,  
  documents=documents,  
  top_n=3,  
)  
print(response)  

```

LiteLLM provides an cohere api compatible `/rerank` endpoint for Rerank calls.
**Setup**
Add this to your litellm proxy config.yaml
```
model_list:  
 - model_name: Salesforce/Llama-Rank-V1  
  litellm_params:  
   model: together_ai/Salesforce/Llama-Rank-V1  
   api_key: os.environ/TOGETHERAI_API_KEY  
 - model_name: rerank-english-v3.0  
  litellm_params:  
   model: azure_ai/rerank-english-v3.0  
   api_key: os.environ/AZURE_AI_API_KEY  
   api_base: os.environ/AZURE_AI_API_BASE  

```

Start litellm
```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

Test request
```
curl http://0.0.0.0:4000/rerank \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "rerank-english-v3.0",  
  "query": "What is the capital of the United States?",  
  "documents": [  
    "Carson City is the capital city of the American state of Nevada.",  
    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
    "Washington, D.C. is the capital of the United States.",  
    "Capital punishment has existed in the United States since before it was a country."  
  ],  
  "top_n": 3  
 }'  

```

Previous
Azure OpenAI
Next
AI/ML API
  * Usage
    * ENV VAR
    * Example Call
  * Passing additional params - max_tokens, temperature
  * Function Calling
  * Supported Models
  * Rerank Endpoint
    * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * AWS Bedrock


On this page
# AWS Bedrock
ALL Bedrock models (Anthropic, Meta, Deepseek, Mistral, Amazon, etc.) are Supported
Property| Details  
---|---  
Description| Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs).  
Provider Route on LiteLLM| `bedrock/`, `bedrock/converse/`, `bedrock/invoke/`, `bedrock/converse_like/`, `bedrock/llama/`, `bedrock/deepseek_r1/`  
Provider Doc| Amazon Bedrock ↗  
Supported OpenAI Endpoints| `/chat/completions`, `/completions`, `/embeddings`, `/images/generations`  
Rerank Endpoint| `/rerank`  
Pass-through Endpoint| Supported  
LiteLLM requires `boto3` to be installed on your system for Bedrock requests
```
pip install boto3>=1.28.57  

```

info
For **Amazon Nova Models** : Bump to v1.53.5+
info
LiteLLM uses boto3 to handle authentication. All these options are supported - https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#credentials.
## Usage​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
 model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

## LiteLLM Proxy Usage​
Here's how to call Bedrock with the LiteLLM Proxy Server
### 1. Setup config.yaml​
```
model_list:  
 - model_name: bedrock-claude-v1  
  litellm_params:  
   model: bedrock/anthropic.claude-instant-v1  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  

```

All possible auth params: 
```
aws_access_key_id: Optional[str],  
aws_secret_access_key: Optional[str],  
aws_session_token: Optional[str],  
aws_region_name: Optional[str],  
aws_session_name: Optional[str],  
aws_profile_name: Optional[str],  
aws_role_name: Optional[str],  
aws_web_identity_token: Optional[str],  
aws_bedrock_runtime_endpoint: Optional[str],  

```

### 2. Start the proxy​
```
litellm --config /path/to/config.yaml  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "bedrock-claude-v1",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="bedrock-claude-v1", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "bedrock-claude-v1",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Set temperature, top p, etc.​
  * SDK
  * PROXY


```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
 model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 temperature=0.7,  
 top_p=1  
)  

```

**Set on yaml**
```
model_list:  
 - model_name: bedrock-claude-v1  
  litellm_params:  
   model: bedrock/anthropic.claude-instant-v1  
   temperature: <your-temp>  
   top_p: <your-top-p>  

```

**Set on request**
```
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="bedrock-claude-v1", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],  
temperature=0.7,  
top_p=1  
)  
  
print(response)  
  

```

## Pass provider-specific params​
If you pass a non-openai param to litellm, we'll assume it's provider-specific and send it as a kwarg in the request body. See more
  * SDK
  * PROXY


```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
 model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 top_k=1 # 👈 PROVIDER-SPECIFIC PARAM  
)  

```

**Set on yaml**
```
model_list:  
 - model_name: bedrock-claude-v1  
  litellm_params:  
   model: bedrock/anthropic.claude-instant-v1  
   top_k: 1 # 👈 PROVIDER-SPECIFIC PARAM  

```

**Set on request**
```
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="bedrock-claude-v1", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],  
temperature=0.7,  
extra_body={  
  top_k=1 # 👈 PROVIDER-SPECIFIC PARAM  
}  
)  
  
print(response)  
  

```

## Usage - Function Calling / Tool calling​
LiteLLM supports tool calling via Bedrock's Converse and Invoke API's.
  * SDK
  * PROXY


```
from litellm import completion  
  
# set env  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  
response = completion(  
  model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto",  
)  
# Add any assertions, here to check response args  
print(response)  
assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)  
assert isinstance(  
  response.choices[0].message.tool_calls[0].function.arguments, str  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-claude-3-7  
  litellm_params:  
   model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0 # for bedrock invoke, specify `bedrock/invoke/<model>`  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer $LITELLM_API_KEY" \  
-d '{  
 "model": "bedrock-claude-3-7",  
 "messages": [  
  {  
   "role": "user",  
   "content": "What'\''s the weather like in Boston today?"  
  }  
 ],  
 "tools": [  
  {  
   "type": "function",  
   "function": {  
    "name": "get_current_weather",  
    "description": "Get the current weather in a given location",  
    "parameters": {  
     "type": "object",  
     "properties": {  
      "location": {  
       "type": "string",  
       "description": "The city and state, e.g. San Francisco, CA"  
      },  
      "unit": {  
       "type": "string",  
       "enum": ["celsius", "fahrenheit"]  
      }  
     },  
     "required": ["location"]  
    }  
   }  
  }  
 ],  
 "tool_choice": "auto"  
}'  
  

```

## Usage - Vision​
```
from litellm import completion  
  
# set env  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
  
def encode_image(image_path):  
  import base64  
  
  with open(image_path, "rb") as image_file:  
    return base64.b64encode(image_file.read()).decode("utf-8")  
  
  
image_path = "../proxy/cached_logo.jpg"  
# Getting the base64 string  
base64_image = encode_image(image_path)  
resp = litellm.completion(  
  model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "Whats in this image?"},  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "data:image/jpeg;base64," + base64_image  
          },  
        },  
      ],  
    }  
  ],  
)  
print(f"\nResponse: {resp}")  

```

## Usage - 'thinking' / 'reasoning content'​
This is currently only supported for Anthropic's Claude 3.7 Sonnet + Deepseek R1.
Works on v1.61.20+.
Returns 2 new fields in `message` and `delta` object:
  * `reasoning_content` - string - The reasoning content of the response
  * `thinking_blocks` - list of objects (Anthropic only) - The thinking blocks of the response


Each object has the following fields:
  * `type` - Literal["thinking"] - The type of thinking block
  * `thinking` - string - The thinking of the response. Also returned in `reasoning_content`
  * `signature` - string - A base64 encoded string, returned by Anthropic.


The `signature` is required by Anthropic on subsequent calls, if 'thinking' content is passed in (only required to use `thinking` with tool calling). Learn more
  * SDK
  * PROXY


```
from litellm import completion  
  
# set env  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
  
resp = completion(  
  model="bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0",  
  messages=[{"role": "user", "content": "What is the capital of France?"}],  
  reasoning_effort="low",  
)  
  
print(resp)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-claude-3-7  
  litellm_params:  
   model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0  
   reasoning_effort: "low" # 👈 EITHER HERE OR ON REQUEST  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "bedrock-claude-3-7",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "reasoning_effort": "low" # 👈 EITHER HERE OR ON CONFIG.YAML  
 }'  

```

**Expected Response**
Same as Anthropic API response.
```
{  
  "id": "chatcmpl-c661dfd7-7530-49c9-b0cc-d5018ba4727d",  
  "created": 1740640366,  
  "model": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",  
  "object": "chat.completion",  
  "system_fingerprint": null,  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": "The capital of France is Paris. It's not only the capital city but also the largest city in France, serving as the country's major cultural, economic, and political center.",  
        "role": "assistant",  
        "tool_calls": null,  
        "function_call": null,  
        "reasoning_content": "The capital of France is Paris. This is a straightforward factual question.",  
        "thinking_blocks": [  
          {  
            "type": "thinking",  
            "thinking": "The capital of France is Paris. This is a straightforward factual question.",  
            "signature": "EqoBCkgIARABGAIiQL2UoU0b1OHYi+yCHpBY7U6FQW8/FcoLewocJQPa2HnmLM+NECy50y44F/kD4SULFXi57buI9fAvyBwtyjlOiO0SDE3+r3spdg6PLOo9PBoMma2ku5OTAoR46j9VIjDRlvNmBvff7YW4WI9oU8XagaOBSxLPxElrhyuxppEn7m6bfT40dqBSTDrfiw4FYB4qEPETTI6TA6wtjGAAqmFqKTo="  
          }  
        ]  
      }  
    }  
  ],  
  "usage": {  
    "completion_tokens": 64,  
    "prompt_tokens": 42,  
    "total_tokens": 106,  
    "completion_tokens_details": null,  
    "prompt_tokens_details": null  
  }  
}  

```

### Pass `thinking` to Anthropic models​
Same as Anthropic API response.
## Usage - Structured Output / JSON mode​
  * SDK
  * PROXY


```
from litellm import completion  
import os   
from pydantic import BaseModel  
  
# set env  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
class CalendarEvent(BaseModel):  
 name: str  
 date: str  
 participants: list[str]  
  
class EventsList(BaseModel):  
  events: list[CalendarEvent]  
  
response = completion(  
 model="bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0", # specify invoke via `bedrock/invoke/anthropic.claude-3-7-sonnet-20250219-v1:0`  
 response_format=EventsList,  
 messages=[  
  {"role": "system", "content": "You are a helpful assistant designed to output JSON."},  
  {"role": "user", "content": "Who won the world series in 2020?"}  
 ],  
)  
print(response.choices[0].message.content)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-claude-3-7  
  litellm_params:  
   model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0 # specify invoke via `bedrock/invoke/<model_name>`   
   aws_access_key_id: os.environ/CUSTOM_AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/CUSTOM_AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/CUSTOM_AWS_REGION_NAME  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it!


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "bedrock-claude-3-7",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful assistant designed to output JSON."  
   },  
   {  
    "role": "user",  
    "content": "Who won the worlde series in 2020?"  
   }  
  ],  
  "response_format": {  
   "type": "json_schema",  
   "json_schema": {  
    "name": "math_reasoning",  
    "description": "reason about maths",  
    "schema": {  
     "type": "object",  
     "properties": {  
      "steps": {  
       "type": "array",  
       "items": {  
        "type": "object",  
        "properties": {  
         "explanation": { "type": "string" },  
         "output": { "type": "string" }  
        },  
        "required": ["explanation", "output"],  
        "additionalProperties": false  
       }  
      },  
      "final_answer": { "type": "string" }  
     },  
     "required": ["steps", "final_answer"],  
     "additionalProperties": false  
    },  
    "strict": true  
   }  
  }  
 }'  

```

## Usage - Latency Optimized Inference​
Valid from v1.65.1+
  * SDK
  * PROXY


```
from litellm import completion  
  
response = completion(  
  model="bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0",  
  messages=[{"role": "user", "content": "What is the capital of France?"}],  
  performanceConfig={"latency": "optimized"},  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-claude-3-7  
  litellm_params:  
   model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0  
   performanceConfig: {"latency": "optimized"} # 👈 EITHER HERE OR ON REQUEST  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it!


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "bedrock-claude-3-7",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "performanceConfig": {"latency": "optimized"} # 👈 EITHER HERE OR ON CONFIG.YAML  
 }'  

```

## Usage - Bedrock Guardrails​
Example of using Bedrock Guardrails with LiteLLM
  * LiteLLM SDK
  * Proxy on request
  * Proxy on config.yaml


```
from litellm import completion  
  
# set env  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
  model="anthropic.claude-v2",  
  messages=[  
    {  
      "content": "where do i buy coffee from? ",  
      "role": "user",  
    }  
  ],  
  max_tokens=10,  
  guardrailConfig={  
    "guardrailIdentifier": "ff6ujrregl1q", # The identifier (ID) for the guardrail.  
    "guardrailVersion": "DRAFT",      # The version of the guardrail.  
    "trace": "disabled",          # The trace behavior for the guardrail. Can either be "disabled" or "enabled"  
  },  
)  

```

```
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="anthropic.claude-v2", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],  
temperature=0.7,  
extra_body={  
  "guardrailConfig": {  
    "guardrailIdentifier": "ff6ujrregl1q", # The identifier (ID) for the guardrail.  
    "guardrailVersion": "DRAFT",      # The version of the guardrail.  
    "trace": "disabled",          # The trace behavior for the guardrail. Can either be "disabled" or "enabled"  
  },  
}  
)  
  
print(response)  

```

  1. Update config.yaml 


```
model_list:  
 - model_name: bedrock-claude-v1  
  litellm_params:  
   model: bedrock/anthropic.claude-instant-v1  
   aws_access_key_id: os.environ/CUSTOM_AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/CUSTOM_AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/CUSTOM_AWS_REGION_NAME  
   guardrailConfig: {  
    "guardrailIdentifier": "ff6ujrregl1q", # The identifier (ID) for the guardrail.  
    "guardrailVersion": "DRAFT",      # The version of the guardrail.  
    "trace": "disabled",          # The trace behavior for the guardrail. Can either be "disabled" or "enabled"  
  }  
  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
  
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="bedrock-claude-v1", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],  
temperature=0.7  
)  
  
print(response)  

```

## Usage - "Assistant Pre-fill"​
If you're using Anthropic's Claude with Bedrock, you can "put words in Claude's mouth" by including an `assistant` role message as the last item in the `messages` array.
> [!IMPORTANT] The returned completion will _**not**_ include your "pre-fill" text, since it is part of the prompt itself. Make sure to prefix Claude's completion with your pre-fill.
```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
messages = [  
  {"role": "user", "content": "How do you say 'Hello' in German? Return your answer as a JSON object, like this:\n\n{ \"Hello\": \"Hallo\" }"},  
  {"role": "assistant", "content": "{"},  
]  
response = completion(model="bedrock/anthropic.claude-v2", messages=messages)  

```

### Example prompt sent to Claude​
```
  
Human: How do you say 'Hello' in German? Return your answer as a JSON object, like this:  
  
{ "Hello": "Hallo" }  
  
Assistant: {  

```

## Usage - "System" messages​
If you're using Anthropic's Claude 2.1 with Bedrock, `system` role messages are properly formatted for you.
```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
messages = [  
  {"role": "system", "content": "You are a snarky assistant."},  
  {"role": "user", "content": "How do I boil water?"},  
]  
response = completion(model="bedrock/anthropic.claude-v2:1", messages=messages)  

```

### Example prompt sent to Claude​
```
You are a snarky assistant.  
  
Human: How do I boil water?  
  
Assistant:  

```

## Usage - Streaming​
```
import os  
from litellm import completion  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
 model="bedrock/anthropic.claude-instant-v1",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 stream=True  
)  
for chunk in response:  
 print(chunk)  

```

#### Example Streaming Output Chunk​
```
{  
 "choices": [  
  {  
   "finish_reason": null,  
   "index": 0,  
   "delta": {  
    "content": "ase can appeal the case to a higher federal court. If a higher federal court rules in a way that conflicts with a ruling from a lower federal court or conflicts with a ruling from a higher state court, the parties involved in the case can appeal the case to the Supreme Court. In order to appeal a case to the Sup"  
   }  
  }  
 ],  
 "created": null,  
 "model": "anthropic.claude-instant-v1",  
 "usage": {  
  "prompt_tokens": null,  
  "completion_tokens": null,  
  "total_tokens": null  
 }  
}  

```

## Cross-region inferencing​
LiteLLM supports Bedrock cross-region inferencing across all supported bedrock models.
  * SDK
  * PROXY


```
from litellm import completion   
import os   
  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
  
litellm.set_verbose = True # 👈 SEE RAW REQUEST   
  
response = completion(  
  model="bedrock/us.anthropic.claude-3-haiku-20240307-v1:0",  
  messages=messages,  
  max_tokens=10,  
  temperature=0.1,  
)  
  
print("Final Response: {}".format(response))  

```

#### 1. Setup config.yaml​
```
model_list:  
 - model_name: bedrock-claude-haiku  
  litellm_params:  
   model: bedrock/us.anthropic.claude-3-haiku-20240307-v1:0  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  

```

#### 2. Start the proxy​
```
litellm --config /path/to/config.yaml  

```

#### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "bedrock-claude-haiku",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="bedrock-claude-haiku", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "bedrock-claude-haiku",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Set 'converse' / 'invoke' route​
info
Supported from LiteLLM Version `v1.53.5`
LiteLLM defaults to the `invoke` route. LiteLLM uses the `converse` route for Bedrock models that support it.
To explicitly set the route, do `bedrock/converse/<model>` or `bedrock/invoke/<model>`.
E.g. 
  * SDK
  * PROXY


```
from litellm import completion  
  
completion(model="bedrock/converse/us.amazon.nova-pro-v1:0")  

```

```
model_list:  
 - model_name: bedrock-model  
  litellm_params:  
   model: bedrock/converse/us.amazon.nova-pro-v1:0  

```

## Alternate user/assistant messages​
Use `user_continue_message` to add a default user message, for cases (e.g. Autogen) where the client might not follow alternating user/assistant messages starting and ending with a user message. 
```
model_list:  
 - model_name: "bedrock-claude"  
  litellm_params:  
   model: "bedrock/anthropic.claude-instant-v1"  
   user_continue_message: {"role": "user", "content": "Please continue"}  

```

OR 
just set `litellm.modify_params=True` and LiteLLM will automatically handle this with a default user_continue_message.
```
model_list:  
 - model_name: "bedrock-claude"  
  litellm_params:  
   model: "bedrock/anthropic.claude-instant-v1"  
  
litellm_settings:  
  modify_params: true  

```

Test it! 
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "bedrock-claude",  
  "messages": [{"role": "assistant", "content": "Hey, how's it going?"}]  
}'  

```

## Usage - PDF / Document Understanding​
LiteLLM supports Document Understanding for Bedrock models - AWS Bedrock Docs.
info
LiteLLM supports ALL Bedrock document types - 
E.g.: "pdf", "csv", "doc", "docx", "xls", "xlsx", "html", "txt", "md"
You can also pass these as either `image_url` or `base64`
### url​
  * SDK
  * PROXY


```
from litellm.utils import supports_pdf_input, completion  
  
# set aws credentials  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
  
# pdf url  
image_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"  
  
# Download the file  
response = requests.get(url)  
file_data = response.content  
  
encoded_file = base64.b64encode(file_data).decode("utf-8")  
  
# model  
model = "bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0"  
  
image_content = [  
  {"type": "text", "text": "What's this file about?"},  
  {  
    "type": "file",  
    "file": {  
      "file_data": f"data:application/pdf;base64,{encoded_file}", # 👈 PDF  
    }  
  },  
]  
  
  
if not supports_pdf_input(model, None):  
  print("Model does not support image input")  
  
response = completion(  
  model=model,  
  messages=[{"role": "user", "content": image_content}],  
)  
assert response is not None  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-model  
  litellm_params:  
   model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  

```

  1. Start the proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "bedrock-model",  
  "messages": [  
    {"role": "user", "content": {"type": "text", "text": "What's this file about?"}},  
    {  
      "type": "file",  
      "file": {  
        "file_data": f"data:application/pdf;base64,{encoded_file}", # 👈 PDF  
      }  
    }  
  ]  
}'  

```

### base64​
  * SDK
  * PROXY


```
from litellm.utils import supports_pdf_input, completion  
  
# set aws credentials  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
  
# pdf url  
image_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"  
response = requests.get(url)  
file_data = response.content  
  
encoded_file = base64.b64encode(file_data).decode("utf-8")  
base64_url = f"data:application/pdf;base64,{encoded_file}"  
  
# model  
model = "bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0"  
  
image_content = [  
  {"type": "text", "text": "What's this file about?"},  
  {  
    "type": "image_url",  
    "image_url": base64_url, # OR {"url": base64_url}  
  },  
]  
  
  
if not supports_pdf_input(model, None):  
  print("Model does not support image input")  
  
response = completion(  
  model=model,  
  messages=[{"role": "user", "content": image_content}],  
)  
assert response is not None  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: bedrock-model  
  litellm_params:  
   model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
   aws_region_name: os.environ/AWS_REGION_NAME  

```

  1. Start the proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "bedrock-model",  
  "messages": [  
    {"role": "user", "content": {"type": "text", "text": "What's this file about?"}},  
    {  
      "type": "image_url",  
      "image_url": "data:application/pdf;base64,{b64_encoded_file}",  
    }  
  ]  
}'  

```

## Bedrock Imported Models (Deepseek, Deepseek R1)​
### Deepseek R1​
This is a separate route, as the chat template is different.
Property| Details  
---|---  
Provider Route| `bedrock/deepseek_r1/{model_arn}`  
Provider Documentation| Bedrock Imported Models, Deepseek Bedrock Imported Model  
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
response = completion(  
  model="bedrock/deepseek_r1/arn:aws:bedrock:us-east-1:086734376398:imported-model/r4c4kewx2s0n", # bedrock/deepseek_r1/{your-model-arn}  
  messages=[{"role": "user", "content": "Tell me a joke"}],  
)  

```

**1. Add to config**
```
model_list:  
  - model_name: DeepSeek-R1-Distill-Llama-70B  
   litellm_params:  
    model: bedrock/deepseek_r1/arn:aws:bedrock:us-east-1:086734376398:imported-model/r4c4kewx2s0n  
  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
   --header 'Authorization: Bearer sk-1234' \  
   --header 'Content-Type: application/json' \  
   --data '{  
      "model": "DeepSeek-R1-Distill-Llama-70B", # 👈 the 'model_name' in config  
      "messages": [  
        {  
        "role": "user",  
        "content": "what llm are you"  
        }  
      ],  
    }'  

```

### Deepseek (not R1)​
Property| Details  
---|---  
Provider Route| `bedrock/llama/{model_arn}`  
Provider Documentation| Bedrock Imported Models, Deepseek Bedrock Imported Model  
Use this route to call Bedrock Imported Models that follow the `llama` Invoke Request / Response spec
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
response = completion(  
  model="bedrock/llama/arn:aws:bedrock:us-east-1:086734376398:imported-model/r4c4kewx2s0n", # bedrock/llama/{your-model-arn}  
  messages=[{"role": "user", "content": "Tell me a joke"}],  
)  

```

**1. Add to config**
```
model_list:  
  - model_name: DeepSeek-R1-Distill-Llama-70B  
   litellm_params:  
    model: bedrock/llama/arn:aws:bedrock:us-east-1:086734376398:imported-model/r4c4kewx2s0n  
  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
   --header 'Authorization: Bearer sk-1234' \  
   --header 'Content-Type: application/json' \  
   --data '{  
      "model": "DeepSeek-R1-Distill-Llama-70B", # 👈 the 'model_name' in config  
      "messages": [  
        {  
        "role": "user",  
        "content": "what llm are you"  
        }  
      ],  
    }'  

```

## Provisioned throughput models​
To use provisioned throughput Bedrock models pass 
  * `model=bedrock/<base-model>`, example `model=bedrock/anthropic.claude-v2`. Set `model` to any of the Supported AWS models
  * `model_id=provisioned-model-arn`


Completion
```
import litellm  
response = litellm.completion(  
  model="bedrock/anthropic.claude-instant-v1",  
  model_id="provisioned-model-arn",  
  messages=[{"content": "Hello, how are you?", "role": "user"}]  
)  

```

Embedding
```
import litellm  
response = litellm.embedding(  
  model="bedrock/amazon.titan-embed-text-v1",  
  model_id="provisioned-model-arn",  
  input=["hi"],  
)  

```

## Supported AWS Bedrock Models​
LiteLLM supports ALL Bedrock models. 
Here's an example of using a bedrock model with LiteLLM. For a complete list, refer to the model cost map
Model Name| Command  
---|---  
Deepseek R1| `completion(model='bedrock/us.deepseek.r1-v1:0', messages=messages)`  
Anthropic Claude-V3.5 Sonnet| `completion(model='bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0', messages=messages)`  
Anthropic Claude-V3 sonnet| `completion(model='bedrock/anthropic.claude-3-sonnet-20240229-v1:0', messages=messages)`  
Anthropic Claude-V3 Haiku| `completion(model='bedrock/anthropic.claude-3-haiku-20240307-v1:0', messages=messages)`  
Anthropic Claude-V3 Opus| `completion(model='bedrock/anthropic.claude-3-opus-20240229-v1:0', messages=messages)`  
Anthropic Claude-V2.1| `completion(model='bedrock/anthropic.claude-v2:1', messages=messages)`  
Anthropic Claude-V2| `completion(model='bedrock/anthropic.claude-v2', messages=messages)`  
Anthropic Claude-Instant V1| `completion(model='bedrock/anthropic.claude-instant-v1', messages=messages)`  
Meta llama3-1-405b| `completion(model='bedrock/meta.llama3-1-405b-instruct-v1:0', messages=messages)`  
Meta llama3-1-70b| `completion(model='bedrock/meta.llama3-1-70b-instruct-v1:0', messages=messages)`  
Meta llama3-1-8b| `completion(model='bedrock/meta.llama3-1-8b-instruct-v1:0', messages=messages)`  
Meta llama3-70b| `completion(model='bedrock/meta.llama3-70b-instruct-v1:0', messages=messages)`  
Meta llama3-8b| `completion(model='bedrock/meta.llama3-8b-instruct-v1:0', messages=messages)`  
Amazon Titan Lite| `completion(model='bedrock/amazon.titan-text-lite-v1', messages=messages)`  
Amazon Titan Express| `completion(model='bedrock/amazon.titan-text-express-v1', messages=messages)`  
Cohere Command| `completion(model='bedrock/cohere.command-text-v14', messages=messages)`  
AI21 J2-Mid| `completion(model='bedrock/ai21.j2-mid-v1', messages=messages)`  
AI21 J2-Ultra| `completion(model='bedrock/ai21.j2-ultra-v1', messages=messages)`  
AI21 Jamba-Instruct| `completion(model='bedrock/ai21.jamba-instruct-v1:0', messages=messages)`  
Meta Llama 2 Chat 13b| `completion(model='bedrock/meta.llama2-13b-chat-v1', messages=messages)`  
Meta Llama 2 Chat 70b| `completion(model='bedrock/meta.llama2-70b-chat-v1', messages=messages)`  
Mistral 7B Instruct| `completion(model='bedrock/mistral.mistral-7b-instruct-v0:2', messages=messages)`  
Mixtral 8x7B Instruct| `completion(model='bedrock/mistral.mixtral-8x7b-instruct-v0:1', messages=messages)`  
## Bedrock Embedding​
### API keys​
This can be set as env variables or passed as **params to litellm.embedding()**
```
import os  
os.environ["AWS_ACCESS_KEY_ID"] = ""    # Access key  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  # Secret access key  
os.environ["AWS_REGION_NAME"] = ""      # us-east-1, us-east-2, us-west-1, us-west-2  

```

### Usage​
```
from litellm import embedding  
response = embedding(  
  model="bedrock/amazon.titan-embed-text-v1",  
  input=["good morning from litellm"],  
)  
print(response)  

```

## Supported AWS Bedrock Embedding Models​
Model Name| Usage| Supported Additional OpenAI params  
---|---|---  
Titan Embeddings V2| `embedding(model="bedrock/amazon.titan-embed-text-v2:0", input=input)`| here  
Titan Embeddings - V1| `embedding(model="bedrock/amazon.titan-embed-text-v1", input=input)`| here  
Titan Multimodal Embeddings| `embedding(model="bedrock/amazon.titan-embed-image-v1", input=input)`| here  
Cohere Embeddings - English| `embedding(model="bedrock/cohere.embed-english-v3", input=input)`| here  
Cohere Embeddings - Multilingual| `embedding(model="bedrock/cohere.embed-multilingual-v3", input=input)`| here  
### Advanced - Drop Unsupported Params​
### Advanced - Pass model/provider-specific Params​
## Image Generation​
Use this for stable diffusion, and amazon nova canvas on bedrock
### Usage​
  * SDK
  * PROXY


```
import os  
from litellm import image_generation  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = image_generation(  
      prompt="A cute baby sea otter",  
      model="bedrock/stability.stable-diffusion-xl-v0",  
    )  
print(f"response: {response}")  

```

**Set optional params**
```
import os  
from litellm import image_generation  
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = image_generation(  
      prompt="A cute baby sea otter",  
      model="bedrock/stability.stable-diffusion-xl-v0",  
      ### OPENAI-COMPATIBLE ###  
      size="128x512", # width=128, height=512  
      ### PROVIDER-SPECIFIC ### see `AmazonStabilityConfig` in bedrock.py for all params  
      seed=30  
    )  
print(f"response: {response}")  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: amazon.nova-canvas-v1:0  
  litellm_params:  
   model: bedrock/amazon.nova-canvas-v1:0  
   aws_region_name: "us-east-1"  
   aws_secret_access_key: my-key # OPTIONAL - all boto3 auth params supported  
   aws_secret_access_id: my-id # OPTIONAL - all boto3 auth params supported  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/v1/images/generations' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_VIRTUAL_KEY' \  
-d '{  
  "model": "amazon.nova-canvas-v1:0",  
  "prompt": "A cute baby sea otter"  
}'  

```

## Supported AWS Bedrock Image Generation Models​
Model Name| Function Call  
---|---  
Stable Diffusion 3 - v0| `embedding(model="bedrock/stability.stability.sd3-large-v1:0", prompt=prompt)`  
Stable Diffusion - v0| `embedding(model="bedrock/stability.stable-diffusion-xl-v0", prompt=prompt)`  
Stable Diffusion - v0| `embedding(model="bedrock/stability.stable-diffusion-xl-v1", prompt=prompt)`  
## Rerank API​
Use Bedrock's Rerank API in the Cohere `/rerank` format. 
Supported Cohere Rerank Params
  * `model` - the foundation model ARN
  * `query` - the query to rerank against
  * `documents` - the list of documents to rerank
  * `top_n` - the number of results to return


  * SDK
  * PROXY


```
from litellm import rerank  
import os   
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = rerank(  
  model="bedrock/arn:aws:bedrock:us-west-2::foundation-model/amazon.rerank-v1:0", # provide the model ARN - get this here https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock/client/list_foundation_models.html  
  query="hello",  
  documents=["hello", "world"],  
  top_n=2,  
)  
  
print(response)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: bedrock-rerank  
   litellm_params:  
    model: bedrock/arn:aws:bedrock:us-west-2::foundation-model/amazon.rerank-v1:0  
    aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  
    aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  
    aws_region_name: os.environ/AWS_REGION_NAME  

```

  1. Start proxy server


```
litellm --config config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/rerank \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "bedrock-rerank",  
  "query": "What is the capital of the United States?",  
  "documents": [  
    "Carson City is the capital city of the American state of Nevada.",  
    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
    "Washington, D.C. is the capital of the United States.",  
    "Capital punishment has existed in the United States since before it was a country."  
  ],  
  "top_n": 3  
  
  
 }'  

```

## Bedrock Application Inference Profile​
Use Bedrock Application Inference Profile to track costs for projects on AWS. 
You can either pass it in the model name - `model="bedrock/arn:...` or as a separate `model_id="arn:..` param.
### Set via `model_id`​
  * SDK
  * PROXY


```
from litellm import completion  
import os   
  
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  
  
response = completion(  
  model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",  
  messages=[{"role": "user", "content": "Hello, how are you?"}],  
  model_id="arn:aws:bedrock:eu-central-1:000000000000:application-inference-profile/a0a0a0a0a0a0",  
)  
  
print(response)  

```

  1. Setup config.yaml 


```
model_list:  
 - model_name: anthropic-claude-3-5-sonnet  
  litellm_params:  
   model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0  
   # You have to set the ARN application inference profile in the model_id parameter  
   model_id: arn:aws:bedrock:eu-central-1:000000000000:application-inference-profile/a0a0a0a0a0a0  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_API_KEY' \  
-d '{  
 "model": "anthropic-claude-3-5-sonnet",  
 "messages": [  
  {  
   "role": "user",  
   "content": [  
    {  
     "type": "text",  
     "text": "List 5 important events in the XIX century"  
    }  
   ]  
  }  
 ]  
}'  

```

## Boto3 - Authentication​
### Passing credentials as parameters - Completion()​
Pass AWS credentials as parameters to litellm.completion
```
import os  
from litellm import completion  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      aws_access_key_id="",  
      aws_secret_access_key="",  
      aws_region_name="",  
)  

```

### Passing extra headers + Custom API Endpoints​
This can be used to override existing headers (e.g. `Authorization`) when calling custom api endpoints
  * SDK
  * PROXY


```
import os  
import litellm  
from litellm import completion  
  
litellm.set_verbose = True # 👈 SEE RAW REQUEST  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      aws_access_key_id="",  
      aws_secret_access_key="",  
      aws_region_name="",  
      aws_bedrock_runtime_endpoint="https://my-fake-endpoint.com",  
      extra_headers={"key": "value"}  
)  

```

  1. Setup config.yaml 


```
model_list:  
  - model_name: bedrock-model  
   litellm_params:  
    model: bedrock/anthropic.claude-instant-v1  
    aws_access_key_id: "",  
    aws_secret_access_key: "",  
    aws_region_name: "",  
    aws_bedrock_runtime_endpoint: "https://my-fake-endpoint.com",  
    extra_headers: {"key": "value"}  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml --detailed_debug  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "bedrock-model",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   {  
    "role": "user",  
    "content": "how can I solve 8x + 7 = -23"  
   }  
  ]  
}'  

```

### SSO Login (AWS Profile)​
  * Set `AWS_PROFILE` environment variable
  * Make bedrock completion call


```
import os  
from litellm import completion  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

or pass `aws_profile_name`:
```
import os  
from litellm import completion  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      aws_profile_name="dev-profile",  
)  

```

### STS (Role-based Auth)​
  * Set `aws_role_name` and `aws_session_name`

LiteLLM Parameter| Boto3 Parameter| Description| Boto3 Documentation  
---|---|---|---  
`aws_access_key_id`| `aws_access_key_id`| AWS access key associated with an IAM user or role| Credentials  
`aws_secret_access_key`| `aws_secret_access_key`| AWS secret key associated with the access key| Credentials  
`aws_role_name`| `RoleArn`| The Amazon Resource Name (ARN) of the role to assume| AssumeRole API  
`aws_session_name`| `RoleSessionName`| An identifier for the assumed role session| AssumeRole API  
Make the bedrock completion call
  * SDK
  * PROXY


```
from litellm import completion  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=messages,  
      max_tokens=10,  
      temperature=0.1,  
      aws_role_name=aws_role_name,  
      aws_session_name="my-test-session",  
    )  

```

If you also need to dynamically set the aws user accessing the role, add the additional args in the completion()/embedding() function
```
from litellm import completion  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=messages,  
      max_tokens=10,  
      temperature=0.1,  
      aws_region_name=aws_region_name,  
      aws_access_key_id=aws_access_key_id,  
      aws_secret_access_key=aws_secret_access_key,  
      aws_role_name=aws_role_name,  
      aws_session_name="my-test-session",  
    )  

```

```
model_list:  
 - model_name: bedrock/*  
  litellm_params:  
   model: bedrock/*  
   aws_role_name: arn:aws:iam::888602223428:role/iam_local_role # AWS RoleArn  
   aws_session_name: "bedrock-session" # AWS RoleSessionName  
   aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID # [OPTIONAL - not required if using role]  
   aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY # [OPTIONAL - not required if using role]  

```

Text to Image : 
```
curl -L -X POST 'http://0.0.0.0:4000/v1/images/generations' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_VIRTUAL_KEY' \  
-d '{  
  "model": "amazon.nova-canvas-v1:0",  
  "prompt": "A cute baby sea otter"  
}'  

```

Color Guided Generation:
```
curl -L -X POST 'http://0.0.0.0:4000/v1/images/generations' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer $LITELLM_VIRTUAL_KEY' \  
-d '{  
  "model": "amazon.nova-canvas-v1:0",  
  "prompt": "A cute baby sea otter",  
  "taskType": "COLOR_GUIDED_GENERATION",  
  "colorGuidedGenerationParams":{"colors":["#FFFFFF"]}  
}'  

```

Model Name| Function Call  
---|---  
Stable Diffusion 3 - v0| `image_generation(model="bedrock/stability.stability.sd3-large-v1:0", prompt=prompt)`  
Stable Diffusion - v0| `image_generation(model="bedrock/stability.stable-diffusion-xl-v0", prompt=prompt)`  
Stable Diffusion - v1| `image_generation(model="bedrock/stability.stable-diffusion-xl-v1", prompt=prompt)`  
Amazon Nova Canvas - v0| `image_generation(model="bedrock/amazon.nova-canvas-v1:0", prompt=prompt)`  
### Passing an external BedrockRuntime.Client as a parameter - Completion()​
This is a deprecated flow. Boto3 is not async. And boto3.client does not let us make the http call through httpx. Pass in your aws params through the method above 👆. See Auth Code Add new auth flow
danger
Experimental - 2024-Jun-23: `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token` will be extracted from boto3.client and be passed into the httpx client 
Pass an external BedrockRuntime.Client object as a parameter to litellm.completion. Useful when using an AWS credentials profile, SSO session, assumed role session, or if environment variables are not available for auth.
Create a client from session credentials:
```
import boto3  
from litellm import completion  
  
bedrock = boto3.client(  
      service_name="bedrock-runtime",  
      region_name="us-east-1",  
      aws_access_key_id="",  
      aws_secret_access_key="",  
      aws_session_token="",  
)  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      aws_bedrock_client=bedrock,  
)  

```

Create a client from AWS profile in `~/.aws/config`:
```
import boto3  
from litellm import completion  
  
dev_session = boto3.Session(profile_name="dev-profile")  
bedrock = dev_session.client(  
      service_name="bedrock-runtime",  
      region_name="us-east-1",  
)  
  
response = completion(  
      model="bedrock/anthropic.claude-instant-v1",  
      messages=[{ "content": "Hello, how are you?","role": "user"}],  
      aws_bedrock_client=bedrock,  
)  

```

## Calling via Internal Proxy (not bedrock url compatible)​
Use the `bedrock/converse_like/model` endpoint to call bedrock converse model via your internal proxy.
  * SDK
  * LiteLLM Proxy


```
from litellm import completion  
  
response = completion(  
  model="bedrock/converse_like/some-model",  
  messages=[{"role": "user", "content": "What's AWS?"}],  
  api_key="sk-1234",  
  api_base="https://some-api-url/models",  
  extra_headers={"test": "hello world"},  
)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: anthropic-claude  
   litellm_params:  
    model: bedrock/converse_like/some-model  
    api_base: https://some-api-url/models  

```

  1. Start proxy server


```
litellm --config config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "anthropic-claude",  
  "messages": [  
   {  
    "role": "system",  
    "content": "You are a helpful math tutor. Guide the user through the solution step by step."  
   },  
   { "content": "Hello, how are you?", "role": "user" }  
  ]  
}'  

```

**Expected Output URL**
```
https://some-api-url/models  

```

Previous
AWS Sagemaker
Next
LiteLLM Proxy (LLM Gateway)
  * Usage
  * LiteLLM Proxy Usage
    * 1. Setup config.yaml
    * 2. Start the proxy
    * 3. Test it
  * Set temperature, top p, etc.
  * Pass provider-specific params
  * Usage - Function Calling / Tool calling
  * Usage - Vision
  * Usage - 'thinking' / 'reasoning content'
    * Pass `thinking` to Anthropic models
  * Usage - Structured Output / JSON mode
  * Usage - Latency Optimized Inference
  * Usage - Bedrock Guardrails
  * Usage - "Assistant Pre-fill"
    * Example prompt sent to Claude
  * Usage - "System" messages
    * Example prompt sent to Claude
  * Usage - Streaming
  * Cross-region inferencing
  * Set 'converse' / 'invoke' route
  * Alternate user/assistant messages
  * Usage - PDF / Document Understanding
    * url
    * base64
  * Bedrock Imported Models (Deepseek, Deepseek R1)
    * Deepseek R1
    * Deepseek (not R1)
  * Provisioned throughput models
  * Supported AWS Bedrock Models
  * Bedrock Embedding
    * API keys
    * Usage
  * Supported AWS Bedrock Embedding Models
    * Advanced - Drop Unsupported Params
    * Advanced - Pass model/provider-specific Params
  * Image Generation
    * Usage
  * Supported AWS Bedrock Image Generation Models
  * Rerank API
  * Bedrock Application Inference Profile
    * Set via `model_id`
  * Boto3 - Authentication
    * Passing credentials as parameters - Completion()
    * Passing extra headers + Custom API Endpoints
    * SSO Login (AWS Profile)
    * STS (Role-based Auth)
    * Passing an external BedrockRuntime.Client as a parameter - Completion()
  * Calling via Internal Proxy (not bedrock url compatible)


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Baseten


On this page
# Baseten
LiteLLM supports any Text-Gen-Interface models on Baseten.
Here's a tutorial on deploying a huggingface TGI model (Llama2, CodeLlama, WizardCoder, Falcon, etc.) on Baseten
### API KEYS​
```
import os   
os.environ["BASETEN_API_KEY"] = ""  

```

### Baseten Models​
Baseten provides infrastructure to deploy and serve ML models https://www.baseten.co/. Use liteLLM to easily call models deployed on Baseten.
Example Baseten Usage - Note: liteLLM supports all models deployed on Baseten
Usage: Pass `model=baseten/<Model ID>`
Model Name| Function Call| Required OS Variables  
---|---|---  
Falcon 7B| `completion(model='baseten/qvv0xeq', messages=messages)`| `os.environ['BASETEN_API_KEY']`  
Wizard LM| `completion(model='baseten/q841o8w', messages=messages)`| `os.environ['BASETEN_API_KEY']`  
MPT 7B Base| `completion(model='baseten/31dxrj3', messages=messages)`| `os.environ['BASETEN_API_KEY']`  
Previous
Aleph Alpha
Next
OpenRouter
  * API KEYS
  * Baseten Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Cerebras


On this page
# Cerebras
https://inference-docs.cerebras.ai/api-reference/chat-completions
tip
**We support ALL Cerebras models, just set`model=cerebras/<any-model-on-cerebras>` as a prefix when sending litellm requests**
## API Key​
```
# env variable  
os.environ['CEREBRAS_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['CEREBRAS_API_KEY'] = ""  
response = completion(  
  model="cerebras/llama3-70b-instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit? (Write in JSON)",  
    }  
  ],  
  max_tokens=10,  
      
  # The prompt should include JSON if 'json_object' is selected; otherwise, you will get error code 400.  
  response_format={ "type": "json_object" },  
  seed=123,  
  stop=["\n\n"],  
  temperature=0.2,  
  top_p=0.9,  
  tool_choice="auto",  
  tools=[],  
  user="user",  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['CEREBRAS_API_KEY'] = ""  
response = completion(  
  model="cerebras/llama3-70b-instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit? (Write in JSON)",  
    }  
  ],  
  stream=True,  
  max_tokens=10,  
  
  # The prompt should include JSON if 'json_object' is selected; otherwise, you will get error code 400.  
  response_format={ "type": "json_object" },   
  seed=123,  
  stop=["\n\n"],  
  temperature=0.2,  
  top_p=0.9,  
  tool_choice="auto",  
  tools=[],  
  user="user",  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage with LiteLLM Proxy Server​
Here's how to call a Cerebras model with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: cerebras/<your-model-name> # add cerebras/ prefix to route as Cerebras provider  
   api_key: api-key         # api key to send your model  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



Previous
LM Studio
Next
Volcano Engine (Volcengine)
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Usage with LiteLLM Proxy Server


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Deepgram


On this page
# Deepgram
LiteLLM supports Deepgram's `/listen` endpoint.
Property| Details  
---|---  
Description| Deepgram's voice AI platform provides APIs for speech-to-text, text-to-speech, and language understanding.  
Provider Route on LiteLLM| `deepgram/`  
Provider Doc| Deepgram ↗  
Supported OpenAI Endpoints| `/audio/transcriptions`  
## Quick Start​
```
from litellm import transcription  
import os   
  
# set api keys   
os.environ["DEEPGRAM_API_KEY"] = ""  
audio_file = open("/path/to/audio.mp3", "rb")  
  
response = transcription(model="deepgram/nova-2", file=audio_file)  
  
print(f"response: {response}")  

```

## LiteLLM Proxy Usage​
### Add model to config​
  1. Add model to config.yaml


```
model_list:  
- model_name: nova-2  
 litellm_params:  
  model: deepgram/nova-2  
  api_key: os.environ/DEEPGRAM_API_KEY  
 model_info:  
  mode: audio_transcription  
    
general_settings:  
 master_key: sk-1234  

```

### Start proxy​
```
litellm --config /path/to/config.yaml   
  
# RUNNING on http://0.0.0.0:4000  

```

### Test​
  * Curl
  * OpenAI


```
curl --location 'http://0.0.0.0:4000/v1/audio/transcriptions' \  
--header 'Authorization: Bearer sk-1234' \  
--form 'file=@"/Users/krrishdholakia/Downloads/gettysburg.wav"' \  
--form 'model="nova-2"'  

```

```
from openai import OpenAI  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
  
audio_file = open("speech.mp3", "rb")  
transcript = client.audio.transcriptions.create(  
 model="nova-2",  
 file=audio_file  
)  

```

Previous
Databricks
Next
IBM watsonx.ai
  * Quick Start
  * LiteLLM Proxy Usage
    * Add model to config
    * Start proxy
    * Test


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Cloudflare Workers AI


On this page
# Cloudflare Workers AI
https://developers.cloudflare.com/workers-ai/models/text-generation/
## API Key​
```
# env variable  
os.environ['CLOUDFLARE_API_KEY'] = "3dnSGlxxxx"  
os.environ['CLOUDFLARE_ACCOUNT_ID'] = "03xxxxx"  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['CLOUDFLARE_API_KEY'] = "3dnSGlxxxx"  
os.environ['CLOUDFLARE_ACCOUNT_ID'] = "03xxxxx"  
  
response = completion(  
  model="cloudflare/@cf/meta/llama-2-7b-chat-int8",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['CLOUDFLARE_API_KEY'] = "3dnSGlxxxx"  
os.environ['CLOUDFLARE_ACCOUNT_ID'] = "03xxxxx"  
  
response = completion(  
  model="cloudflare/@hf/thebloke/codellama-7b-instruct-awq",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Supported Models​
All models listed here https://developers.cloudflare.com/workers-ai/models/text-generation/ are supported
Model Name| Function Call  
---|---  
@cf/meta/llama-2-7b-chat-fp16| `completion(model="mistral/mistral-tiny", messages)`  
@cf/meta/llama-2-7b-chat-int8| `completion(model="mistral/mistral-small", messages)`  
@cf/mistral/mistral-7b-instruct-v0.1| `completion(model="mistral/mistral-medium", messages)`  
@hf/thebloke/codellama-7b-instruct-awq| `completion(model="codellama/codellama-medium", messages)`  
Previous
Xinference [Xorbits Inference]
Next
DeepInfra
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Clarifai


On this page
# Clarifai
Anthropic, OpenAI, Mistral, Llama and Gemini LLMs are Supported on Clarifai. 
danger
Streaming is not yet supported on using clarifai and litellm. Tracking support here: https://github.com/BerriAI/litellm/issues/4162
## Pre-Requisites​
`pip install litellm`
## Required Environment Variables​
To obtain your Clarifai Personal access token follow this link. Optionally the PAT can also be passed in `completion` function.
```
os.environ["CLARIFAI_API_KEY"] = "YOUR_CLARIFAI_PAT" # CLARIFAI_PAT  
  

```

## Usage​
```
import os  
from litellm import completion  
  
os.environ["CLARIFAI_API_KEY"] = ""  
  
response = completion(  
 model="clarifai/mistralai.completion.mistral-large",  
 messages=[{ "content": "Tell me a joke about physics?","role": "user"}]  
)  

```

**Output**
```
{  
  "id": "chatcmpl-572701ee-9ab2-411c-ac75-46c1ba18e781",  
  "choices": [  
   {  
    "finish_reason": "stop",  
    "index": 1,  
    "message": {  
     "content": "Sure, here's a physics joke for you:\n\nWhy can't you trust an atom?\n\nBecause they make up everything!",  
     "role": "assistant"  
    }  
   }  
  ],  
  "created": 1714410197,  
  "model": "https://api.clarifai.com/v2/users/mistralai/apps/completion/models/mistral-large/outputs",  
  "object": "chat.completion",  
  "system_fingerprint": null,  
  "usage": {  
   "prompt_tokens": 14,  
   "completion_tokens": 24,  
   "total_tokens": 38  
  }  
 }  

```

## Clarifai models​
liteLLM supports all models on Clarifai community
Example Usage - Note: liteLLM supports all models deployed on Clarifai
## Llama LLMs​
Model Name| Function Call  
---|---  
clarifai/meta.Llama-2.llama2-7b-chat| `completion('clarifai/meta.Llama-2.llama2-7b-chat', messages)`  
clarifai/meta.Llama-2.llama2-13b-chat| `completion('clarifai/meta.Llama-2.llama2-13b-chat', messages)`  
clarifai/meta.Llama-2.llama2-70b-chat| `completion('clarifai/meta.Llama-2.llama2-70b-chat', messages)`  
clarifai/meta.Llama-2.codeLlama-70b-Python| `completion('clarifai/meta.Llama-2.codeLlama-70b-Python', messages)`  
clarifai/meta.Llama-2.codeLlama-70b-Instruct| `completion('clarifai/meta.Llama-2.codeLlama-70b-Instruct', messages)`  
## Mistral LLMs​
Model Name| Function Call  
---|---  
clarifai/mistralai.completion.mixtral-8x22B| `completion('clarifai/mistralai.completion.mixtral-8x22B', messages)`  
clarifai/mistralai.completion.mistral-large| `completion('clarifai/mistralai.completion.mistral-large', messages)`  
clarifai/mistralai.completion.mistral-medium| `completion('clarifai/mistralai.completion.mistral-medium', messages)`  
clarifai/mistralai.completion.mistral-small| `completion('clarifai/mistralai.completion.mistral-small', messages)`  
clarifai/mistralai.completion.mixtral-8x7B-Instruct-v0_1| `completion('clarifai/mistralai.completion.mixtral-8x7B-Instruct-v0_1', messages)`  
clarifai/mistralai.completion.mistral-7B-OpenOrca| `completion('clarifai/mistralai.completion.mistral-7B-OpenOrca', messages)`  
clarifai/mistralai.completion.openHermes-2-mistral-7B| `completion('clarifai/mistralai.completion.openHermes-2-mistral-7B', messages)`  
## Jurassic LLMs​
Model Name| Function Call  
---|---  
clarifai/ai21.complete.Jurassic2-Grande| `completion('clarifai/ai21.complete.Jurassic2-Grande', messages)`  
clarifai/ai21.complete.Jurassic2-Grande-Instruct| `completion('clarifai/ai21.complete.Jurassic2-Grande-Instruct', messages)`  
clarifai/ai21.complete.Jurassic2-Jumbo-Instruct| `completion('clarifai/ai21.complete.Jurassic2-Jumbo-Instruct', messages)`  
clarifai/ai21.complete.Jurassic2-Jumbo| `completion('clarifai/ai21.complete.Jurassic2-Jumbo', messages)`  
clarifai/ai21.complete.Jurassic2-Large| `completion('clarifai/ai21.complete.Jurassic2-Large', messages)`  
## Wizard LLMs​
Model Name| Function Call  
---|---  
clarifai/wizardlm.generate.wizardCoder-Python-34B| `completion('clarifai/wizardlm.generate.wizardCoder-Python-34B', messages)`  
clarifai/wizardlm.generate.wizardLM-70B| `completion('clarifai/wizardlm.generate.wizardLM-70B', messages)`  
clarifai/wizardlm.generate.wizardLM-13B| `completion('clarifai/wizardlm.generate.wizardLM-13B', messages)`  
clarifai/wizardlm.generate.wizardCoder-15B| `completion('clarifai/wizardlm.generate.wizardCoder-15B', messages)`  
## Anthropic models​
Model Name| Function Call  
---|---  
clarifai/anthropic.completion.claude-v1| `completion('clarifai/anthropic.completion.claude-v1', messages)`  
clarifai/anthropic.completion.claude-instant-1_2| `completion('clarifai/anthropic.completion.claude-instant-1_2', messages)`  
clarifai/anthropic.completion.claude-instant| `completion('clarifai/anthropic.completion.claude-instant', messages)`  
clarifai/anthropic.completion.claude-v2| `completion('clarifai/anthropic.completion.claude-v2', messages)`  
clarifai/anthropic.completion.claude-2_1| `completion('clarifai/anthropic.completion.claude-2_1', messages)`  
clarifai/anthropic.completion.claude-3-opus| `completion('clarifai/anthropic.completion.claude-3-opus', messages)`  
clarifai/anthropic.completion.claude-3-sonnet| `completion('clarifai/anthropic.completion.claude-3-sonnet', messages)`  
## OpenAI GPT LLMs​
Model Name| Function Call  
---|---  
clarifai/openai.chat-completion.GPT-4| `completion('clarifai/openai.chat-completion.GPT-4', messages)`  
clarifai/openai.chat-completion.GPT-3_5-turbo| `completion('clarifai/openai.chat-completion.GPT-3_5-turbo', messages)`  
clarifai/openai.chat-completion.gpt-4-turbo| `completion('clarifai/openai.chat-completion.gpt-4-turbo', messages)`  
clarifai/openai.completion.gpt-3_5-turbo-instruct| `completion('clarifai/openai.completion.gpt-3_5-turbo-instruct', messages)`  
## GCP LLMs​
Model Name| Function Call  
---|---  
clarifai/gcp.generate.gemini-1_5-pro| `completion('clarifai/gcp.generate.gemini-1_5-pro', messages)`  
clarifai/gcp.generate.imagen-2| `completion('clarifai/gcp.generate.imagen-2', messages)`  
clarifai/gcp.generate.code-gecko| `completion('clarifai/gcp.generate.code-gecko', messages)`  
clarifai/gcp.generate.code-bison| `completion('clarifai/gcp.generate.code-bison', messages)`  
clarifai/gcp.generate.text-bison| `completion('clarifai/gcp.generate.text-bison', messages)`  
clarifai/gcp.generate.gemma-2b-it| `completion('clarifai/gcp.generate.gemma-2b-it', messages)`  
clarifai/gcp.generate.gemma-7b-it| `completion('clarifai/gcp.generate.gemma-7b-it', messages)`  
clarifai/gcp.generate.gemini-pro| `completion('clarifai/gcp.generate.gemini-pro', messages)`  
clarifai/gcp.generate.gemma-1_1-7b-it| `completion('clarifai/gcp.generate.gemma-1_1-7b-it', messages)`  
## Cohere LLMs​
Model Name| Function Call  
---|---  
clarifai/cohere.generate.cohere-generate-command| `completion('clarifai/cohere.generate.cohere-generate-command', messages)`  
clarifai/cohere.generate.command-r-plus'| `completion('clarifai/clarifai/cohere.generate.command-r-plus', messages)`  
## Databricks LLMs​
Model Name| Function Call  
---|---  
clarifai/databricks.drbx.dbrx-instruct| `completion('clarifai/databricks.drbx.dbrx-instruct', messages)`  
clarifai/databricks.Dolly-v2.dolly-v2-12b| `completion('clarifai/databricks.Dolly-v2.dolly-v2-12b', messages)`  
## Microsoft LLMs​
Model Name| Function Call  
---|---  
clarifai/microsoft.text-generation.phi-2| `completion('clarifai/microsoft.text-generation.phi-2', messages)`  
clarifai/microsoft.text-generation.phi-1_5| `completion('clarifai/microsoft.text-generation.phi-1_5', messages)`  
## Salesforce models​
Model Name| Function Call  
---|---  
clarifai/salesforce.blip.general-english-image-caption-blip-2| `completion('clarifai/salesforce.blip.general-english-image-caption-blip-2', messages)`  
clarifai/salesforce.xgen.xgen-7b-8k-instruct| `completion('clarifai/salesforce.xgen.xgen-7b-8k-instruct', messages)`  
## Other Top performing LLMs​
Model Name| Function Call  
---|---  
clarifai/deci.decilm.deciLM-7B-instruct| `completion('clarifai/deci.decilm.deciLM-7B-instruct', messages)`  
clarifai/upstage.solar.solar-10_7b-instruct| `completion('clarifai/upstage.solar.solar-10_7b-instruct', messages)`  
clarifai/openchat.openchat.openchat-3_5-1210| `completion('clarifai/openchat.openchat.openchat-3_5-1210', messages)`  
clarifai/togethercomputer.stripedHyena.stripedHyena-Nous-7B| `completion('clarifai/togethercomputer.stripedHyena.stripedHyena-Nous-7B', messages)`  
clarifai/fblgit.una-cybertron.una-cybertron-7b-v2| `completion('clarifai/fblgit.una-cybertron.una-cybertron-7b-v2', messages)`  
clarifai/tiiuae.falcon.falcon-40b-instruct| `completion('clarifai/tiiuae.falcon.falcon-40b-instruct', messages)`  
clarifai/togethercomputer.RedPajama.RedPajama-INCITE-7B-Chat| `completion('clarifai/togethercomputer.RedPajama.RedPajama-INCITE-7B-Chat', messages)`  
clarifai/bigcode.code.StarCoder| `completion('clarifai/bigcode.code.StarCoder', messages)`  
clarifai/mosaicml.mpt.mpt-7b-instruct| `completion('clarifai/mosaicml.mpt.mpt-7b-instruct', messages)`  
Previous
Fireworks AI
Next
VLLM
  * Pre-Requisites
  * Required Environment Variables
  * Usage
  * Clarifai models
  * Llama LLMs
  * Mistral LLMs
  * Jurassic LLMs
  * Wizard LLMs
  * Anthropic models
  * OpenAI GPT LLMs
  * GCP LLMs
  * Cohere LLMs
  * Databricks LLMs
  * Microsoft LLMs
  * Salesforce models
  * Other Top performing LLMs


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Codestral API [Mistral AI]


On this page
# Codestral API [Mistral AI]
Codestral is available in select code-completion plugins but can also be queried directly. See the documentation for more details.
## API Key​
```
# env variable  
os.environ['CODESTRAL_API_KEY']  

```

## FIM / Completions​
info
Official Mistral API Docs: https://docs.mistral.ai/api/#operation/createFIMCompletion
  * No Streaming
  * Streaming


#### Sample Usage​
```
import os  
import litellm  
  
os.environ['CODESTRAL_API_KEY']  
  
response = await litellm.atext_completion(  
  model="text-completion-codestral/codestral-2405",  
  prompt="def is_odd(n): \n return n % 2 == 1 \ndef test_is_odd():",   
  suffix="return True",                       # optional  
  temperature=0,                           # optional  
  top_p=1,                              # optional  
  max_tokens=10,                           # optional  
  min_tokens=10,                           # optional  
  seed=10,                              # optional  
  stop=["return"],                          # optional  
)  

```

#### Expected Response​
```
{  
 "id": "b41e0df599f94bc1a46ea9fcdbc2aabe",  
 "object": "text_completion",  
 "created": 1589478378,  
 "model": "codestral-latest",  
 "choices": [  
  {  
   "text": "\n assert is_odd(1)\n assert",  
   "index": 0,  
   "logprobs": null,  
   "finish_reason": "length"  
  }  
 ],  
 "usage": {  
  "prompt_tokens": 5,  
  "completion_tokens": 7,  
  "total_tokens": 12  
 }  
}  
  

```

#### Sample Usage - Streaming​
```
import os  
import litellm  
  
os.environ['CODESTRAL_API_KEY']  
  
response = await litellm.atext_completion(  
  model="text-completion-codestral/codestral-2405",  
  prompt="def is_odd(n): \n return n % 2 == 1 \ndef test_is_odd():",  
  suffix="return True",  # optional  
  temperature=0,      # optional  
  top_p=1,         # optional  
  stream=True,          
  seed=10,         # optional  
  stop=["return"],     # optional  
)  
  
async for chunk in response:  
  print(chunk)  

```

#### Expected Response​
```
{  
 "id": "726025d3e2d645d09d475bb0d29e3640",  
 "object": "text_completion",  
 "created": 1718659669,  
 "choices": [  
  {  
   "text": "This",  
   "index": 0,  
   "logprobs": null,  
   "finish_reason": null  
  }  
 ],  
 "model": "codestral-2405",   
}  
  

```

### Supported Models​
All models listed here https://docs.mistral.ai/platform/endpoints are supported. We actively maintain the list of models, pricing, token window, etc. here.
Model Name| Function Call  
---|---  
Codestral Latest| `completion(model="text-completion-codestral/codestral-latest", messages)`  
Codestral 2405| `completion(model="text-completion-codestral/codestral-2405", messages)`  
## Chat Completions​
info
Official Mistral API Docs: https://docs.mistral.ai/api/#operation/createChatCompletion
  * No Streaming
  * Streaming


#### Sample Usage​
```
import os  
import litellm  
  
os.environ['CODESTRAL_API_KEY']  
  
response = await litellm.acompletion(  
  model="codestral/codestral-latest",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hey, how's it going?",  
    }  
  ],  
  temperature=0.0,    # optional  
  top_p=1,        # optional  
  max_tokens=10,     # optional  
  safe_prompt=False,   # optional  
  seed=12,        # optional  
)  

```

#### Expected Response​
```
{  
 "id": "chatcmpl-123",  
 "object": "chat.completion",  
 "created": 1677652288,  
 "model": "codestral/codestral-latest",  
 "system_fingerprint": None,  
 "choices": [{  
  "index": 0,  
  "message": {  
   "role": "assistant",  
   "content": "\n\nHello there, how may I assist you today?",  
  },  
  "logprobs": null,  
  "finish_reason": "stop"  
 }],  
 "usage": {  
  "prompt_tokens": 9,  
  "completion_tokens": 12,  
  "total_tokens": 21  
 }  
}  
  
  

```

#### Sample Usage - Streaming​
```
import os  
import litellm  
  
os.environ['CODESTRAL_API_KEY']  
  
response = await litellm.acompletion(  
  model="codestral/codestral-latest",  
  messages=[  
    {  
      "role": "user",  
      "content": "Hey, how's it going?",  
    }  
  ],  
  stream=True,      # optional  
  temperature=0.0,    # optional  
  top_p=1,        # optional  
  max_tokens=10,     # optional  
  safe_prompt=False,   # optional  
  seed=12,        # optional  
)  
async for chunk in response:  
  print(chunk)  

```

#### Expected Response​
```
{  
  "id":"chatcmpl-123",  
  "object":"chat.completion.chunk",  
  "created":1694268190,  
  "model": "codestral/codestral-latest",  
  "system_fingerprint": None,   
  "choices":[  
    {  
      "index":0,  
      "delta":{"role":"assistant","content":"gm"},  
      "logprobs":null,  
    "  finish_reason":null  
    }  
  ]  
}  
  

```

### Supported Models​
All models listed here https://docs.mistral.ai/platform/endpoints are supported. We actively maintain the list of models, pricing, token window, etc. here.
Model Name| Function Call  
---|---  
Codestral Latest| `completion(model="codestral/codestral-latest", messages)`  
Codestral 2405| `completion(model="codestral/codestral-2405", messages)`  
Previous
Mistral AI API
Next
Cohere
  * API Key
  * FIM / Completions
    * Supported Models
  * Chat Completions
    * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Custom API Server (Custom Format)


On this page
# Custom API Server (Custom Format)
Call your custom torch-serve / internal LLM APIs via LiteLLM
info
  * For calling an openai-compatible endpoint, go here
  * For modifying incoming/outgoing calls on proxy, go here


## Quick Start​
```
import litellm  
from litellm import CustomLLM, completion, get_llm_provider  
  
  
class MyCustomLLM(CustomLLM):  
  def completion(self, *args, **kwargs) -> litellm.ModelResponse:  
    return litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{"role": "user", "content": "Hello world"}],  
      mock_response="Hi!",  
    ) # type: ignore  
  
my_custom_llm = MyCustomLLM()  
  
litellm.custom_provider_map = [ # 👈 KEY STEP - REGISTER HANDLER  
    {"provider": "my-custom-llm", "custom_handler": my_custom_llm}  
  ]  
  
resp = completion(  
    model="my-custom-llm/my-fake-model",  
    messages=[{"role": "user", "content": "Hello world!"}],  
  )  
  
assert resp.choices[0].message.content == "Hi!"  

```

## OpenAI Proxy Usage​
  1. Setup your `custom_handler.py` file 


```
import litellm  
from litellm import CustomLLM, completion, get_llm_provider  
  
  
class MyCustomLLM(CustomLLM):  
  def completion(self, *args, **kwargs) -> litellm.ModelResponse:  
    return litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{"role": "user", "content": "Hello world"}],  
      mock_response="Hi!",  
    ) # type: ignore  
  
  async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:  
    return litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{"role": "user", "content": "Hello world"}],  
      mock_response="Hi!",  
    ) # type: ignore  
  
  
my_custom_llm = MyCustomLLM()  

```

  1. Add to `config.yaml`


In the config below, we pass
python_filename: `custom_handler.py` custom_handler_instance_name: `my_custom_llm`. This is defined in Step 1
custom_handler: `custom_handler.my_custom_llm`
```
model_list:  
 - model_name: "test-model"         
  litellm_params:  
   model: "openai/text-embedding-ada-002"  
 - model_name: "my-custom-model"  
  litellm_params:  
   model: "my-custom-llm/my-model"  
  
litellm_settings:  
 custom_provider_map:  
 - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}  

```

```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "my-custom-model",  
  "messages": [{"role": "user", "content": "Say \"this is a test\" in JSON!"}],  
}'  

```

Expected Response
```
{  
  "id": "chatcmpl-06f1b9cd-08bc-43f7-9814-a69173921216",  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": "Hi!",  
        "role": "assistant",  
        "tool_calls": null,  
        "function_call": null  
      }  
    }  
  ],  
  "created": 1721955063,  
  "model": "gpt-3.5-turbo",  
  "object": "chat.completion",  
  "system_fingerprint": null,  
  "usage": {  
    "prompt_tokens": 10,  
    "completion_tokens": 20,  
    "total_tokens": 30  
  }  
}  

```

## Add Streaming Support​
Here's a simple example of returning unix epoch seconds for both completion + streaming use-cases. 
s/o @Eloy Lafuente for this code example.
```
import time  
from typing import Iterator, AsyncIterator  
from litellm.types.utils import GenericStreamingChunk, ModelResponse  
from litellm import CustomLLM, completion, acompletion  
  
class UnixTimeLLM(CustomLLM):  
  def completion(self, *args, **kwargs) -> ModelResponse:  
    return completion(  
      model="test/unixtime",  
      mock_response=str(int(time.time())),  
    ) # type: ignore  
  
  async def acompletion(self, *args, **kwargs) -> ModelResponse:  
    return await acompletion(  
      model="test/unixtime",  
      mock_response=str(int(time.time())),  
    ) # type: ignore  
  
  def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:  
    generic_streaming_chunk: GenericStreamingChunk = {  
      "finish_reason": "stop",  
      "index": 0,  
      "is_finished": True,  
      "text": str(int(time.time())),  
      "tool_use": None,  
      "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},  
    }  
    return generic_streaming_chunk # type: ignore  
  
  async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:  
    generic_streaming_chunk: GenericStreamingChunk = {  
      "finish_reason": "stop",  
      "index": 0,  
      "is_finished": True,  
      "text": str(int(time.time())),  
      "tool_use": None,  
      "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},  
    }  
    yield generic_streaming_chunk # type: ignore  
  
unixtime = UnixTimeLLM()  

```

## Image Generation​
  1. Setup your `custom_handler.py` file 


```
import litellm  
from litellm import CustomLLM  
from litellm.types.utils import ImageResponse, ImageObject  
  
  
class MyCustomLLM(CustomLLM):  
  async def aimage_generation(self, model: str, prompt: str, model_response: ImageResponse, optional_params: dict, logging_obj: Any, timeout: Optional[Union[float, httpx.Timeout]] = None, client: Optional[AsyncHTTPHandler] = None,) -> ImageResponse:  
    return ImageResponse(  
      created=int(time.time()),  
      data=[ImageObject(url="https://example.com/image.png")],  
    )  
  
my_custom_llm = MyCustomLLM()  

```

  1. Add to `config.yaml`


In the config below, we pass
python_filename: `custom_handler.py` custom_handler_instance_name: `my_custom_llm`. This is defined in Step 1
custom_handler: `custom_handler.my_custom_llm`
```
model_list:  
 - model_name: "test-model"         
  litellm_params:  
   model: "openai/text-embedding-ada-002"  
 - model_name: "my-custom-model"  
  litellm_params:  
   model: "my-custom-llm/my-model"  
  
litellm_settings:  
 custom_provider_map:  
 - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}  

```

```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/v1/images/generations' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "my-custom-model",  
  "prompt": "A cute baby sea otter",  
}'  

```

Expected Response
```
{  
  "created": 1721955063,  
  "data": [{"url": "https://example.com/image.png"}],  
}  

```

## Additional Parameters​
Additional parameters are passed inside `optional_params` key in the `completion` or `image_generation` function.
Here's how to set this: 
```
import litellm  
from litellm import CustomLLM, completion, get_llm_provider  
  
  
class MyCustomLLM(CustomLLM):  
  def completion(self, *args, **kwargs) -> litellm.ModelResponse:  
    assert kwargs["optional_params"] == {"my_custom_param": "my-custom-param"} # 👈 CHECK HERE  
    return litellm.completion(  
      model="gpt-3.5-turbo",  
      messages=[{"role": "user", "content": "Hello world"}],  
      mock_response="Hi!",  
    ) # type: ignore  
  
my_custom_llm = MyCustomLLM()  
  
litellm.custom_provider_map = [ # 👈 KEY STEP - REGISTER HANDLER  
    {"provider": "my-custom-llm", "custom_handler": my_custom_llm}  
  ]  
  
resp = completion(model="my-custom-llm/my-model", my_custom_param="my-custom-param")  

```

  1. Setup your `custom_handler.py` file 


```
import litellm  
from litellm import CustomLLM  
from litellm.types.utils import ImageResponse, ImageObject  
  
  
class MyCustomLLM(CustomLLM):  
  async def aimage_generation(self, model: str, prompt: str, model_response: ImageResponse, optional_params: dict, logging_obj: Any, timeout: Optional[Union[float, httpx.Timeout]] = None, client: Optional[AsyncHTTPHandler] = None,) -> ImageResponse:  
    assert optional_params == {"my_custom_param": "my-custom-param"} # 👈 CHECK HERE  
    return ImageResponse(  
      created=int(time.time()),  
      data=[ImageObject(url="https://example.com/image.png")],  
    )  
  
my_custom_llm = MyCustomLLM()  

```

  1. Add to `config.yaml`


In the config below, we pass
python_filename: `custom_handler.py` custom_handler_instance_name: `my_custom_llm`. This is defined in Step 1
custom_handler: `custom_handler.my_custom_llm`
```
model_list:  
 - model_name: "test-model"         
  litellm_params:  
   model: "openai/text-embedding-ada-002"  
 - model_name: "my-custom-model"  
  litellm_params:  
   model: "my-custom-llm/my-model"  
   my_custom_param: "my-custom-param" # 👈 CUSTOM PARAM  
  
litellm_settings:  
 custom_provider_map:  
 - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}  

```

```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/v1/images/generations' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "my-custom-model",  
  "prompt": "A cute baby sea otter",  
}'  

```

## Custom Handler Spec​
```
from litellm.types.utils import GenericStreamingChunk, ModelResponse, ImageResponse  
from typing import Iterator, AsyncIterator, Any, Optional, Union  
from litellm.llms.base import BaseLLM  
  
class CustomLLMError(Exception): # use this for all your exceptions  
  def __init__(  
    self,  
    status_code,  
    message,  
  ):  
    self.status_code = status_code  
    self.message = message  
    super().__init__(  
      self.message  
    ) # Call the base class constructor with the parameters it needs  
  
class CustomLLM(BaseLLM):  
  def __init__(self) -> None:  
    super().__init__()  
  
  def completion(self, *args, **kwargs) -> ModelResponse:  
    raise CustomLLMError(status_code=500, message="Not implemented yet!")  
  
  def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:  
    raise CustomLLMError(status_code=500, message="Not implemented yet!")  
  
  async def acompletion(self, *args, **kwargs) -> ModelResponse:  
    raise CustomLLMError(status_code=500, message="Not implemented yet!")  
  
  async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:  
    raise CustomLLMError(status_code=500, message="Not implemented yet!")  
  
  def image_generation(  
    self,  
    model: str,  
    prompt: str,  
    model_response: ImageResponse,  
    optional_params: dict,  
    logging_obj: Any,  
    timeout: Optional[Union[float, httpx.Timeout]] = None,  
    client: Optional[HTTPHandler] = None,  
  ) -> ImageResponse:  
    raise CustomLLMError(status_code=500, message="Not implemented yet!")  
  
  async def aimage_generation(  
    self,  
    model: str,  
    prompt: str,  
    model_response: ImageResponse,  
    optional_params: dict,  
    logging_obj: Any,  
    timeout: Optional[Union[float, httpx.Timeout]] = None,  
    client: Optional[AsyncHTTPHandler] = None,  
  ) -> ImageResponse:  
    raise CustomLLMError(status_code=500, message="Not implemented yet!")  

```

Previous
Sambanova
Next
Petals
  * Quick Start
  * OpenAI Proxy Usage
  * Add Streaming Support
  * Image Generation
  * Additional Parameters
  * Custom Handler Spec


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Cohere


On this page
# Cohere
## API KEYS​
```
import os   
os.environ["COHERE_API_KEY"] = ""  

```

## Usage​
### LiteLLM Python SDK​
```
from litellm import completion  
  
## set ENV variables  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
# cohere call  
response = completion(  
  model="command-r",   
  messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

#### Streaming​
```
from litellm import completion  
  
## set ENV variables  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
# cohere call  
response = completion(  
  model="command-r",   
  messages = [{ "content": "Hello, how are you?","role": "user"}],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage with LiteLLM Proxy​
Here's how to call Cohere with the LiteLLM Proxy Server
### 1. Save key in your environment​
```
export COHERE_API_KEY="your-api-key"  

```

### 2. Start the proxy​
Define the cohere models you want to use in the config.yaml
```
model_list:  
 - model_name: command-a-03-2025   
  litellm_params:  
   model: command-a-03-2025  
   api_key: "os.environ/COHERE_API_KEY"  

```

```
litellm --config /path/to/config.yaml  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer <your-litellm-api-key>' \  
--data ' {  
   "model": "command-a-03-2025",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy  
response = client.chat.completions.create(model="command-a-03-2025", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

## Supported Models​
Model Name| Function Call  
---|---  
command-a-03-2025| `litellm.completion('command-a-03-2025', messages)`  
command-r-plus-08-2024| `litellm.completion('command-r-plus-08-2024', messages)`  
command-r-08-2024| `litellm.completion('command-r-08-2024', messages)`  
command-r-plus| `litellm.completion('command-r-plus', messages)`  
command-r| `litellm.completion('command-r', messages)`  
command-light| `litellm.completion('command-light', messages)`  
command-nightly| `litellm.completion('command-nightly', messages)`  
## Embedding​
```
from litellm import embedding  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
# cohere call  
response = embedding(  
  model="embed-english-v3.0",   
  input=["good morning from litellm", "this is another item"],   
)  

```

### Setting - Input Type for v3 models​
v3 Models have a required parameter: `input_type`. LiteLLM defaults to `search_document`. It can be one of the following four values:
  * `input_type="search_document"`: (default) Use this for texts (documents) you want to store in your vector database
  * `input_type="search_query"`: Use this for search queries to find the most relevant documents in your vector database
  * `input_type="classification"`: Use this if you use the embeddings as an input for a classification system
  * `input_type="clustering"`: Use this if you use the embeddings for text clustering


https://txt.cohere.com/introducing-embed-v3/
```
from litellm import embedding  
os.environ["COHERE_API_KEY"] = "cohere key"  
  
# cohere call  
response = embedding(  
  model="embed-english-v3.0",   
  input=["good morning from litellm", "this is another item"],   
  input_type="search_document"   
)  

```

### Supported Embedding Models​
Model Name| Function Call  
---|---  
embed-english-v3.0| `embedding(model="embed-english-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-english-light-v3.0| `embedding(model="embed-english-light-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-multilingual-v3.0| `embedding(model="embed-multilingual-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-multilingual-light-v3.0| `embedding(model="embed-multilingual-light-v3.0", input=["good morning from litellm", "this is another item"])`  
embed-english-v2.0| `embedding(model="embed-english-v2.0", input=["good morning from litellm", "this is another item"])`  
embed-english-light-v2.0| `embedding(model="embed-english-light-v2.0", input=["good morning from litellm", "this is another item"])`  
embed-multilingual-v2.0| `embedding(model="embed-multilingual-v2.0", input=["good morning from litellm", "this is another item"])`  
## Rerank​
### Usage​
LiteLLM supports the v1 and v2 clients for Cohere rerank. By default, the `rerank` endpoint uses the v2 client, but you can specify the v1 client by explicitly calling `v1/rerank`
  * LiteLLM SDK Usage
  * LiteLLM Proxy Usage


```
from litellm import rerank  
import os  
  
os.environ["COHERE_API_KEY"] = "sk-.."  
  
query = "What is the capital of the United States?"  
documents = [  
  "Carson City is the capital city of the American state of Nevada.",  
  "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
  "Washington, D.C. is the capital of the United States.",  
  "Capital punishment has existed in the United States since before it was a country.",  
]  
  
response = rerank(  
  model="cohere/rerank-english-v3.0",  
  query=query,  
  documents=documents,  
  top_n=3,  
)  
print(response)  

```

LiteLLM provides an cohere api compatible `/rerank` endpoint for Rerank calls.
**Setup**
Add this to your litellm proxy config.yaml
```
model_list:  
 - model_name: Salesforce/Llama-Rank-V1  
  litellm_params:  
   model: together_ai/Salesforce/Llama-Rank-V1  
   api_key: os.environ/TOGETHERAI_API_KEY  
 - model_name: rerank-english-v3.0  
  litellm_params:  
   model: cohere/rerank-english-v3.0  
   api_key: os.environ/COHERE_API_KEY  

```

Start litellm
```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

Test request
```
curl http://0.0.0.0:4000/rerank \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "rerank-english-v3.0",  
  "query": "What is the capital of the United States?",  
  "documents": [  
    "Carson City is the capital city of the American state of Nevada.",  
    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
    "Washington, D.C. is the capital of the United States.",  
    "Capital punishment has existed in the United States since before it was a country."  
  ],  
  "top_n": 3  
 }'  

```

Previous
Codestral API [Mistral AI]
Next
Anyscale
  * API KEYS
  * Usage
    * LiteLLM Python SDK
  * Usage with LiteLLM Proxy
    * 1. Save key in your environment
    * 2. Start the proxy
    * 3. Test it
  * Supported Models
  * Embedding
    * Setting - Input Type for v3 models
    * Supported Embedding Models
  * Rerank
    * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Databricks


On this page
# Databricks
LiteLLM supports all models on Databricks
tip
**We support ALL Databricks models, just set`model=databricks/<any-model-on-databricks>` as a prefix when sending litellm requests**
## Usage​
  * SDK
  * PROXY


### ENV VAR​
```
import os   
os.environ["DATABRICKS_API_KEY"] = ""  
os.environ["DATABRICKS_API_BASE"] = ""  

```

### Example Call​
```
from litellm import completion  
import os  
## set ENV variables  
os.environ["DATABRICKS_API_KEY"] = "databricks key"  
os.environ["DATABRICKS_API_BASE"] = "databricks base url" # e.g.: https://adb-3064715882934586.6.azuredatabricks.net/serving-endpoints  
  
# Databricks dbrx-instruct call  
response = completion(  
  model="databricks/databricks-dbrx-instruct",   
  messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

  1. Add models to your config.yaml
```
model_list:  
 - model_name: dbrx-instruct  
  litellm_params:  
   model: databricks/databricks-dbrx-instruct  
   api_key: os.environ/DATABRICKS_API_KEY  
   api_base: os.environ/DATABRICKS_API_BASE  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml --debug  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="dbrx-instruct",  
  messages = [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
 ]  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "dbrx-instruct",  
  "messages": [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
   ],  
}'  

```



## Passing additional params - max_tokens, temperature​
See all litellm.completion supported params here
```
# !pip install litellm  
from litellm import completion  
import os  
## set ENV variables  
os.environ["DATABRICKS_API_KEY"] = "databricks key"  
os.environ["DATABRICKS_API_BASE"] = "databricks api base"  
  
# databricks dbrx call  
response = completion(  
  model="databricks/databricks-dbrx-instruct",   
  messages = [{ "content": "Hello, how are you?","role": "user"}],  
  max_tokens=20,  
  temperature=0.5  
)  

```

**proxy**
```
 model_list:  
  - model_name: llama-3  
   litellm_params:  
    model: databricks/databricks-meta-llama-3-70b-instruct  
    api_key: os.environ/DATABRICKS_API_KEY  
    max_tokens: 20  
    temperature: 0.5  

```

## Usage - Thinking / `reasoning_content`​
LiteLLM translates OpenAI's `reasoning_effort` to Anthropic's `thinking` parameter. Code
reasoning_effort| thinking  
---|---  
"low"| "budget_tokens": 1024  
"medium"| "budget_tokens": 2048  
"high"| "budget_tokens": 4096  
Known Limitations:
  * Support for passing thinking blocks back to Claude Issue


  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
# set ENV variables (can also be passed in to .completion() - e.g. `api_base`, `api_key`)  
os.environ["DATABRICKS_API_KEY"] = "databricks key"  
os.environ["DATABRICKS_API_BASE"] = "databricks base url"  
  
resp = completion(  
  model="databricks/databricks-claude-3-7-sonnet",  
  messages=[{"role": "user", "content": "What is the capital of France?"}],  
  reasoning_effort="low",  
)  
  

```

  1. Setup config.yaml


```
- model_name: claude-3-7-sonnet  
 litellm_params:  
  model: databricks/databricks-claude-3-7-sonnet  
  api_key: os.environ/DATABRICKS_API_KEY  
  api_base: os.environ/DATABRICKS_API_BASE  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "claude-3-7-sonnet",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "reasoning_effort": "low"  
 }'  

```

**Expected Response**
```
ModelResponse(  
  id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',  
  created=1740470510,  
  model='claude-3-7-sonnet-20250219',  
  object='chat.completion',  
  system_fingerprint=None,  
  choices=[  
    Choices(  
      finish_reason='stop',  
      index=0,  
      message=Message(  
        content="The capital of France is Paris.",  
        role='assistant',  
        tool_calls=None,  
        function_call=None,  
        provider_specific_fields={  
          'citations': None,  
          'thinking_blocks': [  
            {  
              'type': 'thinking',  
              'thinking': 'The capital of France is Paris. This is a very straightforward factual question.',  
              'signature': 'EuYBCkQYAiJAy6...'  
            }  
          ]  
        }  
      ),  
      thinking_blocks=[  
        {  
          'type': 'thinking',  
          'thinking': 'The capital of France is Paris. This is a very straightforward factual question.',  
          'signature': 'EuYBCkQYAiJAy6AGB...'  
        }  
      ],  
      reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'  
    )  
  ],  
  usage=Usage(  
    completion_tokens=68,  
    prompt_tokens=42,  
    total_tokens=110,  
    completion_tokens_details=None,  
    prompt_tokens_details=PromptTokensDetailsWrapper(  
      audio_tokens=None,  
      cached_tokens=0,  
      text_tokens=None,  
      image_tokens=None  
    ),  
    cache_creation_input_tokens=0,  
    cache_read_input_tokens=0  
  )  
)  

```

### Pass `thinking` to Anthropic models​
You can also pass the `thinking` parameter to Anthropic models.
You can also pass the `thinking` parameter to Anthropic models.
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
# set ENV variables (can also be passed in to .completion() - e.g. `api_base`, `api_key`)  
os.environ["DATABRICKS_API_KEY"] = "databricks key"  
os.environ["DATABRICKS_API_BASE"] = "databricks base url"  
  
response = litellm.completion(  
 model="databricks/databricks-claude-3-7-sonnet",  
 messages=[{"role": "user", "content": "What is the capital of France?"}],  
 thinking={"type": "enabled", "budget_tokens": 1024},  
)  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "databricks/databricks-claude-3-7-sonnet",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "thinking": {"type": "enabled", "budget_tokens": 1024}  
 }'  

```

## Supported Databricks Chat Completion Models​
tip
**We support ALL Databricks models, just set`model=databricks/<any-model-on-databricks>` as a prefix when sending litellm requests**
Model Name| Command  
---|---  
databricks/databricks-claude-3-7-sonnet| `completion(model='databricks/databricks/databricks-claude-3-7-sonnet', messages=messages)`  
databricks-meta-llama-3-1-70b-instruct| `completion(model='databricks/databricks-meta-llama-3-1-70b-instruct', messages=messages)`  
databricks-meta-llama-3-1-405b-instruct| `completion(model='databricks/databricks-meta-llama-3-1-405b-instruct', messages=messages)`  
databricks-dbrx-instruct| `completion(model='databricks/databricks-dbrx-instruct', messages=messages)`  
databricks-meta-llama-3-70b-instruct| `completion(model='databricks/databricks-meta-llama-3-70b-instruct', messages=messages)`  
databricks-llama-2-70b-chat| `completion(model='databricks/databricks-llama-2-70b-chat', messages=messages)`  
databricks-mixtral-8x7b-instruct| `completion(model='databricks/databricks-mixtral-8x7b-instruct', messages=messages)`  
databricks-mpt-30b-instruct| `completion(model='databricks/databricks-mpt-30b-instruct', messages=messages)`  
databricks-mpt-7b-instruct| `completion(model='databricks/databricks-mpt-7b-instruct', messages=messages)`  
## Embedding Models​
### Passing Databricks specific params - 'instruction'​
For embedding models, databricks lets you pass in an additional param 'instruction'. Full Spec
```
# !pip install litellm  
from litellm import embedding  
import os  
## set ENV variables  
os.environ["DATABRICKS_API_KEY"] = "databricks key"  
os.environ["DATABRICKS_API_BASE"] = "databricks url"  
  
# Databricks bge-large-en call  
response = litellm.embedding(  
   model="databricks/databricks-bge-large-en",  
   input=["good morning from litellm"],  
   instruction="Represent this sentence for searching relevant passages:",  
 )  

```

**proxy**
```
 model_list:  
  - model_name: bge-large  
   litellm_params:  
    model: databricks/databricks-bge-large-en  
    api_key: os.environ/DATABRICKS_API_KEY  
    api_base: os.environ/DATABRICKS_API_BASE  
    instruction: "Represent this sentence for searching relevant passages:"  

```

## Supported Databricks Embedding Models​
tip
**We support ALL Databricks models, just set`model=databricks/<any-model-on-databricks>` as a prefix when sending litellm requests**
Model Name| Command  
---|---  
databricks-bge-large-en| `embedding(model='databricks/databricks-bge-large-en', messages=messages)`  
databricks-gte-large-en| `embedding(model='databricks/databricks-gte-large-en', messages=messages)`  
Previous
Hugging Face
Next
Deepgram
  * Usage
    * ENV VAR
    * Example Call
  * Passing additional params - max_tokens, temperature
  * Usage - Thinking / `reasoning_content`
    * Pass `thinking` to Anthropic models
  * Supported Databricks Chat Completion Models
  * Embedding Models
    * Passing Databricks specific params - 'instruction'
  * Supported Databricks Embedding Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * DeepInfra


On this page
# DeepInfra
https://deepinfra.com/
tip
**We support ALL DeepInfra models, just set`model=deepinfra/<any-model-on-deepinfra>` as a prefix when sending litellm requests**
## API Key​
```
# env variable  
os.environ['DEEPINFRA_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['DEEPINFRA_API_KEY'] = ""  
response = completion(  
  model="deepinfra/meta-llama/Llama-2-70b-chat-hf",   
  messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}]  
)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['DEEPINFRA_API_KEY'] = ""  
response = completion(  
  model="deepinfra/meta-llama/Llama-2-70b-chat-hf",   
  messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Chat Models​
Model Name| Function Call  
---|---  
meta-llama/Meta-Llama-3-8B-Instruct| `completion(model="deepinfra/meta-llama/Meta-Llama-3-8B-Instruct", messages)`  
meta-llama/Meta-Llama-3-70B-Instruct| `completion(model="deepinfra/meta-llama/Meta-Llama-3-70B-Instruct", messages)`  
meta-llama/Llama-2-70b-chat-hf| `completion(model="deepinfra/meta-llama/Llama-2-70b-chat-hf", messages)`  
meta-llama/Llama-2-7b-chat-hf| `completion(model="deepinfra/meta-llama/Llama-2-7b-chat-hf", messages)`  
meta-llama/Llama-2-13b-chat-hf| `completion(model="deepinfra/meta-llama/Llama-2-13b-chat-hf", messages)`  
codellama/CodeLlama-34b-Instruct-hf| `completion(model="deepinfra/codellama/CodeLlama-34b-Instruct-hf", messages)`  
mistralai/Mistral-7B-Instruct-v0.1| `completion(model="deepinfra/mistralai/Mistral-7B-Instruct-v0.1", messages)`  
jondurbin/airoboros-l2-70b-gpt4-1.4.1| `completion(model="deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1", messages)`  
Previous
Cloudflare Workers AI
Next
AI21
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Chat Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Galadriel


On this page
# Galadriel
https://docs.galadriel.com/api-reference/chat-completion-API
LiteLLM supports all models on Galadriel.
## API Key​
```
import os   
os.environ['GALADRIEL_API_KEY'] = "your-api-key"  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['GALADRIEL_API_KEY'] = ""  
response = completion(  
  model="galadriel/llama3.1",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['GALADRIEL_API_KEY'] = ""  
response = completion(  
  model="galadriel/llama3.1",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Supported Models​
### Serverless Endpoints​
We support ALL Galadriel AI models, just set `galadriel/` as a prefix when sending completion requests
We support both the complete model name and the simplified name match. 
You can specify the model name either with the full name or with a simplified version e.g. `llama3.1:70b`
Model Name| Simplified Name| Function Call  
---|---|---  
neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8| llama3.1 or llama3.1:8b| `completion(model="galadriel/llama3.1", messages)`  
neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16| llama3.1:70b| `completion(model="galadriel/llama3.1:70b", messages)`  
neuralmagic/Meta-Llama-3.1-405B-Instruct-quantized.w4a16| llama3.1:405b| `completion(model="galadriel/llama3.1:405b", messages)`  
neuralmagic/Mistral-Nemo-Instruct-2407-quantized.w4a16| mistral-nemo or mistral-nemo:12b| `completion(model="galadriel/mistral-nemo", messages)`  
Previous
FriendliAI
Next
Topaz
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Supported Models
    * Serverless Endpoints


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * FriendliAI


On this page
# FriendliAI
info
**We support ALL FriendliAI models, just set`friendliai/` as a prefix when sending completion requests**
Property| Details  
---|---  
Description| The fastest and most efficient inference engine to build production-ready, compound AI systems.  
Provider Route on LiteLLM| `friendliai/`  
Provider Doc| FriendliAI ↗  
Supported OpenAI Endpoints| `/chat/completions`, `/completions`  
## API Key​
```
# env variable  
os.environ['FRIENDLI_TOKEN']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['FRIENDLI_TOKEN'] = ""  
response = completion(  
  model="friendliai/meta-llama-3.1-8b-instruct",  
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['FRIENDLI_TOKEN'] = ""  
response = completion(  
  model="friendliai/meta-llama-3.1-8b-instruct",  
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Supported Models​
We support ALL FriendliAI AI models, just set `friendliai/` as a prefix when sending completion requests
Model Name| Function Call  
---|---  
meta-llama-3.1-8b-instruct| `completion(model="friendliai/meta-llama-3.1-8b-instruct", messages)`  
meta-llama-3.1-70b-instruct| `completion(model="friendliai/meta-llama-3.1-70b-instruct", messages)`  
Previous
Perplexity AI (pplx-api)
Next
Galadriel
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
      * Gemini - Google AI Studio
      * [BETA] Google AI Studio (Gemini) Files API
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Google AI Studio
  * Gemini - Google AI Studio


On this page
# Gemini - Google AI Studio
Property| Details  
---|---  
Description| Google AI Studio is a fully-managed AI development platform for building and using generative AI.  
Provider Route on LiteLLM| `gemini/`  
Provider Doc| Google AI Studio ↗  
API Endpoint for Provider| https://generativelanguage.googleapis.com  
Supported OpenAI Endpoints| `/chat/completions`, `/embeddings`, `/completions`  
Pass-through Endpoint| Supported  
  

## API Keys​
```
import os  
os.environ["GEMINI_API_KEY"] = "your-api-key"  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['GEMINI_API_KEY'] = ""  
response = completion(  
  model="gemini/gemini-pro",   
  messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}]  
)  

```

## Supported OpenAI Params​
  * temperature
  * top_p
  * max_tokens
  * max_completion_tokens
  * stream
  * tools
  * tool_choice
  * functions
  * response_format
  * n
  * stop
  * logprobs
  * frequency_penalty
  * modalities
  * reasoning_content


**Anthropic Params**
  * thinking (used to set max budget tokens across anthropic/gemini models)


**See Updated List**
## Usage - Thinking / `reasoning_content`​
LiteLLM translates OpenAI's `reasoning_effort` to Gemini's `thinking` parameter. Code
**Mapping**
reasoning_effort| thinking  
---|---  
"low"| "budget_tokens": 1024  
"medium"| "budget_tokens": 2048  
"high"| "budget_tokens": 4096  
  * SDK
  * PROXY


```
from litellm import completion  
  
resp = completion(  
  model="gemini/gemini-2.5-flash-preview-04-17",  
  messages=[{"role": "user", "content": "What is the capital of France?"}],  
  reasoning_effort="low",  
)  
  

```

  1. Setup config.yaml


```
- model_name: gemini-2.5-flash  
 litellm_params:  
  model: gemini/gemini-2.5-flash-preview-04-17  
  api_key: os.environ/GEMINI_API_KEY  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "gemini-2.5-flash",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "reasoning_effort": "low"  
 }'  

```

**Expected Response**
```
ModelResponse(  
  id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',  
  created=1740470510,  
  model='claude-3-7-sonnet-20250219',  
  object='chat.completion',  
  system_fingerprint=None,  
  choices=[  
    Choices(  
      finish_reason='stop',  
      index=0,  
      message=Message(  
        content="The capital of France is Paris.",  
        role='assistant',  
        tool_calls=None,  
        function_call=None,  
        reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'  
      ),  
    )  
  ],  
  usage=Usage(  
    completion_tokens=68,  
    prompt_tokens=42,  
    total_tokens=110,  
    completion_tokens_details=None,  
    prompt_tokens_details=PromptTokensDetailsWrapper(  
      audio_tokens=None,  
      cached_tokens=0,  
      text_tokens=None,  
      image_tokens=None  
    ),  
    cache_creation_input_tokens=0,  
    cache_read_input_tokens=0  
  )  
)  

```

### Pass `thinking` to Gemini models​
You can also pass the `thinking` parameter to Gemini models.
This is translated to Gemini's `thinkingConfig` parameter.
  * SDK
  * PROXY


```
response = litellm.completion(  
 model="gemini/gemini-2.5-flash-preview-04-17",  
 messages=[{"role": "user", "content": "What is the capital of France?"}],  
 thinking={"type": "enabled", "budget_tokens": 1024},  
)  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "gemini/gemini-2.5-flash-preview-04-17",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "thinking": {"type": "enabled", "budget_tokens": 1024}  
 }'  

```

## Passing Gemini Specific Params​
### Response schema​
LiteLLM supports sending `response_schema` as a param for Gemini-1.5-Pro on Google AI Studio. 
**Response Schema**
  * SDK
  * PROXY


```
from litellm import completion   
import json   
import os   
  
os.environ['GEMINI_API_KEY'] = ""  
  
messages = [  
  {  
    "role": "user",  
    "content": "List 5 popular cookie recipes."  
  }  
]  
  
response_schema = {  
    "type": "array",  
    "items": {  
      "type": "object",  
      "properties": {  
        "recipe_name": {  
          "type": "string",  
        },  
      },  
      "required": ["recipe_name"],  
    },  
  }  
  
  
completion(  
  model="gemini/gemini-1.5-pro",   
  messages=messages,   
  response_format={"type": "json_object", "response_schema": response_schema} # 👈 KEY CHANGE  
  )  
  
print(json.loads(completion.choices[0].message.content))  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: gemini-pro  
  litellm_params:  
   model: gemini/gemini-1.5-pro  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-pro",  
 "messages": [  
    {"role": "user", "content": "List 5 popular cookie recipes."}  
  ],  
 "response_format": {"type": "json_object", "response_schema": {   
    "type": "array",  
    "items": {  
      "type": "object",  
      "properties": {  
        "recipe_name": {  
          "type": "string",  
        },  
      },  
      "required": ["recipe_name"],  
    },  
  }}  
}  
'  

```

**Validate Schema**
To validate the response_schema, set `enforce_validation: true`.
  * SDK
  * PROXY


```
from litellm import completion, JSONSchemaValidationError  
try:   
  completion(  
  model="gemini/gemini-1.5-pro",   
  messages=messages,   
  response_format={  
    "type": "json_object",   
    "response_schema": response_schema,  
    "enforce_validation": true # 👈 KEY CHANGE  
  }  
  )  
except JSONSchemaValidationError as e:   
  print("Raw Response: {}".format(e.raw_response))  
  raise e  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: gemini-pro  
  litellm_params:  
   model: gemini/gemini-1.5-pro  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-pro",  
 "messages": [  
    {"role": "user", "content": "List 5 popular cookie recipes."}  
  ],  
 "response_format": {"type": "json_object", "response_schema": {   
    "type": "array",  
    "items": {  
      "type": "object",  
      "properties": {  
        "recipe_name": {  
          "type": "string",  
        },  
      },  
      "required": ["recipe_name"],  
    },  
  },   
  "enforce_validation": true  
  }  
}  
'  

```

LiteLLM will validate the response against the schema, and raise a `JSONSchemaValidationError` if the response does not match the schema. 
JSONSchemaValidationError inherits from `openai.APIError`
Access the raw response with `e.raw_response`
### GenerationConfig Params​
To pass additional GenerationConfig params - e.g. `topK`, just pass it in the request body of the call, and LiteLLM will pass it straight through as a key-value pair in the request body. 
**See Gemini GenerationConfigParams**
  * SDK
  * PROXY


```
from litellm import completion   
import json   
import os   
  
os.environ['GEMINI_API_KEY'] = ""  
  
messages = [  
  {  
    "role": "user",  
    "content": "List 5 popular cookie recipes."  
  }  
]  
  
completion(  
  model="gemini/gemini-1.5-pro",   
  messages=messages,   
  topK=1 # 👈 KEY CHANGE  
)  
  
print(json.loads(completion.choices[0].message.content))  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: gemini-pro  
  litellm_params:  
   model: gemini/gemini-1.5-pro  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-pro",  
 "messages": [  
    {"role": "user", "content": "List 5 popular cookie recipes."}  
  ],  
 "topK": 1 # 👈 KEY CHANGE  
}  
'  

```

**Validate Schema**
To validate the response_schema, set `enforce_validation: true`.
  * SDK
  * PROXY


```
from litellm import completion, JSONSchemaValidationError  
try:   
  completion(  
  model="gemini/gemini-1.5-pro",   
  messages=messages,   
  response_format={  
    "type": "json_object",   
    "response_schema": response_schema,  
    "enforce_validation": true # 👈 KEY CHANGE  
  }  
  )  
except JSONSchemaValidationError as e:   
  print("Raw Response: {}".format(e.raw_response))  
  raise e  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: gemini-pro  
  litellm_params:  
   model: gemini/gemini-1.5-pro  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-pro",  
 "messages": [  
    {"role": "user", "content": "List 5 popular cookie recipes."}  
  ],  
 "response_format": {"type": "json_object", "response_schema": {   
    "type": "array",  
    "items": {  
      "type": "object",  
      "properties": {  
        "recipe_name": {  
          "type": "string",  
        },  
      },  
      "required": ["recipe_name"],  
    },  
  },   
  "enforce_validation": true  
  }  
}  
'  

```

## Specifying Safety Settings​
In certain use-cases you may need to make calls to the models and pass safety settings different from the defaults. To do so, simple pass the `safety_settings` argument to `completion` or `acompletion`. For example:
```
response = completion(  
  model="gemini/gemini-pro",   
  messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}],  
  safety_settings=[  
    {  
      "category": "HARM_CATEGORY_HARASSMENT",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_HATE_SPEECH",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_DANGEROUS_CONTENT",  
      "threshold": "BLOCK_NONE",  
    },  
  ]  
)  

```

## Tool Calling​
```
from litellm import completion  
import os  
# set env  
os.environ["GEMINI_API_KEY"] = ".."  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  
response = completion(  
  model="gemini/gemini-1.5-flash",  
  messages=messages,  
  tools=tools,  
)  
# Add any assertions, here to check response args  
print(response)  
assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)  
assert isinstance(  
  response.choices[0].message.tool_calls[0].function.arguments, str  
)  
  
  

```

### Google Search Tool​
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
os.environ["GEMINI_API_KEY"] = ".."  
  
tools = [{"googleSearch": {}}] # 👈 ADD GOOGLE SEARCH  
  
response = completion(  
  model="gemini/gemini-2.0-flash",  
  messages=[{"role": "user", "content": "What is the weather in San Francisco?"}],  
  tools=tools,  
)  
  
print(response)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gemini-2.0-flash  
  litellm_params:  
   model: gemini/gemini-2.0-flash  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-2.0-flash",  
 "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],  
 "tools": [{"googleSearch": {}}]  
}  
'  

```

### Google Search Retrieval​
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
os.environ["GEMINI_API_KEY"] = ".."  
  
tools = [{"googleSearch": {}}] # 👈 ADD GOOGLE SEARCH  
  
response = completion(  
  model="gemini/gemini-2.0-flash",  
  messages=[{"role": "user", "content": "What is the weather in San Francisco?"}],  
  tools=tools,  
)  
  
print(response)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gemini-2.0-flash  
  litellm_params:  
   model: gemini/gemini-2.0-flash  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-2.0-flash",  
 "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],  
 "tools": [{"googleSearch": {}}]  
}  
'  

```

### Code Execution Tool​
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
os.environ["GEMINI_API_KEY"] = ".."  
  
tools = [{"codeExecution": {}}] # 👈 ADD GOOGLE SEARCH  
  
response = completion(  
  model="gemini/gemini-2.0-flash",  
  messages=[{"role": "user", "content": "What is the weather in San Francisco?"}],  
  tools=tools,  
)  
  
print(response)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gemini-2.0-flash  
  litellm_params:  
   model: gemini/gemini-2.0-flash  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-2.0-flash",  
 "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],  
 "tools": [{"codeExecution": {}}]  
}  
'  

```

## JSON Mode​
  * SDK
  * PROXY


```
from litellm import completion   
import json   
import os   
  
os.environ['GEMINI_API_KEY'] = ""  
  
messages = [  
  {  
    "role": "user",  
    "content": "List 5 popular cookie recipes."  
  }  
]  
  
  
  
completion(  
  model="gemini/gemini-1.5-pro",   
  messages=messages,   
  response_format={"type": "json_object"} # 👈 KEY CHANGE  
)  
  
print(json.loads(completion.choices[0].message.content))  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: gemini-pro  
  litellm_params:  
   model: gemini/gemini-1.5-pro  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
 "model": "gemini-pro",  
 "messages": [  
    {"role": "user", "content": "List 5 popular cookie recipes."}  
  ],  
 "response_format": {"type": "json_object"}  
}  
'  

```

# Gemini-Pro-Vision LiteLLM Supports the following image types passed in `url` - Images with direct links - https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg - Image in local storage - ./localimage.jpeg
## Sample Usage​
```
import os  
import litellm  
from dotenv import load_dotenv  
  
# Load the environment variables from .env file  
load_dotenv()  
os.environ["GEMINI_API_KEY"] = os.getenv('GEMINI_API_KEY')  
  
prompt = 'Describe the image in a few sentences.'  
# Note: You can pass here the URL or Path of image directly.  
image_url = 'https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg'  
  
# Create the messages payload according to the documentation  
messages = [  
  {  
    "role": "user",  
    "content": [  
      {  
        "type": "text",  
        "text": prompt  
      },  
      {  
        "type": "image_url",  
        "image_url": {"url": image_url}  
      }  
    ]  
  }  
]  
  
# Make the API call to Gemini model  
response = litellm.completion(  
  model="gemini/gemini-pro-vision",  
  messages=messages,  
)  
  
# Extract the response content  
content = response.get('choices', [{}])[0].get('message', {}).get('content')  
  
# Print the result  
print(content)  

```

## Usage - PDF / Videos / etc. Files​
### Inline Data (e.g. audio stream)​
LiteLLM follows the OpenAI format and accepts sending inline data as an encoded base64 string. 
The format to follow is 
```
data:<mime_type>;base64,<encoded_data>  

```

**LITELLM CALL**
```
import litellm  
from pathlib import Path  
import base64  
import os  
  
os.environ["GEMINI_API_KEY"] = ""   
  
litellm.set_verbose = True # 👈 See Raw call   
  
audio_bytes = Path("speech_vertex.mp3").read_bytes()  
encoded_data = base64.b64encode(audio_bytes).decode("utf-8")  
print("Audio Bytes = {}".format(audio_bytes))  
model = "gemini/gemini-1.5-flash"  
response = litellm.completion(  
  model=model,  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "Please summarize the audio."},  
        {  
          "type": "file",  
          "file": {  
            "file_data": "data:audio/mp3;base64,{}".format(encoded_data), # 👈 SET MIME_TYPE + DATA  
          }  
        },  
      ],  
    }  
  ],  
)  

```

**Equivalent GOOGLE API CALL**
```
# Initialize a Gemini model appropriate for your use case.  
model = genai.GenerativeModel('models/gemini-1.5-flash')  
  
# Create the prompt.  
prompt = "Please summarize the audio."  
  
# Load the samplesmall.mp3 file into a Python Blob object containing the audio  
# file's bytes and then pass the prompt and the audio to Gemini.  
response = model.generate_content([  
  prompt,  
  {  
    "mime_type": "audio/mp3",  
    "data": pathlib.Path('samplesmall.mp3').read_bytes()  
  }  
])  
  
# Output Gemini's response to the prompt and the inline audio.  
print(response.text)  

```

### https:// file​
```
import litellm  
import os  
  
os.environ["GEMINI_API_KEY"] = ""   
  
litellm.set_verbose = True # 👈 See Raw call   
  
model = "gemini/gemini-1.5-flash"  
response = litellm.completion(  
  model=model,  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "Please summarize the file."},  
        {  
          "type": "file",  
          "file": {  
            "file_id": "https://storage...", # 👈 SET THE IMG URL  
            "format": "application/pdf" # OPTIONAL  
          }  
        },  
      ],  
    }  
  ],  
)  

```

### gs:// file​
```
import litellm  
import os  
  
os.environ["GEMINI_API_KEY"] = ""   
  
litellm.set_verbose = True # 👈 See Raw call   
  
model = "gemini/gemini-1.5-flash"  
response = litellm.completion(  
  model=model,  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "Please summarize the file."},  
        {  
          "type": "file",  
          "file": {  
            "file_id": "gs://storage...", # 👈 SET THE IMG URL  
            "format": "application/pdf" # OPTIONAL  
          }  
        },  
      ],  
    }  
  ],  
)  

```

## Chat Models​
tip
**We support ALL Gemini models, just set`model=gemini/<any-model-on-gemini>` as a prefix when sending litellm requests**
Model Name| Function Call| Required OS Variables  
---|---|---  
gemini-pro| `completion(model='gemini/gemini-pro', messages)`| `os.environ['GEMINI_API_KEY']`  
gemini-1.5-pro-latest| `completion(model='gemini/gemini-1.5-pro-latest', messages)`| `os.environ['GEMINI_API_KEY']`  
gemini-2.0-flash| `completion(model='gemini/gemini-2.0-flash', messages)`| `os.environ['GEMINI_API_KEY']`  
gemini-2.0-flash-exp| `completion(model='gemini/gemini-2.0-flash-exp', messages)`| `os.environ['GEMINI_API_KEY']`  
gemini-2.0-flash-lite-preview-02-05| `completion(model='gemini/gemini-2.0-flash-lite-preview-02-05', messages)`| `os.environ['GEMINI_API_KEY']`  
## Context Caching​
Use Google AI Studio context caching is supported by
```
{  
  {  
    "role": "system",  
    "content": ...,  
    "cache_control": {"type": "ephemeral"} # 👈 KEY CHANGE  
  },  
  ...  
}  

```

in your message content block.
### Architecture Diagram​
**Notes:**
  * Relevant code
  * Gemini Context Caching only allows 1 block of continuous messages to be cached. 
  * If multiple non-continuous blocks contain `cache_control` - the first continuous block will be used. (sent to `/cachedContent` in the Gemini format)


  * The raw request to Gemini's `/generateContent` endpoint looks like this: 


```
curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-001:generateContent?key=$GOOGLE_API_KEY" \  
-H 'Content-Type: application/json' \  
-d '{  
   "contents": [  
    {  
     "parts":[{  
      "text": "Please summarize this transcript"  
     }],  
     "role": "user"  
    },  
   ],  
   "cachedContent": "'$CACHE_NAME'"  
  }'  
  

```

### Example Usage​
  * SDK
  * PROXY


```
from litellm import completion   
  
for _ in range(2):   
  resp = completion(  
    model="gemini/gemini-1.5-pro",  
    messages=[  
    # System Message  
      {  
        "role": "system",  
        "content": [  
          {  
            "type": "text",  
            "text": "Here is the full text of a complex legal agreement" * 4000,  
            "cache_control": {"type": "ephemeral"}, # 👈 KEY CHANGE  
          }  
        ],  
      },  
      # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.  
      {  
        "role": "user",  
        "content": [  
          {  
            "type": "text",  
            "text": "What are the key terms and conditions in this agreement?",  
            "cache_control": {"type": "ephemeral"},  
          }  
        ],  
      }]  
  )  
  
  print(resp.usage) # 👈 2nd usage block will be less, since cached tokens used  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: gemini-1.5-pro  
   litellm_params:  
    model: gemini/gemini-1.5-pro  
    api_key: os.environ/GEMINI_API_KEY  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


**See Langchain, OpenAI JS, Llamaindex, etc. examples**
  * Curl
  * OpenAI Python SDK


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gemini-1.5-pro",  
  "messages": [  
    # System Message  
      {  
        "role": "system",  
        "content": [  
          {  
            "type": "text",  
            "text": "Here is the full text of a complex legal agreement" * 4000,  
            "cache_control": {"type": "ephemeral"}, # 👈 KEY CHANGE  
          }  
        ],  
      },  
      # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.  
      {  
        "role": "user",  
        "content": [  
          {  
            "type": "text",  
            "text": "What are the key terms and conditions in this agreement?",  
            "cache_control": {"type": "ephemeral"},  
          }  
        ],  
      }],  
}'  

```

```
import openai  
client = openai.AsyncOpenAI(  
  api_key="anything",      # litellm proxy api key  
  base_url="http://0.0.0.0:4000" # litellm proxy base url  
)  
  
  
response = await client.chat.completions.create(  
  model="gemini-1.5-pro",  
  messages=[  
    {  
      "role": "system",  
      "content": [  
          {  
            "type": "text",  
            "text": "Here is the full text of a complex legal agreement" * 4000,  
            "cache_control": {"type": "ephemeral"}, # 👈 KEY CHANGE  
          }  
      ],  
    },  
    {  
      "role": "user",  
      "content": "what are the key terms and conditions in this agreement?",  
    },  
  ]  
)  
  

```

## Image Generation​
  * SDK
  * PROXY


```
from litellm import completion   
  
response = completion(  
  model="gemini/gemini-2.0-flash-exp-image-generation",  
  messages=[{"role": "user", "content": "Generate an image of a cat"}],  
  modalities=["image", "text"],  
)  
assert response.choices[0].message.content is not None # "data:image/png;base64,e4rr.."  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: gemini-2.0-flash-exp-image-generation  
  litellm_params:  
   model: gemini/gemini-2.0-flash-exp-image-generation  
   api_key: os.environ/GEMINI_API_KEY  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it!


```
curl -L -X POST 'http://localhost:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "gemini-2.0-flash-exp-image-generation",  
  "messages": [{"role": "user", "content": "Generate an image of a cat"}],  
  "modalities": ["image", "text"]  
}'  

```

Previous
VertexAI [Anthropic, Gemini, Model Garden]
Next
[BETA] Google AI Studio (Gemini) Files API
  * API Keys
  * Sample Usage
  * Supported OpenAI Params
  * Usage - Thinking / `reasoning_content`
    * Pass `thinking` to Gemini models
  * Passing Gemini Specific Params
    * Response schema
    * GenerationConfig Params
  * Specifying Safety Settings
  * Tool Calling
    * Google Search Tool
    * Google Search Retrieval
    * Code Execution Tool
  * JSON Mode
  * Sample Usage
  * Usage - PDF / Videos / etc. Files
    * Inline Data (e.g. audio stream)
    * https:// file
    * gs:// file
  * Chat Models
  * Context Caching
    * Architecture Diagram
    * Example Usage
  * Image Generation


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Deepseek


On this page
# Deepseek
https://deepseek.com/
**We support ALL Deepseek models, just set`deepseek/` as a prefix when sending completion requests**
## API Key​
```
# env variable  
os.environ['DEEPSEEK_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['DEEPSEEK_API_KEY'] = ""  
response = completion(  
  model="deepseek/deepseek-chat",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['DEEPSEEK_API_KEY'] = ""  
response = completion(  
  model="deepseek/deepseek-chat",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Supported Models - ALL Deepseek Models Supported!​
We support ALL Deepseek models, just set `deepseek/` as a prefix when sending completion requests
Model Name| Function Call  
---|---  
deepseek-chat| `completion(model="deepseek/deepseek-chat", messages)`  
deepseek-coder| `completion(model="deepseek/deepseek-coder", messages)`  
## Reasoning Models​
Model Name| Function Call  
---|---  
deepseek-reasoner| `completion(model="deepseek/deepseek-reasoner", messages)`  
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
os.environ['DEEPSEEK_API_KEY'] = ""  
resp = completion(  
  model="deepseek/deepseek-reasoner",  
  messages=[{"role": "user", "content": "Tell me a joke."}],  
)  
  
print(  
  resp.choices[0].message.reasoning_content  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: deepseek-reasoner  
  litellm_params:  
    model: deepseek/deepseek-reasoner  
    api_key: os.environ/DEEPSEEK_API_KEY  

```

  1. Run proxy


```
python litellm/proxy/main.py  

```

  1. Test it!


```
curl -L -X POST 'http://0.0.0.0:4000/v1/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "deepseek-reasoner",  
  "messages": [  
   {  
    "role": "user",  
    "content": [  
     {  
      "type": "text",  
      "text": "Hi, how are you ?"  
     }  
    ]  
   }  
  ]  
}'  

```

Previous
🆕 Github
Next
Fireworks AI
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Supported Models - ALL Deepseek Models Supported!
  * Reasoning Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Fireworks AI


On this page
# Fireworks AI
info
**We support ALL Fireworks AI models, just set`fireworks_ai/` as a prefix when sending completion requests**
Property| Details  
---|---  
Description| The fastest and most efficient inference engine to build production-ready, compound AI systems.  
Provider Route on LiteLLM| `fireworks_ai/`  
Provider Doc| Fireworks AI ↗  
Supported OpenAI Endpoints| `/chat/completions`, `/embeddings`, `/completions`, `/audio/transcriptions`  
## Overview​
This guide explains how to integrate LiteLLM with Fireworks AI. You can connect to Fireworks AI in three main ways:
  1. **Using Fireworks AI serverless models** – Easy connection to Fireworks-managed models.
  2. **Connecting to a model in your own Fireworks account** – Access models that are hosted within your Fireworks account.
  3. **Connecting via a direct-route deployment** – A more flexible, customizable connection to a specific Fireworks instance.


## API Key​
```
# env variable  
os.environ['FIREWORKS_AI_API_KEY']  

```

## Sample Usage - Serverless Models​
```
from litellm import completion  
import os  
  
os.environ['FIREWORKS_AI_API_KEY'] = ""  
response = completion(  
  model="fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Serverless Models - Streaming​
```
from litellm import completion  
import os  
  
os.environ['FIREWORKS_AI_API_KEY'] = ""  
response = completion(  
  model="fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Sample Usage - Models in Your Own Fireworks Account​
```
from litellm import completion  
import os  
  
os.environ['FIREWORKS_AI_API_KEY'] = ""  
response = completion(  
  model="fireworks_ai/accounts/fireworks/models/YOUR_MODEL_ID",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Direct-Route Deployment​
```
from litellm import completion  
import os  
  
os.environ['FIREWORKS_AI_API_KEY'] = "YOUR_DIRECT_API_KEY"  
response = completion(  
  model="fireworks_ai/accounts/fireworks/models/qwen2p5-coder-7b#accounts/gitlab/deployments/2fb7764c",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  api_base="https://gitlab-2fb7764c.direct.fireworks.ai/v1"  
)  
print(response)  

```

> **Note:** The above is for the chat interface, if you want to use the text completion interface it's model="text-completion-openai/accounts/fireworks/models/qwen2p5-coder-7b#accounts/gitlab/deployments/2fb7764c"
## Usage with LiteLLM Proxy​
### 1. Set Fireworks AI Models on config.yaml​
```
model_list:  
 - model_name: fireworks-llama-v3-70b-instruct  
  litellm_params:  
   model: fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct  
   api_key: "os.environ/FIREWORKS_AI_API_KEY"  

```

### 2. Start Proxy​
```
litellm --config config.yaml  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "fireworks-llama-v3-70b-instruct",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="fireworks-llama-v3-70b-instruct", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "fireworks-llama-v3-70b-instruct",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Document Inlining​
LiteLLM supports document inlining for Fireworks AI models. This is useful for models that are not vision models, but still need to parse documents/images/etc.
LiteLLM will add `#transform=inline` to the url of the image_url, if the model is not a vision model.**See Code**
  * SDK
  * PROXY


```
from litellm import completion  
import os  
  
os.environ["FIREWORKS_AI_API_KEY"] = "YOUR_API_KEY"  
os.environ["FIREWORKS_AI_API_BASE"] = "https://audio-prod.us-virginia-1.direct.fireworks.ai/v1"  
  
completion = litellm.completion(  
  model="fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "https://storage.googleapis.com/fireworks-public/test/sample_resume.pdf"  
          },  
        },  
        {  
          "type": "text",  
          "text": "What are the candidate's BA and MBA GPAs?",  
        },  
      ],  
    }  
  ],  
)  
print(completion)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: llama-v3p3-70b-instruct  
  litellm_params:  
   model: fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct  
   api_key: os.environ/FIREWORKS_AI_API_KEY  
  #  api_base: os.environ/FIREWORKS_AI_API_BASE [OPTIONAL], defaults to "https://api.fireworks.ai/inference/v1"  

```

  1. Start Proxy


```
litellm --config config.yaml  

```

  1. Test it


```
curl -L -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer YOUR_API_KEY' \  
-d '{"model": "llama-v3p3-70b-instruct",   
  "messages": [      
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "https://storage.googleapis.com/fireworks-public/test/sample_resume.pdf"  
          },  
        },  
        {  
          "type": "text",  
          "text": "What are the candidate's BA and MBA GPAs?",  
        },  
      ],  
    }  
  ]}'  

```

### Disable Auto-add​
If you want to disable the auto-add of `#transform=inline` to the url of the image_url, you can set the `auto_add_transform_inline` to `False` in the `FireworksAIConfig` class.
  * SDK
  * PROXY


```
litellm.disable_add_transform_inline_image_block = True  

```

```
litellm_settings:  
  disable_add_transform_inline_image_block: true  

```

## Supported Models - ALL Fireworks AI Models Supported!​
info
We support ALL Fireworks AI models, just set `fireworks_ai/` as a prefix when sending completion requests
Model Name| Function Call  
---|---  
llama-v3p2-1b-instruct| `completion(model="fireworks_ai/llama-v3p2-1b-instruct", messages)`  
llama-v3p2-3b-instruct| `completion(model="fireworks_ai/llama-v3p2-3b-instruct", messages)`  
llama-v3p2-11b-vision-instruct| `completion(model="fireworks_ai/llama-v3p2-11b-vision-instruct", messages)`  
llama-v3p2-90b-vision-instruct| `completion(model="fireworks_ai/llama-v3p2-90b-vision-instruct", messages)`  
mixtral-8x7b-instruct| `completion(model="fireworks_ai/mixtral-8x7b-instruct", messages)`  
firefunction-v1| `completion(model="fireworks_ai/firefunction-v1", messages)`  
llama-v2-70b-chat| `completion(model="fireworks_ai/llama-v2-70b-chat", messages)`  
## Supported Embedding Models​
info
We support ALL Fireworks AI models, just set `fireworks_ai/` as a prefix when sending embedding requests
Model Name| Function Call  
---|---  
fireworks_ai/nomic-ai/nomic-embed-text-v1.5| `response = litellm.embedding(model="fireworks_ai/nomic-ai/nomic-embed-text-v1.5", input=input_text)`  
fireworks_ai/nomic-ai/nomic-embed-text-v1| `response = litellm.embedding(model="fireworks_ai/nomic-ai/nomic-embed-text-v1", input=input_text)`  
fireworks_ai/WhereIsAI/UAE-Large-V1| `response = litellm.embedding(model="fireworks_ai/WhereIsAI/UAE-Large-V1", input=input_text)`  
fireworks_ai/thenlper/gte-large| `response = litellm.embedding(model="fireworks_ai/thenlper/gte-large", input=input_text)`  
fireworks_ai/thenlper/gte-base| `response = litellm.embedding(model="fireworks_ai/thenlper/gte-base", input=input_text)`  
## Audio Transcription​
### Quick Start​
  * SDK
  * PROXY


```
from litellm import transcription  
import os  
  
os.environ["FIREWORKS_AI_API_KEY"] = "YOUR_API_KEY"  
os.environ["FIREWORKS_AI_API_BASE"] = "https://audio-prod.us-virginia-1.direct.fireworks.ai/v1"  
  
response = transcription(  
  model="fireworks_ai/whisper-v3",  
  audio=audio_file,  
)  

```

Pass API Key/API Base in `.transcription`
  1. Setup config.yaml


```
model_list:  
 - model_name: whisper-v3  
  litellm_params:  
   model: fireworks_ai/whisper-v3  
   api_base: https://audio-prod.us-virginia-1.direct.fireworks.ai/v1  
   api_key: os.environ/FIREWORKS_API_KEY  
  model_info:  
   mode: audio_transcription  

```

  1. Start Proxy


```
litellm --config config.yaml  

```

  1. Test it


```
curl -L -X POST 'http://0.0.0.0:4000/v1/audio/transcriptions' \  
-H 'Authorization: Bearer sk-1234' \  
-F 'file=@"/Users/krrishdholakia/Downloads/gettysburg.wav"' \  
-F 'model="whisper-v3"' \  
-F 'response_format="verbose_json"' \  

```

Previous
Deepseek
Next
Clarifai
  * Overview
  * API Key
  * Sample Usage - Serverless Models
  * Sample Usage - Serverless Models - Streaming
  * Sample Usage - Models in Your Own Fireworks Account
  * Sample Usage - Direct-Route Deployment
  * Usage with LiteLLM Proxy
    * 1. Set Fireworks AI Models on config.yaml
    * 2. Start Proxy
    * 3. Test it
  * Document Inlining
    * Disable Auto-add
  * Supported Models - ALL Fireworks AI Models Supported!
  * Supported Embedding Models
  * Audio Transcription
    * Quick Start


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * 🆕 Github


On this page
# 🆕 Github
https://github.com/marketplace/models
tip
**We support ALL Github models, just set`model=github/<any-model-on-github>` as a prefix when sending litellm requests**
## API Key​
```
# env variable  
os.environ['GITHUB_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['GITHUB_API_KEY'] = ""  
response = completion(  
  model="github/llama3-8b-8192",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['GITHUB_API_KEY'] = ""  
response = completion(  
  model="github/llama3-8b-8192",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage with LiteLLM Proxy​
### 1. Set Github Models on config.yaml​
```
model_list:  
 - model_name: github-llama3-8b-8192 # Model Alias to use for requests  
  litellm_params:  
   model: github/llama3-8b-8192  
   api_key: "os.environ/GITHUB_API_KEY" # ensure you have `GITHUB_API_KEY` in your .env  

```

### 2. Start Proxy​
```
litellm --config config.yaml  

```

### 3. Test it​
Make request to litellm proxy
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "github-llama3-8b-8192",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(model="github-llama3-8b-8192", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "github-llama3-8b-8192",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Supported Models - ALL Github Models Supported!​
We support ALL Github models, just set `github/` as a prefix when sending completion requests
Model Name| Usage  
---|---  
llama-3.1-8b-instant| `completion(model="github/llama-3.1-8b-instant", messages)`  
llama-3.1-70b-versatile| `completion(model="github/llama-3.1-70b-versatile", messages)`  
llama3-8b-8192| `completion(model="github/llama3-8b-8192", messages)`  
llama3-70b-8192| `completion(model="github/llama3-70b-8192", messages)`  
llama2-70b-4096| `completion(model="github/llama2-70b-4096", messages)`  
mixtral-8x7b-32768| `completion(model="github/mixtral-8x7b-32768", messages)`  
gemma-7b-it| `completion(model="github/gemma-7b-it", messages)`  
## Github - Tool / Function Calling Example​
```
# Example dummy function hard coded to return the current weather  
import json  
def get_current_weather(location, unit="fahrenheit"):  
  """Get the current weather in a given location"""  
  if "tokyo" in location.lower():  
    return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})  
  elif "san francisco" in location.lower():  
    return json.dumps(  
      {"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"}  
    )  
  elif "paris" in location.lower():  
    return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})  
  else:  
    return json.dumps({"location": location, "temperature": "unknown"})  
  
  
  
  
# Step 1: send the conversation and available functions to the model  
messages = [  
  {  
    "role": "system",  
    "content": "You are a function calling LLM that uses the data extracted from get_current_weather to answer questions about the weather in San Francisco.",  
  },  
  {  
    "role": "user",  
    "content": "What's the weather like in San Francisco?",  
  },  
]  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {  
            "type": "string",  
            "enum": ["celsius", "fahrenheit"],  
          },  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
response = litellm.completion(  
  model="github/llama3-8b-8192",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto", # auto is default, but we'll be explicit  
)  
print("Response\n", response)  
response_message = response.choices[0].message  
tool_calls = response_message.tool_calls  
  
  
# Step 2: check if the model wanted to call a function  
if tool_calls:  
  # Step 3: call the function  
  # Note: the JSON response may not always be valid; be sure to handle errors  
  available_functions = {  
    "get_current_weather": get_current_weather,  
  }  
  messages.append(  
    response_message  
  ) # extend conversation with assistant's reply  
  print("Response message\n", response_message)  
  # Step 4: send the info for each function call and function response to the model  
  for tool_call in tool_calls:  
    function_name = tool_call.function.name  
    function_to_call = available_functions[function_name]  
    function_args = json.loads(tool_call.function.arguments)  
    function_response = function_to_call(  
      location=function_args.get("location"),  
      unit=function_args.get("unit"),  
    )  
    messages.append(  
      {  
        "tool_call_id": tool_call.id,  
        "role": "tool",  
        "name": function_name,  
        "content": function_response,  
      }  
    ) # extend conversation with function response  
  print(f"messages: {messages}")  
  second_response = litellm.completion(  
    model="github/llama3-8b-8192", messages=messages  
  ) # get a new response from the model where it can see the function response  
  print("second response\n", second_response)  

```

Previous
Groq
Next
Deepseek
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Usage with LiteLLM Proxy
    * 1. Set Github Models on config.yaml
    * 2. Start Proxy
    * 3. Test it
  * Supported Models - ALL Github Models Supported!
  * Github - Tool / Function Calling Example


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
      * Gemini - Google AI Studio
      * [BETA] Google AI Studio (Gemini) Files API
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Google AI Studio
  * [BETA] Google AI Studio (Gemini) Files API


On this page
# [BETA] Google AI Studio (Gemini) Files API
Use this to upload files to Google AI Studio (Gemini).
Useful to pass in large media files to Gemini's `/generateContent` endpoint.
Action| Supported  
---|---  
`create`| Yes  
`delete`| No  
`retrieve`| No  
`list`| No  
## Usage​
  * SDK
  * PROXY


```
import base64  
import requests  
from litellm import completion, create_file  
import os  
  
  
### UPLOAD FILE ###   
  
# Fetch the audio file and convert it to a base64 encoded string  
url = "https://cdn.openai.com/API/docs/audio/alloy.wav"  
response = requests.get(url)  
response.raise_for_status()  
wav_data = response.content  
encoded_string = base64.b64encode(wav_data).decode('utf-8')  
  
  
file = create_file(  
  file=wav_data,  
  purpose="user_data",  
  extra_body={"custom_llm_provider": "gemini"},  
  api_key=os.getenv("GEMINI_API_KEY"),  
)  
  
print(f"file: {file}")  
  
assert file is not None  
  
  
### GENERATE CONTENT ###   
completion = completion(  
  model="gemini-2.0-flash",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {   
          "type": "text",  
          "text": "What is in this recording?"  
        },  
        {  
          "type": "file",  
          "file": {  
            "file_id": file.id,  
            "filename": "my-test-name",  
            "format": "audio/wav"  
          }  
        }  
      ]  
    },  
  ]  
)  
  
print(completion.choices[0].message)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: "gemini-2.0-flash"  
   litellm_params:  
    model: gemini/gemini-2.0-flash  
    api_key: os.environ/GEMINI_API_KEY  

```

  1. Start proxy


```
litellm --config config.yaml  

```

  1. Test it


```
import base64  
import requests  
from openai import OpenAI  
  
client = OpenAI(  
  base_url="http://0.0.0.0:4000",  
  api_key="sk-1234"  
)  
  
# Fetch the audio file and convert it to a base64 encoded string  
url = "https://cdn.openai.com/API/docs/audio/alloy.wav"  
response = requests.get(url)  
response.raise_for_status()  
wav_data = response.content  
encoded_string = base64.b64encode(wav_data).decode('utf-8')  
  
  
file = client.files.create(  
  file=wav_data,  
  purpose="user_data",  
  extra_body={"target_model_names": "gemini-2.0-flash"}  
)  
  
print(f"file: {file}")  
  
assert file is not None  
  
completion = client.chat.completions.create(  
  model="gemini-2.0-flash",  
  modalities=["text", "audio"],  
  audio={"voice": "alloy", "format": "wav"},  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {   
          "type": "text",  
          "text": "What is in this recording?"  
        },  
        {  
          "type": "file",  
          "file": {  
            "file_id": file.id,  
            "filename": "my-test-name",  
            "format": "audio/wav"  
          }  
        }  
      ]  
    },  
  ],  
  extra_body={"drop_params": True}  
)  
  
print(completion.choices[0].message)  

```

Previous
Gemini - Google AI Studio
Next
Anthropic
  * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * LiteLLM Proxy (LLM Gateway)


On this page
# LiteLLM Proxy (LLM Gateway)
Property| Details  
---|---  
Description| LiteLLM Proxy is an OpenAI-compatible gateway that allows you to interact with multiple LLM providers through a unified API. Simply use the `litellm_proxy/` prefix before the model name to route your requests through the proxy.  
Provider Route on LiteLLM| `litellm_proxy/` (add this prefix to the model name, to route any requests to litellm_proxy - e.g. `litellm_proxy/your-model-name`)  
Setup LiteLLM Gateway| LiteLLM Gateway ↗  
Supported Endpoints| `/chat/completions`, `/completions`, `/embeddings`, `/audio/speech`, `/audio/transcriptions`, `/images`, `/rerank`  
## Required Variables​
```
os.environ["LITELLM_PROXY_API_KEY"] = "" # "sk-1234" your litellm proxy api key   
os.environ["LITELLM_PROXY_API_BASE"] = "" # "http://localhost:4000" your litellm proxy api base  

```

## Usage (Non Streaming)​
```
import os   
import litellm  
from litellm import completion  
  
os.environ["LITELLM_PROXY_API_KEY"] = ""  
  
# set custom api base to your proxy  
# either set .env or litellm.api_base  
# os.environ["LITELLM_PROXY_API_BASE"] = ""  
litellm.api_base = "your-openai-proxy-url"  
  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
# litellm proxy call  
response = completion(model="litellm_proxy/your-model-name", messages)  

```

## Usage - passing `api_base`, `api_key` per request​
If you need to set api_base dynamically, just pass it in completions instead - completions(...,api_base="your-proxy-api-base")
```
import os   
import litellm  
from litellm import completion  
  
os.environ["LITELLM_PROXY_API_KEY"] = ""  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
# litellm proxy call  
response = completion(  
  model="litellm_proxy/your-model-name",   
  messages=messages,   
  api_base = "your-litellm-proxy-url",  
  api_key = "your-litellm-proxy-api-key"  
)  

```

## Usage - Streaming​
```
import os   
import litellm  
from litellm import completion  
  
os.environ["LITELLM_PROXY_API_KEY"] = ""  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
# openai call  
response = completion(  
  model="litellm_proxy/your-model-name",   
  messages=messages,  
  api_base = "your-litellm-proxy-url",   
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Embeddings​
```
import litellm  
  
response = litellm.embedding(  
  model="litellm_proxy/your-embedding-model",  
  input="Hello world",  
  api_base="your-litellm-proxy-url",  
  api_key="your-litellm-proxy-api-key"  
)  

```

## Image Generation​
```
import litellm  
  
response = litellm.image_generation(  
  model="litellm_proxy/dall-e-3",  
  prompt="A beautiful sunset over mountains",  
  api_base="your-litellm-proxy-url",  
  api_key="your-litellm-proxy-api-key"  
)  

```

## Audio Transcription​
```
import litellm  
  
response = litellm.transcription(  
  model="litellm_proxy/whisper-1",  
  file="your-audio-file",  
  api_base="your-litellm-proxy-url",  
  api_key="your-litellm-proxy-api-key"  
)  

```

## Text to Speech​
```
import litellm  
  
response = litellm.speech(  
  model="litellm_proxy/tts-1",  
  input="Hello world",  
  api_base="your-litellm-proxy-url",  
  api_key="your-litellm-proxy-api-key"  
)  

```

## Rerank​
```
import litellm  
  
import litellm  
  
response = litellm.rerank(  
  model="litellm_proxy/rerank-english-v2.0",  
  query="What is machine learning?",  
  documents=[  
    "Machine learning is a field of study in artificial intelligence",  
    "Biology is the study of living organisms"  
  ],  
  api_base="your-litellm-proxy-url",  
  api_key="your-litellm-proxy-api-key"  
)  

```

## **Usage with Langchain, LLamaindex, OpenAI Js, Anthropic SDK, Instructor**​
#### Follow this doc to see how to use litellm proxy with langchain, llamaindex, anthropic etc​
Previous
AWS Bedrock
Next
Mistral AI API
  * Required Variables
  * Usage (Non Streaming)
  * Usage - passing `api_base`, `api_key` per request
  * Usage - Streaming
  * Embeddings
  * Image Generation
  * Audio Transcription
  * Text to Speech
  * Rerank
  * **Usage with Langchain, LLamaindex, OpenAI Js, Anthropic SDK, Instructor**


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Jina AI


On this page
# Jina AI
https://jina.ai/embeddings/
Supported endpoints: 
  * /embeddings
  * /rerank


## API Key​
```
# env variable  
os.environ['JINA_AI_API_KEY']  

```

## Sample Usage - Embedding​
  * SDK
  * PROXY


```
from litellm import embedding  
import os  
  
os.environ['JINA_AI_API_KEY'] = ""  
response = embedding(  
  model="jina_ai/jina-embeddings-v3",  
  input=["good morning from litellm"],  
)  
print(response)  

```

  1. Add to config.yaml


```
model_list:  
 - model_name: embedding-model  
  litellm_params:  
   model: jina_ai/jina-embeddings-v3  
   api_key: os.environ/JINA_AI_API_KEY  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000/  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/embeddings' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{"input": ["hello world"], "model": "embedding-model"}'  

```

## Sample Usage - Rerank​
  * SDK
  * PROXY


```
from litellm import rerank  
import os  
  
os.environ["JINA_AI_API_KEY"] = "sk-..."  
  
query = "What is the capital of the United States?"  
documents = [  
  "Carson City is the capital city of the American state of Nevada.",  
  "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
  "Washington, D.C. is the capital of the United States.",  
  "Capital punishment has existed in the United States since before it was a country.",  
]  
  
response = rerank(  
  model="jina_ai/jina-reranker-v2-base-multilingual",  
  query=query,  
  documents=documents,  
  top_n=3,  
)  
print(response)  

```

  1. Add to config.yaml


```
model_list:  
 - model_name: rerank-model  
  litellm_params:  
   model: jina_ai/jina-reranker-v2-base-multilingual  
   api_key: os.environ/JINA_AI_API_KEY  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/rerank' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "model": "rerank-model",  
  "query": "What is the capital of the United States?",  
  "documents": [  
    "Carson City is the capital city of the American state of Nevada.",  
    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
    "Washington, D.C. is the capital of the United States.",  
    "Capital punishment has existed in the United States since before it was a country."  
  ],  
  "top_n": 3  
}'  

```

## Supported Models​
All models listed here https://jina.ai/embeddings/ are supported
## Supported Optional Rerank Parameters​
All cohere rerank parameters are supported. 
## Supported Optional Embeddings Parameters​
```
dimensions  

```

## Provider-specific parameters​
Pass any jina ai specific parameters as a keyword argument to the `embedding` or `rerank` function, e.g. 
  * SDK
  * PROXY


```
response = embedding(  
  model="jina_ai/jina-embeddings-v3",  
  input=["good morning from litellm"],  
  dimensions=1536,  
  my_custom_param="my_custom_value", # any other jina ai specific parameters  
)  

```

```
curl -L -X POST 'http://0.0.0.0:4000/embeddings' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{"input": ["good morning from litellm"], "model": "jina_ai/jina-embeddings-v3", "dimensions": 1536, "my_custom_param": "my_custom_value"}'  

```

Previous
Voyage AI
Next
Aleph Alpha
  * API Key
  * Sample Usage - Embedding
  * Sample Usage - Rerank
  * Supported Models
  * Supported Optional Rerank Parameters
  * Supported Optional Embeddings Parameters
  * Provider-specific parameters


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Groq


On this page
# Groq
https://groq.com/
tip
**We support ALL Groq models, just set`model=groq/<any-model-on-groq>` as a prefix when sending litellm requests**
## API Key​
```
# env variable  
os.environ['GROQ_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['GROQ_API_KEY'] = ""  
response = completion(  
  model="groq/llama3-8b-8192",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['GROQ_API_KEY'] = ""  
response = completion(  
  model="groq/llama3-8b-8192",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage with LiteLLM Proxy​
### 1. Set Groq Models on config.yaml​
```
model_list:  
 - model_name: groq-llama3-8b-8192 # Model Alias to use for requests  
  litellm_params:  
   model: groq/llama3-8b-8192  
   api_key: "os.environ/GROQ_API_KEY" # ensure you have `GROQ_API_KEY` in your .env  

```

### 2. Start Proxy​
```
litellm --config config.yaml  

```

### 3. Test it​
Make request to litellm proxy
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "groq-llama3-8b-8192",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(model="groq-llama3-8b-8192", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "groq-llama3-8b-8192",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Supported Models - ALL Groq Models Supported!​
We support ALL Groq models, just set `groq/` as a prefix when sending completion requests
Model Name| Usage  
---|---  
llama-3.1-8b-instant| `completion(model="groq/llama-3.1-8b-instant", messages)`  
llama-3.1-70b-versatile| `completion(model="groq/llama-3.1-70b-versatile", messages)`  
llama3-8b-8192| `completion(model="groq/llama3-8b-8192", messages)`  
llama3-70b-8192| `completion(model="groq/llama3-70b-8192", messages)`  
llama2-70b-4096| `completion(model="groq/llama2-70b-4096", messages)`  
mixtral-8x7b-32768| `completion(model="groq/mixtral-8x7b-32768", messages)`  
gemma-7b-it| `completion(model="groq/gemma-7b-it", messages)`  
## Groq - Tool / Function Calling Example​
```
# Example dummy function hard coded to return the current weather  
import json  
def get_current_weather(location, unit="fahrenheit"):  
  """Get the current weather in a given location"""  
  if "tokyo" in location.lower():  
    return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})  
  elif "san francisco" in location.lower():  
    return json.dumps(  
      {"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"}  
    )  
  elif "paris" in location.lower():  
    return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})  
  else:  
    return json.dumps({"location": location, "temperature": "unknown"})  
  
  
  
  
# Step 1: send the conversation and available functions to the model  
messages = [  
  {  
    "role": "system",  
    "content": "You are a function calling LLM that uses the data extracted from get_current_weather to answer questions about the weather in San Francisco.",  
  },  
  {  
    "role": "user",  
    "content": "What's the weather like in San Francisco?",  
  },  
]  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {  
            "type": "string",  
            "enum": ["celsius", "fahrenheit"],  
          },  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
response = litellm.completion(  
  model="groq/llama3-8b-8192",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto", # auto is default, but we'll be explicit  
)  
print("Response\n", response)  
response_message = response.choices[0].message  
tool_calls = response_message.tool_calls  
  
  
# Step 2: check if the model wanted to call a function  
if tool_calls:  
  # Step 3: call the function  
  # Note: the JSON response may not always be valid; be sure to handle errors  
  available_functions = {  
    "get_current_weather": get_current_weather,  
  }  
  messages.append(  
    response_message  
  ) # extend conversation with assistant's reply  
  print("Response message\n", response_message)  
  # Step 4: send the info for each function call and function response to the model  
  for tool_call in tool_calls:  
    function_name = tool_call.function.name  
    function_to_call = available_functions[function_name]  
    function_args = json.loads(tool_call.function.arguments)  
    function_response = function_to_call(  
      location=function_args.get("location"),  
      unit=function_args.get("unit"),  
    )  
    messages.append(  
      {  
        "tool_call_id": tool_call.id,  
        "role": "tool",  
        "name": function_name,  
        "content": function_response,  
      }  
    ) # extend conversation with function response  
  print(f"messages: {messages}")  
  second_response = litellm.completion(  
    model="groq/llama3-8b-8192", messages=messages  
  ) # get a new response from the model where it can see the function response  
  print("second response\n", second_response)  

```

## Groq - Vision Example​
Select Groq models support vision. Check out their model list for more details.
  * SDK
  * PROXY


```
from litellm import completion  
  
import os   
from litellm import completion  
  
os.environ["GROQ_API_KEY"] = "your-api-key"  
  
# openai call  
response = completion(  
  model = "groq/llama-3.2-11b-vision-preview",   
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

  1. Add Groq models to config.yaml 


```
model_list:  
 - model_name: groq-llama3-8b-8192 # Model Alias to use for requests  
  litellm_params:  
   model: groq/llama3-8b-8192  
   api_key: "os.environ/GROQ_API_KEY" # ensure you have `GROQ_API_KEY` in your .env  

```

  1. Start Proxy


```
litellm --config config.yaml  

```

  1. Test it


```
import os   
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-1234", # your litellm proxy api key  
)  
  
response = client.chat.completions.create(  
  model = "gpt-4-vision-preview", # use model="llava-hf" to test your custom OpenAI endpoint  
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

## Speech to Text - Whisper​
```
os.environ["GROQ_API_KEY"] = ""  
audio_file = open("/path/to/audio.mp3", "rb")  
  
transcript = litellm.transcription(  
  model="groq/whisper-large-v3",  
  file=audio_file,  
  prompt="Specify context or spelling",  
  temperature=0,  
  response_format="json"  
)  
  
print("response=", transcript)  

```

Previous
Topaz
Next
🆕 Github
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Usage with LiteLLM Proxy
    * 1. Set Groq Models on config.yaml
    * 2. Start Proxy
    * 3. Test it
  * Supported Models - ALL Groq Models Supported!
  * Groq - Tool / Function Calling Example
  * Groq - Vision Example
  * Speech to Text - Whisper


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Infinity


On this page
# Infinity
Property| Details  
---|---  
Description| Infinity is a high-throughput, low-latency REST API for serving text-embeddings, reranking models and clip  
Provider Route on LiteLLM| `infinity/`  
Supported Operations| `/rerank`, `/embeddings`  
Link to Provider Doc| Infinity ↗  
## **Usage - LiteLLM Python SDK**​
```
from litellm import rerank, embedding  
import os  
  
os.environ["INFINITY_API_BASE"] = "http://localhost:8080"  
  
response = rerank(  
  model="infinity/rerank",  
  query="What is the capital of France?",  
  documents=["Paris", "London", "Berlin", "Madrid"],  
)  

```

## **Usage - LiteLLM Proxy**​
LiteLLM provides an cohere api compatible `/rerank` endpoint for Rerank calls.
**Setup**
Add this to your litellm proxy config.yaml
```
model_list:  
 - model_name: custom-infinity-rerank  
  litellm_params:  
   model: infinity/rerank  
   api_base: https://localhost:8080  
   api_key: os.environ/INFINITY_API_KEY  

```

Start litellm
```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

## Test request:​
### Rerank​
```
curl http://0.0.0.0:4000/rerank \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "custom-infinity-rerank",  
  "query": "What is the capital of the United States?",  
  "documents": [  
    "Carson City is the capital city of the American state of Nevada.",  
    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
    "Washington, D.C. is the capital of the United States.",  
    "Capital punishment has existed in the United States since before it was a country."  
  ],  
  "top_n": 3  
 }'  

```

#### Supported Cohere Rerank API Params​
Param| Type| Description  
---|---|---  
`query`| `str`| The query to rerank the documents against  
`documents`| `list[str]`| The documents to rerank  
`top_n`| `int`| The number of documents to return  
`return_documents`| `bool`| Whether to return the documents in the response  
### Usage - Return Documents​
  * SDK
  * PROXY


```
response = rerank(  
  model="infinity/rerank",  
  query="What is the capital of France?",  
  documents=["Paris", "London", "Berlin", "Madrid"],  
  return_documents=True,  
)  

```

```
curl http://0.0.0.0:4000/rerank \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "custom-infinity-rerank",  
  "query": "What is the capital of France?",  
  "documents": [  
    "Paris",  
    "London",  
    "Berlin",  
    "Madrid"  
  ],  
  "return_documents": True,  
 }'  

```

## Pass Provider-specific Params​
Any unmapped params will be passed to the provider as-is.
  * SDK
  * PROXY


```
from litellm import rerank  
import os  
  
os.environ["INFINITY_API_BASE"] = "http://localhost:8080"  
  
response = rerank(  
  model="infinity/rerank",  
  query="What is the capital of France?",  
  documents=["Paris", "London", "Berlin", "Madrid"],  
  raw_scores=True, # 👈 PROVIDER-SPECIFIC PARAM  
)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: custom-infinity-rerank  
  litellm_params:  
   model: infinity/rerank  
   api_base: https://localhost:8080  
   raw_scores: True # 👈 EITHER SET PROVIDER-SPECIFIC PARAMS HERE OR IN REQUEST BODY  

```

  1. Start litellm


```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it!


```
curl http://0.0.0.0:4000/rerank \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "custom-infinity-rerank",  
  "query": "What is the capital of the United States?",  
  "documents": [  
    "Carson City is the capital city of the American state of Nevada.",  
    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
    "Washington, D.C. is the capital of the United States.",  
    "Capital punishment has existed in the United States since before it was a country."  
  ],  
  "raw_scores": True # 👈 PROVIDER-SPECIFIC PARAM  
 }'  

```

## Embeddings​
LiteLLM provides an OpenAI api compatible `/embeddings` endpoint for embedding calls.
**Setup**
Add this to your litellm proxy config.yaml
```
model_list:  
 - model_name: custom-infinity-embedding  
  litellm_params:  
   model: infinity/provider/custom-embedding-v1  
   api_base: http://localhost:8080  
   api_key: os.environ/INFINITY_API_KEY  

```

### Test request:​
```
curl http://0.0.0.0:4000/embeddings \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "custom-infinity-embedding",  
  "input": ["hello"]  
 }'  

```

#### Supported Embedding API Params​
Param| Type| Description  
---|---|---  
`model`| `str`| The embedding model to use  
`input`| `list[str]`| The text inputs to generate embeddings for  
`encoding_format`| `str`| The format to return embeddings in (e.g. "float", "base64")  
`modality`| `str`| The type of input (e.g. "text", "image", "audio")  
### Usage - Basic Examples​
  * SDK
  * PROXY


```
from litellm import embedding  
import os  
  
os.environ["INFINITY_API_BASE"] = "http://localhost:8080"  
  
response = embedding(  
  model="infinity/bge-small",  
  input=["good morning from litellm"]  
)  
  
print(response.data[0]['embedding'])  

```

```
curl http://0.0.0.0:4000/embeddings \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "custom-infinity-embedding",  
  "input": ["hello"]  
 }'  

```

### Usage - OpenAI Client​
  * SDK
  * PROXY


```
from openai import OpenAI  
  
client = OpenAI(  
 api_key="<LITELLM_MASTER_KEY>",  
 base_url="<LITELLM_URL>"  
)  
  
response = client.embeddings.create(  
 model="bge-small",  
 input=["The food was delicious and the waiter..."],  
 encoding_format="float"  
)  
  
print(response.data[0].embedding)  

```

```
curl http://0.0.0.0:4000/embeddings \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "bge-small",  
  "input": ["The food was delicious and the waiter..."],  
  "encoding_format": "float"  
 }'  

```

Previous
VLLM
Next
Xinference [Xorbits Inference]
  * **Usage - LiteLLM Python SDK**
  * **Usage - LiteLLM Proxy**
  * Test request:
    * Rerank
    * Usage - Return Documents
  * Pass Provider-specific Params
  * Embeddings
    * Test request:
    * Usage - Basic Examples
    * Usage - OpenAI Client


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Hugging Face


On this page
# Hugging Face
LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.
  * **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like Together AI and Sambanova. This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the Inference Providers documentation.
  * **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following these steps.


## Supported Models​
### Serverless Inference Providers​
You can check available models for an inference provider by going to huggingface.co/models, clicking the "Other" filter tab, and selecting your desired provider:
![Filter models by Inference Provider](https://docs.litellm.ai/assets/images/hf_filter_inference_providers-b1b026eb51a22a19805b8099df579cae.png)
For example, you can find all Fireworks supported models here.
### Dedicated Inference Endpoints​
Refer to the Inference Endpoints catalog for a list of available models.
## Usage​
  * Serverless Inference Providers
  * Inference Endpoints


### Authentication​
With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.
Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.
```
export HF_TOKEN="hf_xxxxxx"  

```

or alternatively, you can pass your Hugging Face token as a parameter:
```
completion(..., api_key="hf_xxxxxx")  

```

### Getting Started​
To use a Hugging Face model, specify both the provider and model you want to use in the following format:
```
huggingface/<provider>/<hf_org_or_user>/<hf_model>  

```

Where `<hf_org_or_user>/<hf_model>` is the Hugging Face model ID and `<provider>` is the inference provider.  
By default, if you don't specify a provider, LiteLLM will use the HF Inference API.
Examples:
```
# Run DeepSeek-R1 inference through Together AI  
completion(model="huggingface/together/deepseek-ai/DeepSeek-R1",...)  
  
# Run Qwen2.5-72B-Instruct inference through Sambanova  
completion(model="huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct",...)  
  
# Run Llama-3.3-70B-Instruct inference through HF Inference API  
completion(model="huggingface/meta-llama/Llama-3.3-70B-Instruct",...)  

```

![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
### Basic Completion​
Here's an example of chat completion using the DeepSeek-R1 model through Together AI:
```
import os  
from litellm import completion  
  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
response = completion(  
  model="huggingface/together/deepseek-ai/DeepSeek-R1",  
  messages=[  
    {  
      "role": "user",  
      "content": "How many r's are in the word 'strawberry'?",  
    }  
  ],  
)  
print(response)  

```

### Streaming​
Now, let's see what a streaming request looks like.
```
import os  
from litellm import completion  
  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
response = completion(  
  model="huggingface/together/deepseek-ai/DeepSeek-R1",  
  messages=[  
    {  
      "role": "user",  
      "content": "How many r's are in the word `strawberry`?",  
        
    }  
  ],  
  stream=True,  
)  
  
for chunk in response:  
  print(chunk)  

```

### Image Input​
You can also pass images when the model supports it. Here is an example using Llama-3.2-11B-Vision-Instruct model through Sambanova.
```
from litellm import completion  
  
# Set your Hugging Face Token  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "What's in this image?"},  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",  
          }  
        },  
      ],  
    }  
  ]  
  
response = completion(  
  model="huggingface/sambanova/meta-llama/Llama-3.2-11B-Vision-Instruct",   
  messages=messages,  
)  
print(response.choices[0])  

```

### Function Calling​
You can extend the model's capabilities by giving them access to tools. Here is an example with function calling using Qwen2.5-72B-Instruct model through Sambanova.
```
import os  
from litellm import completion  
  
# Set your Hugging Face Token  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
tools = [  
 {  
  "type": "function",  
  "function": {  
   "name": "get_current_weather",  
   "description": "Get the current weather in a given location",  
   "parameters": {  
    "type": "object",  
    "properties": {  
     "location": {  
      "type": "string",  
      "description": "The city and state, e.g. San Francisco, CA",  
     },  
     "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
    },  
    "required": ["location"],  
   },  
  }  
 }  
]  
messages = [  
  {  
    "role": "user",  
    "content": "What's the weather like in Boston today?",  
  }  
]  
  
response = completion(  
  model="huggingface/sambanova/meta-llama/Llama-3.3-70B-Instruct",   
  messages=messages,  
  tools=tools,  
  tool_choice="auto"  
)  
print(response)  

```

![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
### Basic Completion​
After you have deployed your Hugging Face Inference Endpoint on dedicated infrastructure, you can run inference on it by providing the endpoint base URL in `api_base`, and indicating `huggingface/tgi` as the model name.
```
import os  
from litellm import completion  
  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
response = completion(  
  model="huggingface/tgi",  
  messages=[{"content": "Hello, how are you?", "role": "user"}],  
  api_base="https://my-endpoint.endpoints.huggingface.cloud/v1/"  
)  
print(response)  

```

### Streaming​
```
import os  
from litellm import completion  
  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
response = completion(  
  model="huggingface/tgi",  
  messages=[{"content": "Hello, how are you?", "role": "user"}],  
  api_base="https://my-endpoint.endpoints.huggingface.cloud/v1/",  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

### Image Input​
```
import os  
from litellm import completion  
  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "What's in this image?"},  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",  
          }  
        },  
      ],  
    }  
  ]  
response = completion(  
  model="huggingface/tgi",  
  messages=messages,  
  api_base="https://my-endpoint.endpoints.huggingface.cloud/v1/""  
)  
print(response.choices[0])  

```

### Function Calling​
```
import os  
from litellm import completion  
  
os.environ["HF_TOKEN"] = "hf_xxxxxx"  
  
functions = [{  
  "name": "get_weather",  
  "description": "Get the weather in a given location",  
  "parameters": {  
    "type": "object",  
    "properties": {  
      "location": {  
        "type": "string",  
        "description": "The location to get weather for"  
      }  
    },  
    "required": ["location"]  
  }  
}]  
  
response = completion(  
  model="huggingface/tgi",  
  messages=[{"content": "What's the weather like in San Francisco?", "role": "user"}],  
  api_base="https://my-endpoint.endpoints.huggingface.cloud/v1/",  
  functions=functions  
)  
print(response)  

```

## LiteLLM Proxy Server with Hugging Face models​
You can set up a LiteLLM Proxy Server to serve Hugging Face models through any of the supported Inference Providers. Here's how to do it:
### Step 1. Setup the config file​
In this case, we are configuring a proxy to serve `DeepSeek R1` from Hugging Face, using Together AI as the backend Inference Provider.
```
model_list:  
 - model_name: my-r1-model  
  litellm_params:  
   model: huggingface/together/deepseek-ai/DeepSeek-R1  
   api_key: os.environ/HF_TOKEN # ensure you have `HF_TOKEN` in your .env  

```

### Step 2. Start the server​
```
litellm --config /path/to/config.yaml  

```

### Step 3. Make a request to the server​
  * curl
  * python


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-r1-model",  
  "messages": [  
    {  
      "role": "user",  
      "content": "Hello, how are you?"  
    }  
  ]  
}'  

```

```
# pip install openai  
from openai import OpenAI  
  
client = OpenAI(  
  base_url="http://0.0.0.0:4000",  
  api_key="anything",  
)  
  
response = client.chat.completions.create(  
  model="my-r1-model",  
  messages=[  
    {"role": "user", "content": "Hello, how are you?"}  
  ]  
)  
print(response)  

```

## Embedding​
LiteLLM supports Hugging Face's text-embedding-inference models as well.
```
from litellm import embedding  
import os  
os.environ['HF_TOKEN'] = "hf_xxxxxx"  
response = embedding(  
  model='huggingface/microsoft/codebert-base',  
  input=["good morning from litellm"]  
)  

```

# FAQ
**How does billing work with Hugging Face Inference Providers?**
> Billing is centralized on your Hugging Face account, no matter which providers you are using. You are billed the standard provider API rates with no additional markup - Hugging Face simply passes through the provider costs. Note that Hugging Face PRO users get $2 worth of Inference credits every month that can be used across providers.
**Do I need to create an account for each Inference Provider?**
> No, you don't need to create separate accounts. All requests are routed through Hugging Face, so you only need your HF token. This allows you to easily benchmark different providers and choose the one that best fits your needs.
**Will more inference providers be supported by Hugging Face in the future?**
> Yes! New inference providers (and models) are being added gradually.
We welcome any suggestions for improving our Hugging Face integration - Create an issue/Join the Discord!
Previous
Anyscale
Next
Databricks
  * Supported Models
    * Serverless Inference Providers
    * Dedicated Inference Endpoints
  * Usage
    * Authentication
    * Getting Started
    * Basic Completion
    * Streaming
    * Image Input
    * Function Calling
    * Basic Completion
    * Streaming
    * Image Input
    * Function Calling
  * LiteLLM Proxy Server with Hugging Face models
    * Step 1. Setup the config file
    * Step 2. Start the server
    * Step 3. Make a request to the server
  * Embedding


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# Page Not Found
We could not find what you were looking for.
Please contact the owner of the site that linked you to the original URL and let them know their link is broken.
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Nvidia NIM


On this page
# Nvidia NIM
https://docs.api.nvidia.com/nim/reference/
tip
**We support ALL Nvidia NIM models, just set`model=nvidia_nim/<any-model-on-nvidia_nim>` as a prefix when sending litellm requests**
## API Key​
```
# env variable  
os.environ['NVIDIA_NIM_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['NVIDIA_NIM_API_KEY'] = ""  
response = completion(  
  model="nvidia_nim/meta/llama3-70b-instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ],  
  temperature=0.2,    # optional  
  top_p=0.9,       # optional  
  frequency_penalty=0.1, # optional  
  presence_penalty=0.1,  # optional  
  max_tokens=10,     # optional  
  stop=["\n\n"],     # optional  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['NVIDIA_NIM_API_KEY'] = ""  
response = completion(  
  model="nvidia_nim/meta/llama3-70b-instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ],  
  stream=True,  
  temperature=0.2,    # optional  
  top_p=0.9,       # optional  
  frequency_penalty=0.1, # optional  
  presence_penalty=0.1,  # optional  
  max_tokens=10,     # optional  
  stop=["\n\n"],     # optional  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage - embedding​
```
import litellm  
import os  
  
response = litellm.embedding(  
  model="nvidia_nim/nvidia/nv-embedqa-e5-v5",        # add `nvidia_nim/` prefix to model so litellm knows to route to Nvidia NIM  
  input=["good morning from litellm"],  
  encoding_format = "float",   
  user_id = "user-1234",  
  
  # Nvidia NIM Specific Parameters  
  input_type = "passage", # Optional  
  truncate = "NONE" # Optional  
)  
print(response)  

```

## **Usage - LiteLLM Proxy Server**​
Here's how to call an Nvidia NIM Endpoint with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: nvidia_nim/<your-model-name> # add nvidia_nim/ prefix to route as Nvidia NIM provider  
   api_key: api-key         # api key to send your model  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



## Supported Models - 💥 ALL Nvidia NIM Models Supported!​
We support ALL `nvidia_nim` models, just set `nvidia_nim/` as a prefix when sending completion requests
Model Name| Function Call  
---|---  
nvidia/nemotron-4-340b-reward| `completion(model="nvidia_nim/nvidia/nemotron-4-340b-reward", messages)`  
01-ai/yi-large| `completion(model="nvidia_nim/01-ai/yi-large", messages)`  
aisingapore/sea-lion-7b-instruct| `completion(model="nvidia_nim/aisingapore/sea-lion-7b-instruct", messages)`  
databricks/dbrx-instruct| `completion(model="nvidia_nim/databricks/dbrx-instruct", messages)`  
google/gemma-7b| `completion(model="nvidia_nim/google/gemma-7b", messages)`  
google/gemma-2b| `completion(model="nvidia_nim/google/gemma-2b", messages)`  
google/codegemma-1.1-7b| `completion(model="nvidia_nim/google/codegemma-1.1-7b", messages)`  
google/codegemma-7b| `completion(model="nvidia_nim/google/codegemma-7b", messages)`  
google/recurrentgemma-2b| `completion(model="nvidia_nim/google/recurrentgemma-2b", messages)`  
ibm/granite-34b-code-instruct| `completion(model="nvidia_nim/ibm/granite-34b-code-instruct", messages)`  
ibm/granite-8b-code-instruct| `completion(model="nvidia_nim/ibm/granite-8b-code-instruct", messages)`  
mediatek/breeze-7b-instruct| `completion(model="nvidia_nim/mediatek/breeze-7b-instruct", messages)`  
meta/codellama-70b| `completion(model="nvidia_nim/meta/codellama-70b", messages)`  
meta/llama2-70b| `completion(model="nvidia_nim/meta/llama2-70b", messages)`  
meta/llama3-8b| `completion(model="nvidia_nim/meta/llama3-8b", messages)`  
meta/llama3-70b| `completion(model="nvidia_nim/meta/llama3-70b", messages)`  
microsoft/phi-3-medium-4k-instruct| `completion(model="nvidia_nim/microsoft/phi-3-medium-4k-instruct", messages)`  
microsoft/phi-3-mini-128k-instruct| `completion(model="nvidia_nim/microsoft/phi-3-mini-128k-instruct", messages)`  
microsoft/phi-3-mini-4k-instruct| `completion(model="nvidia_nim/microsoft/phi-3-mini-4k-instruct", messages)`  
microsoft/phi-3-small-128k-instruct| `completion(model="nvidia_nim/microsoft/phi-3-small-128k-instruct", messages)`  
microsoft/phi-3-small-8k-instruct| `completion(model="nvidia_nim/microsoft/phi-3-small-8k-instruct", messages)`  
mistralai/codestral-22b-instruct-v0.1| `completion(model="nvidia_nim/mistralai/codestral-22b-instruct-v0.1", messages)`  
mistralai/mistral-7b-instruct| `completion(model="nvidia_nim/mistralai/mistral-7b-instruct", messages)`  
mistralai/mistral-7b-instruct-v0.3| `completion(model="nvidia_nim/mistralai/mistral-7b-instruct-v0.3", messages)`  
mistralai/mixtral-8x7b-instruct| `completion(model="nvidia_nim/mistralai/mixtral-8x7b-instruct", messages)`  
mistralai/mixtral-8x22b-instruct| `completion(model="nvidia_nim/mistralai/mixtral-8x22b-instruct", messages)`  
mistralai/mistral-large| `completion(model="nvidia_nim/mistralai/mistral-large", messages)`  
nvidia/nemotron-4-340b-instruct| `completion(model="nvidia_nim/nvidia/nemotron-4-340b-instruct", messages)`  
seallms/seallm-7b-v2.5| `completion(model="nvidia_nim/seallms/seallm-7b-v2.5", messages)`  
snowflake/arctic| `completion(model="nvidia_nim/snowflake/arctic", messages)`  
upstage/solar-10.7b-instruct| `completion(model="nvidia_nim/upstage/solar-10.7b-instruct", messages)`  
Previous
Predibase
Next
xAI
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Usage - embedding
  * **Usage - LiteLLM Proxy Server**
  * Supported Models - 💥 ALL Nvidia NIM Models Supported!


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Mistral AI API


On this page
# Mistral AI API
https://docs.mistral.ai/api/
## API Key​
```
# env variable  
os.environ['MISTRAL_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['MISTRAL_API_KEY'] = ""  
response = completion(  
  model="mistral/mistral-tiny",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['MISTRAL_API_KEY'] = ""  
response = completion(  
  model="mistral/mistral-tiny",   
  messages=[  
    {"role": "user", "content": "hello from litellm"}  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage with LiteLLM Proxy​
### 1. Set Mistral Models on config.yaml​
```
model_list:  
 - model_name: mistral-small-latest  
  litellm_params:  
   model: mistral/mistral-small-latest  
   api_key: "os.environ/MISTRAL_API_KEY" # ensure you have `MISTRAL_API_KEY` in your .env  

```

### 2. Start Proxy​
```
litellm --config config.yaml  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "mistral-small-latest",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(model="mistral-small-latest", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "mistral-small-latest",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Supported Models​
info
All models listed here https://docs.mistral.ai/platform/endpoints are supported. We actively maintain the list of models, pricing, token window, etc. here.
Model Name| Function Call  
---|---  
Mistral Small| `completion(model="mistral/mistral-small-latest", messages)`  
Mistral Medium| `completion(model="mistral/mistral-medium-latest", messages)`  
Mistral Large 2| `completion(model="mistral/mistral-large-2407", messages)`  
Mistral Large Latest| `completion(model="mistral/mistral-large-latest", messages)`  
Mistral 7B| `completion(model="mistral/open-mistral-7b", messages)`  
Mixtral 8x7B| `completion(model="mistral/open-mixtral-8x7b", messages)`  
Mixtral 8x22B| `completion(model="mistral/open-mixtral-8x22b", messages)`  
Codestral| `completion(model="mistral/codestral-latest", messages)`  
Mistral NeMo| `completion(model="mistral/open-mistral-nemo", messages)`  
Mistral NeMo 2407| `completion(model="mistral/open-mistral-nemo-2407", messages)`  
Codestral Mamba| `completion(model="mistral/open-codestral-mamba", messages)`  
Codestral Mamba| `completion(model="mistral/codestral-mamba-latest"", messages)`  
## Function Calling​
```
from litellm import completion  
  
# set env  
os.environ["MISTRAL_API_KEY"] = "your-api-key"  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  
response = completion(  
  model="mistral/mistral-large-latest",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto",  
)  
# Add any assertions, here to check response args  
print(response)  
assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)  
assert isinstance(  
  response.choices[0].message.tool_calls[0].function.arguments, str  
)  

```

## Sample Usage - Embedding​
```
from litellm import embedding  
import os  
  
os.environ['MISTRAL_API_KEY'] = ""  
response = embedding(  
  model="mistral/mistral-embed",  
  input=["good morning from litellm"],  
)  
print(response)  

```

## Supported Models​
All models listed here https://docs.mistral.ai/platform/endpoints are supported
Model Name| Function Call  
---|---  
Mistral Embeddings| `embedding(model="mistral/mistral-embed", input)`  
Previous
LiteLLM Proxy (LLM Gateway)
Next
Codestral API [Mistral AI]
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Usage with LiteLLM Proxy
    * 1. Set Mistral Models on config.yaml
    * 2. Start Proxy
    * 3. Test it
  * Supported Models
  * Function Calling
  * Sample Usage - Embedding
  * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Ollama


On this page
# Ollama
LiteLLM supports all models from Ollama
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
info
We recommend using ollama_chat for better responses.
## Pre-requisites​
Ensure you have your ollama server running
## Example usage​
```
from litellm import completion  
  
response = completion(  
  model="ollama/llama2",   
  messages=[{ "content": "respond in 20 words. who are you?","role": "user"}],   
  api_base="http://localhost:11434"  
)  
print(response)  
  

```

## Example usage - Streaming​
```
from litellm import completion  
  
response = completion(  
  model="ollama/llama2",   
  messages=[{ "content": "respond in 20 words. who are you?","role": "user"}],   
  api_base="http://localhost:11434",  
  stream=True  
)  
print(response)  
for chunk in response:  
  print(chunk['choices'][0]['delta'])  
  

```

## Example usage - Streaming + Acompletion​
Ensure you have async_generator installed for using ollama acompletion with streaming
```
pip install async_generator  

```

```
async def async_ollama():  
  response = await litellm.acompletion(  
    model="ollama/llama2",   
    messages=[{ "content": "what's the weather" ,"role": "user"}],   
    api_base="http://localhost:11434",   
    stream=True  
  )  
  async for chunk in response:  
    print(chunk)  
  
# call async_ollama  
import asyncio  
asyncio.run(async_ollama())  
  

```

## Example Usage - JSON Mode​
To use ollama JSON Mode pass `format="json"` to `litellm.completion()`
```
from litellm import completion  
response = completion(  
 model="ollama/llama2",  
 messages=[  
   {  
     "role": "user",  
     "content": "respond in json, what's the weather"  
   }  
 ],  
 max_tokens=10,  
 format = "json"  
)  

```

## Example Usage - Tool Calling​
To use ollama tool calling, pass `tools=[{..}]` to `litellm.completion()`
  * SDK
  * PROXY


```
from litellm import completion  
import litellm   
  
## [OPTIONAL] REGISTER MODEL - not all ollama models support function calling, litellm defaults to json mode tool calls if native tool calling not supported.  
  
# litellm.register_model(model_cost={  
#         "ollama_chat/llama3.1": {   
#          "supports_function_calling": true  
#         },  
#       })  
  
tools = [  
 {  
  "type": "function",  
  "function": {  
   "name": "get_current_weather",  
   "description": "Get the current weather in a given location",  
   "parameters": {  
    "type": "object",  
    "properties": {  
     "location": {  
      "type": "string",  
      "description": "The city and state, e.g. San Francisco, CA",  
     },  
     "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
    },  
    "required": ["location"],  
   },  
  }  
 }  
]  
  
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  
  
response = completion(  
 model="ollama_chat/llama3.1",  
 messages=messages,  
 tools=tools  
)  

```

  1. Setup config.yaml 


```
model_list:  
 - model_name: "llama3.1"         
  litellm_params:  
   model: "ollama_chat/llama3.1"  
   keep_alive: "8m" # Optional: Overrides default keep_alive, use -1 for Forever  
  model_info:  
   supports_function_calling: true  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{  
  "model": "llama3.1",  
 "messages": [  
  {  
   "role": "user",  
   "content": "What'\''s the weather like in Boston today?"  
  }  
 ],  
 "tools": [  
  {  
   "type": "function",  
   "function": {  
    "name": "get_current_weather",  
    "description": "Get the current weather in a given location",  
    "parameters": {  
     "type": "object",  
     "properties": {  
      "location": {  
       "type": "string",  
       "description": "The city and state, e.g. San Francisco, CA"  
      },  
      "unit": {  
       "type": "string",  
       "enum": ["celsius", "fahrenheit"]  
      }  
     },  
     "required": ["location"]  
    }  
   }  
  }  
 ],  
 "tool_choice": "auto",  
 "stream": true  
}'  

```

## Using Ollama FIM on `/v1/completions`​
LiteLLM supports calling Ollama's `/api/generate` endpoint on `/v1/completions` requests. 
  * SDK
  * PROXY


```
import litellm   
litellm._turn_on_debug() # turn on debug to see the request  
from litellm import completion  
  
response = completion(  
  model="ollama/llama3.1",  
  prompt="Hello, world!",  
  api_base="http://localhost:11434"  
)  
print(response)  

```

  1. Setup config.yaml 


```
model_list:  
 - model_name: "llama3.1"         
  litellm_params:  
   model: "ollama/llama3.1"  
   api_base: "http://localhost:11434"  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml --detailed_debug  
  
# RUNNING ON http://0.0.0.0:4000   

```

  1. Test it! 


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="anything", # 👈 PROXY KEY (can be anything, if master_key not set)  
  base_url="http://0.0.0.0:4000" # 👈 PROXY BASE URL  
)  
  
response = client.completions.create(  
  model="ollama/llama3.1",  
  prompt="Hello, world!",  
  api_base="http://localhost:11434"  
)  
print(response)  

```

## Using ollama `api/chat`​
In order to send ollama requests to `POST /api/chat` on your ollama server, set the model prefix to `ollama_chat`
```
from litellm import completion  
  
response = completion(  
  model="ollama_chat/llama2",   
  messages=[{ "content": "respond in 20 words. who are you?","role": "user"}],   
)  
print(response)  

```

## Ollama Models​
Ollama supported models: https://github.com/ollama/ollama
Model Name| Function Call  
---|---  
Mistral| `completion(model='ollama/mistral', messages, api_base="http://localhost:11434", stream=True)`  
Mistral-7B-Instruct-v0.1| `completion(model='ollama/mistral-7B-Instruct-v0.1', messages, api_base="http://localhost:11434", stream=False)`  
Mistral-7B-Instruct-v0.2| `completion(model='ollama/mistral-7B-Instruct-v0.2', messages, api_base="http://localhost:11434", stream=False)`  
Mixtral-8x7B-Instruct-v0.1| `completion(model='ollama/mistral-8x7B-Instruct-v0.1', messages, api_base="http://localhost:11434", stream=False)`  
Mixtral-8x22B-Instruct-v0.1| `completion(model='ollama/mixtral-8x22B-Instruct-v0.1', messages, api_base="http://localhost:11434", stream=False)`  
Llama2 7B| `completion(model='ollama/llama2', messages, api_base="http://localhost:11434", stream=True)`  
Llama2 13B| `completion(model='ollama/llama2:13b', messages, api_base="http://localhost:11434", stream=True)`  
Llama2 70B| `completion(model='ollama/llama2:70b', messages, api_base="http://localhost:11434", stream=True)`  
Llama2 Uncensored| `completion(model='ollama/llama2-uncensored', messages, api_base="http://localhost:11434", stream=True)`  
Code Llama| `completion(model='ollama/codellama', messages, api_base="http://localhost:11434", stream=True)`  
Llama2 Uncensored| `completion(model='ollama/llama2-uncensored', messages, api_base="http://localhost:11434", stream=True)`  
Meta LLaMa3 8B| `completion(model='ollama/llama3', messages, api_base="http://localhost:11434", stream=False)`  
Meta LLaMa3 70B| `completion(model='ollama/llama3:70b', messages, api_base="http://localhost:11434", stream=False)`  
Orca Mini| `completion(model='ollama/orca-mini', messages, api_base="http://localhost:11434", stream=True)`  
Vicuna| `completion(model='ollama/vicuna', messages, api_base="http://localhost:11434", stream=True)`  
Nous-Hermes| `completion(model='ollama/nous-hermes', messages, api_base="http://localhost:11434", stream=True)`  
Nous-Hermes 13B| `completion(model='ollama/nous-hermes:13b', messages, api_base="http://localhost:11434", stream=True)`  
Wizard Vicuna Uncensored| `completion(model='ollama/wizard-vicuna', messages, api_base="http://localhost:11434", stream=True)`  
### JSON Schema support​
  * SDK
  * PROXY


```
from litellm import completion  
  
response = completion(  
  model="ollama_chat/deepseek-r1",   
  messages=[{ "content": "respond in 20 words. who are you?","role": "user"}],   
  response_format={"type": "json_schema", "json_schema": {"schema": {"type": "object", "properties": {"name": {"type": "string"}}}}},  
)  
print(response)  

```

  1. Setup config.yaml 


```
model_list:  
 - model_name: "deepseek-r1"         
  litellm_params:  
   model: "ollama_chat/deepseek-r1"  
   api_base: "http://localhost:11434"  

```

  1. Start proxy 


```
litellm --config /path/to/config.yaml  
  
# RUNNING ON http://0.0.0.0:4000  

```

  1. Test it! 


```
from pydantic import BaseModel  
from openai import OpenAI  
  
client = OpenAI(  
  api_key="anything", # 👈 PROXY KEY (can be anything, if master_key not set)  
  base_url="http://0.0.0.0:4000" # 👈 PROXY BASE URL  
)  
  
class Step(BaseModel):  
  explanation: str  
  output: str  
  
class MathReasoning(BaseModel):  
  steps: list[Step]  
  final_answer: str  
  
completion = client.beta.chat.completions.parse(  
  model="deepseek-r1",  
  messages=[  
    {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},  
    {"role": "user", "content": "how can I solve 8x + 7 = -23"}  
  ],  
  response_format=MathReasoning,  
)  
  
math_reasoning = completion.choices[0].message.parsed  

```

## Ollama Vision Models​
Model Name| Function Call  
---|---  
llava| `completion('ollama/llava', messages)`  
#### Using Ollama Vision Models​
Call `ollama/llava` in the same input/output format as OpenAI `gpt-4-vision`
LiteLLM Supports the following image types passed in `url`
  * Base64 encoded svgs


**Example Request**
```
import litellm  
  
response = litellm.completion(  
 model = "ollama/llava",  
 messages=[  
   {  
     "role": "user",  
     "content": [  
             {  
               "type": "text",  
               "text": "Whats in this image?"  
             },  
             {  
               "type": "image_url",  
               "image_url": {  
               "url": "iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDCeta2jxYbr69evk4MHDyY7d+7MjhMnTiTPnz9Pfv/+nfQT2ggpO2dMF8cghuoM7Ygj5iWCqRlGFml0QC/ftGmTmzt3rmsaKDsgBSPh0/8yPeLLBihLkOKJc0jp8H8vUzcxIA1k6QJ/c78tWEyj5P3o4u9+jywNPdJi5rAH9x0KHcl4Hg570eQp3+vHXGyrmEeigzQsQsjavXt38ujRo44LQuDDhw+TW7duRS1HGgMxhNXHgflaNTOsHyKvHK5Ijo2jbFjJBQK9YwFd6RVMzfgRBmEfP37suBBm/p49e1qjEP2mwTViNRo0VJWH1deMXcNK08uUjVUu7s/zRaL+oLNxz1bpANco4npUgX4G2eFbpDFyQoQxojBCpEGSytmOH8qrH5Q9vuzD6ofQylkCUmh8DBAr+q8JCyVNtWQIidKQE9wNtLSQnS4jDSsxNHogzFuQBw4cyM61UKVsjfr3ooBkPSqqQHesUPWVtzi9/vQi1T+rJj7WiTz4Pt/l3LxUkr5P2VYZaZ4URpsE+st/dujQoaBBYokbrz/8TJNQYLSonrPS9kUaSkPeZyj1AWSj+d+VBoy1pIWVNed8P0Ll/ee5HdGRhrHhR5GGN0r4LGZBaj8oFDJitBTJzIZgFcmU0Y8ytWMZMzJOaXUSrUs5RxKnrxmbb5YXO9VGUhtpXldhEUogFr3IzIsvlpmdosVcGVGXFWp2oU9kLFL3dEkSz6NHEY1sjSRdIuDFWEhd8KxFqsRi1uM/nz9/zpxnwlESONdg6dKlbsaMGS4EHFHtjFIDHwKOo46l4TxSuxgDzi+rE2jg+BaFruOX4HXa0Nnf1lwAPufZeF8/r6zD97WK2qFnGjBxTw5qNGPxT+5T/r7/7RawFC3j4vTp09koCxkeHjqbHJqArmH5UrFKKksnxrK7FuRIs8STfBZv+luugXZ2pR/pP9Ois4z+TiMzUUkUjD0iEi1fzX8GmXyuxUBRcaUfykV0YZnlJGKQpOiGB76x5GeWkWWJc3mOrK6S7xdND+W5N6XyaRgtWJFe13GkaZnKOsYqGdOVVVbGupsyA/l7emTLHi7vwTdirNEt0qxnzAvBFcnQF16xh/TMpUuXHDowhlA9vQVraQhkudRdzOnK+04ZSP3DUhVSP61YsaLtd/ks7ZgtPcXqPqEafHkdqa84X6aCeL7YWlv6edGFHb+ZFICPlljHhg0bKuk0CSvVznWsotRu433alNdFrqG45ejoaPCaUkWERpLXjzFL2Rpllp7PJU2a/v7Ab8N05/9t27Z16KUqoFGsxnI9EosS2niSYg9SpU6B4JgTrvVW1flt1sT+0ADIJU2maXzcUTraGCRaL1Wp9rUMk16PMom8QhruxzvZIegJjFU7LLCePfS8uaQdPny4jTTL0dbee5mYokQsXTIWNY46kuMbnt8Kmec+LGWtOVIl9cT1rCB0V8WqkjAsRwta93TbwNYoGKsUSChN44lgBNCoHLHzquYKrU6qZ8lolCIN0Rh6cP0Q3U6I6IXILYOQI513hJaSKAorFpuHXJNfVlpRtmYBk1Su1obZr5dnKAO+L10Hrj3WZW+E3qh6IszE37F6EB+68mGpvKm4eb9bFrlzrok7fvr0Kfv727dvWRmdVTJHw0qiiCUSZ6wCK+7XL/AcsgNyL74DQQ730sv78Su7+t/A36MdY0sW5o40ahslXr58aZ5HtZB8GH64m9EmMZ7FpYw4T6QnrZfgenrhFxaSiSGXtPnz57e9TkNZLvTjeqhr734CNtrK41L40sUQckmj1lGKQ0rC37x544r8eNXRpnVE3ZZY7zXo8NomiO0ZUCj2uHz58rbXoZ6gc0uA+F6ZeKS/jhRDUq8MKrTho9fEkihMmhxtBI1DxKFY9XLpVcSkfoi8JGnToZO5sU5aiDQIW716ddt7ZLYtMQlhECdBGXZZMWldY5BHm5xgAroWj4C0hbYkSc/jBmggIrXJWlZM6pSETsEPGqZOndr2uuuR5rF169a2HoHPdurUKZM4CO1WTPqaDaAd+GFGKdIQkxAn9RuEWcTRyN2KSUgiSgF5aWzPTeA/lN5rZubMmR2bE4SIC4nJoltgAV/dVefZm72AtctUCJU2CMJ327hxY9t7EHbkyJFseq+EJSY16RPo3Dkq1kkr7+q0bNmyDuLQcZBEPYmHVdOBiJyIlrRDq41YPWfXOxUysi5fvtyaj+2BpcnsUV/oSoEMOk2CQGlr4ckhBwaetBhjCwH0ZHtJROPJkyc7UjcYLDjmrH7ADTEBXFfOYmB0k9oYBOjJ8b4aOYSe7QkKcYhFlq3QYLQhSidNmtS2RATwy8YOM3EQJsUjKiaWZ+vZToUQgzhkHXudb/PW5YMHD9yZM2faPsMwoc7RciYJXbGuBqJ1UIGKKLv915jsvgtJxCZDubdXr165mzdvtr1Hz5LONA8jrUwKPqsmVesKa49S3Q4WxmRPUEYdTjgiUcfUwLx589ySJUva3oMkP6IYddq6HMS4o55xBJBUeRjzfa4Zdeg56QZ43LhxoyPo7Lf1kNt7oO8wWAbNwaYjIv5lhyS7kRf96dvm5Jah8vfvX3flyhX35cuX6HfzFHOToS1H4BenCaHvO8pr8iDuwoUL7tevX+b5ZdbBair0xkFIlFDlW4ZknEClsp/TzXyAKVOmmHWFVSbDNw1l1+4f90U6IY/q4V27dpnE9bJ+v87QEydjqx/UamVVPRG+mwkNTYN+9tjkwzEx+atCm/X9WvWtDtAb68Wy9LXa1UmvCDDIpPkyOQ5ZwSzJ4jMrvFcr0rSjOUh+GcT4LSg5ugkW1Io0/SCDQBojh0hPlaJdah+tkVYrnTZowP8iq1F1TgMBBauufyB33x1v+NWFYmT5KmppgHC+NkAgbmRkpD3yn9QIseXymoTQFGQmIOKTxiZIWpvAatenVqRVXf2nTrAWMsPnKrMZHz6bJq5jvce6QK8J1cQNgKxlJapMPdZSR64/UivS9NztpkVEdKcrs5alhhWP9NeqlfWopzhZScI6QxseegZRGeg5a8C3Re1Mfl1ScP36ddcUaMuv24iOJtz7sbUjTS4qBvKmstYJoUauiuD3k5qhyr7QdUHMeCgLa1Ear9NquemdXgmum4fvJ6w1lqsuDhNrg1qSpleJK7K3TF0Q2jSd94uSZ60kK1e3qyVpQK6PVWXp2/FC3mp6jBhKKOiY2h3gtUV64TWM6wDETRPLDfSakXmH3w8g9Jlug8ZtTt4kVF0kLUYYmCCtD/DrQ5YhMGbA9L3ucdjh0y8kOHW5gU/VEEmJTcL4Pz/f7mgoAbYkAAAAAElFTkSuQmCC"  
               }  
             }  
           ]  
   }  
 ],  
)  
print(response)  

```

## LiteLLM/Ollama Docker Image​
For Ollama LiteLLM Provides a Docker Image for an OpenAI API compatible server for local LLMs - llama2, mistral, codellama
![Chat on WhatsApp](https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square) ![Chat on Discord](https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square)
### An OpenAI API compatible server for local LLMs - llama2, mistral, codellama​
### Quick Start:​
Docker Hub: For ARM Processors: https://hub.docker.com/repository/docker/litellm/ollama/general For Intel/AMD Processors: to be added
```
docker pull litellm/ollama  

```

```
docker run --name ollama litellm/ollama  

```

#### Test the server container​
On the docker container run the `test.py` file using `python3 test.py`
### Making a request to this server​
```
import openai  
  
api_base = f"http://0.0.0.0:4000" # base url for server  
  
openai.api_base = api_base  
openai.api_key = "temp-key"  
print(openai.api_base)  
  
  
print(f'LiteLLM: response from proxy with streaming')  
response = openai.chat.completions.create(  
  model="ollama/llama2",   
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, acknowledge that you got it"  
    }  
  ],  
  stream=True  
)  
  
for chunk in response:  
  print(f'LiteLLM: streaming response from proxy {chunk}')  

```

### Responses from this server​
```
{  
 "object": "chat.completion",  
 "choices": [  
  {  
   "finish_reason": "stop",  
   "index": 0,  
   "message": {  
    "content": " Hello! I acknowledge receipt of your test request. Please let me know if there's anything else I can assist you with.",  
    "role": "assistant",  
    "logprobs": null  
   }  
  }  
 ],  
 "id": "chatcmpl-403d5a85-2631-4233-92cb-01e6dffc3c39",  
 "created": 1696992706.619709,  
 "model": "ollama/llama2",  
 "usage": {  
  "prompt_tokens": 18,  
  "completion_tokens": 25,  
  "total_tokens": 43  
 }  
}  

```

## Calling Docker Container (host.docker.internal)​
Follow these instructions
Previous
Triton Inference Server
Next
Perplexity AI (pplx-api)
  * Pre-requisites
  * Example usage
  * Example usage - Streaming
  * Example usage - Streaming + Acompletion
  * Example Usage - JSON Mode
  * Example Usage - Tool Calling
  * Using Ollama FIM on `/v1/completions`
  * Using ollama `api/chat`
  * Ollama Models
    * JSON Schema support
  * Ollama Vision Models
  * LiteLLM/Ollama Docker Image
    * An OpenAI API compatible server for local LLMs - llama2, mistral, codellama
    * Quick Start:
    * Making a request to this server
    * Responses from this server
  * Calling Docker Container (host.docker.internal)


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * LM Studio


On this page
# LM Studio
https://lmstudio.ai/docs/basics/server
tip
**We support ALL LM Studio models, just set`model=lm_studio/<any-model-on-lmstudio>` as a prefix when sending litellm requests**
Property| Details  
---|---  
Description| Discover, download, and run local LLMs.  
Provider Route on LiteLLM| `lm_studio/`  
Provider Doc| LM Studio ↗  
Supported OpenAI Endpoints| `/chat/completions`, `/embeddings`, `/completions`  
## API Key​
```
# env variable  
os.environ['LM_STUDIO_API_BASE']  
os.environ['LM_STUDIO_API_KEY'] # optional, default is empty  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['LM_STUDIO_API_BASE'] = ""  
  
response = completion(  
  model="lm_studio/llama-3-8b-instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ]  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['LM_STUDIO_API_KEY'] = ""  
response = completion(  
  model="lm_studio/llama-3-8b-instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ],  
  stream=True,  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage with LiteLLM Proxy Server​
Here's how to call a LM Studio model with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: lm_studio/<your-model-name> # add lm_studio/ prefix to route as LM Studio provider  
   api_key: api-key         # api key to send your model  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



## Supported Parameters​
See Supported Parameters for supported parameters.
## Embedding​
```
from litellm import embedding  
import os   
  
os.environ['LM_STUDIO_API_BASE'] = "http://localhost:8000"  
response = embedding(  
  model="lm_studio/jina-embeddings-v3",  
  input=["Hello world"],  
)  
print(response)  

```

Previous
xAI
Next
Cerebras
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Usage with LiteLLM Proxy Server
  * Supported Parameters
  * Embedding


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * OpenAI


On this page
# OpenAI
LiteLLM supports OpenAI Chat + Embedding calls.
### Required API Keys​
```
import os   
os.environ["OPENAI_API_KEY"] = "your-api-key"  

```

### Usage​
```
import os   
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
# openai call  
response = completion(  
  model = "gpt-4o",   
  messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

### Usage - LiteLLM Proxy Server​
Here's how to call OpenAI models with the LiteLLM Proxy Server
### 1. Save key in your environment​
```
export OPENAI_API_KEY=""  

```

### 2. Start the proxy​
  * config.yaml
  * config.yaml - proxy all OpenAI models
  * CLI


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: openai/gpt-3.5-turbo             # The `openai/` prefix will call openai.chat.completions.create  
   api_key: os.environ/OPENAI_API_KEY  
 - model_name: gpt-3.5-turbo-instruct  
  litellm_params:  
   model: text-completion-openai/gpt-3.5-turbo-instruct # The `text-completion-openai/` prefix will call openai.completions.create  
   api_key: os.environ/OPENAI_API_KEY  

```

Use this to add all openai models with one API Key. **WARNING: This will not do any load balancing** This means requests to `gpt-4`, `gpt-3.5-turbo` , `gpt-4-turbo-preview` will all go through this route 
```
model_list:  
 - model_name: "*"       # all requests where model not in your config go to this deployment  
  litellm_params:  
   model: openai/*      # set `openai/` to use the openai route  
   api_key: os.environ/OPENAI_API_KEY  

```

```
$ litellm --model gpt-3.5-turbo  
  
# Server running on http://0.0.0.0:4000  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "gpt-3.5-turbo",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

### Optional Keys - OpenAI Organization, OpenAI API Base​
```
import os   
os.environ["OPENAI_ORGANIZATION"] = "your-org-id"    # OPTIONAL  
os.environ["OPENAI_API_BASE"] = "openaiai-api-base"   # OPTIONAL  

```

### OpenAI Chat Completion Models​
Model Name| Function Call  
---|---  
gpt-4.1| `response = completion(model="gpt-4.1", messages=messages)`  
gpt-4.1-mini| `response = completion(model="gpt-4.1-mini", messages=messages)`  
gpt-4.1-nano| `response = completion(model="gpt-4.1-nano", messages=messages)`  
o4-mini| `response = completion(model="o4-mini", messages=messages)`  
o3-mini| `response = completion(model="o3-mini", messages=messages)`  
o3| `response = completion(model="o3", messages=messages)`  
o1-mini| `response = completion(model="o1-mini", messages=messages)`  
o1-preview| `response = completion(model="o1-preview", messages=messages)`  
gpt-4o-mini| `response = completion(model="gpt-4o-mini", messages=messages)`  
gpt-4o-mini-2024-07-18| `response = completion(model="gpt-4o-mini-2024-07-18", messages=messages)`  
gpt-4o| `response = completion(model="gpt-4o", messages=messages)`  
gpt-4o-2024-08-06| `response = completion(model="gpt-4o-2024-08-06", messages=messages)`  
gpt-4o-2024-05-13| `response = completion(model="gpt-4o-2024-05-13", messages=messages)`  
gpt-4-turbo| `response = completion(model="gpt-4-turbo", messages=messages)`  
gpt-4-turbo-preview| `response = completion(model="gpt-4-0125-preview", messages=messages)`  
gpt-4-0125-preview| `response = completion(model="gpt-4-0125-preview", messages=messages)`  
gpt-4-1106-preview| `response = completion(model="gpt-4-1106-preview", messages=messages)`  
gpt-3.5-turbo-1106| `response = completion(model="gpt-3.5-turbo-1106", messages=messages)`  
gpt-3.5-turbo| `response = completion(model="gpt-3.5-turbo", messages=messages)`  
gpt-3.5-turbo-0301| `response = completion(model="gpt-3.5-turbo-0301", messages=messages)`  
gpt-3.5-turbo-0613| `response = completion(model="gpt-3.5-turbo-0613", messages=messages)`  
gpt-3.5-turbo-16k| `response = completion(model="gpt-3.5-turbo-16k", messages=messages)`  
gpt-3.5-turbo-16k-0613| `response = completion(model="gpt-3.5-turbo-16k-0613", messages=messages)`  
gpt-4| `response = completion(model="gpt-4", messages=messages)`  
gpt-4-0314| `response = completion(model="gpt-4-0314", messages=messages)`  
gpt-4-0613| `response = completion(model="gpt-4-0613", messages=messages)`  
gpt-4-32k| `response = completion(model="gpt-4-32k", messages=messages)`  
gpt-4-32k-0314| `response = completion(model="gpt-4-32k-0314", messages=messages)`  
gpt-4-32k-0613| `response = completion(model="gpt-4-32k-0613", messages=messages)`  
These also support the `OPENAI_API_BASE` environment variable, which can be used to specify a custom API endpoint.
## OpenAI Vision Models​
Model Name| Function Call  
---|---  
gpt-4o| `response = completion(model="gpt-4o", messages=messages)`  
gpt-4-turbo| `response = completion(model="gpt-4-turbo", messages=messages)`  
gpt-4-vision-preview| `response = completion(model="gpt-4-vision-preview", messages=messages)`  
#### Usage​
```
import os   
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
# openai call  
response = completion(  
  model = "gpt-4-vision-preview",   
  messages=[  
    {  
      "role": "user",  
      "content": [  
              {  
                "type": "text",  
                "text": "What’s in this image?"  
              },  
              {  
                "type": "image_url",  
                "image_url": {  
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"  
                }  
              }  
            ]  
    }  
  ],  
)  
  

```

## PDF File Parsing​
OpenAI has a new `file` message type that allows you to pass in a PDF file and have it parsed into a structured output. Read more
  * SDK
  * PROXY


```
import base64  
from litellm import completion  
  
with open("draconomicon.pdf", "rb") as f:  
  data = f.read()  
  
base64_string = base64.b64encode(data).decode("utf-8")  
  
completion = completion(  
  model="gpt-4o",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "file",  
          "file": {  
            "filename": "draconomicon.pdf",  
            "file_data": f"data:application/pdf;base64,{base64_string}",  
          }  
        },  
        {  
          "type": "text",  
          "text": "What is the first dragon in the book?",  
        }  
      ],  
    },  
  ],  
)  
  
print(completion.choices[0].message.content)  

```

  1. Setup config.yaml


```
model_list:  
 - model_name: openai-model  
  litellm_params:  
   model: gpt-4o  
   api_key: os.environ/OPENAI_API_KEY  

```

  1. Start the proxy


```
litellm --config config.yaml  

```

  1. Test it!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-d '{   
  "model": "openai-model",  
  "messages": [  
    {"role": "user", "content": [  
      {  
        "type": "file",  
        "file": {  
          "filename": "draconomicon.pdf",  
          "file_data": f"data:application/pdf;base64,{base64_string}",  
        }  
      }  
    ]}  
  ]  
}'  

```

## OpenAI Fine Tuned Models​
Model Name| Function Call  
---|---  
fine tuned `gpt-4-0613`| `response = completion(model="ft:gpt-4-0613", messages=messages)`  
fine tuned `gpt-4o-2024-05-13`| `response = completion(model="ft:gpt-4o-2024-05-13", messages=messages)`  
fine tuned `gpt-3.5-turbo-0125`| `response = completion(model="ft:gpt-3.5-turbo-0125", messages=messages)`  
fine tuned `gpt-3.5-turbo-1106`| `response = completion(model="ft:gpt-3.5-turbo-1106", messages=messages)`  
fine tuned `gpt-3.5-turbo-0613`| `response = completion(model="ft:gpt-3.5-turbo-0613", messages=messages)`  
## OpenAI Audio Transcription​
LiteLLM supports OpenAI Audio Transcription endpoint.
Supported models:
Model Name| Function Call  
---|---  
`whisper-1`| `response = completion(model="whisper-1", file=audio_file)`  
`gpt-4o-transcribe`| `response = completion(model="gpt-4o-transcribe", file=audio_file)`  
`gpt-4o-mini-transcribe`| `response = completion(model="gpt-4o-mini-transcribe", file=audio_file)`  
  * SDK
  * PROXY


```
from litellm import transcription  
import os   
  
# set api keys   
os.environ["OPENAI_API_KEY"] = ""  
audio_file = open("/path/to/audio.mp3", "rb")  
  
response = transcription(model="gpt-4o-transcribe", file=audio_file)  
  
print(f"response: {response}")  

```

  1. Setup config.yaml


```
model_list:  
- model_name: gpt-4o-transcribe  
 litellm_params:  
  model: gpt-4o-transcribe  
  api_key: os.environ/OPENAI_API_KEY  
 model_info:  
  mode: audio_transcription  
    
general_settings:  
 master_key: sk-1234  

```

  1. Start the proxy


```
litellm --config config.yaml  

```

  1. Test it!


```
curl --location 'http://0.0.0.0:8000/v1/audio/transcriptions' \  
--header 'Authorization: Bearer sk-1234' \  
--form 'file=@"/Users/krrishdholakia/Downloads/gettysburg.wav"' \  
--form 'model="gpt-4o-transcribe"'  

```

## Advanced​
### Getting OpenAI API Response Headers​
Set `litellm.return_response_headers = True` to get raw response headers from OpenAI
You can expect to always get the `_response_headers` field from `litellm.completion()`, `litellm.embedding()` functions
  * litellm.completion
  * litellm.completion + stream
  * litellm.embedding


```
litellm.return_response_headers = True  
  
# /chat/completion  
response = completion(  
  model="gpt-4o-mini",  
  messages=[  
    {  
      "role": "user",  
      "content": "hi",  
    }  
  ],  
)  
print(f"response: {response}")  
print("_response_headers=", response._response_headers)  

```

```
litellm.return_response_headers = True  
  
# /chat/completion  
response = completion(  
  model="gpt-4o-mini",  
  stream=True,  
  messages=[  
    {  
      "role": "user",  
      "content": "hi",  
    }  
  ],  
)  
print(f"response: {response}")  
print("response_headers=", response._response_headers)  
for chunk in response:  
  print(chunk)  

```

```
litellm.return_response_headers = True  
  
# embedding  
embedding_response = litellm.embedding(  
  model="text-embedding-ada-002",  
  input="hello",  
)  
  
embedding_response_headers = embedding_response._response_headers  
print("embedding_response_headers=", embedding_response_headers)  

```

Expected Response Headers from OpenAI
```
{  
 "date": "Sat, 20 Jul 2024 22:05:23 GMT",  
 "content-type": "application/json",  
 "transfer-encoding": "chunked",  
 "connection": "keep-alive",  
 "access-control-allow-origin": "*",  
 "openai-model": "text-embedding-ada-002",  
 "openai-organization": "*****",  
 "openai-processing-ms": "20",  
 "openai-version": "2020-10-01",  
 "strict-transport-security": "max-age=15552000; includeSubDomains; preload",  
 "x-ratelimit-limit-requests": "5000",  
 "x-ratelimit-limit-tokens": "5000000",  
 "x-ratelimit-remaining-requests": "4999",  
 "x-ratelimit-remaining-tokens": "4999999",  
 "x-ratelimit-reset-requests": "12ms",  
 "x-ratelimit-reset-tokens": "0s",  
 "x-request-id": "req_cc37487bfd336358231a17034bcfb4d9",  
 "cf-cache-status": "DYNAMIC",  
 "set-cookie": "__cf_bm=E_FJY8fdAIMBzBE2RZI2.OkMIO3lf8Hz.ydBQJ9m3q8-1721513123-1.0.1.1-6OK0zXvtd5s9Jgqfz66cU9gzQYpcuh_RLaUZ9dOgxR9Qeq4oJlu.04C09hOTCFn7Hg.k.2tiKLOX24szUE2shw; path=/; expires=Sat, 20-Jul-24 22:35:23 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, *cfuvid=SDndIImxiO3U0aBcVtoy1TBQqYeQtVDo1L6*Nlpp7EU-1721513123215-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None",  
 "x-content-type-options": "nosniff",  
 "server": "cloudflare",  
 "cf-ray": "8a66409b4f8acee9-SJC",  
 "content-encoding": "br",  
 "alt-svc": "h3=\":443\"; ma=86400"  
}  

```

### Parallel Function calling​
See a detailed walthrough of parallel function calling with litellm here
```
import litellm  
import json  
# set openai api key  
import os  
os.environ['OPENAI_API_KEY'] = "" # litellm reads OPENAI_API_KEY from .env and sends the request  
# Example dummy function hard coded to return the same weather  
# In production, this could be your backend API or an external API  
def get_current_weather(location, unit="fahrenheit"):  
  """Get the current weather in a given location"""  
  if "tokyo" in location.lower():  
    return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})  
  elif "san francisco" in location.lower():  
    return json.dumps({"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"})  
  elif "paris" in location.lower():  
    return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})  
  else:  
    return json.dumps({"location": location, "temperature": "unknown"})  
  
messages = [{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}]  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
  
response = litellm.completion(  
  model="gpt-3.5-turbo-1106",  
  messages=messages,  
  tools=tools,  
  tool_choice="auto", # auto is default, but we'll be explicit  
)  
print("\nLLM Response1:\n", response)  
response_message = response.choices[0].message  
tool_calls = response.choices[0].message.tool_calls  

```

### Setting `extra_headers` for completion calls​
```
import os   
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
response = completion(  
  model = "gpt-3.5-turbo",   
  messages=[{ "content": "Hello, how are you?","role": "user"}],  
  extra_headers={"AI-Resource Group": "ishaan-resource"}  
)  

```

### Setting Organization-ID for completion calls​
This can be set in one of the following ways:
  * Environment Variable `OPENAI_ORGANIZATION`
  * Params to `litellm.completion(model=model, organization="your-organization-id")`
  * Set as `litellm.organization="your-organization-id"`


```
import os   
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
os.environ["OPENAI_ORGANIZATION"] = "your-org-id" # OPTIONAL  
  
response = completion(  
  model = "gpt-3.5-turbo",   
  messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

### Set `ssl_verify=False`​
This is done by setting your own `httpx.Client`
  * For `litellm.completion` set `litellm.client_session=httpx.Client(verify=False)`
  * For `litellm.acompletion` set `litellm.aclient_session=AsyncClient.Client(verify=False)`


```
import litellm, httpx  
  
# for completion  
litellm.client_session = httpx.Client(verify=False)  
response = litellm.completion(  
  model="gpt-3.5-turbo",  
  messages=messages,  
)  
  
# for acompletion  
litellm.aclient_session = httpx.AsyncClient(verify=False)  
response = litellm.acompletion(  
  model="gpt-3.5-turbo",  
  messages=messages,  
)  

```

### Using OpenAI Proxy with LiteLLM​
```
import os   
import litellm  
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = ""  
  
# set custom api base to your proxy  
# either set .env or litellm.api_base  
# os.environ["OPENAI_API_BASE"] = ""  
litellm.api_base = "your-openai-proxy-url"  
  
  
messages = [{ "content": "Hello, how are you?","role": "user"}]  
  
# openai call  
response = completion("openai/your-model-name", messages)  

```

If you need to set api_base dynamically, just pass it in completions instead - `completions(...,api_base="your-proxy-api-base")`
For more check out setting API Base/Keys
### Forwarding Org ID for Proxy requests​
Forward openai Org ID's from the client to OpenAI with `forward_openai_org_id` param. 
  1. Setup config.yaml 


```
model_list:  
 - model_name: "gpt-3.5-turbo"  
  litellm_params:  
   model: gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  
  
general_settings:  
  forward_openai_org_id: true # 👈 KEY CHANGE  

```

  1. Start Proxy


```
litellm --config config.yaml --detailed_debug  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Make OpenAI call


```
from openai import OpenAI  
client = OpenAI(  
  api_key="sk-1234",  
  organization="my-special-org",  
  base_url="http://0.0.0.0:4000"  
)  
  
client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])  

```

In your logs you should see the forwarded org id
```
LiteLLM:DEBUG: utils.py:255 - Request to litellm:  
LiteLLM:DEBUG: utils.py:255 - litellm.acompletion(... organization='my-special-org',)  

```

Previous
Supported Models & Providers
Next
OpenAI (Text Completion)
  * Required API Keys
  * Usage
  * Usage - LiteLLM Proxy Server
  * 1. Save key in your environment
  * 2. Start the proxy
  * 3. Test it
  * Optional Keys - OpenAI Organization, OpenAI API Base
  * OpenAI Chat Completion Models
  * OpenAI Vision Models
  * PDF File Parsing
  * OpenAI Fine Tuned Models
  * OpenAI Audio Transcription
  * Advanced
    * Getting OpenAI API Response Headers
    * Parallel Function calling
    * Setting `extra_headers` for completion calls
    * Setting Organization-ID for completion calls
    * Set `ssl_verify=False`
    * Using OpenAI Proxy with LiteLLM
    * Forwarding Org ID for Proxy requests


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search`⌘``K`
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * NLP Cloud


On this page
# NLP Cloud
LiteLLM supports all LLMs on NLP Cloud.
## API Keys​
```
import os   
  
os.environ["NLP_CLOUD_API_KEY"] = "your-api-key"  

```

## Sample Usage​
```
import os  
from litellm import completion   
  
# set env  
os.environ["NLP_CLOUD_API_KEY"] = "your-api-key"   
  
messages = [{"role": "user", "content": "Hey! how's it going?"}]  
response = completion(model="dolphin", messages=messages)  
print(response)  

```

## streaming​
Just set `stream=True` when calling completion.
```
import os  
from litellm import completion   
  
# set env  
os.environ["NLP_CLOUD_API_KEY"] = "your-api-key"   
  
messages = [{"role": "user", "content": "Hey! how's it going?"}]  
response = completion(model="dolphin", messages=messages, stream=True)  
for chunk in response:  
  print(chunk["choices"][0]["delta"]["content"]) # same as openai format  

```

## non-dolphin models​
By default, LiteLLM will map `dolphin` and `chatdolphin` to nlp cloud. 
If you're trying to call any other model (e.g. GPT-J, Llama-2, etc.) with nlp cloud, just set it as your custom llm provider. 
```
import os  
from litellm import completion   
  
# set env - [OPTIONAL] replace with your nlp cloud key  
os.environ["NLP_CLOUD_API_KEY"] = "your-api-key"   
  
messages = [{"role": "user", "content": "Hey! how's it going?"}]  
  
# e.g. to call Llama2 on NLP Cloud  
response = completion(model="nlp_cloud/finetuned-llama-2-70b", messages=messages, stream=True)  
for chunk in response:  
  print(chunk["choices"][0]["delta"]["content"]) # same as openai format  

```

Previous
AI21
Next
Replicate
  * API Keys
  * Sample Usage
  * streaming
  * non-dolphin models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * OpenAI-Compatible Endpoints


On this page
# OpenAI-Compatible Endpoints
To call models hosted behind an openai proxy, make 2 changes:
  1. For `/chat/completions`: Put `openai/` in front of your model name, so litellm knows you're trying to call an openai `/chat/completions` endpoint. 
  2. For `/completions`: Put `text-completion-openai/` in front of your model name, so litellm knows you're trying to call an openai `/completions` endpoint. [NOT REQUIRED for `openai/` endpoints called via `/v1/completions` route].
  3. **Do NOT** add anything additional to the base url e.g. `/v1/embedding`. LiteLLM uses the openai-client to make these calls, and that automatically adds the relevant endpoints. 


## Usage - completion​
```
import litellm  
import os  
  
response = litellm.completion(  
  model="openai/mistral",        # add `openai/` prefix to model so litellm knows to route to OpenAI  
  api_key="sk-1234",         # api key to your openai compatible endpoint  
  api_base="http://0.0.0.0:4000",   # set API Base of your Custom OpenAI Endpoint  
  messages=[  
        {  
          "role": "user",  
          "content": "Hey, how's it going?",  
        }  
  ],  
)  
print(response)  

```

## Usage - embedding​
```
import litellm  
import os  
  
response = litellm.embedding(  
  model="openai/GPT-J",        # add `openai/` prefix to model so litellm knows to route to OpenAI  
  api_key="sk-1234",         # api key to your openai compatible endpoint  
  api_base="http://0.0.0.0:4000",   # set API Base of your Custom OpenAI Endpoint  
  input=["good morning from litellm"]  
)  
print(response)  

```

## Usage with LiteLLM Proxy Server​
Here's how to call an OpenAI-Compatible Endpoint with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: openai/<your-model-name> # add openai/ prefix to route as OpenAI provider  
   api_base: <model-api-base>    # add api base for OpenAI compatible provider  
   api_key: api-key         # api key to send your model  

```

info
If you see `Not Found Error` when testing make sure your `api_base` has the `/v1` postfix
Example: `http://vllm-endpoint.xyz/v1`
  2. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  3. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



### Advanced - Disable System Messages​
Some VLLM models (e.g. gemma) don't support system messages. To map those requests to 'user' messages, use the `supports_system_message` flag. 
```
model_list:  
- model_name: my-custom-model  
  litellm_params:  
   model: openai/google/gemma  
   api_base: http://my-custom-base  
   api_key: ""   
   supports_system_message: False # 👈 KEY CHANGE  

```

Previous
OpenAI (Text Completion)
Next
Azure OpenAI
  * Usage - completion
  * Usage - embedding
  * Usage with LiteLLM Proxy Server
    * Advanced - Disable System Messages


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# Page Not Found
We could not find what you were looking for.
Please contact the owner of the site that linked you to the original URL and let them know their link is broken.
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# Page Not Found
We could not find what you were looking for.
Please contact the owner of the site that linked you to the original URL and let them know their link is broken.
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * OpenRouter


On this page
# OpenRouter
LiteLLM supports all the text / chat / vision models from OpenRouter
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
## Usage​
```
import os  
from litellm import completion  
os.environ["OPENROUTER_API_KEY"] = ""  
os.environ["OPENROUTER_API_BASE"] = "" # [OPTIONAL] defaults to https://openrouter.ai/api/v1  
  
  
os.environ["OR_SITE_URL"] = "" # [OPTIONAL]  
os.environ["OR_APP_NAME"] = "" # [OPTIONAL]  
  
response = completion(  
      model="openrouter/google/palm-2-chat-bison",  
      messages=messages,  
    )  

```

## OpenRouter Completion Models​
🚨 LiteLLM supports ALL OpenRouter models, send `model=openrouter/<your-openrouter-model>` to send it to open router. See all openrouter models here
Model Name| Function Call  
---|---  
openrouter/openai/gpt-3.5-turbo| `completion('openrouter/openai/gpt-3.5-turbo', messages)`  
openrouter/openai/gpt-3.5-turbo-16k| `completion('openrouter/openai/gpt-3.5-turbo-16k', messages)`  
openrouter/openai/gpt-4| `completion('openrouter/openai/gpt-4', messages)`  
openrouter/openai/gpt-4-32k| `completion('openrouter/openai/gpt-4-32k', messages)`  
openrouter/anthropic/claude-2| `completion('openrouter/anthropic/claude-2', messages)`  
openrouter/anthropic/claude-instant-v1| `completion('openrouter/anthropic/claude-instant-v1', messages)`  
openrouter/google/palm-2-chat-bison| `completion('openrouter/google/palm-2-chat-bison', messages)`  
openrouter/google/palm-2-codechat-bison| `completion('openrouter/google/palm-2-codechat-bison', messages)`  
openrouter/meta-llama/llama-2-13b-chat| `completion('openrouter/meta-llama/llama-2-13b-chat', messages)`  
openrouter/meta-llama/llama-2-70b-chat| `completion('openrouter/meta-llama/llama-2-70b-chat', messages)`  
## Passing OpenRouter Params - transforms, models, route​
Pass `transforms`, `models`, `route`as arguments to `litellm.completion()`
```
import os  
from litellm import completion  
os.environ["OPENROUTER_API_KEY"] = ""  
  
response = completion(  
      model="openrouter/google/palm-2-chat-bison",  
      messages=messages,  
      transforms = [""],  
      route= ""  
    )  

```

Previous
Baseten
Next
Sambanova
  * Usage
  * OpenRouter Completion Models
  * Passing OpenRouter Params - transforms, models, route


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Sambanova


On this page
# Sambanova
https://cloud.sambanova.ai/
tip
**We support ALL Sambanova models, just set`model=sambanova/<any-model-on-sambanova>` as a prefix when sending litellm requests. For the complete supported model list, visit https://docs.sambanova.ai/cloud/docs/get-started/supported-models **
## API Key​
```
# env variable  
os.environ['SAMBANOVA_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['SAMBANOVA_API_KEY'] = ""  
response = completion(  
  model="sambanova/Meta-Llama-3.1-8B-Instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What do you know about sambanova.ai. Give your response in json format",  
    }  
  ],  
  max_tokens=10,  
  response_format={ "type": "json_object" },  
  stop=["\n\n"],  
  temperature=0.2,  
  top_p=0.9,  
  tool_choice="auto",  
  tools=[],  
  user="user",  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['SAMBANOVA_API_KEY'] = ""  
response = completion(  
  model="sambanova/Meta-Llama-3.1-8B-Instruct",  
  messages=[  
    {  
      "role": "user",  
      "content": "What do you know about sambanova.ai. Give your response in json format",  
    }  
  ],  
  stream=True,  
  max_tokens=10,  
  response_format={ "type": "json_object" },  
  stop=["\n\n"],  
  temperature=0.2,  
  top_p=0.9,  
  tool_choice="auto",  
  tools=[],  
  user="user",  
)  
  
for chunk in response:  
  print(chunk)  

```

## Usage with LiteLLM Proxy Server​
Here's how to call a Sambanova model with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: sambanova/<your-model-name> # add sambanova/ prefix to route as Sambanova provider  
   api_key: api-key         # api key to send your model  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



Previous
OpenRouter
Next
Custom API Server (Custom Format)
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Usage with LiteLLM Proxy Server


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Snowflake


On this page
# Snowflake
Property| Details  
---|---  
Description| The Snowflake Cortex LLM REST API lets you access the COMPLETE function via HTTP POST requests  
Provider Route on LiteLLM| `snowflake/`  
Link to Provider Doc| Snowflake ↗  
Base URL| https://{account-id}.snowflakecomputing.com/api/v2/cortex/inference:complete/  
Supported OpenAI Endpoints| `/chat/completions`, `/completions`  
Currently, Snowflake's REST API does not have an endpoint for `snowflake-arctic-embed` embedding models. If you want to use these embedding models with Litellm, you can call them through our Hugging Face provider. 
Find the Arctic Embed models here on Hugging Face.
## Supported OpenAI Parameters​
```
  "temperature",  
  "max_tokens",  
  "top_p",  
  "response_format"  

```

## API KEYS​
Snowflake does have API keys. Instead, you access the Snowflake API with your JWT token and account identifier.
```
import os   
os.environ["SNOWFLAKE_JWT"] = "YOUR JWT"  
os.environ["SNOWFLAKE_ACCOUNT_ID"] = "YOUR ACCOUNT IDENTIFIER"  

```

## Usage​
```
from litellm import completion  
  
## set ENV variables  
os.environ["SNOWFLAKE_JWT"] = "YOUR JWT"  
os.environ["SNOWFLAKE_ACCOUNT_ID"] = "YOUR ACCOUNT IDENTIFIER"  
  
# Snowflake call  
response = completion(  
  model="snowflake/mistral-7b",   
  messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

## Usage with LiteLLM Proxy​
#### 1. Required env variables​
```
export SNOWFLAKE_JWT=""  
export SNOWFLAKE_ACCOUNT_ID = ""  

```

#### 2. Start the proxy~​
```
model_list:  
 - model_name: mistral-7b  
  litellm_params:  
    model: snowflake/mistral-7b  
    api_key: YOUR_API_KEY  
    api_base: https://YOUR-ACCOUNT-ID.snowflakecomputing.com/api/v2/cortex/inference:complete  
  

```

```
litellm --config /path/to/config.yaml  

```

#### 3. Test it​
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "snowflake/mistral-7b",  
   "messages": [  
    {  
     "role": "user",  
     "content": "Hello, how are you?"  
    }  
   ]  
  }  
'  

```

Previous
Petals
Next
Exception Mapping
  * Supported OpenAI Parameters
  * API KEYS
  * Usage
  * Usage with LiteLLM Proxy


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Replicate


On this page
# Replicate
LiteLLM supports all models on Replicate
## Usage​
  * SDK
  * PROXY


### API KEYS​
```
import os   
os.environ["REPLICATE_API_KEY"] = ""  

```

### Example Call​
```
from litellm import completion  
import os  
## set ENV variables  
os.environ["REPLICATE_API_KEY"] = "replicate key"  
  
# replicate llama-3 call  
response = completion(  
  model="replicate/meta/meta-llama-3-8b-instruct",   
  messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

  1. Add models to your config.yaml
```
model_list:  
 - model_name: llama-3  
  litellm_params:  
   model: replicate/meta/meta-llama-3-8b-instruct  
   api_key: os.environ/REPLICATE_API_KEY  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml --debug  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="llama-3",  
  messages = [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
 ]  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "llama-3",  
  "messages": [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
   ],  
}'  

```



### Expected Replicate Call​
This is the call litellm will make to replicate, from the above example: 
```
  
POST Request Sent from LiteLLM:  
curl -X POST \  
https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct \  
-H 'Authorization: Token your-api-key' -H 'Content-Type: application/json' \  
-d '{'version': 'meta/meta-llama-3-8b-instruct', 'input': {'prompt': '<|start_header_id|>system<|end_header_id|>\n\nBe a good human!<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat do you know about earth?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n'}}'  

```

## Advanced Usage - Prompt Formatting​
LiteLLM has prompt template mappings for all `meta-llama` llama3 instruct models. **See Code**
To apply a custom prompt template: 
  * SDK
  * PROXY


```
import litellm  
  
import os   
os.environ["REPLICATE_API_KEY"] = ""  
  
# Create your own custom prompt template   
litellm.register_prompt_template(  
    model="togethercomputer/LLaMA-2-7B-32K",  
    initial_prompt_value="You are a good assistant" # [OPTIONAL]  
    roles={  
      "system": {  
        "pre_message": "[INST] <<SYS>>\n", # [OPTIONAL]  
        "post_message": "\n<</SYS>>\n [/INST]\n" # [OPTIONAL]  
      },  
      "user": {   
        "pre_message": "[INST] ", # [OPTIONAL]  
        "post_message": " [/INST]" # [OPTIONAL]  
      },   
      "assistant": {  
        "pre_message": "\n" # [OPTIONAL]  
        "post_message": "\n" # [OPTIONAL]  
      }  
    }  
    final_prompt_value="Now answer as best you can:" # [OPTIONAL]  
)  
  
def test_replicate_custom_model():  
  model = "replicate/togethercomputer/LLaMA-2-7B-32K"  
  response = completion(model=model, messages=messages)  
  print(response['choices'][0]['message']['content'])  
  return response  
  
test_replicate_custom_model()  

```

```
# Model-specific parameters  
model_list:  
 - model_name: mistral-7b # model alias  
  litellm_params: # actual params for litellm.completion()  
   model: "replicate/mistralai/Mistral-7B-Instruct-v0.1"   
   api_key: os.environ/REPLICATE_API_KEY  
   initial_prompt_value: "\n"  
   roles: {"system":{"pre_message":"<|im_start|>system\n", "post_message":"<|im_end|>"}, "assistant":{"pre_message":"<|im_start|>assistant\n","post_message":"<|im_end|>"}, "user":{"pre_message":"<|im_start|>user\n","post_message":"<|im_end|>"}}  
   final_prompt_value: "\n"  
   bos_token: "<s>"  
   eos_token: "</s>"  
   max_tokens: 4096  

```

## Advanced Usage - Calling Replicate Deployments​
Calling a deployed replicate LLM Add the `replicate/deployments/` prefix to your model, so litellm will call the `deployments` endpoint. This will call `ishaan-jaff/ishaan-mistral` deployment on replicate
```
response = completion(  
  model="replicate/deployments/ishaan-jaff/ishaan-mistral",   
  messages= [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

Replicate Cold Boots
Replicate responses can take 3-5 mins due to replicate cold boots, if you're trying to debug try making the request with `litellm.set_verbose=True`. More info on replicate cold boots
## Replicate Models​
liteLLM supports all replicate LLMs
For replicate models ensure to add a `replicate/` prefix to the `model` arg. liteLLM detects it using this arg. 
Below are examples on how to call replicate LLMs using liteLLM 
Model Name| Function Call| Required OS Variables  
---|---|---  
replicate/llama-2-70b-chat| `completion(model='replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf', messages)`| `os.environ['REPLICATE_API_KEY']`  
a16z-infra/llama-2-13b-chat| `completion(model='replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52', messages)`| `os.environ['REPLICATE_API_KEY']`  
replicate/vicuna-13b| `completion(model='replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b', messages)`| `os.environ['REPLICATE_API_KEY']`  
daanelson/flan-t5-large| `completion(model='replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f', messages)`| `os.environ['REPLICATE_API_KEY']`  
custom-llm| `completion(model='replicate/custom-llm-version-id', messages)`| `os.environ['REPLICATE_API_KEY']`  
replicate deployment| `completion(model='replicate/deployments/ishaan-jaff/ishaan-mistral', messages)`| `os.environ['REPLICATE_API_KEY']`  
## Passing additional params - max_tokens, temperature​
See all litellm.completion supported params here
```
# !pip install litellm  
from litellm import completion  
import os  
## set ENV variables  
os.environ["REPLICATE_API_KEY"] = "replicate key"  
  
# replicate llama-2 call  
response = completion(  
  model="replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf",   
  messages = [{ "content": "Hello, how are you?","role": "user"}],  
  max_tokens=20,  
  temperature=0.5  
)  

```

**proxy**
```
 model_list:  
  - model_name: llama-3  
   litellm_params:  
    model: replicate/meta/meta-llama-3-8b-instruct  
    api_key: os.environ/REPLICATE_API_KEY  
    max_tokens: 20  
    temperature: 0.5  

```

## Passings Replicate specific params​
Send params not supported by `litellm.completion()` but supported by Replicate by passing them to `litellm.completion`
Example `seed`, `min_tokens` are Replicate specific param
```
# !pip install litellm  
from litellm import completion  
import os  
## set ENV variables  
os.environ["REPLICATE_API_KEY"] = "replicate key"  
  
# replicate llama-2 call  
response = completion(  
  model="replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf",   
  messages = [{ "content": "Hello, how are you?","role": "user"}],  
  seed=-1,  
  min_tokens=2,  
  top_k=20,  
)  

```

**proxy**
```
 model_list:  
  - model_name: llama-3  
   litellm_params:  
    model: replicate/meta/meta-llama-3-8b-instruct  
    api_key: os.environ/REPLICATE_API_KEY  
    min_tokens: 2  
    top_k: 20  

```

Previous
NLP Cloud
Next
Together AI
  * Usage
    * API KEYS
    * Example Call
    * Expected Replicate Call
  * Advanced Usage - Prompt Formatting
  * Advanced Usage - Calling Replicate Deployments
  * Replicate Models
  * Passing additional params - max_tokens, temperature
  * Passings Replicate specific params


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Predibase


On this page
# Predibase
LiteLLM supports all models on Predibase
## Usage​
  * SDK
  * PROXY


### API KEYS​
```
import os   
os.environ["PREDIBASE_API_KEY"] = ""  

```

### Example Call​
```
from litellm import completion  
import os  
## set ENV variables  
os.environ["PREDIBASE_API_KEY"] = "predibase key"  
os.environ["PREDIBASE_TENANT_ID"] = "predibase tenant id"  
  
# predibase llama-3 call  
response = completion(  
  model="predibase/llama-3-8b-instruct",   
  messages = [{ "content": "Hello, how are you?","role": "user"}]  
)  

```

  1. Add models to your config.yaml
```
model_list:  
 - model_name: llama-3  
  litellm_params:  
   model: predibase/llama-3-8b-instruct  
   api_key: os.environ/PREDIBASE_API_KEY  
   tenant_id: os.environ/PREDIBASE_TENANT_ID  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml --debug  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="llama-3",  
  messages = [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
 ]  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "llama-3",  
  "messages": [  
   {  
     "role": "system",  
     "content": "Be a good human!"  
   },  
   {  
     "role": "user",  
     "content": "What do you know about earth?"  
   }  
   ],  
}'  

```



## Advanced Usage - Prompt Formatting​
LiteLLM has prompt template mappings for all `meta-llama` llama3 instruct models. **See Code**
To apply a custom prompt template: 
  * SDK
  * PROXY


```
import litellm  
  
import os   
os.environ["PREDIBASE_API_KEY"] = ""  
  
# Create your own custom prompt template   
litellm.register_prompt_template(  
    model="togethercomputer/LLaMA-2-7B-32K",  
    initial_prompt_value="You are a good assistant" # [OPTIONAL]  
    roles={  
      "system": {  
        "pre_message": "[INST] <<SYS>>\n", # [OPTIONAL]  
        "post_message": "\n<</SYS>>\n [/INST]\n" # [OPTIONAL]  
      },  
      "user": {   
        "pre_message": "[INST] ", # [OPTIONAL]  
        "post_message": " [/INST]" # [OPTIONAL]  
      },   
      "assistant": {  
        "pre_message": "\n" # [OPTIONAL]  
        "post_message": "\n" # [OPTIONAL]  
      }  
    }  
    final_prompt_value="Now answer as best you can:" # [OPTIONAL]  
)  
  
def predibase_custom_model():  
  model = "predibase/togethercomputer/LLaMA-2-7B-32K"  
  response = completion(model=model, messages=messages)  
  print(response['choices'][0]['message']['content'])  
  return response  
  
predibase_custom_model()  

```

```
# Model-specific parameters  
model_list:  
 - model_name: mistral-7b # model alias  
  litellm_params: # actual params for litellm.completion()  
   model: "predibase/mistralai/Mistral-7B-Instruct-v0.1"   
   api_key: os.environ/PREDIBASE_API_KEY  
   initial_prompt_value: "\n"  
   roles: {"system":{"pre_message":"<|im_start|>system\n", "post_message":"<|im_end|>"}, "assistant":{"pre_message":"<|im_start|>assistant\n","post_message":"<|im_end|>"}, "user":{"pre_message":"<|im_start|>user\n","post_message":"<|im_end|>"}}  
   final_prompt_value: "\n"  
   bos_token: "<s>"  
   eos_token: "</s>"  
   max_tokens: 4096  

```

## Passing additional params - max_tokens, temperature​
See all litellm.completion supported params here
```
# !pip install litellm  
from litellm import completion  
import os  
## set ENV variables  
os.environ["PREDIBASE_API_KEY"] = "predibase key"  
  
# predibae llama-3 call  
response = completion(  
  model="predibase/llama3-8b-instruct",   
  messages = [{ "content": "Hello, how are you?","role": "user"}],  
  max_tokens=20,  
  temperature=0.5  
)  

```

**proxy**
```
 model_list:  
  - model_name: llama-3  
   litellm_params:  
    model: predibase/llama-3-8b-instruct  
    api_key: os.environ/PREDIBASE_API_KEY  
    max_tokens: 20  
    temperature: 0.5  

```

## Passings Predibase specific params - adapter_id, adapter_source,​
Send params not supported by `litellm.completion()` but supported by Predibase by passing them to `litellm.completion`
Example `adapter_id`, `adapter_source` are Predibase specific param - See List
```
# !pip install litellm  
from litellm import completion  
import os  
## set ENV variables  
os.environ["PREDIBASE_API_KEY"] = "predibase key"  
  
# predibase llama3 call  
response = completion(  
  model="predibase/llama-3-8b-instruct",   
  messages = [{ "content": "Hello, how are you?","role": "user"}],  
  adapter_id="my_repo/3",  
  adapter_source="pbase",  
)  

```

**proxy**
```
 model_list:  
  - model_name: llama-3  
   litellm_params:  
    model: predibase/llama-3-8b-instruct  
    api_key: os.environ/PREDIBASE_API_KEY  
    adapter_id: my_repo/3  
    adapter_source: pbase  

```

Previous
IBM watsonx.ai
Next
Nvidia NIM
  * Usage
    * API KEYS
    * Example Call
  * Advanced Usage - Prompt Formatting
  * Passing additional params - max_tokens, temperature
  * Passings Predibase specific params - adapter_id, adapter_source,


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Perplexity AI (pplx-api)


On this page
# Perplexity AI (pplx-api)
https://www.perplexity.ai
## API Key​
```
# env variable  
os.environ['PERPLEXITYAI_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['PERPLEXITYAI_API_KEY'] = ""  
response = completion(  
  model="perplexity/sonar-pro",   
  messages=messages  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['PERPLEXITYAI_API_KEY'] = ""  
response = completion(  
  model="perplexity/sonar-pro",   
  messages=messages,  
  stream=True  
)  
  
for chunk in response:  
  print(chunk)  

```

## Supported Models​
All models listed here https://docs.perplexity.ai/docs/model-cards are supported. Just do `model=perplexity/<model-name>`.
Model Name| Function Call  
---|---  
sonar-deep-research| `completion(model="perplexity/sonar-deep-research", messages)`  
sonar-reasoning-pro| `completion(model="perplexity/sonar-reasoning-pro", messages)`  
sonar-reasoning| `completion(model="perplexity/sonar-reasoning", messages)`  
sonar-pro| `completion(model="perplexity/sonar-pro", messages)`  
sonar| `completion(model="perplexity/sonar", messages)`  
r1-1776| `completion(model="perplexity/r1-1776", messages)`  
info
For more information about passing provider-specific parameters, go here
Previous
Ollama
Next
FriendliAI
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Petals


On this page
# Petals
Petals: https://github.com/bigscience-workshop/petals
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
## Pre-Requisites​
Ensure you have `petals` installed
```
pip install git+https://github.com/bigscience-workshop/petals  

```

## Usage​
Ensure you add `petals/` as a prefix for all petals LLMs. This sets the custom_llm_provider to petals
```
from litellm import completion  
  
response = completion(  
  model="petals/petals-team/StableBeluga2",   
  messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  
  
print(response)  

```

## Usage with Streaming​
```
response = completion(  
  model="petals/petals-team/StableBeluga2",   
  messages=[{ "content": "Hello, how are you?","role": "user"}],  
  stream=True  
)  
  
print(response)  
for chunk in response:  
 print(chunk)  

```

### Model Details​
Model Name| Function Call  
---|---  
petals-team/StableBeluga| `completion('petals/petals-team/StableBeluga2', messages)`  
huggyllama/llama-65b| `completion('petals/huggyllama/llama-65b', messages)`  
Previous
Custom API Server (Custom Format)
Next
Snowflake
  * Pre-Requisites
  * Usage
  * Usage with Streaming
    * Model Details


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * OpenAI (Text Completion)


On this page
# OpenAI (Text Completion)
LiteLLM supports OpenAI text completion models
### Required API Keys​
```
import os   
os.environ["OPENAI_API_KEY"] = "your-api-key"  

```

### Usage​
```
import os   
from litellm import completion  
  
os.environ["OPENAI_API_KEY"] = "your-api-key"  
  
# openai call  
response = completion(  
  model = "gpt-3.5-turbo-instruct",   
  messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

### Usage - LiteLLM Proxy Server​
Here's how to call OpenAI models with the LiteLLM Proxy Server
### 1. Save key in your environment​
```
export OPENAI_API_KEY=""  

```

### 2. Start the proxy​
  * config.yaml
  * config.yaml - proxy all OpenAI models
  * CLI


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: openai/gpt-3.5-turbo             # The `openai/` prefix will call openai.chat.completions.create  
   api_key: os.environ/OPENAI_API_KEY  
 - model_name: gpt-3.5-turbo-instruct  
  litellm_params:  
   model: text-completion-openai/gpt-3.5-turbo-instruct # The `text-completion-openai/` prefix will call openai.completions.create  
   api_key: os.environ/OPENAI_API_KEY  

```

Use this to add all openai models with one API Key. **WARNING: This will not do any load balancing** This means requests to `gpt-4`, `gpt-3.5-turbo` , `gpt-4-turbo-preview` will all go through this route 
```
model_list:  
 - model_name: "*"       # all requests where model not in your config go to this deployment  
  litellm_params:  
   model: openai/*      # set `openai/` to use the openai route  
   api_key: os.environ/OPENAI_API_KEY  

```

```
$ litellm --model gpt-3.5-turbo-instruct  
  
# Server running on http://0.0.0.0:4000  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo-instruct",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo-instruct", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "gpt-3.5-turbo-instruct",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## OpenAI Text Completion Models / Instruct Models​
Model Name| Function Call  
---|---  
gpt-3.5-turbo-instruct| `response = completion(model="gpt-3.5-turbo-instruct", messages=messages)`  
gpt-3.5-turbo-instruct-0914| `response = completion(model="gpt-3.5-turbo-instruct-0914", messages=messages)`  
text-davinci-003| `response = completion(model="text-davinci-003", messages=messages)`  
ada-001| `response = completion(model="ada-001", messages=messages)`  
curie-001| `response = completion(model="curie-001", messages=messages)`  
babbage-001| `response = completion(model="babbage-001", messages=messages)`  
babbage-002| `response = completion(model="babbage-002", messages=messages)`  
davinci-002| `response = completion(model="davinci-002", messages=messages)`  
Previous
OpenAI
Next
OpenAI-Compatible Endpoints
  * Required API Keys
  * Usage
  * Usage - LiteLLM Proxy Server
  * 1. Save key in your environment
  * 2. Start the proxy
  * 3. Test it
  * OpenAI Text Completion Models / Instruct Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Topaz


On this page
# Topaz
Property| Details  
---|---  
Description| Professional-grade photo and video editing powered by AI.  
Provider Route on LiteLLM| `topaz/`  
Provider Doc| Topaz ↗  
API Endpoint for Provider| https://api.topazlabs.com  
Supported OpenAI Endpoints| `/image/variations`  
## Quick Start​
```
from litellm import image_variation  
import os   
  
os.environ["TOPAZ_API_KEY"] = ""  
response = image_variation(  
  model="topaz/Standard V2", image=image_url  
)  

```

## Supported OpenAI Params​
  * `response_format`
  * `size` (widthxheight)


Previous
Galadriel
Next
Groq
  * Quick Start
  * Supported OpenAI Params


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Together AI


On this page
# Together AI
LiteLLM supports all models on Together AI. 
## API Keys​
```
import os   
os.environ["TOGETHERAI_API_KEY"] = "your-api-key"  

```

## Sample Usage​
```
from litellm import completion   
  
os.environ["TOGETHERAI_API_KEY"] = "your-api-key"  
  
messages = [{"role": "user", "content": "Write me a poem about the blue sky"}]  
  
completion(model="together_ai/togethercomputer/Llama-2-7B-32K-Instruct", messages=messages)  

```

## Together AI Models​
liteLLM supports `non-streaming` and `streaming` requests to all models on https://api.together.xyz/
Example TogetherAI Usage - Note: liteLLM supports all models deployed on TogetherAI
### Llama LLMs - Chat​
Model Name| Function Call| Required OS Variables  
---|---|---  
togethercomputer/llama-2-70b-chat| `completion('together_ai/togethercomputer/llama-2-70b-chat', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
### Llama LLMs - Language / Instruct​
Model Name| Function Call| Required OS Variables  
---|---|---  
togethercomputer/llama-2-70b| `completion('together_ai/togethercomputer/llama-2-70b', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
togethercomputer/LLaMA-2-7B-32K| `completion('together_ai/togethercomputer/LLaMA-2-7B-32K', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
togethercomputer/Llama-2-7B-32K-Instruct| `completion('together_ai/togethercomputer/Llama-2-7B-32K-Instruct', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
togethercomputer/llama-2-7b| `completion('together_ai/togethercomputer/llama-2-7b', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
### Falcon LLMs​
Model Name| Function Call| Required OS Variables  
---|---|---  
togethercomputer/falcon-40b-instruct| `completion('together_ai/togethercomputer/falcon-40b-instruct', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
togethercomputer/falcon-7b-instruct| `completion('together_ai/togethercomputer/falcon-7b-instruct', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
### Alpaca LLMs​
Model Name| Function Call| Required OS Variables  
---|---|---  
togethercomputer/alpaca-7b| `completion('together_ai/togethercomputer/alpaca-7b', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
### Other Chat LLMs​
Model Name| Function Call| Required OS Variables  
---|---|---  
HuggingFaceH4/starchat-alpha| `completion('together_ai/HuggingFaceH4/starchat-alpha', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
### Code LLMs​
Model Name| Function Call| Required OS Variables  
---|---|---  
togethercomputer/CodeLlama-34b| `completion('together_ai/togethercomputer/CodeLlama-34b', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
togethercomputer/CodeLlama-34b-Instruct| `completion('together_ai/togethercomputer/CodeLlama-34b-Instruct', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
togethercomputer/CodeLlama-34b-Python| `completion('together_ai/togethercomputer/CodeLlama-34b-Python', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
defog/sqlcoder| `completion('together_ai/defog/sqlcoder', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
NumbersStation/nsql-llama-2-7B| `completion('together_ai/NumbersStation/nsql-llama-2-7B', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
WizardLM/WizardCoder-15B-V1.0| `completion('together_ai/WizardLM/WizardCoder-15B-V1.0', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
WizardLM/WizardCoder-Python-34B-V1.0| `completion('together_ai/WizardLM/WizardCoder-Python-34B-V1.0', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
### Language LLMs​
Model Name| Function Call| Required OS Variables  
---|---|---  
NousResearch/Nous-Hermes-Llama2-13b| `completion('together_ai/NousResearch/Nous-Hermes-Llama2-13b', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
Austism/chronos-hermes-13b| `completion('together_ai/Austism/chronos-hermes-13b', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
upstage/SOLAR-0-70b-16bit| `completion('together_ai/upstage/SOLAR-0-70b-16bit', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
WizardLM/WizardLM-70B-V1.0| `completion('together_ai/WizardLM/WizardLM-70B-V1.0', messages)`| `os.environ['TOGETHERAI_API_KEY']`  
## Prompt Templates​
Using a chat model on Together AI with it's own prompt format?
### Using Llama2 Instruct models​
If you're using Together AI's Llama2 variants( `model=togethercomputer/llama-2..-instruct`), LiteLLM can automatically translate between the OpenAI prompt format and the TogetherAI Llama2 one (`[INST]..[/INST]`). 
```
from litellm import completion   
  
# set env variable   
os.environ["TOGETHERAI_API_KEY"] = ""  
  
messages = [{"role": "user", "content": "Write me a poem about the blue sky"}]  
  
completion(model="together_ai/togethercomputer/Llama-2-7B-32K-Instruct", messages=messages)  

```

### Using another model​
You can create a custom prompt template on LiteLLM (and we welcome PRs to add them to the main repo 🤗)
Let's make one for `OpenAssistant/llama2-70b-oasst-sft-v10`!
The accepted template format is: Reference
```
"""  
<|im_start|>system  
{system_message}<|im_end|>  
<|im_start|>user  
{prompt}<|im_end|>  
<|im_start|>assistant  
"""  

```

Let's register our custom prompt template: Implementation Code
```
import litellm   
  
litellm.register_prompt_template(  
    model="OpenAssistant/llama2-70b-oasst-sft-v10",  
    roles={  
      "system": {  
        "pre_message": "[<|im_start|>system",  
        "post_message": "\n"  
      },  
      "user": {  
        "pre_message": "<|im_start|>user",  
        "post_message": "\n"  
      },   
      "assistant": {  
        "pre_message": "<|im_start|>assistant",  
        "post_message": "\n"  
      }  
    }  
  )  

```

Let's use it! 
```
from litellm import completion   
  
# set env variable   
os.environ["TOGETHERAI_API_KEY"] = ""  
  
messages=[{"role":"user", "content": "Write me a poem about the blue sky"}]  
  
completion(model="together_ai/OpenAssistant/llama2-70b-oasst-sft-v10", messages=messages)  

```

**Complete Code**
```
import litellm   
from litellm import completion  
  
# set env variable   
os.environ["TOGETHERAI_API_KEY"] = ""  
  
litellm.register_prompt_template(  
    model="OpenAssistant/llama2-70b-oasst-sft-v10",  
    roles={  
      "system": {  
        "pre_message": "[<|im_start|>system",  
        "post_message": "\n"  
      },  
      "user": {  
        "pre_message": "<|im_start|>user",  
        "post_message": "\n"  
      },   
      "assistant": {  
        "pre_message": "<|im_start|>assistant",  
        "post_message": "\n"  
      }  
    }  
  )  
  
messages=[{"role":"user", "content": "Write me a poem about the blue sky"}]  
  
response = completion(model="together_ai/OpenAssistant/llama2-70b-oasst-sft-v10", messages=messages)  
  
print(response)  

```

**Output**
```
{  
 "choices": [  
  {  
   "finish_reason": "stop",  
   "index": 0,  
   "message": {  
    "content": ".\n\nThe sky is a canvas of blue,\nWith clouds that drift and move,",  
    "role": "assistant",  
    "logprobs": null  
   }  
  }  
 ],  
 "created": 1693941410.482018,  
 "model": "OpenAssistant/llama2-70b-oasst-sft-v10",  
 "usage": {  
  "prompt_tokens": 7,  
  "completion_tokens": 16,  
  "total_tokens": 23  
 },  
 "litellm_call_id": "f21315db-afd6-4c1e-b43a-0b5682de4b06"  
}  

```

## Rerank​
### Usage​
  * LiteLLM SDK Usage
  * LiteLLM Proxy Usage


```
from litellm import rerank  
import os  
  
os.environ["TOGETHERAI_API_KEY"] = "sk-.."  
  
query = "What is the capital of the United States?"  
documents = [  
  "Carson City is the capital city of the American state of Nevada.",  
  "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
  "Washington, D.C. is the capital of the United States.",  
  "Capital punishment has existed in the United States since before it was a country.",  
]  
  
response = rerank(  
  model="together_ai/rerank-english-v3.0",  
  query=query,  
  documents=documents,  
  top_n=3,  
)  
print(response)  

```

LiteLLM provides an cohere api compatible `/rerank` endpoint for Rerank calls.
**Setup**
Add this to your litellm proxy config.yaml
```
model_list:  
 - model_name: Salesforce/Llama-Rank-V1  
  litellm_params:  
   model: together_ai/Salesforce/Llama-Rank-V1  
   api_key: os.environ/TOGETHERAI_API_KEY  

```

Start litellm
```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

Test request
```
curl http://0.0.0.0:4000/rerank \  
 -H "Authorization: Bearer sk-1234" \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "Salesforce/Llama-Rank-V1",  
  "query": "What is the capital of the United States?",  
  "documents": [  
    "Carson City is the capital city of the American state of Nevada.",  
    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",  
    "Washington, D.C. is the capital of the United States.",  
    "Capital punishment has existed in the United States since before it was a country."  
  ],  
  "top_n": 3  
 }'  

```

Previous
Replicate
Next
Voyage AI
  * API Keys
  * Sample Usage
  * Together AI Models
    * Llama LLMs - Chat
    * Llama LLMs - Language / Instruct
    * Falcon LLMs
    * Alpaca LLMs
    * Other Chat LLMs
    * Code LLMs
    * Language LLMs
  * Prompt Templates
    * Using Llama2 Instruct models
    * Using another model
  * Rerank
    * Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Triton Inference Server


On this page
# Triton Inference Server
LiteLLM supports Embedding Models on Triton Inference Servers
Property| Details  
---|---  
Description| NVIDIA Triton Inference Server  
Provider Route on LiteLLM| `triton/`  
Supported Operations| `/chat/completion`, `/completion`, `/embedding`  
Supported Triton endpoints| `/infer`, `/generate`, `/embeddings`  
Link to Provider Doc| Triton Inference Server ↗  
## Triton `/generate` - Chat Completion​
  * SDK
  * PROXY


Use the `triton/` prefix to route to triton server
```
from litellm import completion  
response = completion(  
  model="triton/llama-3-8b-instruct",  
  messages=[{"role": "user", "content": "who are u?"}],  
  max_tokens=10,  
  api_base="http://localhost:8000/generate",  
)  

```

  1. Add models to your config.yaml
```
model_list:  
 - model_name: my-triton-model  
  litellm_params:  
   model: triton/<your-triton-model>"  
   api_base: https://your-triton-api-base/triton/generate  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml --detailed_debug  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
from openai import OpenAI  
  
# set base_url to your proxy server  
# set api_key to send to proxy server  
client = OpenAI(api_key="<proxy-api-key>", base_url="http://0.0.0.0:4000")  
  
response = client.chat.completions.create(  
  model="my-triton-model",  
  messages=[{"role": "user", "content": "who are u?"}],  
  max_tokens=10,  
)  
  
print(response)  
  

```

`--header` is optional, only required if you're using litellm proxy with Virtual Keys
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer sk-1234' \  
--data ' {  
"model": "my-triton-model",  
"messages": [{"role": "user", "content": "who are u?"}]  
}'  
  

```



## Triton `/infer` - Chat Completion​
  * SDK
  * PROXY


Use the `triton/` prefix to route to triton server
```
from litellm import completion  
  
  
response = completion(  
  model="triton/llama-3-8b-instruct",  
  messages=[{"role": "user", "content": "who are u?"}],  
  max_tokens=10,  
  api_base="http://localhost:8000/infer",  
)  

```

  1. Add models to your config.yaml
```
model_list:  
 - model_name: my-triton-model  
  litellm_params:  
   model: triton/<your-triton-model>"  
   api_base: https://your-triton-api-base/triton/infer  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml --detailed_debug  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
from openai import OpenAI  
  
# set base_url to your proxy server  
# set api_key to send to proxy server  
client = OpenAI(api_key="<proxy-api-key>", base_url="http://0.0.0.0:4000")  
  
response = client.chat.completions.create(  
  model="my-triton-model",  
  messages=[{"role": "user", "content": "who are u?"}],  
  max_tokens=10,  
)  
  
print(response)  
  

```

`--header` is optional, only required if you're using litellm proxy with Virtual Keys
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer sk-1234' \  
--data ' {  
"model": "my-triton-model",  
"messages": [{"role": "user", "content": "who are u?"}]  
}'  
  

```



## Triton `/embeddings` - Embedding​
  * SDK
  * PROXY


Use the `triton/` prefix to route to triton server
```
from litellm import embedding  
import os  
  
response = await litellm.aembedding(  
  model="triton/<your-triton-model>",                              
  api_base="https://your-triton-api-base/triton/embeddings", # /embeddings endpoint you want litellm to call on your server  
  input=["good morning from litellm"],  
)  

```

  1. Add models to your config.yaml
```
model_list:  
 - model_name: my-triton-model  
  litellm_params:  
   model: triton/<your-triton-model>"  
   api_base: https://your-triton-api-base/triton/embeddings  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml --detailed_debug  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
from openai import OpenAI  
  
# set base_url to your proxy server  
# set api_key to send to proxy server  
client = OpenAI(api_key="<proxy-api-key>", base_url="http://0.0.0.0:4000")  
  
response = client.embeddings.create(  
  input=["hello from litellm"],  
  model="my-triton-model"  
)  
  
print(response)  
  

```

`--header` is optional, only required if you're using litellm proxy with Virtual Keys
```
curl --location 'http://0.0.0.0:4000/embeddings' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer sk-1234' \  
--data ' {  
"model": "my-triton-model",  
"input": ["write a litellm poem"]  
}'  
  

```



Previous
Volcano Engine (Volcengine)
Next
Ollama
  * Triton `/generate` - Chat Completion
  * Triton `/infer` - Chat Completion
  * Triton `/embeddings` - Embedding


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * VertexAI [Anthropic, Gemini, Model Garden]


On this page
# VertexAI [Anthropic, Gemini, Model Garden]
## Overview​
Property| Details  
---|---  
Description| Vertex AI is a fully-managed AI development platform for building and using generative AI.  
Provider Route on LiteLLM| `vertex_ai/`  
Link to Provider Doc| Vertex AI ↗  
Base URL| https://{vertex_location}-aiplatform.googleapis.com/  
Supported Operations| `/chat/completions`, `/completions`, `/embeddings`, `/audio/speech`, `/fine_tuning`, `/batches`, `/files`, `/images`  
  
  
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
## `vertex_ai/` route​
The `vertex_ai/` route uses uses VertexAI's REST API.
```
from litellm import completion  
import json   
  
## GET CREDENTIALS   
## RUN ##   
# !gcloud auth application-default login - run this to add vertex credentials to your env  
## OR ##   
file_path = 'path/to/vertex_ai_service_account.json'  
  
# Load the JSON file  
with open(file_path, 'r') as file:  
  vertex_credentials = json.load(file)  
  
# Convert to JSON string  
vertex_credentials_json = json.dumps(vertex_credentials)  
  
## COMPLETION CALL   
response = completion(  
 model="vertex_ai/gemini-pro",  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
 vertex_credentials=vertex_credentials_json  
)  

```

### **System Message**​
```
from litellm import completion  
import json   
  
## GET CREDENTIALS   
file_path = 'path/to/vertex_ai_service_account.json'  
  
# Load the JSON file  
with open(file_path, 'r') as file:  
  vertex_credentials = json.load(file)  
  
# Convert to JSON string  
vertex_credentials_json = json.dumps(vertex_credentials)  
  
  
response = completion(  
 model="vertex_ai/gemini-pro",  
 messages=[{"content": "You are a good bot.","role": "system"}, {"content": "Hello, how are you?","role": "user"}],   
 vertex_credentials=vertex_credentials_json  
)  

```

### **Function Calling**​
Force Gemini to make tool calls with `tool_choice="required"`.
```
from litellm import completion  
import json   
  
## GET CREDENTIALS   
file_path = 'path/to/vertex_ai_service_account.json'  
  
# Load the JSON file  
with open(file_path, 'r') as file:  
  vertex_credentials = json.load(file)  
  
# Convert to JSON string  
vertex_credentials_json = json.dumps(vertex_credentials)  
  
  
messages = [  
  {  
    "role": "system",  
    "content": "Your name is Litellm Bot, you are a helpful assistant",  
  },  
  # User asks for their name and weather in San Francisco  
  {  
    "role": "user",  
    "content": "Hello, what is your name and can you tell me the weather?",  
  },  
]  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          }  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
  
data = {  
  "model": "vertex_ai/gemini-1.5-pro-preview-0514"),  
  "messages": messages,  
  "tools": tools,  
  "tool_choice": "required",  
  "vertex_credentials": vertex_credentials_json  
}  
  
## COMPLETION CALL   
print(completion(**data))  

```

### **JSON Schema**​
From v`1.40.1+` LiteLLM supports sending `response_schema` as a param for Gemini-1.5-Pro on Vertex AI. For other models (e.g. `gemini-1.5-flash` or `claude-3-5-sonnet`), LiteLLM adds the schema to the message list with a user-controlled prompt.
**Response Schema**
  * SDK
  * PROXY


```
from litellm import completion   
import json   
  
## SETUP ENVIRONMENT  
# !gcloud auth application-default login - run this to add vertex credentials to your env  
  
messages = [  
  {  
    "role": "user",  
    "content": "List 5 popular cookie recipes."  
  }  
]  
  
response_schema = {  
    "type": "array",  
    "items": {  
      "type": "object",  
      "properties": {  
        "recipe_name": {  
          "type": "string",  
        },  
      },  
      "required": ["recipe_name"],  
    },  
  }  
  
  
completion(  
  model="vertex_ai/gemini-1.5-pro",   
  messages=messages,   
  response_format={"type": "json_object", "response_schema": response_schema} # 👈 KEY CHANGE  
  )  
  
print(json.loads(completion.choices[0].message.content))  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: gemini-pro  
  litellm_params:  
   model: vertex_ai/gemini-1.5-pro  
   vertex_project: "project-id"  
   vertex_location: "us-central1"  
   vertex_credentials: "/path/to/service_account.json" # [OPTIONAL] Do this OR `!gcloud auth application-default login` - run this to add vertex credentials to your env  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
 "model": "gemini-pro",  
 "messages": [  
    {"role": "user", "content": "List 5 popular cookie recipes."}  
  ],  
 "response_format": {"type": "json_object", "response_schema": {   
    "type": "array",  
    "items": {  
      "type": "object",  
      "properties": {  
        "recipe_name": {  
          "type": "string",  
        },  
      },  
      "required": ["recipe_name"],  
    },  
  }}  
}  
'  

```

**Validate Schema**
To validate the response_schema, set `enforce_validation: true`.
  * SDK
  * PROXY


```
from litellm import completion, JSONSchemaValidationError  
try:   
  completion(  
  model="vertex_ai/gemini-1.5-pro",   
  messages=messages,   
  response_format={  
    "type": "json_object",   
    "response_schema": response_schema,  
    "enforce_validation": true # 👈 KEY CHANGE  
  }  
  )  
except JSONSchemaValidationError as e:   
  print("Raw Response: {}".format(e.raw_response))  
  raise e  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: gemini-pro  
  litellm_params:  
   model: vertex_ai/gemini-1.5-pro  
   vertex_project: "project-id"  
   vertex_location: "us-central1"  
   vertex_credentials: "/path/to/service_account.json" # [OPTIONAL] Do this OR `!gcloud auth application-default login` - run this to add vertex credentials to your env  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request!


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
 "model": "gemini-pro",  
 "messages": [  
    {"role": "user", "content": "List 5 popular cookie recipes."}  
  ],  
 "response_format": {"type": "json_object", "response_schema": {   
    "type": "array",  
    "items": {  
      "type": "object",  
      "properties": {  
        "recipe_name": {  
          "type": "string",  
        },  
      },  
      "required": ["recipe_name"],  
    },  
  },   
  "enforce_validation": true  
  }  
}  
'  

```

LiteLLM will validate the response against the schema, and raise a `JSONSchemaValidationError` if the response does not match the schema. 
JSONSchemaValidationError inherits from `openai.APIError`
Access the raw response with `e.raw_response`
**Add to prompt yourself**
```
from litellm import completion   
  
## GET CREDENTIALS   
file_path = 'path/to/vertex_ai_service_account.json'  
  
# Load the JSON file  
with open(file_path, 'r') as file:  
  vertex_credentials = json.load(file)  
  
# Convert to JSON string  
vertex_credentials_json = json.dumps(vertex_credentials)  
  
messages = [  
  {  
    "role": "user",  
    "content": """  
List 5 popular cookie recipes.  
  
Using this JSON schema:  
  
  Recipe = {"recipe_name": str}  
  
Return a `list[Recipe]`  
    """  
  }  
]  
  
completion(model="vertex_ai/gemini-1.5-flash-preview-0514", messages=messages, response_format={ "type": "json_object" })  

```

### **Grounding - Web Search**​
Add Google Search Result grounding to vertex ai calls. 
**Relevant VertexAI Docs**
See the grounding metadata with `response_obj._hidden_params["vertex_ai_grounding_metadata"]`
  * SDK
  * PROXY


```
from litellm import completion   
  
## SETUP ENVIRONMENT  
# !gcloud auth application-default login - run this to add vertex credentials to your env  
  
tools = [{"googleSearch": {}}] # 👈 ADD GOOGLE SEARCH  
  
resp = litellm.completion(  
          model="vertex_ai/gemini-1.0-pro-001",  
          messages=[{"role": "user", "content": "Who won the world cup?"}],  
          tools=tools,  
        )  
  
print(resp)  

```

  * OpenAI Python SDK
  * cURL


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-1234", # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000/v1/" # point to litellm proxy  
)  
  
response = client.chat.completions.create(  
  model="gemini-pro",  
  messages=[{"role": "user", "content": "Who won the world cup?"}],  
  tools=[{"googleSearch": {}}],  
)  
  
print(response)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gemini-pro",  
  "messages": [  
   {"role": "user", "content": "Who won the world cup?"}  
  ],  
  "tools": [  
    {  
      "googleSearch": {}   
    }  
  ]  
 }'  
  

```

You can also use the `enterpriseWebSearch` tool for an enterprise compliant search.
  * SDK
  * PROXY


```
from litellm import completion   
  
## SETUP ENVIRONMENT  
# !gcloud auth application-default login - run this to add vertex credentials to your env  
  
tools = [{"enterpriseWebSearch": {}}] # 👈 ADD GOOGLE ENTERPRISE SEARCH  
  
resp = litellm.completion(  
          model="vertex_ai/gemini-1.0-pro-001",  
          messages=[{"role": "user", "content": "Who won the world cup?"}],  
          tools=tools,  
        )  
  
print(resp)  

```

  * OpenAI Python SDK
  * cURL


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-1234", # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000/v1/" # point to litellm proxy  
)  
  
response = client.chat.completions.create(  
  model="gemini-pro",  
  messages=[{"role": "user", "content": "Who won the world cup?"}],  
  tools=[{"enterpriseWebSearch": {}}],  
)  
  
print(response)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gemini-pro",  
  "messages": [  
   {"role": "user", "content": "Who won the world cup?"}  
  ],  
  "tools": [  
    {  
      "enterpriseWebSearch": {}   
    }  
  ]  
 }'  
  

```

#### **Moving from Vertex AI SDK to LiteLLM (GROUNDING)**​
If this was your initial VertexAI Grounding code,
```
import vertexai  
from vertexai.generative_models import GenerativeModel, GenerationConfig, Tool, grounding  
  
  
vertexai.init(project=project_id, location="us-central1")  
  
model = GenerativeModel("gemini-1.5-flash-001")  
  
# Use Google Search for grounding  
tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())  
  
prompt = "When is the next total solar eclipse in US?"  
response = model.generate_content(  
  prompt,  
  tools=[tool],  
  generation_config=GenerationConfig(  
    temperature=0.0,  
  ),  
)  
  
print(response)  

```

then, this is what it looks like now
```
from litellm import completion  
  
  
# !gcloud auth application-default login - run this to add vertex credentials to your env  
  
tools = [{"googleSearch": {"disable_attributon": False}}] # 👈 ADD GOOGLE SEARCH  
  
resp = litellm.completion(  
          model="vertex_ai/gemini-1.0-pro-001",  
          messages=[{"role": "user", "content": "Who won the world cup?"}],  
          tools=tools,  
          vertex_project="project-id"  
        )  
  
print(resp)  

```

### **Thinking /`reasoning_content`**​
LiteLLM translates OpenAI's `reasoning_effort` to Gemini's `thinking` parameter. Code
**Mapping**
reasoning_effort| thinking  
---|---  
"low"| "budget_tokens": 1024  
"medium"| "budget_tokens": 2048  
"high"| "budget_tokens": 4096  
  * SDK
  * PROXY


```
from litellm import completion  
  
# !gcloud auth application-default login - run this to add vertex credentials to your env  
  
resp = completion(  
  model="vertex_ai/gemini-2.5-flash-preview-04-17",  
  messages=[{"role": "user", "content": "What is the capital of France?"}],  
  reasoning_effort="low",  
  vertex_project="project-id",  
  vertex_location="us-central1"  
)  
  

```

  1. Setup config.yaml


```
- model_name: gemini-2.5-flash  
 litellm_params:  
  model: vertex_ai/gemini-2.5-flash-preview-04-17  
  vertex_credentials: {"project_id": "project-id", "location": "us-central1", "project_key": "project-key"}  
  vertex_project: "project-id"  
  vertex_location: "us-central1"  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "gemini-2.5-flash",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "reasoning_effort": "low"  
 }'  

```

**Expected Response**
```
ModelResponse(  
  id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',  
  created=1740470510,  
  model='claude-3-7-sonnet-20250219',  
  object='chat.completion',  
  system_fingerprint=None,  
  choices=[  
    Choices(  
      finish_reason='stop',  
      index=0,  
      message=Message(  
        content="The capital of France is Paris.",  
        role='assistant',  
        tool_calls=None,  
        function_call=None,  
        reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'  
      ),  
    )  
  ],  
  usage=Usage(  
    completion_tokens=68,  
    prompt_tokens=42,  
    total_tokens=110,  
    completion_tokens_details=None,  
    prompt_tokens_details=PromptTokensDetailsWrapper(  
      audio_tokens=None,  
      cached_tokens=0,  
      text_tokens=None,  
      image_tokens=None  
    ),  
    cache_creation_input_tokens=0,  
    cache_read_input_tokens=0  
  )  
)  

```

#### Pass `thinking` to Gemini models​
You can also pass the `thinking` parameter to Gemini models.
This is translated to Gemini's `thinkingConfig` parameter.
  * SDK
  * PROXY


```
from litellm import completion  
  
# !gcloud auth application-default login - run this to add vertex credentials to your env  
  
response = litellm.completion(  
 model="vertex_ai/gemini-2.5-flash-preview-04-17",  
 messages=[{"role": "user", "content": "What is the capital of France?"}],  
 thinking={"type": "enabled", "budget_tokens": 1024},  
 vertex_project="project-id",  
 vertex_location="us-central1"  
)  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer $LITELLM_KEY" \  
 -d '{  
  "model": "vertex_ai/gemini-2.5-flash-preview-04-17",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "thinking": {"type": "enabled", "budget_tokens": 1024}  
 }'  

```

### **Context Caching**​
Use Vertex AI context caching is supported by calling provider api directly. (Unified Endpoint support coming soon.).
**Go straight to provider**
## Pre-requisites​
  * `pip install google-cloud-aiplatform` (pre-installed on proxy docker image)
  * Authentication: 
    * run `gcloud auth application-default login` See Google Cloud Docs
    * Alternatively you can set `GOOGLE_APPLICATION_CREDENTIALS`
Here's how: **Jump to Code**
      * Create a service account on GCP
      * Export the credentials as a json
      * load the json and json.dump the json as a string
      * store the json string in your environment as `GOOGLE_APPLICATION_CREDENTIALS`


## Sample Usage​
```
import litellm  
litellm.vertex_project = "hardy-device-38811" # Your Project ID  
litellm.vertex_location = "us-central1" # proj location  
  
response = litellm.completion(model="gemini-pro", messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}])  

```

## Usage with LiteLLM Proxy Server​
Here's how to use Vertex AI with the LiteLLM Proxy Server
  1. Modify the config.yaml 
     * Different location per model
     * One location all vertex models
Use this when you need to set a different location for each vertex model
```
model_list:  
 - model_name: gemini-vision  
  litellm_params:  
   model: vertex_ai/gemini-1.0-pro-vision-001  
   vertex_project: "project-id"  
   vertex_location: "us-central1"  
 - model_name: gemini-vision  
  litellm_params:  
   model: vertex_ai/gemini-1.0-pro-vision-001  
   vertex_project: "project-id2"  
   vertex_location: "us-east"  

```

Use this when you have one vertex location for all models
```
litellm_settings:   
 vertex_project: "hardy-device-38811" # Your Project ID  
 vertex_location: "us-central1" # proj location  
  
model_list:   
 -model_name: team1-gemini-pro  
 litellm_params:   
  model: gemini-pro  

```

  2. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  3. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="team1-gemini-pro",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "team1-gemini-pro",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



## Authentication - vertex_project, vertex_location, etc.​
Set your vertex credentials via:
  * dynamic params OR
  * env vars 


### **Dynamic Params**​
You can set:
  * `vertex_credentials` (str) - can be a json string or filepath to your vertex ai service account.json
  * `vertex_location` (str) - place where vertex model is deployed (us-central1, asia-southeast1, etc.)
  * `vertex_project` Optional[str] - use if vertex project different from the one in vertex_credentials


as dynamic params for a `litellm.completion` call. 
  * SDK
  * PROXY


```
from litellm import completion  
import json   
  
## GET CREDENTIALS   
file_path = 'path/to/vertex_ai_service_account.json'  
  
# Load the JSON file  
with open(file_path, 'r') as file:  
  vertex_credentials = json.load(file)  
  
# Convert to JSON string  
vertex_credentials_json = json.dumps(vertex_credentials)  
  
  
response = completion(  
 model="vertex_ai/gemini-pro",  
 messages=[{"content": "You are a good bot.","role": "system"}, {"content": "Hello, how are you?","role": "user"}],   
 vertex_credentials=vertex_credentials_json,  
 vertex_project="my-special-project",   
 vertex_location="my-special-location"  
)  

```

```
model_list:  
  - model_name: gemini-1.5-pro  
   litellm_params:  
    model: gemini-1.5-pro  
    vertex_credentials: os.environ/VERTEX_FILE_PATH_ENV_VAR # os.environ["VERTEX_FILE_PATH_ENV_VAR"] = "/path/to/service_account.json"   
    vertex_project: "my-special-project"  
    vertex_location: "my-special-location:  

```

### **Environment Variables**​
You can set:
  * `GOOGLE_APPLICATION_CREDENTIALS` - store the filepath for your service_account.json in here (used by vertex sdk directly).
  * VERTEXAI_LOCATION - place where vertex model is deployed (us-central1, asia-southeast1, etc.)
  * VERTEXAI_PROJECT - Optional[str] - use if vertex project different from the one in vertex_credentials


  1. GOOGLE_APPLICATION_CREDENTIALS


```
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service_account.json"  

```

  1. VERTEXAI_LOCATION


```
export VERTEXAI_LOCATION="us-central1" # can be any vertex location  

```

  1. VERTEXAI_PROJECT


```
export VERTEXAI_PROJECT="my-test-project" # ONLY use if model project is different from service account project  

```

## Specifying Safety Settings​
In certain use-cases you may need to make calls to the models and pass safety settings different from the defaults. To do so, simple pass the `safety_settings` argument to `completion` or `acompletion`. For example:
### Set per model/request​
  * SDK
  * Proxy


```
response = completion(  
  model="vertex_ai/gemini-pro",   
  messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}]  
  safety_settings=[  
    {  
      "category": "HARM_CATEGORY_HARASSMENT",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_HATE_SPEECH",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_DANGEROUS_CONTENT",  
      "threshold": "BLOCK_NONE",  
    },  
  ]  
)  

```

**Option 1: Set in config**
```
model_list:  
 - model_name: gemini-experimental  
  litellm_params:  
   model: vertex_ai/gemini-experimental  
   vertex_project: litellm-epic  
   vertex_location: us-central1  
   safety_settings:  
   - category: HARM_CATEGORY_HARASSMENT  
    threshold: BLOCK_NONE  
   - category: HARM_CATEGORY_HATE_SPEECH  
    threshold: BLOCK_NONE  
   - category: HARM_CATEGORY_SEXUALLY_EXPLICIT  
    threshold: BLOCK_NONE  
   - category: HARM_CATEGORY_DANGEROUS_CONTENT  
    threshold: BLOCK_NONE  

```

**Option 2: Set on call**
```
response = client.chat.completions.create(  
  model="gemini-experimental",  
  messages=[  
    {  
      "role": "user",  
      "content": "Can you write exploits?",  
    }  
  ],  
  max_tokens=8192,  
  stream=False,  
  temperature=0.0,  
  
  extra_body={  
    "safety_settings": [  
      {  
        "category": "HARM_CATEGORY_HARASSMENT",  
        "threshold": "BLOCK_NONE",  
      },  
      {  
        "category": "HARM_CATEGORY_HATE_SPEECH",  
        "threshold": "BLOCK_NONE",  
      },  
      {  
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",  
        "threshold": "BLOCK_NONE",  
      },  
      {  
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",  
        "threshold": "BLOCK_NONE",  
      },  
    ],  
  }  
)  

```

### Set Globally​
  * SDK
  * Proxy


```
import litellm   
  
litellm.set_verbose = True 👈 See RAW REQUEST/RESPONSE   
  
litellm.vertex_ai_safety_settings = [  
    {  
      "category": "HARM_CATEGORY_HARASSMENT",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_HATE_SPEECH",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",  
      "threshold": "BLOCK_NONE",  
    },  
    {  
      "category": "HARM_CATEGORY_DANGEROUS_CONTENT",  
      "threshold": "BLOCK_NONE",  
    },  
  ]  
response = completion(  
  model="vertex_ai/gemini-pro",   
  messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}]  
)  

```

```
model_list:  
 - model_name: gemini-experimental  
  litellm_params:  
   model: vertex_ai/gemini-experimental  
   vertex_project: litellm-epic  
   vertex_location: us-central1  
  
litellm_settings:  
  vertex_ai_safety_settings:  
   - category: HARM_CATEGORY_HARASSMENT  
    threshold: BLOCK_NONE  
   - category: HARM_CATEGORY_HATE_SPEECH  
    threshold: BLOCK_NONE  
   - category: HARM_CATEGORY_SEXUALLY_EXPLICIT  
    threshold: BLOCK_NONE  
   - category: HARM_CATEGORY_DANGEROUS_CONTENT  
    threshold: BLOCK_NONE  

```

## Set Vertex Project & Vertex Location​
All calls using Vertex AI require the following parameters:
  * Your Project ID


```
import os, litellm   
  
# set via env var  
os.environ["VERTEXAI_PROJECT"] = "hardy-device-38811" # Your Project ID`  
  
### OR ###  
  
# set directly on module   
litellm.vertex_project = "hardy-device-38811" # Your Project ID`  

```

  * Your Project Location


```
import os, litellm   
  
# set via env var  
os.environ["VERTEXAI_LOCATION"] = "us-central1 # Your Location  
  
### OR ###  
  
# set directly on module   
litellm.vertex_location = "us-central1 # Your Location  

```

## Anthropic​
Model Name| Function Call  
---|---  
claude-3-opus@20240229| `completion('vertex_ai/claude-3-opus@20240229', messages)`  
claude-3-5-sonnet@20240620| `completion('vertex_ai/claude-3-5-sonnet@20240620', messages)`  
claude-3-sonnet@20240229| `completion('vertex_ai/claude-3-sonnet@20240229', messages)`  
claude-3-haiku@20240307| `completion('vertex_ai/claude-3-haiku@20240307', messages)`  
claude-3-7-sonnet@20250219| `completion('vertex_ai/claude-3-7-sonnet@20250219', messages)`  
### Usage​
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ""  
  
model = "claude-3-sonnet@20240229"  
  
vertex_ai_project = "your-vertex-project" # can also set this as os.environ["VERTEXAI_PROJECT"]  
vertex_ai_location = "your-vertex-location" # can also set this as os.environ["VERTEXAI_LOCATION"]  
  
response = completion(  
  model="vertex_ai/" + model,  
  messages=[{"role": "user", "content": "hi"}],  
  temperature=0.7,  
  vertex_ai_project=vertex_ai_project,  
  vertex_ai_location=vertex_ai_location,  
)  
print("\nModel Response", response)  

```

**1. Add to config**
```
model_list:  
  - model_name: anthropic-vertex  
   litellm_params:  
    model: vertex_ai/claude-3-sonnet@20240229  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-east-1"  
  - model_name: anthropic-vertex  
   litellm_params:  
    model: vertex_ai/claude-3-sonnet@20240229  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-west-1"  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
   --header 'Authorization: Bearer sk-1234' \  
   --header 'Content-Type: application/json' \  
   --data '{  
      "model": "anthropic-vertex", # 👈 the 'model_name' in config  
      "messages": [  
        {  
        "role": "user",  
        "content": "what llm are you"  
        }  
      ],  
    }'  

```

### Usage - `thinking` / `reasoning_content`​
  * SDK
  * PROXY


```
from litellm import completion  
  
resp = completion(  
  model="vertex_ai/claude-3-7-sonnet-20250219",  
  messages=[{"role": "user", "content": "What is the capital of France?"}],  
  thinking={"type": "enabled", "budget_tokens": 1024},  
)  
  

```

  1. Setup config.yaml


```
- model_name: claude-3-7-sonnet-20250219  
 litellm_params:  
  model: vertex_ai/claude-3-7-sonnet-20250219  
  vertex_ai_project: "my-test-project"  
  vertex_ai_location: "us-west-1"  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "claude-3-7-sonnet-20250219",  
  "messages": [{"role": "user", "content": "What is the capital of France?"}],  
  "thinking": {"type": "enabled", "budget_tokens": 1024}  
 }'  

```

**Expected Response**
```
ModelResponse(  
  id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',  
  created=1740470510,  
  model='claude-3-7-sonnet-20250219',  
  object='chat.completion',  
  system_fingerprint=None,  
  choices=[  
    Choices(  
      finish_reason='stop',  
      index=0,  
      message=Message(  
        content="The capital of France is Paris.",  
        role='assistant',  
        tool_calls=None,  
        function_call=None,  
        provider_specific_fields={  
          'citations': None,  
          'thinking_blocks': [  
            {  
              'type': 'thinking',  
              'thinking': 'The capital of France is Paris. This is a very straightforward factual question.',  
              'signature': 'EuYBCkQYAiJAy6...'  
            }  
          ]  
        }  
      ),  
      thinking_blocks=[  
        {  
          'type': 'thinking',  
          'thinking': 'The capital of France is Paris. This is a very straightforward factual question.',  
          'signature': 'EuYBCkQYAiJAy6AGB...'  
        }  
      ],  
      reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'  
    )  
  ],  
  usage=Usage(  
    completion_tokens=68,  
    prompt_tokens=42,  
    total_tokens=110,  
    completion_tokens_details=None,  
    prompt_tokens_details=PromptTokensDetailsWrapper(  
      audio_tokens=None,  
      cached_tokens=0,  
      text_tokens=None,  
      image_tokens=None  
    ),  
    cache_creation_input_tokens=0,  
    cache_read_input_tokens=0  
  )  
)  

```

## Llama 3 API​
Model Name| Function Call  
---|---  
meta/llama3-405b-instruct-maas| `completion('vertex_ai/meta/llama3-405b-instruct-maas', messages)`  
### Usage​
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ""  
  
model = "meta/llama3-405b-instruct-maas"  
  
vertex_ai_project = "your-vertex-project" # can also set this as os.environ["VERTEXAI_PROJECT"]  
vertex_ai_location = "your-vertex-location" # can also set this as os.environ["VERTEXAI_LOCATION"]  
  
response = completion(  
  model="vertex_ai/" + model,  
  messages=[{"role": "user", "content": "hi"}],  
  vertex_ai_project=vertex_ai_project,  
  vertex_ai_location=vertex_ai_location,  
)  
print("\nModel Response", response)  

```

**1. Add to config**
```
model_list:  
  - model_name: anthropic-llama  
   litellm_params:  
    model: vertex_ai/meta/llama3-405b-instruct-maas  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-east-1"  
  - model_name: anthropic-llama  
   litellm_params:  
    model: vertex_ai/meta/llama3-405b-instruct-maas  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-west-1"  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
   --header 'Authorization: Bearer sk-1234' \  
   --header 'Content-Type: application/json' \  
   --data '{  
      "model": "anthropic-llama", # 👈 the 'model_name' in config  
      "messages": [  
        {  
        "role": "user",  
        "content": "what llm are you"  
        }  
      ],  
    }'  

```

## Mistral API​
**Supported OpenAI Params**
Model Name| Function Call  
---|---  
mistral-large@latest| `completion('vertex_ai/mistral-large@latest', messages)`  
mistral-large@2407| `completion('vertex_ai/mistral-large@2407', messages)`  
mistral-nemo@latest| `completion('vertex_ai/mistral-nemo@latest', messages)`  
codestral@latest| `completion('vertex_ai/codestral@latest', messages)`  
codestral@@2405| `completion('vertex_ai/codestral@2405', messages)`  
### Usage​
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ""  
  
model = "mistral-large@2407"  
  
vertex_ai_project = "your-vertex-project" # can also set this as os.environ["VERTEXAI_PROJECT"]  
vertex_ai_location = "your-vertex-location" # can also set this as os.environ["VERTEXAI_LOCATION"]  
  
response = completion(  
  model="vertex_ai/" + model,  
  messages=[{"role": "user", "content": "hi"}],  
  vertex_ai_project=vertex_ai_project,  
  vertex_ai_location=vertex_ai_location,  
)  
print("\nModel Response", response)  

```

**1. Add to config**
```
model_list:  
  - model_name: vertex-mistral  
   litellm_params:  
    model: vertex_ai/mistral-large@2407  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-east-1"  
  - model_name: vertex-mistral  
   litellm_params:  
    model: vertex_ai/mistral-large@2407  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-west-1"  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
   --header 'Authorization: Bearer sk-1234' \  
   --header 'Content-Type: application/json' \  
   --data '{  
      "model": "vertex-mistral", # 👈 the 'model_name' in config  
      "messages": [  
        {  
        "role": "user",  
        "content": "what llm are you"  
        }  
      ],  
    }'  

```

### Usage - Codestral FIM​
Call Codestral on VertexAI via the OpenAI `/v1/completion` endpoint for FIM tasks. 
Note: You can also call Codestral via `/chat/completion`.
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
# os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ""  
# OR run `!gcloud auth print-access-token` in your terminal  
  
model = "codestral@2405"  
  
vertex_ai_project = "your-vertex-project" # can also set this as os.environ["VERTEXAI_PROJECT"]  
vertex_ai_location = "your-vertex-location" # can also set this as os.environ["VERTEXAI_LOCATION"]  
  
response = text_completion(  
  model="vertex_ai/" + model,  
  vertex_ai_project=vertex_ai_project,  
  vertex_ai_location=vertex_ai_location,  
  prompt="def is_odd(n): \n return n % 2 == 1 \ndef test_is_odd():",   
  suffix="return True",                       # optional  
  temperature=0,                           # optional  
  top_p=1,                              # optional  
  max_tokens=10,                           # optional  
  min_tokens=10,                           # optional  
  seed=10,                              # optional  
  stop=["return"],                          # optional  
)  
  
print("\nModel Response", response)  

```

**1. Add to config**
```
model_list:  
  - model_name: vertex-codestral  
   litellm_params:  
    model: vertex_ai/codestral@2405  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-east-1"  
  - model_name: vertex-codestral  
   litellm_params:  
    model: vertex_ai/codestral@2405  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-west-1"  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl -X POST 'http://0.0.0.0:4000/completions' \  
   -H 'Authorization: Bearer sk-1234' \  
   -H 'Content-Type: application/json' \  
   -d '{  
      "model": "vertex-codestral", # 👈 the 'model_name' in config  
      "prompt": "def is_odd(n): \n return n % 2 == 1 \ndef test_is_odd():",   
      "suffix":"return True",                       # optional  
      "temperature":0,                           # optional  
      "top_p":1,                              # optional  
      "max_tokens":10,                           # optional  
      "min_tokens":10,                           # optional  
      "seed":10,                              # optional  
      "stop":["return"],                          # optional  
    }'  

```

## AI21 Models​
Model Name| Function Call  
---|---  
jamba-1.5-mini@001| `completion(model='vertex_ai/jamba-1.5-mini@001', messages)`  
jamba-1.5-large@001| `completion(model='vertex_ai/jamba-1.5-large@001', messages)`  
### Usage​
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ""  
  
model = "meta/jamba-1.5-mini@001"  
  
vertex_ai_project = "your-vertex-project" # can also set this as os.environ["VERTEXAI_PROJECT"]  
vertex_ai_location = "your-vertex-location" # can also set this as os.environ["VERTEXAI_LOCATION"]  
  
response = completion(  
  model="vertex_ai/" + model,  
  messages=[{"role": "user", "content": "hi"}],  
  vertex_ai_project=vertex_ai_project,  
  vertex_ai_location=vertex_ai_location,  
)  
print("\nModel Response", response)  

```

**1. Add to config**
```
model_list:  
  - model_name: jamba-1.5-mini  
   litellm_params:  
    model: vertex_ai/jamba-1.5-mini@001  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-east-1"  
  - model_name: jamba-1.5-large  
   litellm_params:  
    model: vertex_ai/jamba-1.5-large@001  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-west-1"  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
   --header 'Authorization: Bearer sk-1234' \  
   --header 'Content-Type: application/json' \  
   --data '{  
      "model": "jamba-1.5-large",  
      "messages": [  
        {  
        "role": "user",  
        "content": "what llm are you"  
        }  
      ],  
    }'  

```

## Gemini Pro​
Model Name| Function Call  
---|---  
gemini-pro| `completion('gemini-pro', messages)`, `completion('vertex_ai/gemini-pro', messages)`  
## Fine-tuned Models​
You can call fine-tuned Vertex AI Gemini models through LiteLLM
Property| Details  
---|---  
Provider Route| `vertex_ai/gemini/{MODEL_ID}`  
Vertex Documentation| Vertex AI - Fine-tuned Gemini Models  
Supported Operations| `/chat/completions`, `/completions`, `/embeddings`, `/images`  
To use a model that follows the `/gemini` request/response format, simply set the model parameter as 
Model parameter for calling fine-tuned gemini models
```
model="vertex_ai/gemini/<your-finetuned-model>"  

```

  * LiteLLM Python SDK
  * LiteLLM Proxy


Example
```
import litellm  
import os  
  
## set ENV variables  
os.environ["VERTEXAI_PROJECT"] = "hardy-device-38811"  
os.environ["VERTEXAI_LOCATION"] = "us-central1"  
  
response = litellm.completion(  
 model="vertex_ai/gemini/<your-finetuned-model>", # e.g. vertex_ai/gemini/4965075652664360960  
 messages=[{ "content": "Hello, how are you?","role": "user"}],  
)  

```

  1. Add Vertex Credentials to your env 


Authenticate to Vertex AI
```
!gcloud auth application-default login  

```

  1. Setup config.yaml 


Add to litellm config
```
- model_name: finetuned-gemini  
 litellm_params:  
  model: vertex_ai/gemini/<ENDPOINT_ID>  
  vertex_project: <PROJECT_ID>  
  vertex_location: <LOCATION>  

```

  1. Test it! 


  * OpenAI Python SDK
  * curl


Example request
```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="your-litellm-key",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="finetuned-gemini",  
  messages=[  
    {"role": "user", "content": "hi"}  
  ]  
)  
print(response)  

```

Example request
```
curl --location 'https://0.0.0.0:4000/v1/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: <LITELLM_KEY>' \  
--data '{"model": "finetuned-gemini" ,"messages":[{"role": "user", "content":[{"type": "text", "text": "hi"}]}]}'  

```

## Model Garden​
tip
All OpenAI compatible models from Vertex Model Garden are supported. 
#### Using Model Garden​
**Almost all Vertex Model Garden models are OpenAI compatible.**
  * OpenAI Compatible Models
  * Non-OpenAI Compatible Models


Property| Details  
---|---  
Provider Route| `vertex_ai/openai/{MODEL_ID}`  
Vertex Documentation| Vertex Model Garden - OpenAI Chat Completions, Vertex Model Garden  
Supported Operations| `/chat/completions`, `/embeddings`  
  * SDK
  * Proxy


```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["VERTEXAI_PROJECT"] = "hardy-device-38811"  
os.environ["VERTEXAI_LOCATION"] = "us-central1"  
  
response = completion(  
 model="vertex_ai/openai/<your-endpoint-id>",   
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

**1. Add to config**
```
model_list:  
  - model_name: llama3-1-8b-instruct  
   litellm_params:  
    model: vertex_ai/openai/5464397967697903616  
    vertex_ai_project: "my-test-project"  
    vertex_ai_location: "us-east-1"  

```

**2. Start proxy**
```
litellm --config /path/to/config.yaml  
  
# RUNNING at http://0.0.0.0:4000  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
   --header 'Authorization: Bearer sk-1234' \  
   --header 'Content-Type: application/json' \  
   --data '{  
      "model": "llama3-1-8b-instruct", # 👈 the 'model_name' in config  
      "messages": [  
        {  
        "role": "user",  
        "content": "what llm are you"  
        }  
      ],  
    }'  

```

```
from litellm import completion  
import os  
  
## set ENV variables  
os.environ["VERTEXAI_PROJECT"] = "hardy-device-38811"  
os.environ["VERTEXAI_LOCATION"] = "us-central1"  
  
response = completion(  
 model="vertex_ai/<your-endpoint-id>",   
 messages=[{ "content": "Hello, how are you?","role": "user"}]  
)  

```

## Gemini Pro Vision​
Model Name| Function Call  
---|---  
gemini-pro-vision| `completion('gemini-pro-vision', messages)`, `completion('vertex_ai/gemini-pro-vision', messages)`  
## Gemini 1.5 Pro (and Vision)​
Model Name| Function Call  
---|---  
gemini-1.5-pro| `completion('gemini-1.5-pro', messages)`, `completion('vertex_ai/gemini-1.5-pro', messages)`  
gemini-1.5-flash-preview-0514| `completion('gemini-1.5-flash-preview-0514', messages)`, `completion('vertex_ai/gemini-1.5-flash-preview-0514', messages)`  
gemini-1.5-pro-preview-0514| `completion('gemini-1.5-pro-preview-0514', messages)`, `completion('vertex_ai/gemini-1.5-pro-preview-0514', messages)`  
#### Using Gemini Pro Vision​
Call `gemini-pro-vision` in the same input/output format as OpenAI `gpt-4-vision`
LiteLLM Supports the following image types passed in `url`
  * Images with Cloud Storage URIs - gs://cloud-samples-data/generative-ai/image/boats.jpeg
  * Images with direct links - https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg
  * Videos with Cloud Storage URIs - https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4
  * Base64 Encoded Local Images


**Example Request - image url**
  * Images with direct links
  * Local Base64 Images


```
import litellm  
  
response = litellm.completion(  
 model = "vertex_ai/gemini-pro-vision",  
 messages=[  
   {  
     "role": "user",  
     "content": [  
             {  
               "type": "text",  
               "text": "Whats in this image?"  
             },  
             {  
               "type": "image_url",  
               "image_url": {  
               "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"  
               }  
             }  
           ]  
   }  
 ],  
)  
print(response)  

```

```
import litellm  
  
def encode_image(image_path):  
  import base64  
  
  with open(image_path, "rb") as image_file:  
    return base64.b64encode(image_file.read()).decode("utf-8")  
  
image_path = "cached_logo.jpg"  
# Getting the base64 string  
base64_image = encode_image(image_path)  
response = litellm.completion(  
  model="vertex_ai/gemini-pro-vision",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "Whats in this image?"},  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "data:image/jpeg;base64," + base64_image  
          },  
        },  
      ],  
    }  
  ],  
)  
print(response)  

```

## Usage - Function Calling​
LiteLLM supports Function Calling for Vertex AI gemini models. 
```
from litellm import completion  
import os  
# set env  
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ".."  
os.environ["VERTEX_AI_PROJECT"] = ".."  
os.environ["VERTEX_AI_LOCATION"] = ".."  
  
tools = [  
  {  
    "type": "function",  
    "function": {  
      "name": "get_current_weather",  
      "description": "Get the current weather in a given location",  
      "parameters": {  
        "type": "object",  
        "properties": {  
          "location": {  
            "type": "string",  
            "description": "The city and state, e.g. San Francisco, CA",  
          },  
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},  
        },  
        "required": ["location"],  
      },  
    },  
  }  
]  
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]  
  
response = completion(  
  model="vertex_ai/gemini-pro-vision",  
  messages=messages,  
  tools=tools,  
)  
# Add any assertions, here to check response args  
print(response)  
assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)  
assert isinstance(  
  response.choices[0].message.tool_calls[0].function.arguments, str  
)  
  

```

## Usage - PDF / Videos / Audio etc. Files​
Pass any file supported by Vertex AI, through LiteLLM. 
LiteLLM Supports the following file types passed in url. 
Using `file` message type for VertexAI is live from v1.65.1+ 
```
Files with Cloud Storage URIs - gs://cloud-samples-data/generative-ai/image/boats.jpeg  
Files with direct links - https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg  
Videos with Cloud Storage URIs - https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4  
Base64 Encoded Local Files  

```

  * SDK
  * PROXY


### **Using`gs://` or any URL**​
```
from litellm import completion  
  
response = completion(  
  model="vertex_ai/gemini-1.5-flash",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "You are a very professional document summarization specialist. Please summarize the given document."},  
        {  
          "type": "file",  
          "file": {  
            "file_id": "gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf",  
            "format": "application/pdf" # OPTIONAL - specify mime-type  
          }  
        },  
      ],  
    }  
  ],  
  max_tokens=300,  
)  
  
print(response.choices[0])  

```

### **using base64**​
```
from litellm import completion  
import base64  
import requests  
  
# URL of the file  
url = "https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf"  
  
# Download the file  
response = requests.get(url)  
file_data = response.content  
  
encoded_file = base64.b64encode(file_data).decode("utf-8")  
  
response = completion(  
  model="vertex_ai/gemini-1.5-flash",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {"type": "text", "text": "You are a very professional document summarization specialist. Please summarize the given document."},  
        {  
          "type": "file",  
          "file": {  
            "file_data": f"data:application/pdf;base64,{encoded_file}", # 👈 PDF  
          }   
        },  
        {  
          "type": "audio_input",  
          "audio_input {  
            "audio_input": f"data:audio/mp3;base64,{encoded_file}", # 👈 AUDIO File ('file' message works as too)  
          }   
        },  
      ],  
    }  
  ],  
  max_tokens=300,  
)  
  
print(response.choices[0])  

```

  1. Add model to config 


```
- model_name: gemini-1.5-flash  
 litellm_params:  
  model: vertex_ai/gemini-1.5-flash  
  vertex_credentials: "/path/to/service_account.json"  

```

  1. Start Proxy


```
litellm --config /path/to/config.yaml  

```

  1. Test it! 


**Using`gs://`**
```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "gemini-1.5-flash",  
  "messages": [  
   {  
    "role": "user",  
    "content": [  
     {  
      "type": "text",  
      "text": "You are a very professional document summarization specialist. Please summarize the given document"  
     },  
     {  
        "type": "file",  
        "file": {  
          "file_id": "gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf",  
          "format": "application/pdf" # OPTIONAL  
        }  
      }  
     }  
    ]  
   }  
  ],  
  "max_tokens": 300  
 }'  
  

```

```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \  
 -d '{  
  "model": "gemini-1.5-flash",  
  "messages": [  
   {  
    "role": "user",  
    "content": [  
     {  
      "type": "text",  
      "text": "You are a very professional document summarization specialist. Please summarize the given document"  
     },  
     {  
        "type": "file",  
        "file": {  
          "file_data": f"data:application/pdf;base64,{encoded_file}", # 👈 PDF  
        },  
      },  
      {  
        "type": "audio_input",  
        "audio_input {  
          "audio_input": f"data:audio/mp3;base64,{encoded_file}", # 👈 AUDIO File ('file' message works as too)  
        }   
      },  
  ]  
   }  
  ],  
  "max_tokens": 300  
 }'  
  

```

## Chat Models​
Model Name| Function Call  
---|---  
chat-bison-32k| `completion('chat-bison-32k', messages)`  
chat-bison| `completion('chat-bison', messages)`  
chat-bison@001| `completion('chat-bison@001', messages)`  
## Code Chat Models​
Model Name| Function Call  
---|---  
codechat-bison| `completion('codechat-bison', messages)`  
codechat-bison-32k| `completion('codechat-bison-32k', messages)`  
codechat-bison@001| `completion('codechat-bison@001', messages)`  
## Text Models​
Model Name| Function Call  
---|---  
text-bison| `completion('text-bison', messages)`  
text-bison@001| `completion('text-bison@001', messages)`  
## Code Text Models​
Model Name| Function Call  
---|---  
code-bison| `completion('code-bison', messages)`  
code-bison@001| `completion('code-bison@001', messages)`  
code-gecko@001| `completion('code-gecko@001', messages)`  
code-gecko@latest| `completion('code-gecko@latest', messages)`  
## **Embedding Models**​
#### Usage - Embedding​
  * SDK
  * LiteLLM PROXY


```
import litellm  
from litellm import embedding  
litellm.vertex_project = "hardy-device-38811" # Your Project ID  
litellm.vertex_location = "us-central1" # proj location  
  
response = embedding(  
  model="vertex_ai/textembedding-gecko",  
  input=["good morning from litellm"],  
)  
print(response)  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: snowflake-arctic-embed-m-long-1731622468876  
  litellm_params:  
   model: vertex_ai/<your-model-id>  
   vertex_project: "adroit-crow-413218"  
   vertex_location: "us-central1"  
   vertex_credentials: adroit-crow-413218-a956eef1a2a8.json   
  
litellm_settings:  
 drop_params: True  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request using OpenAI Python SDK, Langchain Python SDK


```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
response = client.embeddings.create(  
  model="snowflake-arctic-embed-m-long-1731622468876",   
  input = ["good morning from litellm", "this is another item"],  
)  
  
print(response)  

```

#### Supported Embedding Models​
All models listed here are supported
Model Name| Function Call  
---|---  
text-embedding-004| `embedding(model="vertex_ai/text-embedding-004", input)`  
text-multilingual-embedding-002| `embedding(model="vertex_ai/text-multilingual-embedding-002", input)`  
textembedding-gecko| `embedding(model="vertex_ai/textembedding-gecko", input)`  
textembedding-gecko-multilingual| `embedding(model="vertex_ai/textembedding-gecko-multilingual", input)`  
textembedding-gecko-multilingual@001| `embedding(model="vertex_ai/textembedding-gecko-multilingual@001", input)`  
textembedding-gecko@001| `embedding(model="vertex_ai/textembedding-gecko@001", input)`  
textembedding-gecko@003| `embedding(model="vertex_ai/textembedding-gecko@003", input)`  
text-embedding-preview-0409| `embedding(model="vertex_ai/text-embedding-preview-0409", input)`  
text-multilingual-embedding-preview-0409| `embedding(model="vertex_ai/text-multilingual-embedding-preview-0409", input)`  
Fine-tuned OR Custom Embedding models| `embedding(model="vertex_ai/<your-model-id>", input)`  
### Supported OpenAI (Unified) Params​
param| type| vertex equivalent  
---|---|---  
`input`| **string or List[string]**| `instances`  
`dimensions`| **int**| `output_dimensionality`  
`input_type`| **Literal["RETRIEVAL_QUERY","RETRIEVAL_DOCUMENT", "SEMANTIC_SIMILARITY", "CLASSIFICATION", "CLUSTERING", "QUESTION_ANSWERING", "FACT_VERIFICATION"]**| `task_type`  
#### Usage with OpenAI (Unified) Params​
  * SDK
  * LiteLLM PROXY


```
response = litellm.embedding(  
  model="vertex_ai/text-embedding-004",  
  input=["good morning from litellm", "gm"]  
  input_type = "RETRIEVAL_DOCUMENT",  
  dimensions=1,  
)  

```

```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
response = client.embeddings.create(  
  model="text-embedding-004",   
  input = ["good morning from litellm", "gm"],  
  dimensions=1,  
  extra_body = {  
    "input_type": "RETRIEVAL_QUERY",  
  }  
)  
  
print(response)  

```

### Supported Vertex Specific Params​
param| type  
---|---  
`auto_truncate`| **bool**  
`task_type`| **Literal["RETRIEVAL_QUERY","RETRIEVAL_DOCUMENT", "SEMANTIC_SIMILARITY", "CLASSIFICATION", "CLUSTERING", "QUESTION_ANSWERING", "FACT_VERIFICATION"]**  
`title`| **str**  
#### Usage with Vertex Specific Params (Use `task_type` and `title`)​
You can pass any vertex specific params to the embedding model. Just pass them to the embedding function like this: 
Relevant Vertex AI doc with all embedding params
  * SDK
  * LiteLLM PROXY


```
response = litellm.embedding(  
  model="vertex_ai/text-embedding-004",  
  input=["good morning from litellm", "gm"]  
  task_type = "RETRIEVAL_DOCUMENT",  
  title = "test",  
  dimensions=1,  
  auto_truncate=True,  
)  

```

```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
response = client.embeddings.create(  
  model="text-embedding-004",   
  input = ["good morning from litellm", "gm"],  
  dimensions=1,  
  extra_body = {  
    "task_type": "RETRIEVAL_QUERY",  
    "auto_truncate": True,  
    "title": "test",  
  }  
)  
  
print(response)  

```

## **Multi-Modal Embeddings**​
Known Limitations:
  * Only supports 1 image / video / image per request
  * Only supports GCS or base64 encoded images / videos


### Usage​
  * SDK
  * LiteLLM PROXY (Unified Endpoint)
  * LiteLLM PROXY (Vertex SDK)


Using GCS Images
```
response = await litellm.aembedding(  
  model="vertex_ai/multimodalembedding@001",  
  input="gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png" # will be sent as a gcs image  
)  

```

Using base 64 encoded images
```
response = await litellm.aembedding(  
  model="vertex_ai/multimodalembedding@001",  
  input="data:image/jpeg;base64,..." # will be sent as a base64 encoded image  
)  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: multimodalembedding@001  
  litellm_params:  
   model: vertex_ai/multimodalembedding@001  
   vertex_project: "adroit-crow-413218"  
   vertex_location: "us-central1"  
   vertex_credentials: adroit-crow-413218-a956eef1a2a8.json   
  
litellm_settings:  
 drop_params: True  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request use OpenAI Python SDK, Langchain Python SDK


  * OpenAI SDK
  * Langchain


Requests with GCS Image / Video URI
```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
# # request sent to model set on litellm proxy, `litellm --model`  
response = client.embeddings.create(  
  model="multimodalembedding@001",   
  input = "gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png",  
)  
  
print(response)  

```

Requests with base64 encoded images
```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
# # request sent to model set on litellm proxy, `litellm --model`  
response = client.embeddings.create(  
  model="multimodalembedding@001",   
  input = "data:image/jpeg;base64,...",  
)  
  
print(response)  

```

Requests with GCS Image / Video URI
```
from langchain_openai import OpenAIEmbeddings  
  
embeddings_models = "multimodalembedding@001"  
  
embeddings = OpenAIEmbeddings(  
  model="multimodalembedding@001",  
  base_url="http://0.0.0.0:4000",  
  api_key="sk-1234", # type: ignore  
)  
  
  
query_result = embeddings.embed_query(  
  "gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png"  
)  
print(query_result)  
  

```

Requests with base64 encoded images
```
from langchain_openai import OpenAIEmbeddings  
  
embeddings_models = "multimodalembedding@001"  
  
embeddings = OpenAIEmbeddings(  
  model="multimodalembedding@001",  
  base_url="http://0.0.0.0:4000",  
  api_key="sk-1234", # type: ignore  
)  
  
  
query_result = embeddings.embed_query(  
  "data:image/jpeg;base64,..."  
)  
print(query_result)  
  

```

  1. Add model to config.yaml


```
default_vertex_config:  
 vertex_project: "adroit-crow-413218"  
 vertex_location: "us-central1"  
 vertex_credentials: adroit-crow-413218-a956eef1a2a8.json   

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request use OpenAI Python SDK


```
import vertexai  
  
from vertexai.vision_models import Image, MultiModalEmbeddingModel, Video  
from vertexai.vision_models import VideoSegmentConfig  
from google.auth.credentials import Credentials  
  
  
LITELLM_PROXY_API_KEY = "sk-1234"  
LITELLM_PROXY_BASE = "http://0.0.0.0:4000/vertex-ai"  
  
import datetime  
  
class CredentialsWrapper(Credentials):  
  def __init__(self, token=None):  
    super().__init__()  
    self.token = token  
    self.expiry = None # or set to a future date if needed  
      
  def refresh(self, request):  
    pass  
    
  def apply(self, headers, token=None):  
    headers['Authorization'] = f'Bearer {self.token}'  
  
  @property  
  def expired(self):  
    return False # Always consider the token as non-expired  
  
  @property  
  def valid(self):  
    return True # Always consider the credentials as valid  
  
credentials = CredentialsWrapper(token=LITELLM_PROXY_API_KEY)  
  
vertexai.init(  
  project="adroit-crow-413218",  
  location="us-central1",  
  api_endpoint=LITELLM_PROXY_BASE,  
  credentials = credentials,  
  api_transport="rest",  
    
)  
  
model = MultiModalEmbeddingModel.from_pretrained("multimodalembedding")  
image = Image.load_from_file(  
  "gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png"  
)  
  
embeddings = model.get_embeddings(  
  image=image,  
  contextual_text="Colosseum",  
  dimension=1408,  
)  
print(f"Image Embedding: {embeddings.image_embedding}")  
print(f"Text Embedding: {embeddings.text_embedding}")  

```

### Text + Image + Video Embeddings​
  * SDK
  * LiteLLM PROXY (Unified Endpoint)


Text + Image 
```
response = await litellm.aembedding(  
  model="vertex_ai/multimodalembedding@001",  
  input=["hey", "gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png"] # will be sent as a gcs image  
)  

```

Text + Video 
```
response = await litellm.aembedding(  
  model="vertex_ai/multimodalembedding@001",  
  input=["hey", "gs://my-bucket/embeddings/supermarket-video.mp4"] # will be sent as a gcs image  
)  

```

Image + Video 
```
response = await litellm.aembedding(  
  model="vertex_ai/multimodalembedding@001",  
  input=["gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png", "gs://my-bucket/embeddings/supermarket-video.mp4"] # will be sent as a gcs image  
)  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: multimodalembedding@001  
  litellm_params:  
   model: vertex_ai/multimodalembedding@001  
   vertex_project: "adroit-crow-413218"  
   vertex_location: "us-central1"  
   vertex_credentials: adroit-crow-413218-a956eef1a2a8.json   
  
litellm_settings:  
 drop_params: True  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request use OpenAI Python SDK, Langchain Python SDK


Text + Image 
```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
# # request sent to model set on litellm proxy, `litellm --model`  
response = client.embeddings.create(  
  model="multimodalembedding@001",   
  input = ["hey", "gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png"],  
)  
  
print(response)  

```

Text + Video 
```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
# # request sent to model set on litellm proxy, `litellm --model`  
response = client.embeddings.create(  
  model="multimodalembedding@001",   
  input = ["hey", "gs://my-bucket/embeddings/supermarket-video.mp4"],  
)  
  
print(response)  

```

Image + Video 
```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
# # request sent to model set on litellm proxy, `litellm --model`  
response = client.embeddings.create(  
  model="multimodalembedding@001",   
  input = ["gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png", "gs://my-bucket/embeddings/supermarket-video.mp4"],  
)  
  
print(response)  

```

## **Image Generation Models**​
Usage 
```
response = await litellm.aimage_generation(  
  prompt="An olympic size swimming pool",  
  model="vertex_ai/imagegeneration@006",  
  vertex_ai_project="adroit-crow-413218",  
  vertex_ai_location="us-central1",  
)  

```

**Generating multiple images**
Use the `n` parameter to pass how many images you want generated
```
response = await litellm.aimage_generation(  
  prompt="An olympic size swimming pool",  
  model="vertex_ai/imagegeneration@006",  
  vertex_ai_project="adroit-crow-413218",  
  vertex_ai_location="us-central1",  
  n=1,  
)  

```

### Supported Image Generation Models​
Model Name| FUsage  
---|---  
`imagen-3.0-generate-001`| `litellm.image_generation('vertex_ai/imagen-3.0-generate-001', prompt)`  
`imagen-3.0-fast-generate-001`| `litellm.image_generation('vertex_ai/imagen-3.0-fast-generate-001', prompt)`  
`imagegeneration@006`| `litellm.image_generation('vertex_ai/imagegeneration@006', prompt)`  
`imagegeneration@005`| `litellm.image_generation('vertex_ai/imagegeneration@005', prompt)`  
`imagegeneration@002`| `litellm.image_generation('vertex_ai/imagegeneration@002', prompt)`  
## **Text to Speech APIs**​
info
LiteLLM supports calling Vertex AI Text to Speech API in the OpenAI text to speech API format
### Usage - Basic​
  * SDK
  * LiteLLM PROXY (Unified Endpoint)


Vertex AI does not support passing a `model` param - so passing `model=vertex_ai/` is the only required param
**Sync Usage**
```
speech_file_path = Path(__file__).parent / "speech_vertex.mp3"  
response = litellm.speech(  
  model="vertex_ai/",  
  input="hello what llm guardrail do you have",  
)  
response.stream_to_file(speech_file_path)  

```

**Async Usage**
```
speech_file_path = Path(__file__).parent / "speech_vertex.mp3"  
response = litellm.aspeech(  
  model="vertex_ai/",  
  input="hello what llm guardrail do you have",  
)  
response.stream_to_file(speech_file_path)  

```

  1. Add model to config.yaml


```
model_list:  
 - model_name: vertex-tts  
  litellm_params:  
   model: vertex_ai/ # Vertex AI does not support passing a `model` param - so passing `model=vertex_ai/` is the only required param  
   vertex_project: "adroit-crow-413218"  
   vertex_location: "us-central1"  
   vertex_credentials: adroit-crow-413218-a956eef1a2a8.json   
  
litellm_settings:  
 drop_params: True  

```

  1. Start Proxy 


```
$ litellm --config /path/to/config.yaml  

```

  1. Make Request use OpenAI Python SDK


```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
# see supported values for "voice" on vertex here:   
# https://console.cloud.google.com/vertex-ai/generative/speech/text-to-speech  
response = client.audio.speech.create(  
  model = "vertex-tts",  
  input="the quick brown fox jumped over the lazy dogs",  
  voice={'languageCode': 'en-US', 'name': 'en-US-Studio-O'}  
)  
print("response from proxy", response)  

```

### Usage - `ssml` as input​
Pass your `ssml` as input to the `input` param, if it contains `<speak>`, it will be automatically detected and passed as `ssml` to the Vertex AI API
If you need to force your `input` to be passed as `ssml`, set `use_ssml=True`
  * SDK
  * LiteLLM PROXY (Unified Endpoint)


Vertex AI does not support passing a `model` param - so passing `model=vertex_ai/` is the only required param
```
speech_file_path = Path(__file__).parent / "speech_vertex.mp3"  
  
  
ssml = """  
<speak>  
  <p>Hello, world!</p>  
  <p>This is a test of the <break strength="medium" /> text-to-speech API.</p>  
</speak>  
"""  
  
response = litellm.speech(  
  input=ssml,  
  model="vertex_ai/test",  
  voice={  
    "languageCode": "en-UK",  
    "name": "en-UK-Studio-O",  
  },  
  audioConfig={  
    "audioEncoding": "LINEAR22",  
    "speakingRate": "10",  
  },  
)  
response.stream_to_file(speech_file_path)  

```

```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
ssml = """  
<speak>  
  <p>Hello, world!</p>  
  <p>This is a test of the <break strength="medium" /> text-to-speech API.</p>  
</speak>  
"""  
  
# see supported values for "voice" on vertex here:   
# https://console.cloud.google.com/vertex-ai/generative/speech/text-to-speech  
response = client.audio.speech.create(  
  model = "vertex-tts",  
  input=ssml,  
  voice={'languageCode': 'en-US', 'name': 'en-US-Studio-O'},  
)  
print("response from proxy", response)  

```

### Forcing SSML Usage​
You can force the use of SSML by setting the `use_ssml` parameter to `True`. This is useful when you want to ensure that your input is treated as SSML, even if it doesn't contain the `<speak>` tags.
Here are examples of how to force SSML usage:
  * SDK
  * LiteLLM PROXY (Unified Endpoint)


Vertex AI does not support passing a `model` param - so passing `model=vertex_ai/` is the only required param
```
speech_file_path = Path(__file__).parent / "speech_vertex.mp3"  
  
  
ssml = """  
<speak>  
  <p>Hello, world!</p>  
  <p>This is a test of the <break strength="medium" /> text-to-speech API.</p>  
</speak>  
"""  
  
response = litellm.speech(  
  input=ssml,  
  use_ssml=True,  
  model="vertex_ai/test",  
  voice={  
    "languageCode": "en-UK",  
    "name": "en-UK-Studio-O",  
  },  
  audioConfig={  
    "audioEncoding": "LINEAR22",  
    "speakingRate": "10",  
  },  
)  
response.stream_to_file(speech_file_path)  

```

```
import openai  
  
client = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")  
  
ssml = """  
<speak>  
  <p>Hello, world!</p>  
  <p>This is a test of the <break strength="medium" /> text-to-speech API.</p>  
</speak>  
"""  
  
# see supported values for "voice" on vertex here:   
# https://console.cloud.google.com/vertex-ai/generative/speech/text-to-speech  
response = client.audio.speech.create(  
  model = "vertex-tts",  
  input=ssml, # pass as None since OpenAI SDK requires this param  
  voice={'languageCode': 'en-US', 'name': 'en-US-Studio-O'},  
  extra_body={"use_ssml": True},  
)  
print("response from proxy", response)  

```

## **Batch APIs**​
Just add the following Vertex env vars to your environment. 
```
# GCS Bucket settings, used to store batch prediction files in  
export GCS_BUCKET_NAME = "litellm-testing-bucket" # the bucket you want to store batch prediction files in  
export GCS_PATH_SERVICE_ACCOUNT="/path/to/service_account.json" # path to your service account json file  
  
# Vertex /batch endpoint settings, used for LLM API requests  
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service_account.json" # path to your service account json file  
export VERTEXAI_LOCATION="us-central1" # can be any vertex location  
export VERTEXAI_PROJECT="my-test-project"   

```

### Usage​
#### 1. Create a file of batch requests for vertex​
LiteLLM expects the file to follow the **OpenAI batches files format**
Each `body` in the file should be an **OpenAI API request**
Create a file called `vertex_batch_completions.jsonl` in the current working directory, the `model` should be the Vertex AI model name
```
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-1.5-flash-001", "messages": [{"role": "system", "content": "You are a helpful assistant."},{"role": "user", "content": "Hello world!"}],"max_tokens": 10}}  
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-1.5-flash-001", "messages": [{"role": "system", "content": "You are an unhelpful assistant."},{"role": "user", "content": "Hello world!"}],"max_tokens": 10}}  

```

#### 2. Upload a File of batch requests​
For `vertex_ai` litellm will upload the file to the provided `GCS_BUCKET_NAME`
```
import os  
oai_client = OpenAI(  
  api_key="sk-1234",        # litellm proxy API key  
  base_url="http://localhost:4000" # litellm proxy base url  
)  
file_name = "vertex_batch_completions.jsonl" #   
_current_dir = os.path.dirname(os.path.abspath(__file__))  
file_path = os.path.join(_current_dir, file_name)  
file_obj = oai_client.files.create(  
  file=open(file_path, "rb"),  
  purpose="batch",  
  extra_body={"custom_llm_provider": "vertex_ai"}, # tell litellm to use vertex_ai for this file upload  
)  

```

**Expected Response**
```
{  
  "id": "gs://litellm-testing-bucket/litellm-vertex-files/publishers/google/models/gemini-1.5-flash-001/d3f198cd-c0d1-436d-9b1e-28e3f282997a",  
  "bytes": 416,  
  "created_at": 1733392026,  
  "filename": "litellm-vertex-files/publishers/google/models/gemini-1.5-flash-001/d3f198cd-c0d1-436d-9b1e-28e3f282997a",  
  "object": "file",  
  "purpose": "batch",  
  "status": "uploaded",  
  "status_details": null  
}  

```

#### 3. Create a batch​
```
batch_input_file_id = file_obj.id # use `file_obj` from step 2  
create_batch_response = oai_client.batches.create(  
  completion_window="24h",  
  endpoint="/v1/chat/completions",  
  input_file_id=batch_input_file_id, # example input_file_id = "gs://litellm-testing-bucket/litellm-vertex-files/publishers/google/models/gemini-1.5-flash-001/c2b1b785-252b-448c-b180-033c4c63b3ce"  
  extra_body={"custom_llm_provider": "vertex_ai"}, # tell litellm to use `vertex_ai` for this batch request  
)  

```

**Expected Response**
```
{  
  "id": "3814889423749775360",  
  "completion_window": "24hrs",  
  "created_at": 1733392026,  
  "endpoint": "",  
  "input_file_id": "gs://litellm-testing-bucket/litellm-vertex-files/publishers/google/models/gemini-1.5-flash-001/d3f198cd-c0d1-436d-9b1e-28e3f282997a",  
  "object": "batch",  
  "status": "validating",  
  "cancelled_at": null,  
  "cancelling_at": null,  
  "completed_at": null,  
  "error_file_id": null,  
  "errors": null,  
  "expired_at": null,  
  "expires_at": null,  
  "failed_at": null,  
  "finalizing_at": null,  
  "in_progress_at": null,  
  "metadata": null,  
  "output_file_id": "gs://litellm-testing-bucket/litellm-vertex-files/publishers/google/models/gemini-1.5-flash-001",  
  "request_counts": null  
}  

```

#### 4. Retrieve a batch​
```
retrieved_batch = oai_client.batches.retrieve(  
  batch_id=create_batch_response.id,  
  extra_body={"custom_llm_provider": "vertex_ai"}, # tell litellm to use `vertex_ai` for this batch request  
)  

```

**Expected Response**
```
{  
  "id": "3814889423749775360",  
  "completion_window": "24hrs",  
  "created_at": 1736500100,  
  "endpoint": "",  
  "input_file_id": "gs://example-bucket-1-litellm/litellm-vertex-files/publishers/google/models/gemini-1.5-flash-001/7b2e47f5-3dd4-436d-920f-f9155bbdc952",  
  "object": "batch",  
  "status": "completed",  
  "cancelled_at": null,  
  "cancelling_at": null,  
  "completed_at": null,  
  "error_file_id": null,  
  "errors": null,  
  "expired_at": null,  
  "expires_at": null,  
  "failed_at": null,  
  "finalizing_at": null,  
  "in_progress_at": null,  
  "metadata": null,  
  "output_file_id": "gs://example-bucket-1-litellm/litellm-vertex-files/publishers/google/models/gemini-1.5-flash-001",  
  "request_counts": null  
}  

```

## **Fine Tuning APIs**​
Property| Details  
---|---  
Description| Create Fine Tuning Jobs in Vertex AI (`/tuningJobs`) using OpenAI Python SDK  
Vertex Fine Tuning Documentation| Vertex Fine Tuning  
### Usage​
#### 1. Add `finetune_settings` to your config.yaml​
```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
# 👇 Key change: For /fine_tuning/jobs endpoints  
finetune_settings:  
 - custom_llm_provider: "vertex_ai"  
  vertex_project: "adroit-crow-413218"  
  vertex_location: "us-central1"  
  vertex_credentials: "/Users/ishaanjaffer/Downloads/adroit-crow-413218-a956eef1a2a8.json"  

```

#### 2. Create a Fine Tuning Job​
  * OpenAI Python SDK
  * curl


```
ft_job = await client.fine_tuning.jobs.create(  
  model="gemini-1.0-pro-002",         # Vertex model you want to fine-tune  
  training_file="gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl",         # file_id from create file response  
  extra_body={"custom_llm_provider": "vertex_ai"}, # tell litellm proxy which provider to use  
)  

```

```
curl http://localhost:4000/v1/fine_tuning/jobs \  
  -H "Content-Type: application/json" \  
  -H "Authorization: Bearer sk-1234" \  
  -d '{  
  "custom_llm_provider": "vertex_ai",  
  "model": "gemini-1.0-pro-002",  
  "training_file": "gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl"  
  }'  

```

**Advanced use case - Passing`adapter_size` to the Vertex AI API**
Set hyper_parameters, such as `n_epochs`, `learning_rate_multiplier` and `adapter_size`. See Vertex Advanced Hyperparameters
  * OpenAI Python SDK
  * curl


```
  
ft_job = client.fine_tuning.jobs.create(  
  model="gemini-1.0-pro-002",         # Vertex model you want to fine-tune  
  training_file="gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl",         # file_id from create file response  
  hyperparameters={  
    "n_epochs": 3,           # epoch_count on Vertex  
    "learning_rate_multiplier": 0.1,  # learning_rate_multiplier on Vertex  
    "adapter_size": "ADAPTER_SIZE_ONE" # type: ignore, vertex specific hyperparameter  
  },  
  extra_body={  
    "custom_llm_provider": "vertex_ai",  
  },  
)  

```

```
curl http://localhost:4000/v1/fine_tuning/jobs \  
  -H "Content-Type: application/json" \  
  -H "Authorization: Bearer sk-1234" \  
  -d '{  
  "custom_llm_provider": "vertex_ai",  
  "model": "gemini-1.0-pro-002",  
  "training_file": "gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl",  
  "hyperparameters": {  
    "n_epochs": 3,  
    "learning_rate_multiplier": 0.1,  
    "adapter_size": "ADAPTER_SIZE_ONE"  
  }  
  }'  

```

## Extra​
### Using `GOOGLE_APPLICATION_CREDENTIALS`​
Here's the code for storing your service account credentials as `GOOGLE_APPLICATION_CREDENTIALS` environment variable:
```
import os   
import tempfile  
  
def load_vertex_ai_credentials():  
 # Define the path to the vertex_key.json file  
 print("loading vertex ai credentials")  
 filepath = os.path.dirname(os.path.abspath(__file__))  
 vertex_key_path = filepath + "/vertex_key.json"  
  
 # Read the existing content of the file or create an empty dictionary  
 try:  
   with open(vertex_key_path, "r") as file:  
     # Read the file content  
     print("Read vertexai file path")  
     content = file.read()  
  
     # If the file is empty or not valid JSON, create an empty dictionary  
     if not content or not content.strip():  
       service_account_key_data = {}  
     else:  
       # Attempt to load the existing JSON content  
       file.seek(0)  
       service_account_key_data = json.load(file)  
 except FileNotFoundError:  
   # If the file doesn't exist, create an empty dictionary  
   service_account_key_data = {}  
  
 # Create a temporary file  
 with tempfile.NamedTemporaryFile(mode="w+", delete=False) as temp_file:  
   # Write the updated content to the temporary file  
   json.dump(service_account_key_data, temp_file, indent=2)  
  
 # Export the temporary file as GOOGLE_APPLICATION_CREDENTIALS  
 os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath(temp_file.name)  

```

### Using GCP Service Account​
info
Trying to deploy LiteLLM on Google Cloud Run? Tutorial here
  1. Figure out the Service Account bound to the Google Cloud Run service


  1. Get the FULL EMAIL address of the corresponding Service Account
  2. Next, go to IAM & Admin > Manage Resources , select your top-level project that houses your Google Cloud Run Service


Click `Add Principal`
  1. Specify the Service Account as the principal and Vertex AI User as the role


Once that's done, when you deploy the new container in the Google Cloud Run service, LiteLLM will have automatic access to all Vertex AI endpoints.
s/o @Darien Kindlund for this tutorial
Previous
AI/ML API
Next
Gemini - Google AI Studio
  * Overview
  * `vertex_ai/` route
    * **System Message**
    * **Function Calling**
    * **JSON Schema**
    * **Grounding - Web Search**
    * **Thinking /`reasoning_content`**
    * **Context Caching**
  * Pre-requisites
  * Sample Usage
  * Usage with LiteLLM Proxy Server
  * Authentication - vertex_project, vertex_location, etc.
    * **Dynamic Params**
    * **Environment Variables**
  * Specifying Safety Settings
    * Set per model/request
    * Set Globally
  * Set Vertex Project & Vertex Location
  * Anthropic
    * Usage
    * Usage - `thinking` / `reasoning_content`
  * Llama 3 API
    * Usage
  * Mistral API
    * Usage
    * Usage - Codestral FIM
  * AI21 Models
    * Usage
  * Gemini Pro
  * Fine-tuned Models
  * Model Garden
  * Gemini Pro Vision
  * Gemini 1.5 Pro (and Vision)
  * Usage - Function Calling
  * Usage - PDF / Videos / Audio etc. Files
    * **Using`gs://` or any URL**
    * **using base64**
  * Chat Models
  * Code Chat Models
  * Text Models
  * Code Text Models
  * **Embedding Models**
    * Supported OpenAI (Unified) Params
    * Supported Vertex Specific Params
  * **Multi-Modal Embeddings**
    * Usage
    * Text + Image + Video Embeddings
  * **Image Generation Models**
    * Supported Image Generation Models
  * **Text to Speech APIs**
    * Usage - Basic
    * Usage - `ssml` as input
    * Forcing SSML Usage
  * **Batch APIs**
    * Usage
  * **Fine Tuning APIs**
    * Usage
  * Extra
    * Using `GOOGLE_APPLICATION_CREDENTIALS`
    * Using GCP Service Account


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * VLLM


On this page
# VLLM
LiteLLM supports all models on VLLM.
Property| Details  
---|---  
Description| vLLM is a fast and easy-to-use library for LLM inference and serving. Docs  
Provider Route on LiteLLM| `hosted_vllm/` (for OpenAI compatible server), `vllm/` (for vLLM sdk usage)  
Provider Doc| vLLM ↗  
Supported Endpoints| `/chat/completions`, `/embeddings`, `/completions`  
# Quick Start
## Usage - litellm.completion (calling OpenAI compatible endpoint)​
vLLM Provides an OpenAI compatible endpoints - here's how to call it with LiteLLM 
In order to use litellm to call a hosted vllm server add the following to your completion call
  * `model="hosted_vllm/<your-vllm-model-name>"`
  * `api_base = "your-hosted-vllm-server"`


```
import litellm   
  
response = litellm.completion(  
      model="hosted_vllm/facebook/opt-125m", # pass the vllm model name  
      messages=messages,  
      api_base="https://hosted-vllm-api.co",  
      temperature=0.2,  
      max_tokens=80)  
  
print(response)  

```

## Usage - LiteLLM Proxy Server (calling OpenAI compatible endpoint)​
Here's how to call an OpenAI-Compatible Endpoint with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: hosted_vllm/facebook/opt-125m # add hosted_vllm/ prefix to route as OpenAI provider  
   api_base: https://hosted-vllm-api.co   # add api base for OpenAI compatible provider  

```

  2. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  3. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



## Embeddings​
  * SDK
  * PROXY


```
from litellm import embedding    
import os  
  
os.environ["HOSTED_VLLM_API_BASE"] = "http://localhost:8000"  
  
  
embedding = embedding(model="hosted_vllm/facebook/opt-125m", input=["Hello world"])  
  
print(embedding)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: my-model  
   litellm_params:  
    model: hosted_vllm/facebook/opt-125m # add hosted_vllm/ prefix to route as OpenAI provider  
    api_base: https://hosted-vllm-api.co   # add api base for OpenAI compatible provider  

```

  1. Start the proxy 


```
$ litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


```
curl -L -X POST 'http://0.0.0.0:4000/embeddings' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{"input": ["hello world"], "model": "my-model"}'  

```

See OpenAI SDK/Langchain/etc. examples
## Send Video URL to VLLM​
Example Implementation from VLLM here
  * (Unified) Files Message
  * (VLLM-specific) Video Message


Use this to send a video url to VLLM + Gemini in the same format, using OpenAI's `files` message type.
There are two ways to send a video url to VLLM:
  1. Pass the video url directly


```
{"type": "file", "file": {"file_id": video_url}},  

```

  1. Pass the video data as base64


```
{"type": "file", "file": {"file_data": f"data:video/mp4;base64,{video_data_base64}"}}  

```

  * SDK
  * PROXY


```
from litellm import completion  
  
messages=[  
  {  
    "role": "user",  
    "content": [  
      {  
        "type": "text",  
        "text": "Summarize the following video"  
      },  
      {  
        "type": "file",  
        "file": {  
          "file_id": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"  
        }  
      }  
    ]  
  }  
]  
  
# call vllm   
os.environ["HOSTED_VLLM_API_BASE"] = "https://hosted-vllm-api.co"  
os.environ["HOSTED_VLLM_API_KEY"] = "" # [optional], if your VLLM server requires an API key  
response = completion(  
  model="hosted_vllm/qwen", # pass the vllm model name  
  messages=messages,  
)  
  
# call gemini   
os.environ["GEMINI_API_KEY"] = "your-gemini-api-key"  
response = completion(  
  model="gemini/gemini-1.5-flash", # pass the gemini model name  
  messages=messages,  
)  
  
print(response)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: my-model  
   litellm_params:  
    model: hosted_vllm/qwen # add hosted_vllm/ prefix to route as OpenAI provider  
    api_base: https://hosted-vllm-api.co   # add api base for OpenAI compatible provider  
  - model_name: my-gemini-model  
   litellm_params:  
    model: gemini/gemini-1.5-flash # add gemini/ prefix to route as Google AI Studio provider  
    api_key: os.environ/GEMINI_API_KEY  

```

  1. Start the proxy 


```
$ litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


```
curl -X POST http://0.0.0.0:4000/chat/completions \  
-H "Authorization: Bearer sk-1234" \  
-H "Content-Type: application/json" \  
-d '{  
  "model": "my-model",  
  "messages": [  
    {"role": "user", "content":   
      [  
        {"type": "text", "text": "Summarize the following video"},  
        {"type": "file", "file": {"file_id": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"}}  
      ]  
    }  
  ]  
}'  

```

Use this to send a video url to VLLM in it's native message format (`video_url`).
There are two ways to send a video url to VLLM:
  1. Pass the video url directly


```
{"type": "video_url", "video_url": {"url": video_url}},  

```

  1. Pass the video data as base64


```
{"type": "video_url", "video_url": {"url": f"data:video/mp4;base64,{video_data_base64}"}}  

```

  * SDK
  * PROXY


```
from litellm import completion  
  
response = completion(  
      model="hosted_vllm/qwen", # pass the vllm model name  
      messages=[  
        {  
          "role": "user",  
          "content": [  
            {  
              "type": "text",  
              "text": "Summarize the following video"  
            },  
            {  
              "type": "video_url",  
              "video_url": {  
                "url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"  
              }  
            }  
          ]  
        }  
      ],  
      api_base="https://hosted-vllm-api.co")  
  
print(response)  

```

  1. Setup config.yaml


```
model_list:  
  - model_name: my-model  
   litellm_params:  
    model: hosted_vllm/qwen # add hosted_vllm/ prefix to route as OpenAI provider  
    api_base: https://hosted-vllm-api.co   # add api base for OpenAI compatible provider  

```

  1. Start the proxy 


```
$ litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it! 


```
curl -X POST http://0.0.0.0:4000/chat/completions \  
-H "Authorization: Bearer sk-1234" \  
-H "Content-Type: application/json" \  
-d '{  
  "model": "my-model",  
  "messages": [  
    {"role": "user", "content":   
      [  
        {"type": "text", "text": "Summarize the following video"},  
        {"type": "video_url", "video_url": {"url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"}}  
      ]  
    }  
  ]  
}'  

```

## (Deprecated) for `vllm pip package`​
### Using - `litellm.completion`​
```
pip install litellm vllm  

```

```
import litellm   
  
response = litellm.completion(  
      model="vllm/facebook/opt-125m", # add a vllm prefix so litellm knows the custom_llm_provider==vllm  
      messages=messages,  
      temperature=0.2,  
      max_tokens=80)  
  
print(response)  

```

### Batch Completion​
```
from litellm import batch_completion  
  
model_name = "facebook/opt-125m"  
provider = "vllm"  
messages = [[{"role": "user", "content": "Hey, how's it going"}] for _ in range(5)]  
  
response_list = batch_completion(  
      model=model_name,   
      custom_llm_provider=provider, # can easily switch to huggingface, replicate, together ai, sagemaker, etc.  
      messages=messages,  
      temperature=0.2,  
      max_tokens=80,  
    )  
print(response_list)  

```

### Prompt Templates​
For models with special prompt templates (e.g. Llama2), we format the prompt to fit their template.
**What if we don't support a model you need?** You can also specify you're own custom prompt formatting, in case we don't have your model covered yet. 
**Does this mean you have to specify a prompt for all models?** No. By default we'll concatenate your message content to make a prompt (expected format for Bloom, T-5, Llama-2 base models, etc.)
**Default Prompt Template**
```
def default_pt(messages):  
  return " ".join(message["content"] for message in messages)  

```

Code for how prompt templates work in LiteLLM
#### Models we already have Prompt Templates for​
Model Name| Works for Models| Function Call  
---|---|---  
meta-llama/Llama-2-7b-chat| All meta-llama llama2 chat models| `completion(model='vllm/meta-llama/Llama-2-7b', messages=messages, api_base="your_api_endpoint")`  
tiiuae/falcon-7b-instruct| All falcon instruct models| `completion(model='vllm/tiiuae/falcon-7b-instruct', messages=messages, api_base="your_api_endpoint")`  
mosaicml/mpt-7b-chat| All mpt chat models| `completion(model='vllm/mosaicml/mpt-7b-chat', messages=messages, api_base="your_api_endpoint")`  
codellama/CodeLlama-34b-Instruct-hf| All codellama instruct models| `completion(model='vllm/codellama/CodeLlama-34b-Instruct-hf', messages=messages, api_base="your_api_endpoint")`  
WizardLM/WizardCoder-Python-34B-V1.0| All wizardcoder models| `completion(model='vllm/WizardLM/WizardCoder-Python-34B-V1.0', messages=messages, api_base="your_api_endpoint")`  
Phind/Phind-CodeLlama-34B-v2| All phind-codellama models| `completion(model='vllm/Phind/Phind-CodeLlama-34B-v2', messages=messages, api_base="your_api_endpoint")`  
#### Custom prompt templates​
```
# Create your own custom prompt template works   
litellm.register_prompt_template(  
  model="togethercomputer/LLaMA-2-7B-32K",  
  roles={  
      "system": {  
        "pre_message": "[INST] <<SYS>>\n",  
        "post_message": "\n<</SYS>>\n [/INST]\n"  
      },  
      "user": {   
        "pre_message": "[INST] ",  
        "post_message": " [/INST]\n"  
      },   
      "assistant": {  
        "pre_message": "\n",  
        "post_message": "\n",  
      }  
    } # tell LiteLLM how you want to map the openai messages to this model  
)  
  
def test_vllm_custom_model():  
  model = "vllm/togethercomputer/LLaMA-2-7B-32K"  
  response = completion(model=model, messages=messages)  
  print(response['choices'][0]['message']['content'])  
  return response  
  
test_vllm_custom_model()  

```

Implementation Code
Previous
Clarifai
Next
Infinity
  * Usage - litellm.completion (calling OpenAI compatible endpoint)
  * Usage - LiteLLM Proxy Server (calling OpenAI compatible endpoint)
  * Embeddings
  * Send Video URL to VLLM
  * (Deprecated) for `vllm pip package`
    * Using - `litellm.completion`
    * Batch Completion
    * Prompt Templates


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * IBM watsonx.ai


On this page
# IBM watsonx.ai
LiteLLM supports all IBM watsonx.ai foundational models and embeddings.
## Environment Variables​
```
os.environ["WATSONX_URL"] = "" # (required) Base URL of your WatsonX instance  
# (required) either one of the following:  
os.environ["WATSONX_APIKEY"] = "" # IBM cloud API key  
os.environ["WATSONX_TOKEN"] = "" # IAM auth token  
# optional - can also be passed as params to completion() or embedding()  
os.environ["WATSONX_PROJECT_ID"] = "" # Project ID of your WatsonX instance  
os.environ["WATSONX_DEPLOYMENT_SPACE_ID"] = "" # ID of your deployment space to use deployed models  
os.environ["WATSONX_ZENAPIKEY"] = "" # Zen API key (use for long-term api token)  

```

See here for more information on how to get an access token to authenticate to watsonx.ai.
## Usage​
![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)
```
import os  
from litellm import completion  
  
os.environ["WATSONX_URL"] = ""  
os.environ["WATSONX_APIKEY"] = ""  
  
## Call WATSONX `/text/chat` endpoint - supports function calling  
response = completion(  
 model="watsonx/meta-llama/llama-3-1-8b-instruct",  
 messages=[{ "content": "what is your favorite colour?","role": "user"}],  
 project_id="<my-project-id>" # or pass with os.environ["WATSONX_PROJECT_ID"]  
)  
  
## Call WATSONX `/text/generation` endpoint - not all models support /chat route.   
response = completion(  
 model="watsonx/ibm/granite-13b-chat-v2",  
 messages=[{ "content": "what is your favorite colour?","role": "user"}],  
 project_id="<my-project-id>"  
)  

```

## Usage - Streaming​
```
import os  
from litellm import completion  
  
os.environ["WATSONX_URL"] = ""  
os.environ["WATSONX_APIKEY"] = ""  
os.environ["WATSONX_PROJECT_ID"] = ""  
  
response = completion(  
 model="watsonx/meta-llama/llama-3-1-8b-instruct",  
 messages=[{ "content": "what is your favorite colour?","role": "user"}],  
 stream=True  
)  
for chunk in response:  
 print(chunk)  

```

#### Example Streaming Output Chunk​
```
{  
 "choices": [  
  {  
   "finish_reason": null,  
   "index": 0,  
   "delta": {  
    "content": "I don't have a favorite color, but I do like the color blue. What's your favorite color?"  
   }  
  }  
 ],  
 "created": null,  
 "model": "watsonx/ibm/granite-13b-chat-v2",  
 "usage": {  
  "prompt_tokens": null,  
  "completion_tokens": null,  
  "total_tokens": null  
 }  
}  

```

## Usage - Models in deployment spaces​
Models that have been deployed to a deployment space (e.g.: tuned models) can be called using the `deployment/<deployment_id>` format (where `<deployment_id>` is the ID of the deployed model in your deployment space). 
The ID of your deployment space must also be set in the environment variable `WATSONX_DEPLOYMENT_SPACE_ID` or passed to the function as `space_id=<deployment_space_id>`. 
```
import litellm  
response = litellm.completion(  
  model="watsonx/deployment/<deployment_id>",  
  messages=[{"content": "Hello, how are you?", "role": "user"}],  
  space_id="<deployment_space_id>"  
)  

```

## Usage - Embeddings​
LiteLLM also supports making requests to IBM watsonx.ai embedding models. The credential needed for this is the same as for completion.
```
from litellm import embedding  
  
response = embedding(  
  model="watsonx/ibm/slate-30m-english-rtrvr",  
  input=["What is the capital of France?"],  
  project_id="<my-project-id>"  
)  
print(response)  
# EmbeddingResponse(model='ibm/slate-30m-english-rtrvr', data=[{'object': 'embedding', 'index': 0, 'embedding': [-0.037463713, -0.02141933, -0.02851813, 0.015519324, ..., -0.0021367231, -0.01704561, -0.001425816, 0.0035238306]}], object='list', usage=Usage(prompt_tokens=8, total_tokens=8))  

```

## OpenAI Proxy Usage​
Here's how to call IBM watsonx.ai with the LiteLLM Proxy Server
### 1. Save keys in your environment​
```
export WATSONX_URL=""  
export WATSONX_APIKEY=""  
export WATSONX_PROJECT_ID=""  

```

### 2. Start the proxy​
  * CLI
  * config.yaml


```
$ litellm --model watsonx/meta-llama/llama-3-8b-instruct  
  
# Server running on http://0.0.0.0:4000  

```

```
model_list:  
 - model_name: llama-3-8b  
  litellm_params:  
   # all params accepted by litellm.completion()  
   model: watsonx/meta-llama/llama-3-8b-instruct  
   api_key: "os.environ/WATSONX_API_KEY" # does os.getenv("WATSONX_API_KEY")  

```

### 3. Test it​
  * Curl Request
  * OpenAI v1.0.0+
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "llama-3-8b",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what is your favorite colour?"  
    }  
   ]  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="llama-3-8b", messages=[  
  {  
    "role": "user",  
    "content": "what is your favorite colour?"  
  }  
])  
  
print(response)  
  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000", # set openai_api_base to the LiteLLM Proxy  
  model = "llama-3-8b",  
  temperature=0.1  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

## Authentication​
### Passing credentials as parameters​
You can also pass the credentials as parameters to the completion and embedding functions.
```
import os  
from litellm import completion  
  
response = completion(  
      model="watsonx/ibm/granite-13b-chat-v2",  
      messages=[{ "content": "What is your favorite color?","role": "user"}],  
      url="",  
      api_key="",  
      project_id=""  
)  

```

## Supported IBM watsonx.ai Models​
Here are some examples of models available in IBM watsonx.ai that you can use with LiteLLM:
Mode Name| Command  
---|---  
Flan T5 XXL| `completion(model=watsonx/google/flan-t5-xxl, messages=messages)`  
Flan Ul2| `completion(model=watsonx/google/flan-ul2, messages=messages)`  
Mt0 XXL| `completion(model=watsonx/bigscience/mt0-xxl, messages=messages)`  
Gpt Neox| `completion(model=watsonx/eleutherai/gpt-neox-20b, messages=messages)`  
Mpt 7B Instruct2| `completion(model=watsonx/ibm/mpt-7b-instruct2, messages=messages)`  
Starcoder| `completion(model=watsonx/bigcode/starcoder, messages=messages)`  
Llama 2 70B Chat| `completion(model=watsonx/meta-llama/llama-2-70b-chat, messages=messages)`  
Llama 2 13B Chat| `completion(model=watsonx/meta-llama/llama-2-13b-chat, messages=messages)`  
Granite 13B Instruct| `completion(model=watsonx/ibm/granite-13b-instruct-v1, messages=messages)`  
Granite 13B Chat| `completion(model=watsonx/ibm/granite-13b-chat-v1, messages=messages)`  
Flan T5 XL| `completion(model=watsonx/google/flan-t5-xl, messages=messages)`  
Granite 13B Chat V2| `completion(model=watsonx/ibm/granite-13b-chat-v2, messages=messages)`  
Granite 13B Instruct V2| `completion(model=watsonx/ibm/granite-13b-instruct-v2, messages=messages)`  
Elyza Japanese Llama 2 7B Instruct| `completion(model=watsonx/elyza/elyza-japanese-llama-2-7b-instruct, messages=messages)`  
Mixtral 8X7B Instruct V01 Q| `completion(model=watsonx/ibm-mistralai/mixtral-8x7b-instruct-v01-q, messages=messages)`  
For a list of all available models in watsonx.ai, see here.
## Supported IBM watsonx.ai Embedding Models​
Model Name| Function Call  
---|---  
Slate 30m| `embedding(model="watsonx/ibm/slate-30m-english-rtrvr", input=input)`  
Slate 125m| `embedding(model="watsonx/ibm/slate-125m-english-rtrvr", input=input)`  
For a list of all available embedding models in watsonx.ai, see here.
Previous
Deepgram
Next
Predibase
  * Environment Variables
  * Usage
  * Usage - Streaming
  * Usage - Models in deployment spaces
  * Usage - Embeddings
  * OpenAI Proxy Usage
    * 1. Save keys in your environment
    * 2. Start the proxy
    * 3. Test it
  * Authentication
    * Passing credentials as parameters
  * Supported IBM watsonx.ai Models
  * Supported IBM watsonx.ai Embedding Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Voyage AI


On this page
# Voyage AI
https://docs.voyageai.com/embeddings/
## API Key​
```
# env variable  
os.environ['VOYAGE_API_KEY']  

```

## Sample Usage - Embedding​
```
from litellm import embedding  
import os  
  
os.environ['VOYAGE_API_KEY'] = ""  
response = embedding(  
  model="voyage/voyage-3-large",  
  input=["good morning from litellm"],  
)  
print(response)  

```

## Supported Models​
All models listed here https://docs.voyageai.com/embeddings/#models-and-specifics are supported
Model Name| Function Call  
---|---  
voyage-3-large| `embedding(model="voyage/voyage-3-large", input)`  
voyage-3| `embedding(model="voyage/voyage-3", input)`  
voyage-3-lite| `embedding(model="voyage/voyage-3-lite", input)`  
voyage-code-3| `embedding(model="voyage/voyage-code-3", input)`  
voyage-finance-2| `embedding(model="voyage/voyage-finance-2", input)`  
voyage-law-2| `embedding(model="voyage/voyage-law-2", input)`  
voyage-code-2| `embedding(model="voyage/voyage-code-2", input)`  
voyage-multilingual-2| `embedding(model="voyage/voyage-multilingual-2	", input)`  
voyage-large-2-instruct| `embedding(model="voyage/voyage-large-2-instruct", input)`  
voyage-large-2| `embedding(model="voyage/voyage-large-2", input)`  
voyage-2| `embedding(model="voyage/voyage-2", input)`  
voyage-lite-02-instruct| `embedding(model="voyage/voyage-lite-02-instruct", input)`  
voyage-01| `embedding(model="voyage/voyage-01", input)`  
voyage-lite-01| `embedding(model="voyage/voyage-lite-01", input)`  
voyage-lite-01-instruct| `embedding(model="voyage/voyage-lite-01-instruct", input)`  
Previous
Together AI
Next
Jina AI
  * API Key
  * Sample Usage - Embedding
  * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Volcano Engine (Volcengine)


On this page
# Volcano Engine (Volcengine)
https://www.volcengine.com/docs/82379/1263482
tip
**We support ALL Volcengine NIM models, just set`model=volcengine/<any-model-on-volcengine>` as a prefix when sending litellm requests**
## API Key​
```
# env variable  
os.environ['VOLCENGINE_API_KEY']  

```

## Sample Usage​
```
from litellm import completion  
import os  
  
os.environ['VOLCENGINE_API_KEY'] = ""  
response = completion(  
  model="volcengine/<OUR_ENDPOINT_ID>",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ],  
  temperature=0.2,    # optional  
  top_p=0.9,       # optional  
  frequency_penalty=0.1, # optional  
  presence_penalty=0.1,  # optional  
  max_tokens=10,     # optional  
  stop=["\n\n"],     # optional  
)  
print(response)  

```

## Sample Usage - Streaming​
```
from litellm import completion  
import os  
  
os.environ['VOLCENGINE_API_KEY'] = ""  
response = completion(  
  model="volcengine/<OUR_ENDPOINT_ID>",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ],  
  stream=True,  
  temperature=0.2,    # optional  
  top_p=0.9,       # optional  
  frequency_penalty=0.1, # optional  
  presence_penalty=0.1,  # optional  
  max_tokens=10,     # optional  
  stop=["\n\n"],     # optional  
)  
  
for chunk in response:  
  print(chunk)  

```

## Supported Models - 💥 ALL Volcengine NIM Models Supported!​
We support ALL `volcengine` models, just set `volcengine/<OUR_ENDPOINT_ID>` as a prefix when sending completion requests
## Sample Usage - LiteLLM Proxy​
### Config.yaml setting​
```
model_list:  
 - model_name: volcengine-model  
  litellm_params:  
   model: volcengine/<OUR_ENDPOINT_ID>  
   api_key: os.environ/VOLCENGINE_API_KEY  

```

### Send Request​
```
curl --location 'http://localhost:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "volcengine-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "here is my api key. openai_api_key=sk-1234"  
    }  
  ]  
}'  

```

Previous
Cerebras
Next
Triton Inference Server
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Supported Models - 💥 ALL Volcengine NIM Models Supported!
  * Sample Usage - LiteLLM Proxy
    * Config.yaml setting
    * Send Request


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * xAI


On this page
# xAI
https://docs.x.ai/docs
tip
**We support ALL xAI models, just set`model=xai/<any-model-on-xai>` as a prefix when sending litellm requests**
## API Key​
```
# env variable  
os.environ['XAI_API_KEY']  

```

## Sample Usage​
LiteLLM python sdk usage - Non-streaming
```
from litellm import completion  
import os  
  
os.environ['XAI_API_KEY'] = ""  
response = completion(  
  model="xai/grok-3-mini-beta",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ],  
  max_tokens=10,  
  response_format={ "type": "json_object" },  
  seed=123,  
  stop=["\n\n"],  
  temperature=0.2,  
  top_p=0.9,  
  tool_choice="auto",  
  tools=[],  
  user="user",  
)  
print(response)  

```

## Sample Usage - Streaming​
LiteLLM python sdk usage - Streaming
```
from litellm import completion  
import os  
  
os.environ['XAI_API_KEY'] = ""  
response = completion(  
  model="xai/grok-3-mini-beta",  
  messages=[  
    {  
      "role": "user",  
      "content": "What's the weather like in Boston today in Fahrenheit?",  
    }  
  ],  
  stream=True,  
  max_tokens=10,  
  response_format={ "type": "json_object" },  
  seed=123,  
  stop=["\n\n"],  
  temperature=0.2,  
  top_p=0.9,  
  tool_choice="auto",  
  tools=[],  
  user="user",  
)  
  
for chunk in response:  
  print(chunk)  

```

## Sample Usage - Vision​
LiteLLM python sdk usage - Vision
```
import os   
from litellm import completion  
  
os.environ["XAI_API_KEY"] = "your-api-key"  
  
response = completion(  
  model="xai/grok-2-vision-latest",  
  messages=[  
    {  
      "role": "user",  
      "content": [  
        {  
          "type": "image_url",  
          "image_url": {  
            "url": "https://science.nasa.gov/wp-content/uploads/2023/09/web-first-images-release.png",  
            "detail": "high",  
          },  
        },  
        {  
          "type": "text",  
          "text": "What's in this image?",  
        },  
      ],  
    },  
  ],  
)  

```

## Usage with LiteLLM Proxy Server​
Here's how to call a XAI model with the LiteLLM Proxy Server
  1. Modify the config.yaml 
```
model_list:  
 - model_name: my-model  
  litellm_params:  
   model: xai/<your-model-name> # add xai/ prefix to route as XAI provider  
   api_key: api-key         # api key to send your model  

```



  1. Start the proxy 
```
$ litellm --config /path/to/config.yaml  

```

  2. Send Request to LiteLLM Proxy Server
     * OpenAI Python v1.0.0+
     * curl
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="my-model",  
  messages = [  
    {  
      "role": "user",  
      "content": "what llm are you"  
    }  
  ],  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "my-model",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
}'  

```



## Reasoning Usage​
LiteLLM supports reasoning usage for xAI models.
  * LiteLLM Python SDK
  * LiteLLM Proxy - OpenAI SDK Usage


reasoning with xai/grok-3-mini-beta
```
import litellm  
response = litellm.completion(  
  model="xai/grok-3-mini-beta",  
  messages=[{"role": "user", "content": "What is 101*3?"}],  
  reasoning_effort="low",  
)  
  
print("Reasoning Content:")  
print(response.choices[0].message.reasoning_content)  
  
print("\nFinal Response:")  
print(completion.choices[0].message.content)  
  
print("\nNumber of completion tokens (input):")  
print(completion.usage.completion_tokens)  
  
print("\nNumber of reasoning tokens (input):")  
print(completion.usage.completion_tokens_details.reasoning_tokens)  

```

reasoning with xai/grok-3-mini-beta
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",       # pass litellm proxy key, if you're using virtual keys  
  base_url="http://0.0.0.0:4000" # litellm-proxy-base url  
)  
  
response = client.chat.completions.create(  
  model="xai/grok-3-mini-beta",  
  messages=[{"role": "user", "content": "What is 101*3?"}],  
  reasoning_effort="low",  
)  
  
print("Reasoning Content:")  
print(response.choices[0].message.reasoning_content)  
  
print("\nFinal Response:")  
print(completion.choices[0].message.content)  
  
print("\nNumber of completion tokens (input):")  
print(completion.usage.completion_tokens)  
  
print("\nNumber of reasoning tokens (input):")  
print(completion.usage.completion_tokens_details.reasoning_tokens)  

```

**Example Response:**
```
Reasoning Content:  
Let me calculate 101 multiplied by 3:  
101 * 3 = 303.  
I can double-check that: 100 * 3 is 300, and 1 * 3 is 3, so 300 + 3 = 303. Yes, that's correct.  
  
Final Response:  
The result of 101 multiplied by 3 is 303.  
  
Number of completion tokens (input):  
14  
  
Number of reasoning tokens (input):  
310  

```

Previous
Nvidia NIM
Next
LM Studio
  * API Key
  * Sample Usage
  * Sample Usage - Streaming
  * Sample Usage - Vision
  * Usage with LiteLLM Proxy Server
  * Reasoning Usage


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
      * Life of a Request
      * What is stored in the DB
      * High Availability Setup (Resolve DB Deadlocks)
      * Router Architecture (Fallbacks / Retries)
      * User Management Hierarchy
      * Control Model Access with OIDC (Azure AD/Keycloak/etc.)
      * Image URL Handling
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Architecture
  * Life of a Request


On this page
# Life of a Request
## High Level architecture​
### Request Flow​
  1. **User Sends Request** : The process begins when a user sends a request to the LiteLLM Proxy Server (Gateway).
  2. **Virtual Keys**: At this stage the `Bearer` token in the request is checked to ensure it is valid and under it's budget. Here is the list of checks that run for each request
     * 2.1 Check if the Virtual Key exists in Redis Cache or In Memory Cache
     * 2.2 **If not in Cache** , Lookup Virtual Key in DB
  3. **Rate Limiting** : The MaxParallelRequestsHandler checks the **rate limit (rpm/tpm)** for the the following components:
     * Global Server Rate Limit
     * Virtual Key Rate Limit
     * User Rate Limit
     * Team Limit
  4. **LiteLLM`proxy_server.py`** : Contains the `/chat/completions` and `/embeddings` endpoints. Requests to these endpoints are sent through the LiteLLM Router
  5. **LiteLLM Router**: The LiteLLM Router handles Load balancing, Fallbacks, Retries for LLM API deployments.
  6. **litellm.completion() / litellm.embedding()** : The litellm Python SDK is used to call the LLM in the OpenAI API format (Translation and parameter mapping)
  7. **Post-Request Processing** : After the response is sent back to the client, the following **asynchronous** tasks are performed:
     * Logging to Lunary, MLflow, LangFuse or other logging destinations
     * The MaxParallelRequestsHandler updates the rpm/tpm usage for the 
       * Global Server Rate Limit
       * Virtual Key Rate Limit
       * User Rate Limit
       * Team Limit
     * The `_ProxyDBLogger` updates spend / usage in the LiteLLM database. Here is everything tracked in the DB per request


## Frequently Asked Questions​
  1. Is a db transaction tied to the lifecycle of request?
     * No, a db transaction is not tied to the lifecycle of a request.
     * The check if a virtual key is valid relies on a DB read if it's not in cache.
     * All other DB transactions are async in background tasks


Previous
Demo App
Next
What is stored in the DB
  * High Level architecture
    * Request Flow
  * Frequently Asked Questions


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
      * Logging
      * StandardLoggingPayload Specification
      * Team/Key Based Logging
      * 📈 Prometheus metrics
      * Alerting / Webhooks
      * PagerDuty Alerting
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Logging, Alerting, Metrics
  * Alerting / Webhooks


On this page
# Alerting / Webhooks
Get alerts for:
Category| Alert Type  
---|---  
**LLM Performance**|  Hanging API calls, Slow API calls, Failed API calls, Model outage alerting  
**Budget & Spend**| Budget tracking per key/user, Soft budget alerts, Weekly & Monthly spend reports per Team/Tag  
**System Health**|  Failed database read/writes  
**Daily Reports**|  Top 5 slowest LLM deployments, Top 5 LLM deployments with most failed requests, Weekly & Monthly spend per Team/Tag  
Works across: 
  * Slack
  * Discord
  * Microsoft Teams


## Quick Start​
Set up a slack alert channel to receive alerts from proxy.
### Step 1: Add a Slack Webhook URL to env​
Get a slack webhook url from https://api.slack.com/messaging/webhooks
You can also use Discord Webhooks, see here
Set `SLACK_WEBHOOK_URL` in your proxy env to enable Slack alerts.
```
export SLACK_WEBHOOK_URL="https://hooks.slack.com/services/<>/<>/<>"  

```

### Step 2: Setup Proxy​
```
general_settings:   
  alerting: ["slack"]  
  alerting_threshold: 300 # sends alerts if requests hang for 5min+ and responses take 5min+   
  spend_report_frequency: "1d" # [Optional] set as 1d, 2d, 30d .... Specify how often you want a Spend Report to be sent  
    
  # [OPTIONAL ALERTING ARGS]  
  alerting_args:  
    daily_report_frequency: 43200 # 12 hours in seconds  
    report_check_interval: 3600  # 1 hour in seconds  
    budget_alert_ttl: 86400    # 24 hours in seconds  
    outage_alert_ttl: 60      # 1 minute in seconds  
    region_outage_alert_ttl: 60  # 1 minute in seconds  
    minor_outage_alert_threshold: 5   
    major_outage_alert_threshold: 10  
    max_outage_alert_list_size: 1000  
    log_to_console: false  
    

```

Start proxy 
```
$ litellm --config /path/to/config.yaml  

```

### Step 3: Test it!​
```
curl -X GET 'http://0.0.0.0:4000/health/services?service=slack' \  
-H 'Authorization: Bearer sk-1234'  

```

## Advanced​
### Redacting Messages from Alerts​
By default alerts show the `messages/input` passed to the LLM. If you want to redact this from slack alerting set the following setting on your config
```
general_settings:  
 alerting: ["slack"]  
 alert_types: ["spend_reports"]   
  
litellm_settings:  
 redact_messages_in_exceptions: True  

```

### Soft Budget Alerts for Virtual Keys​
Use this to send an alert when a key/team is close to it's budget running out
Step 1. Create a virtual key with a soft budget
Set the `soft_budget` to 0.001
```
curl -X 'POST' \  
 'http://localhost:4000/key/generate' \  
 -H 'accept: application/json' \  
 -H 'x-goog-api-key: sk-1234' \  
 -H 'Content-Type: application/json' \  
 -d '{  
 "key_alias": "prod-app1",  
 "team_id": "113c1a22-e347-4506-bfb2-b320230ea414",  
 "soft_budget": 0.001  
}'  

```

Step 2. Send a request to the proxy with the virtual key
```
curl http://0.0.0.0:4000/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer sk-Nb5eCf427iewOlbxXIH4Ow" \  
-d '{  
 "model": "openai/gpt-4",  
 "messages": [  
  {  
   "role": "user",  
   "content": "this is a test request, write a short poem"  
  }  
 ]  
}'  
  

```

Step 3. Check slack for Expected Alert
### Add Metadata to alerts​
Add alerting metadata to proxy calls for debugging. 
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [],   
  extra_body={  
    "metadata": {  
      "alerting_metadata": {  
        "hello": "world"  
      }  
    }  
  }  
)  

```

**Expected Response**
### Select specific alert types​
Set `alert_types` if you want to Opt into only specific alert types. When alert_types is not set, all Default Alert Types are enabled.
👉 **See all alert types here**
```
general_settings:  
 alerting: ["slack"]  
 alert_types: [  
  "llm_exceptions",  
  "llm_too_slow",  
  "llm_requests_hanging",  
  "budget_alerts",  
  "spend_reports",  
  "db_exceptions",  
  "daily_reports",  
  "cooldown_deployment",  
  "new_model_added",  
 ]   

```

### Map slack channels to alert type​
Use this if you want to set specific channels per alert type
**This allows you to do the following**
```
llm_exceptions -> go to slack channel #llm-exceptions  
spend_reports -> go to slack channel #llm-spend-reports  

```

Set `alert_to_webhook_url` on your config.yaml
  * 1 channel per alert
  * multiple channels per alert


```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
general_settings:   
 master_key: sk-1234  
 alerting: ["slack"]  
 alerting_threshold: 0.0001 # (Seconds) set an artificially low threshold for testing alerting  
 alert_to_webhook_url: {  
  "llm_exceptions": "https://hooks.slack.com/services/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",  
  "llm_too_slow": "https://hooks.slack.com/services/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",  
  "llm_requests_hanging": "https://hooks.slack.com/services/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",  
  "budget_alerts": "https://hooks.slack.com/services/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",  
  "db_exceptions": "https://hooks.slack.com/services/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",  
  "daily_reports": "https://hooks.slack.com/services/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",  
  "spend_reports": "https://hooks.slack.com/services/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",  
  "cooldown_deployment": "https://hooks.slack.com/services/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",  
  "new_model_added": "https://hooks.slack.com/services/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",  
  "outage_alerts": "https://hooks.slack.com/services/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",  
 }  
  
litellm_settings:  
 success_callback: ["langfuse"]  

```

Provide multiple slack channels for a given alert type
```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
general_settings:   
 master_key: sk-1234  
 alerting: ["slack"]  
 alerting_threshold: 0.0001 # (Seconds) set an artificially low threshold for testing alerting  
 alert_to_webhook_url: {  
  "llm_exceptions": ["os.environ/SLACK_WEBHOOK_URL", "os.environ/SLACK_WEBHOOK_URL_2"],  
  "llm_too_slow": ["https://webhook.site/7843a980-a494-4967-80fb-d502dbc16886", "https://webhook.site/28cfb179-f4fb-4408-8129-729ff55cf213"],  
  "llm_requests_hanging": ["os.environ/SLACK_WEBHOOK_URL_5", "os.environ/SLACK_WEBHOOK_URL_6"],  
  "budget_alerts": ["os.environ/SLACK_WEBHOOK_URL_7", "os.environ/SLACK_WEBHOOK_URL_8"],  
  "db_exceptions": ["os.environ/SLACK_WEBHOOK_URL_9", "os.environ/SLACK_WEBHOOK_URL_10"],  
  "daily_reports": ["os.environ/SLACK_WEBHOOK_URL_11", "os.environ/SLACK_WEBHOOK_URL_12"],  
  "spend_reports": ["os.environ/SLACK_WEBHOOK_URL_13", "os.environ/SLACK_WEBHOOK_URL_14"],  
  "cooldown_deployment": ["os.environ/SLACK_WEBHOOK_URL_15", "os.environ/SLACK_WEBHOOK_URL_16"],  
  "new_model_added": ["os.environ/SLACK_WEBHOOK_URL_17", "os.environ/SLACK_WEBHOOK_URL_18"],  
  "outage_alerts": ["os.environ/SLACK_WEBHOOK_URL_19", "os.environ/SLACK_WEBHOOK_URL_20"],  
 }  
  
litellm_settings:  
 success_callback: ["langfuse"]  

```

Test it - send a valid llm request - expect to see a `llm_too_slow` alert in it's own slack channel
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gpt-4",  
  "messages": [  
   {"role": "user", "content": "Hello, Claude gm!"}  
  ]  
}'  

```

### MS Teams Webhooks​
MS Teams provides a slack compatible webhook url that you can use for alerting
##### Quick Start​
  1. Get a webhook url for your Microsoft Teams channel 
  2. Add it to your .env


```
SLACK_WEBHOOK_URL="https://berriai.webhook.office.com/webhookb2/...6901/IncomingWebhook/b55fa0c2a48647be8e6effedcd540266/e04b1092-4a3e-44a2-ab6b-29a0a4854d1d"  

```

  1. Add it to your litellm config 


```
model_list:   
  model_name: "azure-model"  
  litellm_params:  
    model: "azure/gpt-35-turbo"  
    api_key: "my-bad-key" # 👈 bad key  
  
general_settings:   
  alerting: ["slack"]  
  alerting_threshold: 300 # sends alerts if requests hang for 5min+ and responses take 5min+   

```

  1. Run health check!


Call the proxy `/health/services` endpoint to test if your alerting connection is correctly setup.
```
curl --location 'http://0.0.0.0:4000/health/services?service=slack' \  
--header 'Authorization: Bearer sk-1234'  

```

**Expected Response**
### Discord Webhooks​
Discord provides a slack compatible webhook url that you can use for alerting
##### Quick Start​
  1. Get a webhook url for your discord channel 
  2. Append `/slack` to your discord webhook - it should look like


```
"https://discord.com/api/webhooks/1240030362193760286/cTLWt5ATn1gKmcy_982rl5xmYHsrM1IWJdmCL1AyOmU9JdQXazrp8L1_PYgUtgxj8x4f/slack"  

```

  1. Add it to your litellm config 


```
model_list:   
  model_name: "azure-model"  
  litellm_params:  
    model: "azure/gpt-35-turbo"  
    api_key: "my-bad-key" # 👈 bad key  
  
general_settings:   
  alerting: ["slack"]  
  alerting_threshold: 300 # sends alerts if requests hang for 5min+ and responses take 5min+   
  
environment_variables:  
  SLACK_WEBHOOK_URL: "https://discord.com/api/webhooks/1240030362193760286/cTLWt5ATn1gKmcy_982rl5xmYHsrM1IWJdmCL1AyOmU9JdQXazrp8L1_PYgUtgxj8x4f/slack"  

```

## [BETA] Webhooks for Budget Alerts​
**Note** : This is a beta feature, so the spec might change.
Set a webhook to get notified for budget alerts. 
  1. Setup config.yaml


Add url to your environment, for testing you can use a link from here
```
export WEBHOOK_URL="https://webhook.site/6ab090e8-c55f-4a23-b075-3209f5c57906"  

```

Add 'webhook' to config.yaml
```
general_settings:   
 alerting: ["webhook"] # 👈 KEY CHANGE  

```

  1. Start proxy


```
litellm --config /path/to/config.yaml  
  
# RUNNING on http://0.0.0.0:4000  

```

  1. Test it!


```
curl -X GET --location 'http://0.0.0.0:4000/health/services?service=webhook' \  
--header 'Authorization: Bearer sk-1234'  

```

**Expected Response**
```
{  
 "spend": 1, # the spend for the 'event_group'  
 "max_budget": 0, # the 'max_budget' set for the 'event_group'  
 "token": "88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b",  
 "user_id": "default_user_id",  
 "team_id": null,  
 "user_email": null,  
 "key_alias": null,  
 "projected_exceeded_data": null,  
 "projected_spend": null,  
 "event": "budget_crossed", # Literal["budget_crossed", "threshold_crossed", "projected_limit_exceeded"]  
 "event_group": "user",  
 "event_message": "User Budget: Budget Crossed"  
}  

```

### API Spec for Webhook Event​
  * `spend` _float_ : The current spend amount for the 'event_group'.
  * `max_budget` _float or null_ : The maximum allowed budget for the 'event_group'. null if not set. 
  * `token` _str_ : A hashed value of the key, used for authentication or identification purposes.
  * `customer_id` _str or null_ : The ID of the customer associated with the event (optional).
  * `internal_user_id` _str or null_ : The ID of the internal user associated with the event (optional).
  * `team_id` _str or null_ : The ID of the team associated with the event (optional).
  * `user_email` _str or null_ : The email of the internal user associated with the event (optional).
  * `key_alias` _str or null_ : An alias for the key associated with the event (optional).
  * `projected_exceeded_date` _str or null_ : The date when the budget is projected to be exceeded, returned when 'soft_budget' is set for key (optional).
  * `projected_spend` _float or null_ : The projected spend amount, returned when 'soft_budget' is set for key (optional).
  * `event` _Literal["budget_crossed", "threshold_crossed", "projected_limit_exceeded"]_ : The type of event that triggered the webhook. Possible values are:
    * "spend_tracked": Emitted whenever spend is tracked for a customer id. 
    * "budget_crossed": Indicates that the spend has exceeded the max budget.
    * "threshold_crossed": Indicates that spend has crossed a threshold (currently sent when 85% and 95% of budget is reached).
    * "projected_limit_exceeded": For "key" only - Indicates that the projected spend is expected to exceed the soft budget threshold.
  * `event_group` _Literal["customer", "internal_user", "key", "team", "proxy"]_ : The group associated with the event. Possible values are:
    * "customer": The event is related to a specific customer
    * "internal_user": The event is related to a specific internal user.
    * "key": The event is related to a specific key.
    * "team": The event is related to a team.
    * "proxy": The event is related to a proxy.
  * `event_message` _str_ : A human-readable description of the event.


## Region-outage alerting (✨ Enterprise feature)​
info
Get a free 2-week license
Setup alerts if a provider region is having an outage. 
```
general_settings:  
  alerting: ["slack"]  
  alert_types: ["region_outage_alerts"]   

```

By default this will trigger if multiple models in a region fail 5+ requests in 1 minute. '400' status code errors are not counted (i.e. BadRequestErrors).
Control thresholds with: 
```
general_settings:  
  alerting: ["slack"]  
  alert_types: ["region_outage_alerts"]   
  alerting_args:  
    region_outage_alert_ttl: 60 # time-window in seconds  
    minor_outage_alert_threshold: 5 # number of errors to trigger a minor alert  
    major_outage_alert_threshold: 10 # number of errors to trigger a major alert  

```

## **All Possible Alert Types**​
👉 **Here is how you can set specific alert types**
LLM-related Alerts
Alert Type| Description| Default On  
---|---|---  
`llm_exceptions`| Alerts for LLM API exceptions| ✅  
`llm_too_slow`| Notifications for LLM responses slower than the set threshold| ✅  
`llm_requests_hanging`| Alerts for LLM requests that are not completing| ✅  
`cooldown_deployment`| Alerts when a deployment is put into cooldown| ✅  
`new_model_added`| Notifications when a new model is added to litellm proxy through /model/new| ✅  
`outage_alerts`| Alerts when a specific LLM deployment is facing an outage| ✅  
`region_outage_alerts`| Alerts when a specific LLM region is facing an outage. Example us-east-1| ✅  
Budget and Spend Alerts
Alert Type| Description| Default On  
---|---|---  
`budget_alerts`| Notifications related to budget limits or thresholds| ✅  
`spend_reports`| Periodic reports on spending across teams or tags| ✅  
`failed_tracking_spend`| Alerts when spend tracking fails| ✅  
`daily_reports`| Daily Spend reports| ✅  
`fallback_reports`| Weekly Reports on LLM fallback occurrences| ✅  
Database Alerts
Alert Type| Description| Default On  
---|---|---  
`db_exceptions`| Notifications for database-related exceptions| ✅  
Management Endpoint Alerts - Virtual Key, Team, Internal User
Alert Type| Description| Default On  
---|---|---  
`new_virtual_key_created`| Notifications when a new virtual key is created| ❌  
`virtual_key_updated`| Alerts when a virtual key is modified| ❌  
`virtual_key_deleted`| Notifications when a virtual key is removed| ❌  
`new_team_created`| Alerts for the creation of a new team| ❌  
`team_updated`| Notifications when team details are modified| ❌  
`team_deleted`| Alerts when a team is deleted| ❌  
`new_internal_user_created`| Notifications for new internal user accounts| ❌  
`internal_user_updated`| Alerts when an internal user's details are changed| ❌  
`internal_user_deleted`| Notifications when an internal user account is removed| ❌  
## `alerting_args` Specification​
Parameter| Default| Description  
---|---|---  
`daily_report_frequency`| 43200 (12 hours)| Frequency of receiving deployment latency/failure reports in seconds  
`report_check_interval`| 3600 (1 hour)| How often to check if a report should be sent (background process) in seconds  
`budget_alert_ttl`| 86400 (24 hours)| Cache TTL for budget alerts to prevent spam when budget is crossed  
`outage_alert_ttl`| 60 (1 minute)| Time window for collecting model outage errors in seconds  
`region_outage_alert_ttl`| 60 (1 minute)| Time window for collecting region-based outage errors in seconds  
`minor_outage_alert_threshold`| 5| Number of errors that trigger a minor outage alert (400 errors not counted)  
`major_outage_alert_threshold`| 10| Number of errors that trigger a major outage alert (400 errors not counted)  
`max_outage_alert_list_size`| 1000| Maximum number of errors to store in cache per model/region  
`log_to_console`| false| If true, prints alerting payload to console as a `.warning` log.  
Previous
📈 Prometheus metrics
Next
PagerDuty Alerting
  * Quick Start
    * Step 1: Add a Slack Webhook URL to env
    * Step 2: Setup Proxy
    * Step 3: Test it!
  * Advanced
    * Redacting Messages from Alerts
    * Soft Budget Alerts for Virtual Keys
    * Add Metadata to alerts
    * Select specific alert types
    * Map slack channels to alert type
    * MS Teams Webhooks
    * Discord Webhooks
  * BETA Webhooks for Budget Alerts
    * API Spec for Webhook Event
  * Region-outage alerting (✨ Enterprise feature)
  * **All Possible Alert Types**
  * `alerting_args` Specification


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
    * OpenAI
    * OpenAI (Text Completion)
    * OpenAI-Compatible Endpoints
    * Azure OpenAI
    * Azure AI Studio
    * AI/ML API
    * VertexAI [Anthropic, Gemini, Model Garden]
    * Google AI Studio
    * Anthropic
    * AWS Sagemaker
    * AWS Bedrock
    * LiteLLM Proxy (LLM Gateway)
    * Mistral AI API
    * Codestral API [Mistral AI]
    * Cohere
    * Anyscale
    * Hugging Face
    * Databricks
    * Deepgram
    * IBM watsonx.ai
    * Predibase
    * Nvidia NIM
    * xAI
    * LM Studio
    * Cerebras
    * Volcano Engine (Volcengine)
    * Triton Inference Server
    * Ollama
    * Perplexity AI (pplx-api)
    * FriendliAI
    * Galadriel
    * Topaz
    * Groq
    * 🆕 Github
    * Deepseek
    * Fireworks AI
    * Clarifai
    * VLLM
    * Infinity
    * Xinference [Xorbits Inference]
    * Cloudflare Workers AI
    * DeepInfra
    * AI21
    * NLP Cloud
    * Replicate
    * Together AI
    * Voyage AI
    * Jina AI
    * Aleph Alpha
    * Baseten
    * OpenRouter
    * Sambanova
    * Custom API Server (Custom Format)
    * Petals
    * Snowflake
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * Supported Models & Providers
  * Xinference [Xorbits Inference]


On this page
# Xinference [Xorbits Inference]
https://inference.readthedocs.io/en/latest/index.html
## API Base, Key​
```
# env variable  
os.environ['XINFERENCE_API_BASE'] = "http://127.0.0.1:9997/v1"  
os.environ['XINFERENCE_API_KEY'] = "anything" #[optional] no api key required  

```

## Sample Usage - Embedding​
```
from litellm import embedding  
import os  
  
os.environ['XINFERENCE_API_BASE'] = "http://127.0.0.1:9997/v1"  
response = embedding(  
  model="xinference/bge-base-en",  
  input=["good morning from litellm"],  
)  
print(response)  

```

## Sample Usage `api_base` param​
```
from litellm import embedding  
import os  
  
response = embedding(  
  model="xinference/bge-base-en",  
  api_base="http://127.0.0.1:9997/v1",  
  input=["good morning from litellm"],  
)  
print(response)  

```

## Supported Models​
All models listed here https://inference.readthedocs.io/en/latest/models/builtin/embedding/index.html are supported
Model Name| Function Call  
---|---  
bge-base-en| `embedding(model="xinference/bge-base-en", input)`  
bge-base-en-v1.5| `embedding(model="xinference/bge-base-en-v1.5", input)`  
bge-base-zh| `embedding(model="xinference/bge-base-zh", input)`  
bge-base-zh-v1.5| `embedding(model="xinference/bge-base-zh-v1.5", input)`  
bge-large-en| `embedding(model="xinference/bge-large-en", input)`  
bge-large-en-v1.5| `embedding(model="xinference/bge-large-en-v1.5", input)`  
bge-large-zh| `embedding(model="xinference/bge-large-zh", input)`  
bge-large-zh-noinstruct| `embedding(model="xinference/bge-large-zh-noinstruct", input)`  
bge-large-zh-v1.5| `embedding(model="xinference/bge-large-zh-v1.5", input)`  
bge-small-en-v1.5| `embedding(model="xinference/bge-small-en-v1.5", input)`  
bge-small-zh| `embedding(model="xinference/bge-small-zh", input)`  
bge-small-zh-v1.5| `embedding(model="xinference/bge-small-zh-v1.5", input)`  
e5-large-v2| `embedding(model="xinference/e5-large-v2", input)`  
gte-base| `embedding(model="xinference/gte-base", input)`  
gte-large| `embedding(model="xinference/gte-large", input)`  
jina-embeddings-v2-base-en| `embedding(model="xinference/jina-embeddings-v2-base-en", input)`  
jina-embeddings-v2-small-en| `embedding(model="xinference/jina-embeddings-v2-small-en", input)`  
multilingual-e5-large| `embedding(model="xinference/multilingual-e5-large", input)`  
Previous
Infinity
Next
Cloudflare Workers AI
  * API Base, Key
  * Sample Usage - Embedding
  * Sample Usage `api_base` param
  * Supported Models


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
      * Virtual Keys
      * OIDC - JWT-based Auth
      * [Beta] Service Accounts
      * Role-based Access Controls (RBAC)
      * Custom Auth
      * IP Address Filtering
      * Email Notifications
      * Attribute Management changes to Users
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Authentication
  * Role-based Access Controls (RBAC)


On this page
# Role-based Access Controls (RBAC)
Role-based access control (RBAC) is based on Organizations, Teams and Internal User Roles
  * `Organizations` are the top-level entities that contain Teams.
  * `Team` - A Team is a collection of multiple `Internal Users`
  * `Internal Users` - users that can create keys, make LLM API calls, view usage on LiteLLM 
  * `Roles` define the permissions of an `Internal User`
  * `Virtual Keys` - Keys are used for authentication to the LiteLLM API. Keys are tied to a `Internal User` and `Team`


## Roles​
Role Type| Role Name| Permissions  
---|---|---  
**Admin**| `proxy_admin`|  Admin over the platform  
| `proxy_admin_viewer`| Can login, view all keys, view all spend. **Cannot** create keys/delete keys/add new users  
**Organization**| `org_admin`|  Admin over the organization. Can create teams and users within their organization  
**Internal User**| `internal_user`|  Can login, view/create/delete their own keys, view their spend. **Cannot** add new users  
| `internal_user_viewer`| Can login, view their own keys, view their own spend. **Cannot** create/delete keys, add new users  
## Onboarding Organizations​
### 1. Creating a new Organization​
Any user with role=`proxy_admin` can create a new organization
**Usage**
**API Reference for /organization/new**
```
curl --location 'http://0.0.0.0:4000/organization/new' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "organization_alias": "marketing_department",  
    "models": ["gpt-4"],  
    "max_budget": 20  
  }'  

```

Expected Response 
```
{  
 "organization_id": "ad15e8ca-12ae-46f4-8659-d02debef1b23",  
 "organization_alias": "marketing_department",  
 "budget_id": "98754244-3a9c-4b31-b2e9-c63edc8fd7eb",  
 "metadata": {},  
 "models": [  
  "gpt-4"  
 ],  
 "created_by": "109010464461339474872",  
 "updated_by": "109010464461339474872",  
 "created_at": "2024-10-08T18:30:24.637000Z",  
 "updated_at": "2024-10-08T18:30:24.637000Z"  
}  

```

### 2. Adding an `org_admin` to an Organization​
Create a user (ishaan@berri.ai) as an `org_admin` for the `marketing_department` Organization (from step 1)
Users with the following roles can call `/organization/member_add`
  * `proxy_admin`
  * `org_admin` only within their own organization


```
curl -X POST 'http://0.0.0.0:4000/organization/member_add' \  
  -H 'Authorization: Bearer sk-1234' \  
  -H 'Content-Type: application/json' \  
  -d '{"organization_id": "ad15e8ca-12ae-46f4-8659-d02debef1b23", "member": {"role": "org_admin", "user_id": "ishaan@berri.ai"}}'  

```

Now a user with user_id = `ishaan@berri.ai` and role = `org_admin` has been created in the `marketing_department` Organization
Create a Virtual Key for user_id = `ishaan@berri.ai`. The User can then use the Virtual key for their Organization Admin Operations
```
curl --location 'http://0.0.0.0:4000/key/generate' \  
    --header 'Authorization: Bearer sk-1234' \  
    --header 'Content-Type: application/json' \  
    --data '{  
      "user_id": "ishaan@berri.ai"  
  }'  

```

Expected Response 
```
{  
 "models": [],  
 "user_id": "ishaan@berri.ai",  
 "key": "sk-7shH8TGMAofR4zQpAAo6kQ",  
 "key_name": "sk-...o6kQ",  
}  

```

### 3. `Organization Admin` - Create a Team​
The organization admin will use the virtual key created in step 2 to create a `Team` within the `marketing_department` Organization
```
curl --location 'http://0.0.0.0:4000/team/new' \  
  --header 'Authorization: Bearer sk-7shH8TGMAofR4zQpAAo6kQ' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "team_alias": "engineering_team",  
    "organization_id": "ad15e8ca-12ae-46f4-8659-d02debef1b23"  
  }'  

```

This will create the team `engineering_team` within the `marketing_department` Organization
Expected Response 
```
{  
 "team_alias": "engineering_team",  
 "team_id": "01044ee8-441b-45f4-be7d-c70e002722d8",  
 "organization_id": "ad15e8ca-12ae-46f4-8659-d02debef1b23",  
}  

```

### `Organization Admin` - Add an `Internal User`​
The organization admin will use the virtual key created in step 2 to add an Internal User to the `engineering_team` Team. 
  * We will assign role=`internal_user` so the user can create Virtual Keys for themselves
  * `team_id` is from step 3


```
curl -X POST 'http://0.0.0.0:4000/team/member_add' \  
  -H 'Authorization: Bearer sk-1234' \  
  -H 'Content-Type: application/json' \  
  -d '{"team_id": "01044ee8-441b-45f4-be7d-c70e002722d8", "member": {"role": "internal_user", "user_id": "krrish@berri.ai"}}'  
  

```

Previous
[Beta] Service Accounts
Next
Custom Auth
  * Roles
  * Onboarding Organizations
    * 1. Creating a new Organization
    * 2. Adding an `org_admin` to an Organization
    * 3. `Organization Admin` - Create a Team
    * `Organization Admin` - Add an `Internal User`


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
      * 💸 Spend Tracking
      * Custom LLM Pricing
      * Billing
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Spend Tracking
  * Billing


On this page
# Billing
Bill internal teams, external customers for their usage
**🚨 Requirements**
  * Setup Lago, for usage-based billing. We recommend following their Stripe tutorial


Steps:
  * Connect the proxy to Lago
  * Set the id you want to bill for (customers, internal users, teams)
  * Start! 


## Quick Start​
Bill internal teams for their usage
### 1. Connect proxy to Lago​
Set 'lago' as a callback on your proxy config.yaml
```
model_list:  
 - model_name: fake-openai-endpoint  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
litellm_settings:  
 callbacks: ["lago"] # 👈 KEY CHANGE  
  
general_settings:  
 master_key: sk-1234  

```

Add your Lago keys to the environment
```
export LAGO_API_BASE="http://localhost:3000" # self-host - https://docs.getlago.com/guide/self-hosted/docker#run-the-app  
export LAGO_API_KEY="3e29d607-de54-49aa-a019-ecf585729070" # Get key - https://docs.getlago.com/guide/self-hosted/docker#find-your-api-key  
export LAGO_API_EVENT_CODE="openai_tokens" # name of lago billing code  
export LAGO_API_CHARGE_BY="team_id" # 👈 Charges 'team_id' attached to proxy key  

```

Start proxy 
```
litellm --config /path/to/config.yaml  

```

### 2. Create Key for Internal Team​
```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data-raw '{"team_id": "my-unique-id"}' # 👈 Internal Team's ID  

```

Response Object:
```
{  
 "key": "sk-tXL0wt5-lOOVK9sfY2UacA",  
}  

```

### 3. Start billing!​
  * Curl
  * OpenAI Python SDK
  * Langchain


```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer sk-tXL0wt5-lOOVK9sfY2UacA' \ # 👈 Team's Key  
--data ' {  
   "model": "fake-openai-endpoint",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="sk-tXL0wt5-lOOVK9sfY2UacA", # 👈 Team's Key  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
import os   
  
os.environ["OPENAI_API_KEY"] = "sk-tXL0wt5-lOOVK9sfY2UacA" # 👈 Team's Key  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "gpt-3.5-turbo",  
  temperature=0.1,  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

**See Results on Lago**
## Advanced - Lago Logging object​
This is what LiteLLM will log to Lagos
```
{  
  "event": {  
   "transaction_id": "<generated_unique_id>",  
   "external_customer_id": <selected_id>, # either 'end_user_id', 'user_id', or 'team_id'. Default 'end_user_id'.   
   "code": os.getenv("LAGO_API_EVENT_CODE"),   
   "properties": {  
     "input_tokens": <number>,  
     "output_tokens": <number>,  
     "model": <string>,  
     "response_cost": <number>, # 👈 LITELLM CALCULATED RESPONSE COST - https://github.com/BerriAI/litellm/blob/d43f75150a65f91f60dc2c0c9462ce3ffc713c1f/litellm/utils.py#L1473  
   }  
  }  
}  

```

## Advanced - Bill Customers, Internal Users​
For:
  * Customers (id passed via 'user' param in /chat/completion call) = 'end_user_id'
  * Internal Users (id set when creating keys) = 'user_id' 
  * Teams (id set when creating keys) = 'team_id' 


  * Customer Billing
  * Internal User Billing


  1. Set 'LAGO_API_CHARGE_BY' to 'end_user_id'
```
export LAGO_API_CHARGE_BY="end_user_id"  

```

  2. Test it!
     * Curl
     * OpenAI Python SDK
     * Langchain
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
   "user": "my_customer_id" # 👈 whatever your customer id is  
  }  
'  

```

```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
], user="my_customer_id") # 👈 whatever your customer id is  
  
print(response)  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
import os   
  
os.environ["OPENAI_API_KEY"] = "anything"  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "gpt-3.5-turbo",  
  temperature=0.1,  
  extra_body={  
    "user": "my_customer_id" # 👈 whatever your customer id is  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```



  1. Set 'LAGO_API_CHARGE_BY' to 'user_id'


```
export LAGO_API_CHARGE_BY="user_id"  

```

  1. Create a key for that user 


```
curl 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data-raw '{"user_id": "my-unique-id"}' # 👈 Internal User's id  

```

Response Object:
```
{  
 "key": "sk-tXL0wt5-lOOVK9sfY2UacA",  
}  

```

  1. Make API Calls with that Key 


```
import openai  
client = openai.OpenAI(  
  api_key="sk-tXL0wt5-lOOVK9sfY2UacA", # 👈 Generated key  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
])  
  
print(response)  

```

Previous
Custom LLM Pricing
Next
💰 Budgets, Rate Limits
  * Quick Start
    * 1. Connect proxy to Lago
    * 2. Create Key for Internal Team
    * 3. Start billing!
  * Advanced - Lago Logging object
  * Advanced - Bill Customers, Internal Users


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# Page Not Found
We could not find what you were looking for.
Please contact the owner of the site that linked you to the original URL and let them know their link is broken.
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Caching


On this page
# Caching
note
For OpenAI/Anthropic Prompt Caching, go here
Cache LLM Responses. LiteLLM's caching system stores and reuses LLM responses to save costs and reduce latency. When you make the same request twice, the cached response is returned instead of calling the LLM API again.
### Supported Caches​
  * In Memory Cache
  * Redis Cache 
  * Qdrant Semantic Cache
  * Redis Semantic Cache
  * s3 Bucket Cache 


## Quick Start​
  * redis cache
  * Qdrant Semantic cache
  * s3 cache
  * redis semantic cache


Caching can be enabled by adding the `cache` key in the `config.yaml`
#### Step 1: Add `cache` to the config.yaml​
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
 - model_name: text-embedding-ada-002  
  litellm_params:  
   model: text-embedding-ada-002  
  
litellm_settings:  
 set_verbose: True  
 cache: True     # set cache responses to True, litellm defaults to using a redis cache  

```

#### [OPTIONAL] Step 1.5: Add redis namespaces, default ttl​
#### Namespace​
If you want to create some folder for your keys, you can set a namespace, like this:
```
litellm_settings:  
 cache: true   
 cache_params:    # set cache params for redis  
  type: redis  
  namespace: "litellm.caching.caching"  

```

and keys will be stored like:
```
litellm.caching.caching:<hash>  

```

#### Redis Cluster​
  * Set on config.yaml
  * Set on .env


```
model_list:  
 - model_name: "*"  
  litellm_params:  
   model: "*"  
  
  
litellm_settings:  
 cache: True  
 cache_params:  
  type: redis  
  redis_startup_nodes: [{"host": "127.0.0.1", "port": "7001"}]   

```

You can configure redis cluster in your .env by setting `REDIS_CLUSTER_NODES` in your .env
**Example`REDIS_CLUSTER_NODES`** value
```
REDIS_CLUSTER_NODES = "[{"host": "127.0.0.1", "port": "7001"}, {"host": "127.0.0.1", "port": "7003"}, {"host": "127.0.0.1", "port": "7004"}, {"host": "127.0.0.1", "port": "7005"}, {"host": "127.0.0.1", "port": "7006"}, {"host": "127.0.0.1", "port": "7007"}]"  

```

note
Example python script for setting redis cluster nodes in .env:
```
# List of startup nodes  
startup_nodes = [  
  {"host": "127.0.0.1", "port": "7001"},  
  {"host": "127.0.0.1", "port": "7003"},  
  {"host": "127.0.0.1", "port": "7004"},  
  {"host": "127.0.0.1", "port": "7005"},  
  {"host": "127.0.0.1", "port": "7006"},  
  {"host": "127.0.0.1", "port": "7007"},  
]  
  
# set startup nodes in environment variables  
os.environ["REDIS_CLUSTER_NODES"] = json.dumps(startup_nodes)  
print("REDIS_CLUSTER_NODES", os.environ["REDIS_CLUSTER_NODES"])  

```

#### Redis Sentinel​
  * Set on config.yaml
  * Set on .env


```
model_list:  
 - model_name: "*"  
  litellm_params:  
   model: "*"  
  
  
litellm_settings:  
 cache: true  
 cache_params:  
  type: "redis"  
  service_name: "mymaster"  
  sentinel_nodes: [["localhost", 26379]]  
  sentinel_password: "password" # [OPTIONAL]  

```

You can configure redis sentinel in your .env by setting `REDIS_SENTINEL_NODES` in your .env
**Example`REDIS_SENTINEL_NODES`** value
```
REDIS_SENTINEL_NODES='[["localhost", 26379]]'  
REDIS_SERVICE_NAME = "mymaster"  
REDIS_SENTINEL_PASSWORD = "password"  

```

note
Example python script for setting redis cluster nodes in .env:
```
# List of startup nodes  
sentinel_nodes = [["localhost", 26379]]  
  
# set startup nodes in environment variables  
os.environ["REDIS_SENTINEL_NODES"] = json.dumps(sentinel_nodes)  
print("REDIS_SENTINEL_NODES", os.environ["REDIS_SENTINEL_NODES"])  

```

#### TTL​
```
litellm_settings:  
 cache: true   
 cache_params:    # set cache params for redis  
  type: redis  
  ttl: 600 # will be cached on redis for 600s  
  # default_in_memory_ttl: Optional[float], default is None. time in seconds.   
  # default_in_redis_ttl: Optional[float], default is None. time in seconds.   

```

#### SSL​
just set `REDIS_SSL="True"` in your .env, and LiteLLM will pick this up. 
```
REDIS_SSL="True"  

```

For quick testing, you can also use REDIS_URL, eg.:
```
REDIS_URL="rediss://.."  

```

but we **don't** recommend using REDIS_URL in prod. We've noticed a performance difference between using it vs. redis_host, port, etc. 
#### Step 2: Add Redis Credentials to .env​
Set either `REDIS_URL` or the `REDIS_HOST` in your os environment, to enable caching.
```
REDIS_URL = ""    # REDIS_URL='redis://username:password@hostname:port/database'  
## OR ##   
REDIS_HOST = ""    # REDIS_HOST='redis-18841.c274.us-east-1-3.ec2.cloud.redislabs.com'  
REDIS_PORT = ""    # REDIS_PORT='18841'  
REDIS_PASSWORD = ""  # REDIS_PASSWORD='liteLlmIsAmazing'  

```

**Additional kwargs**  
You can pass in any additional redis.Redis arg, by storing the variable + value in your os environment, like this: 
```
REDIS_<redis-kwarg-name> = ""  

```

**See how it's read from the environment**
#### Step 3: Run proxy with config​
```
$ litellm --config /path/to/config.yaml  

```

Caching can be enabled by adding the `cache` key in the `config.yaml`
#### Step 1: Add `cache` to the config.yaml​
```
model_list:  
 - model_name: fake-openai-endpoint  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
 - model_name: openai-embedding  
  litellm_params:  
   model: openai/text-embedding-3-small  
   api_key: os.environ/OPENAI_API_KEY  
  
litellm_settings:  
 set_verbose: True  
 cache: True     # set cache responses to True, litellm defaults to using a redis cache  
 cache_params:  
  type: qdrant-semantic  
  qdrant_semantic_cache_embedding_model: openai-embedding # the model should be defined on the model_list  
  qdrant_collection_name: test_collection  
  qdrant_quantization_config: binary  
  similarity_threshold: 0.8  # similarity threshold for semantic cache  

```

#### Step 2: Add Qdrant Credentials to your .env​
```
QDRANT_API_KEY = "16rJUMBRx*************"  
QDRANT_API_BASE = "https://5392d382-45*********.cloud.qdrant.io"  

```

#### Step 3: Run proxy with config​
```
$ litellm --config /path/to/config.yaml  

```

#### Step 4. Test it​
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "fake-openai-endpoint",  
  "messages": [  
   {"role": "user", "content": "Hello"}  
  ]  
 }'  

```

**Expect to see`x-litellm-semantic-similarity` in the response headers when semantic caching is one**
#### Step 1: Add `cache` to the config.yaml​
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
 - model_name: text-embedding-ada-002  
  litellm_params:  
   model: text-embedding-ada-002  
  
litellm_settings:  
 set_verbose: True  
 cache: True     # set cache responses to True  
 cache_params:    # set cache params for s3  
  type: s3  
  s3_bucket_name: cache-bucket-litellm  # AWS Bucket Name for S3  
  s3_region_name: us-west-2       # AWS Region Name for S3  
  s3_aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID # us os.environ/<variable name> to pass environment variables. This is AWS Access Key ID for S3  
  s3_aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY # AWS Secret Access Key for S3  
  s3_endpoint_url: https://s3.amazonaws.com # [OPTIONAL] S3 endpoint URL, if you want to use Backblaze/cloudflare s3 buckets  

```

#### Step 2: Run proxy with config​
```
$ litellm --config /path/to/config.yaml  

```

Caching can be enabled by adding the `cache` key in the `config.yaml`
#### Step 1: Add `cache` to the config.yaml​
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
 - model_name: azure-embedding-model  
  litellm_params:  
   model: azure/azure-embedding-model  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: "2023-07-01-preview"  
  
litellm_settings:  
 set_verbose: True  
 cache: True     # set cache responses to True, litellm defaults to using a redis cache  
 cache_params:  
  type: "redis-semantic"   
  similarity_threshold: 0.8  # similarity threshold for semantic cache  
  redis_semantic_cache_embedding_model: azure-embedding-model # set this to a model_name set in model_list  

```

#### Step 2: Add Redis Credentials to .env​
Set either `REDIS_URL` or the `REDIS_HOST` in your os environment, to enable caching.
```
REDIS_URL = ""    # REDIS_URL='redis://username:password@hostname:port/database'  
## OR ##   
REDIS_HOST = ""    # REDIS_HOST='redis-18841.c274.us-east-1-3.ec2.cloud.redislabs.com'  
REDIS_PORT = ""    # REDIS_PORT='18841'  
REDIS_PASSWORD = ""  # REDIS_PASSWORD='liteLlmIsAmazing'  

```

**Additional kwargs**  
You can pass in any additional redis.Redis arg, by storing the variable + value in your os environment, like this: 
```
REDIS_<redis-kwarg-name> = ""  

```

#### Step 3: Run proxy with config​
```
$ litellm --config /path/to/config.yaml  

```

## Usage​
### Basic​
  * /chat/completions
  * /embeddings


Send the same request twice:
```
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -d '{  
   "model": "gpt-3.5-turbo",  
   "messages": [{"role": "user", "content": "write a poem about litellm!"}],  
   "temperature": 0.7  
  }'  
  
curl http://0.0.0.0:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -d '{  
   "model": "gpt-3.5-turbo",  
   "messages": [{"role": "user", "content": "write a poem about litellm!"}],  
   "temperature": 0.7  
  }'  

```

Send the same request twice:
```
curl --location 'http://0.0.0.0:4000/embeddings' \  
 --header 'Content-Type: application/json' \  
 --data ' {  
 "model": "text-embedding-ada-002",  
 "input": ["write a litellm poem"]  
 }'  
  
curl --location 'http://0.0.0.0:4000/embeddings' \  
 --header 'Content-Type: application/json' \  
 --data ' {  
 "model": "text-embedding-ada-002",  
 "input": ["write a litellm poem"]  
 }'  

```

### Dynamic Cache Controls​
Parameter| Type| Description  
---|---|---  
`ttl`|  _Optional(int)_|  Will cache the response for the user-defined amount of time (in seconds)  
`s-maxage`| _Optional(int)_|  Will only accept cached responses that are within user-defined range (in seconds)  
`no-cache`| _Optional(bool)_|  Will not store the response in cache.  
`no-store`| _Optional(bool)_|  Will not cache the response  
`namespace`|  _Optional(str)_|  Will cache the response under a user-defined namespace  
Each cache parameter can be controlled on a per-request basis. Here are examples for each parameter:
### `ttl`​
Set how long (in seconds) to cache a response.
  * OpenAI Python SDK
  * curl


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="your-api-key",  
  base_url="http://0.0.0.0:4000"  
)  
  
chat_completion = client.chat.completions.create(  
  messages=[{"role": "user", "content": "Hello"}],  
  model="gpt-3.5-turbo",  
  extra_body={  
    "cache": {  
      "ttl": 300 # Cache response for 5 minutes  
    }  
  }  
)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "cache": {"ttl": 300},  
  "messages": [  
   {"role": "user", "content": "Hello"}  
  ]  
 }'  

```

### `s-maxage`​
Only accept cached responses that are within the specified age (in seconds).
  * OpenAI Python SDK
  * curl


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="your-api-key",  
  base_url="http://0.0.0.0:4000"  
)  
  
chat_completion = client.chat.completions.create(  
  messages=[{"role": "user", "content": "Hello"}],  
  model="gpt-3.5-turbo",  
  extra_body={  
    "cache": {  
      "s-maxage": 600 # Only use cache if less than 10 minutes old  
    }  
  }  
)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "cache": {"s-maxage": 600},  
  "messages": [  
   {"role": "user", "content": "Hello"}  
  ]  
 }'  

```

### `no-cache`​
Force a fresh response, bypassing the cache.
  * OpenAI Python SDK
  * curl


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="your-api-key",  
  base_url="http://0.0.0.0:4000"  
)  
  
chat_completion = client.chat.completions.create(  
  messages=[{"role": "user", "content": "Hello"}],  
  model="gpt-3.5-turbo",  
  extra_body={  
    "cache": {  
      "no-cache": True # Skip cache check, get fresh response  
    }  
  }  
)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "cache": {"no-cache": true},  
  "messages": [  
   {"role": "user", "content": "Hello"}  
  ]  
 }'  

```

### `no-store`​
Will not store the response in cache.
  * OpenAI Python SDK
  * curl


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="your-api-key",  
  base_url="http://0.0.0.0:4000"  
)  
  
chat_completion = client.chat.completions.create(  
  messages=[{"role": "user", "content": "Hello"}],  
  model="gpt-3.5-turbo",  
  extra_body={  
    "cache": {  
      "no-store": True # Don't cache this response  
    }  
  }  
)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "cache": {"no-store": true},  
  "messages": [  
   {"role": "user", "content": "Hello"}  
  ]  
 }'  

```

### `namespace`​
Store the response under a specific cache namespace.
  * OpenAI Python SDK
  * curl


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="your-api-key",  
  base_url="http://0.0.0.0:4000"  
)  
  
chat_completion = client.chat.completions.create(  
  messages=[{"role": "user", "content": "Hello"}],  
  model="gpt-3.5-turbo",  
  extra_body={  
    "cache": {  
      "namespace": "my-custom-namespace" # Store in custom namespace  
    }  
  }  
)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "cache": {"namespace": "my-custom-namespace"},  
  "messages": [  
   {"role": "user", "content": "Hello"}  
  ]  
 }'  

```

## Set cache for proxy, but not on the actual llm api call​
Use this if you just want to enable features like rate limiting, and loadbalancing across multiple instances.
Set `supported_call_types: []` to disable caching on the actual api call. 
```
litellm_settings:  
 cache: True  
 cache_params:  
  type: redis  
  supported_call_types: []   

```

## Debugging Caching - `/cache/ping`​
LiteLLM Proxy exposes a `/cache/ping` endpoint to test if the cache is working as expected
**Usage**
```
curl --location 'http://0.0.0.0:4000/cache/ping' -H "Authorization: Bearer sk-1234"  

```

**Expected Response - when cache healthy**
```
{  
  "status": "healthy",  
  "cache_type": "redis",  
  "ping_response": true,  
  "set_cache_response": "success",  
  "litellm_cache_params": {  
    "supported_call_types": "['completion', 'acompletion', 'embedding', 'aembedding', 'atranscription', 'transcription']",  
    "type": "redis",  
    "namespace": "None"  
  },  
  "redis_cache_params": {  
    "redis_client": "Redis<ConnectionPool<Connection<host=redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com,port=16337,db=0>>>",  
    "redis_kwargs": "{'url': 'redis://:******@redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com:16337'}",  
    "async_redis_conn_pool": "BlockingConnectionPool<Connection<host=redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com,port=16337,db=0>>",  
    "redis_version": "7.2.0"  
  }  
}  

```

## Advanced​
### Control Call Types Caching is on for - (`/chat/completion`, `/embeddings`, etc.)​
By default, caching is on for all call types. You can control which call types caching is on for by setting `supported_call_types` in `cache_params`
**Cache will only be on for the call types specified in`supported_call_types`**
```
litellm_settings:  
 cache: True  
 cache_params:  
  type: redis  
  supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]  
             # /chat/completions, /completions, /embeddings, /audio/transcriptions  

```

### Set Cache Params on config.yaml​
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
 - model_name: text-embedding-ada-002  
  litellm_params:  
   model: text-embedding-ada-002  
  
litellm_settings:  
 set_verbose: True  
 cache: True     # set cache responses to True, litellm defaults to using a redis cache  
 cache_params:     # cache_params are optional  
  type: "redis" # The type of cache to initialize. Can be "local" or "redis". Defaults to "local".  
  host: "localhost" # The host address for the Redis cache. Required if type is "redis".  
  port: 6379 # The port number for the Redis cache. Required if type is "redis".  
  password: "your_password" # The password for the Redis cache. Required if type is "redis".  
    
  # Optional configurations  
  supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]  
           # /chat/completions, /completions, /embeddings, /audio/transcriptions  

```

### Deleting Cache Keys - `/cache/delete`​
In order to delete a cache key, send a request to `/cache/delete` with the `keys` you want to delete
Example 
```
curl -X POST "http://0.0.0.0:4000/cache/delete" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{"keys": ["586bf3f3c1bf5aecb55bd9996494d3bbc69eb58397163add6d49537762a7548d", "key2"]}'  

```

```
# {"status":"success"}  

```

#### Viewing Cache Keys from responses​
You can view the cache_key in the response headers, on cache hits the cache key is sent as the `x-litellm-cache-key` response headers
```
curl -i --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "user": "ishan",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what is litellm"  
    }  
  ],  
}'  

```

Response from litellm proxy 
```
date: Thu, 04 Apr 2024 17:37:21 GMT  
content-type: application/json  
x-litellm-cache-key: 586bf3f3c1bf5aecb55bd9996494d3bbc69eb58397163add6d49537762a7548d  
  
{  
  "id": "chatcmpl-9ALJTzsBlXR9zTxPvzfFFtFbFtG6T",  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": "I'm sorr.."  
        "role": "assistant"  
      }  
    }  
  ],  
  "created": 1712252235,  
}  
         

```

### **Set Caching Default Off - Opt in only**​
  1. **Set`mode: default_off` for caching**


```
model_list:  
 - model_name: fake-openai-endpoint  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
  
# default off mode  
litellm_settings:  
 set_verbose: True  
 cache: True  
 cache_params:  
  mode: default_off # 👈 Key change cache is default_off  

```

  1. **Opting in to cache when cache is default off**


  * OpenAI Python SDK
  * curl


```
import os  
from openai import OpenAI  
  
client = OpenAI(api_key=<litellm-api-key>, base_url="http://0.0.0.0:4000")  
  
chat_completion = client.chat.completions.create(  
  messages=[  
    {  
      "role": "user",  
      "content": "Say this is a test",  
    }  
  ],  
  model="gpt-3.5-turbo",  
  extra_body = {    # OpenAI python accepts extra args in extra_body  
    "cache": {"use-cache": True}  
  }  
)  

```

```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "cache": {"use-cache": True}  
  "messages": [  
   {"role": "user", "content": "Say this is a test"}  
  ]  
 }'  

```

### Turn on `batch_redis_requests`​
**What it does?** When a request is made:
  * Check if a key starting with `litellm:<hashed_api_key>:<call_type>:` exists in-memory, if no - get the last 100 cached requests for this key and store it
  * New requests are stored with this `litellm:..` as the namespace


**Why?** Reduce number of redis GET requests. This improved latency by 46% in prod load tests. 
**Usage**
```
litellm_settings:  
 cache: true  
 cache_params:  
  type: redis  
  ... # remaining redis args (host, port, etc.)  
 callbacks: ["batch_redis_requests"] # 👈 KEY CHANGE!  

```

**SEE CODE**
## Supported `cache_params` on proxy config.yaml​
```
cache_params:  
 # ttl   
 ttl: Optional[float]  
 default_in_memory_ttl: Optional[float]  
 default_in_redis_ttl: Optional[float]  
  
 # Type of cache (options: "local", "redis", "s3")  
 type: s3  
  
 # List of litellm call types to cache for  
 # Options: "completion", "acompletion", "embedding", "aembedding"  
 supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]  
           # /chat/completions, /completions, /embeddings, /audio/transcriptions  
  
 # Redis cache parameters  
 host: localhost # Redis server hostname or IP address  
 port: "6379" # Redis server port (as a string)  
 password: secret_password # Redis server password  
 namespace: Optional[str] = None,  
   
  
 # S3 cache parameters  
 s3_bucket_name: your_s3_bucket_name # Name of the S3 bucket  
 s3_region_name: us-west-2 # AWS region of the S3 bucket  
 s3_api_version: 2006-03-01 # AWS S3 API version  
 s3_use_ssl: true # Use SSL for S3 connections (options: true, false)  
 s3_verify: true # SSL certificate verification for S3 connections (options: true, false)  
 s3_endpoint_url: https://s3.amazonaws.com # S3 endpoint URL  
 s3_aws_access_key_id: your_access_key # AWS Access Key ID for S3  
 s3_aws_secret_access_key: your_secret_key # AWS Secret Access Key for S3  
 s3_aws_session_token: your_session_token # AWS Session Token for temporary credentials  
  

```

## Advanced - user api key cache ttl​
Configure how long the in-memory cache stores the key object (prevents db requests)
```
general_settings:  
 user_api_key_cache_ttl: <your-number> #time in seconds  

```

By default this value is set to 60s.
Previous
Post-Call Rules
Next
Supported Models & Providers
  * Supported Caches
  * Quick Start
  * Usage
    * Basic
    * Dynamic Cache Controls
    * `ttl`
    * `s-maxage`
    * `no-cache`
    * `no-store`
    * `namespace`
  * Set cache for proxy, but not on the actual llm api call
  * Debugging Caching - `/cache/ping`
  * Advanced
    * Control Call Types Caching is on for - (`/chat/completion`, `/embeddings`, etc.)
    * Set Cache Params on config.yaml
    * Deleting Cache Keys - `/cache/delete`
    * **Set Caching Default Off - Opt in only**
    * Turn on `batch_redis_requests`
  * Supported `cache_params` on proxy config.yaml
  * Advanced - user api key cache ttl


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
      * Modify / Reject Incoming Requests
      * Post-Call Rules
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Create Custom Plugins
  * Modify / Reject Incoming Requests


On this page
# Modify / Reject Incoming Requests
  * Modify data before making llm api calls on proxy
  * Reject data before making llm api calls / before returning the response 
  * Enforce 'user' param for all openai endpoint calls


See a complete example with our parallel request rate limiter
## Quick Start​
  1. In your Custom Handler add a new `async_pre_call_hook` function


This function is called just before a litellm completion call is made, and allows you to modify the data going into the litellm call **See Code**
```
from litellm.integrations.custom_logger import CustomLogger  
import litellm  
from litellm.proxy.proxy_server import UserAPIKeyAuth, DualCache  
from typing import Optional, Literal  
  
# This file includes the custom callbacks for LiteLLM Proxy  
# Once defined, these can be passed in proxy_config.yaml  
class MyCustomHandler(CustomLogger): # https://docs.litellm.ai/docs/observability/custom_callback#callback-class  
  # Class variables or attributes  
  def __init__(self):  
    pass  
  
  #### CALL HOOKS - proxy only ####   
  
  async def async_pre_call_hook(self, user_api_key_dict: UserAPIKeyAuth, cache: DualCache, data: dict, call_type: Literal[  
      "completion",  
      "text_completion",  
      "embeddings",  
      "image_generation",  
      "moderation",  
      "audio_transcription",  
    ]):   
    data["model"] = "my-new-model"  
    return data   
  
  async def async_post_call_failure_hook(  
    self,   
    request_data: dict,  
    original_exception: Exception,   
    user_api_key_dict: UserAPIKeyAuth  
  ):  
    pass  
  
  async def async_post_call_success_hook(  
    self,  
    data: dict,  
    user_api_key_dict: UserAPIKeyAuth,  
    response,  
  ):  
    pass  
  
  async def async_moderation_hook( # call made in parallel to llm api call  
    self,  
    data: dict,  
    user_api_key_dict: UserAPIKeyAuth,  
    call_type: Literal["completion", "embeddings", "image_generation", "moderation", "audio_transcription"],  
  ):  
    pass  
  
  async def async_post_call_streaming_hook(  
    self,  
    user_api_key_dict: UserAPIKeyAuth,  
    response: str,  
  ):  
    pass  
  
  aasync def async_post_call_streaming_iterator_hook(  
    self,  
    user_api_key_dict: UserAPIKeyAuth,  
    response: Any,  
    request_data: dict,  
  ) -> AsyncGenerator[ModelResponseStream, None]:  
    """  
    Passes the entire stream to the guardrail  
  
    This is useful for plugins that need to see the entire stream.  
    """  
    async for item in response:  
      yield item  
  
proxy_handler_instance = MyCustomHandler()  

```

  1. Add this file to your proxy config


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
  
litellm_settings:  
 callbacks: custom_callbacks.proxy_handler_instance # sets litellm.callbacks = [proxy_handler_instance]  

```

  1. Start the server + test the request


```
$ litellm /path/to/config.yaml  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "good morning good sir"  
    }  
  ],  
  "user": "ishaan-app",  
  "temperature": 0.2  
  }'  

```

## [BETA] _NEW_ async_moderation_hook​
Run a moderation check in parallel to the actual LLM API call. 
In your Custom Handler add a new `async_moderation_hook` function
  * This is currently only supported for `/chat/completion` calls. 
  * This function runs in parallel to the actual LLM API call. 
  * If your `async_moderation_hook` raises an Exception, we will return that to the user. 


info
We might need to update the function schema in the future, to support multiple endpoints (e.g. accept a call_type). Please keep that in mind, while trying this feature
See a complete example with our Llama Guard content moderation hook
```
from litellm.integrations.custom_logger import CustomLogger  
import litellm  
from fastapi import HTTPException  
  
# This file includes the custom callbacks for LiteLLM Proxy  
# Once defined, these can be passed in proxy_config.yaml  
class MyCustomHandler(CustomLogger): # https://docs.litellm.ai/docs/observability/custom_callback#callback-class  
  # Class variables or attributes  
  def __init__(self):  
    pass  
  
  #### ASYNC ####   
    
  async def async_log_pre_api_call(self, model, messages, kwargs):  
    pass  
  
  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):  
    pass  
  
  async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):  
    pass  
  
  #### CALL HOOKS - proxy only ####   
  
  async def async_pre_call_hook(self, user_api_key_dict: UserAPIKeyAuth, cache: DualCache, data: dict, call_type: Literal["completion", "embeddings"]):  
    data["model"] = "my-new-model"  
    return data   
    
  async def async_moderation_hook( ### 👈 KEY CHANGE ###  
    self,  
    data: dict,  
  ):  
    messages = data["messages"]  
    print(messages)  
    if messages[0]["content"] == "hello world":   
      raise HTTPException(  
          status_code=400, detail={"error": "Violated content safety policy"}  
        )  
  
proxy_handler_instance = MyCustomHandler()  

```

  1. Add this file to your proxy config


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
  
litellm_settings:  
 callbacks: custom_callbacks.proxy_handler_instance # sets litellm.callbacks = [proxy_handler_instance]  

```

  1. Start the server + test the request


```
$ litellm /path/to/config.yaml  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "Hello world"  
    }  
  ],  
  }'  

```

## Advanced - Enforce 'user' param​
Set `enforce_user_param` to true, to require all calls to the openai endpoints to have the 'user' param. 
**See Code**
```
general_settings:  
 enforce_user_param: True  

```

**Result**
## Advanced - Return rejected message as response​
For chat completions and text completion calls, you can return a rejected message as a user response. 
Do this by returning a string. LiteLLM takes care of returning the response in the correct format depending on the endpoint and if it's streaming/non-streaming.
For non-chat/text completion endpoints, this response is returned as a 400 status code exception. 
### 1. Create Custom Handler​
```
from litellm.integrations.custom_logger import CustomLogger  
import litellm  
from litellm.utils import get_formatted_prompt  
  
# This file includes the custom callbacks for LiteLLM Proxy  
# Once defined, these can be passed in proxy_config.yaml  
class MyCustomHandler(CustomLogger):  
  def __init__(self):  
    pass  
  
  #### CALL HOOKS - proxy only ####   
  
  async def async_pre_call_hook(self, user_api_key_dict: UserAPIKeyAuth, cache: DualCache, data: dict, call_type: Literal[  
      "completion",  
      "text_completion",  
      "embeddings",  
      "image_generation",  
      "moderation",  
      "audio_transcription",  
    ]) -> Optional[dict, str, Exception]:   
    formatted_prompt = get_formatted_prompt(data=data, call_type=call_type)  
  
    if "Hello world" in formatted_prompt:  
      return "This is an invalid response"  
  
    return data   
  
proxy_handler_instance = MyCustomHandler()  

```

### 2. Update config.yaml​
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
  
litellm_settings:  
 callbacks: custom_callbacks.proxy_handler_instance # sets litellm.callbacks = [proxy_handler_instance]  

```

### 3. Test it!​
```
$ litellm /path/to/config.yaml  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --data ' {  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "Hello world"  
    }  
  ],  
  }'  

```

**Expected Response**
```
{  
  "id": "chatcmpl-d00bbede-2d90-4618-bf7b-11a1c23cf360",  
  "choices": [  
    {  
      "finish_reason": "stop",  
      "index": 0,  
      "message": {  
        "content": "This is an invalid response.", # 👈 REJECTED RESPONSE  
        "role": "assistant"  
      }  
    }  
  ],  
  "created": 1716234198,  
  "model": null,  
  "object": "chat.completion",  
  "system_fingerprint": null,  
  "usage": {}  
}  

```

Previous
[BETA] OpenID Connect (OIDC)
Next
Post-Call Rules
  * Quick Start
  * BETA _NEW_ async_moderation_hook
  * Advanced - Enforce 'user' param
  * Advanced - Return rejected message as response
    * 1. Create Custom Handler
    * 2. Update config.yaml
    * 3. Test it!


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
      * Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples
      * Clientside LLM Credentials
      * Request Headers
      * Response Headers
      * Model Discovery
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Making LLM Requests
  * Clientside LLM Credentials


On this page
# Clientside LLM Credentials
### Pass User LLM API Keys, Fallbacks​
Allow your end-users to pass their model list, api base, OpenAI API key (any LiteLLM supported provider) to make requests 
**Note** This is not related to virtual keys. This is for when you want to pass in your users actual LLM API keys. 
info
**You can pass a litellm.RouterConfig as`user_config` , See all supported params here https://github.com/BerriAI/litellm/blob/main/litellm/types/router.py **
#### Step 1: Define user model list & config​
```
import os  
  
user_config = {  
  'model_list': [  
    {  
      'model_name': 'user-azure-instance',  
      'litellm_params': {  
        'model': 'azure/chatgpt-v-2',  
        'api_key': os.getenv('AZURE_API_KEY'),  
        'api_version': os.getenv('AZURE_API_VERSION'),  
        'api_base': os.getenv('AZURE_API_BASE'),  
        'timeout': 10,  
      },  
      'tpm': 240000,  
      'rpm': 1800,  
    },  
    {  
      'model_name': 'user-openai-instance',  
      'litellm_params': {  
        'model': 'gpt-3.5-turbo',  
        'api_key': os.getenv('OPENAI_API_KEY'),  
        'timeout': 10,  
      },  
      'tpm': 240000,  
      'rpm': 1800,  
    },  
  ],  
  'num_retries': 2,  
  'allowed_fails': 3,  
  'fallbacks': [  
    {  
      'user-azure-instance': ['user-openai-instance']  
    }  
  ]  
}  
  
  

```

#### Step 2: Send user_config in `extra_body`​
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
# send request to `user-azure-instance`  
response = client.chat.completions.create(model="user-azure-instance", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],   
  extra_body={  
   "user_config": user_config  
  }  
) # 👈 User config  
  
print(response)  

```

#### Step 1: Define user model list & config​
```
const os = require('os');  
  
const userConfig = {  
  model_list: [  
    {  
      model_name: 'user-azure-instance',  
      litellm_params: {  
        model: 'azure/chatgpt-v-2',  
        api_key: process.env.AZURE_API_KEY,  
        api_version: process.env.AZURE_API_VERSION,  
        api_base: process.env.AZURE_API_BASE,  
        timeout: 10,  
      },  
      tpm: 240000,  
      rpm: 1800,  
    },  
    {  
      model_name: 'user-openai-instance',  
      litellm_params: {  
        model: 'gpt-3.5-turbo',  
        api_key: process.env.OPENAI_API_KEY,  
        timeout: 10,  
      },  
      tpm: 240000,  
      rpm: 1800,  
    },  
  ],  
  num_retries: 2,  
  allowed_fails: 3,  
  fallbacks: [  
    {  
      'user-azure-instance': ['user-openai-instance']  
    }  
  ]  
};  

```

#### Step 2: Send `user_config` as a param to `openai.chat.completions.create`​
```
const { OpenAI } = require('openai');  
  
const openai = new OpenAI({  
 apiKey: "sk-1234",  
 baseURL: "http://0.0.0.0:4000"  
});  
  
async function main() {  
 const chatCompletion = await openai.chat.completions.create({  
  messages: [{ role: 'user', content: 'Say this is a test' }],  
  model: 'gpt-3.5-turbo',  
  user_config: userConfig // # 👈 User config  
 });  
}  
  
main();  

```

### Pass User LLM API Keys / API Base​
Allows your users to pass in their OpenAI API key/API base (any LiteLLM supported provider) to make requests 
Here's how to do it: 
#### 1. Enable configurable clientside auth credentials for a provider​
```
model_list:  
 - model_name: "fireworks_ai/*"  
  litellm_params:  
   model: "fireworks_ai/*"  
   configurable_clientside_auth_params: ["api_base"]  
   # OR   
   configurable_clientside_auth_params: [{"api_base": "^https://litellm.*direct\.fireworks\.ai/v1$"}] # 👈 regex  

```

Specify any/all auth params you want the user to be able to configure:
  * api_base (✅ regex supported)
  * api_key
  * base_url 


(check provider docs for provider-specific auth params - e.g. `vertex_project`)
#### 2. Test it!​
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],   
  extra_body={"api_key": "my-bad-key", "api_base": "https://litellm-dev.direct.fireworks.ai/v1"}) # 👈 clientside credentials  
  
print(response)  

```

More examples: 
Pass in the litellm_params (E.g. api_key, api_base, etc.) via the `extra_body` parameter in the OpenAI client. 
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [  
  {  
    "role": "user",  
    "content": "this is a test request, write a short poem"  
  }  
],   
  extra_body={  
   "api_key": "my-azure-key",  
   "api_base": "my-azure-base",  
   "api_version": "my-azure-version"  
  }) # 👈 User Key  
  
print(response)  

```

For JS, the OpenAI client accepts passing params in the `create(..)` body as normal.
```
const { OpenAI } = require('openai');  
  
const openai = new OpenAI({  
 apiKey: "sk-1234",  
 baseURL: "http://0.0.0.0:4000"  
});  
  
async function main() {  
 const chatCompletion = await openai.chat.completions.create({  
  messages: [{ role: 'user', content: 'Say this is a test' }],  
  model: 'gpt-3.5-turbo',  
  api_key: "my-bad-key" // 👈 User Key  
 });  
}  
  
main();  

```

### Pass provider-specific params (e.g. Region, Project ID, etc.)​
Specify the region, project id, etc. to use for making requests to Vertex AI on the clientside.
Any value passed in the Proxy's request body, will be checked by LiteLLM against the mapped openai / litellm auth params. 
Unmapped params, will be assumed to be provider-specific params, and will be passed through to the provider in the LLM API's request body.
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={ # pass any additional litellm_params here  
    vertex_ai_location: "us-east1"   
  }  
)  
  
print(response)  

```

Previous
Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples
Next
Request Headers
  * Pass User LLM API Keys, Fallbacks
  * Pass User LLM API Keys / API Base
  * Pass provider-specific params (e.g. Region, Project ID, etc.)


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
      * Overview
      * File Management
      * All settings
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Config.yaml
  * Overview


On this page
# Overview
Set model list, `api_base`, `api_key`, `temperature` & proxy server settings (`master-key`) on the config.yaml. 
Param Name| Description  
---|---  
`model_list`| List of supported models on the server, with model-specific configs  
`router_settings`| litellm Router settings, example `routing_strategy="least-busy"` **see all**  
`litellm_settings`|  litellm Module settings, example `litellm.drop_params=True`, `litellm.set_verbose=True`, `litellm.api_base`, `litellm.cache` **see all**  
`general_settings`|  Server settings, example setting `master_key: sk-my_special_key`  
`environment_variables`| Environment Variables example, `REDIS_HOST`, `REDIS_PORT`  
**Complete List:** Check the Swagger UI docs on `<your-proxy-url>/#/config.yaml` (e.g. http://0.0.0.0:4000/#/config.yaml), for everything you can pass in the config.yaml.
## Quick Start​
Set a model alias for your deployments. 
In the `config.yaml` the model_name parameter is the user-facing name to use for your deployment. 
In the config below:
  * `model_name`: the name to pass TO litellm from the external client 
  * `litellm_params.model`: the model string passed to the litellm.completion() function


E.g.: 
  * `model=vllm-models` will route to `openai/facebook/opt-125m`. 
  * `model=gpt-3.5-turbo` will load balance between `azure/gpt-turbo-small-eu` and `azure/gpt-turbo-small-ca`


```
model_list:  
 - model_name: gpt-3.5-turbo ### RECEIVED MODEL NAME ###  
  litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input  
   model: azure/gpt-turbo-small-eu ### MODEL NAME sent to `litellm.completion()` ###  
   api_base: https://my-endpoint-europe-berri-992.openai.azure.com/  
   api_key: "os.environ/AZURE_API_KEY_EU" # does os.getenv("AZURE_API_KEY_EU")  
   rpm: 6   # [OPTIONAL] Rate limit for this deployment: in requests per minute (rpm)  
 - model_name: bedrock-claude-v1   
  litellm_params:  
   model: bedrock/anthropic.claude-instant-v1  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/gpt-turbo-small-ca  
   api_base: https://my-endpoint-canada-berri992.openai.azure.com/  
   api_key: "os.environ/AZURE_API_KEY_CA"  
   rpm: 6  
 - model_name: anthropic-claude  
  litellm_params:   
   model: bedrock/anthropic.claude-instant-v1  
   ### [OPTIONAL] SET AWS REGION ###  
   aws_region_name: us-east-1  
 - model_name: vllm-models  
  litellm_params:  
   model: openai/facebook/opt-125m # the `openai/` prefix tells litellm it's openai compatible  
   api_base: http://0.0.0.0:4000/v1  
   api_key: none  
   rpm: 1440  
  model_info:   
   version: 2  
   
 # Use this if you want to make requests to `claude-3-haiku-20240307`,`claude-3-opus-20240229`,`claude-2.1` without defining them on the config.yaml  
 # Default models  
 # Works for ALL Providers and needs the default provider credentials in .env  
 - model_name: "*"   
  litellm_params:  
   model: "*"  
  
litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py  
 drop_params: True  
 success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env  
  
general_settings:   
 master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)  
 alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env  

```

info
For more provider-specific info, go here
#### Step 2: Start Proxy with config​
```
$ litellm --config /path/to/config.yaml  

```

tip
Run with `--detailed_debug` if you need detailed debug logs 
```
$ litellm --config /path/to/config.yaml --detailed_debug  

```

#### Step 3: Test it​
Sends request to model where `model_name=gpt-3.5-turbo` on config.yaml. 
If multiple with `model_name=gpt-3.5-turbo` does Load Balancing
**Langchain, OpenAI SDK Usage Examples**
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
  }  
'  

```

## LLM configs `model_list`​
### Model-specific params (API Base, Keys, Temperature, Max Tokens, Organization, Headers etc.)​
You can use the config to save model-specific information like api_base, api_key, temperature, max_tokens, etc. 
**All input params**
**Step 1** : Create a `config.yaml` file
```
model_list:  
 - model_name: gpt-4-team1  
  litellm_params: # params for litellm.completion() - https://docs.litellm.ai/docs/completion/input#input---request-body  
   model: azure/chatgpt-v-2  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
   api_version: "2023-05-15"  
   azure_ad_token: eyJ0eXAiOiJ  
   seed: 12  
   max_tokens: 20  
 - model_name: gpt-4-team2  
  litellm_params:  
   model: azure/gpt-4  
   api_key: sk-123  
   api_base: https://openai-gpt-4-test-v-2.openai.azure.com/  
   temperature: 0.2  
 - model_name: openai-gpt-3.5  
  litellm_params:  
   model: openai/gpt-3.5-turbo  
   extra_headers: {"AI-Resource Group": "ishaan-resource"}  
   api_key: sk-123  
   organization: org-ikDc4ex8NB  
   temperature: 0.2  
 - model_name: mistral-7b  
  litellm_params:  
   model: ollama/mistral  
   api_base: your_ollama_api_base  

```

**Step 2** : Start server with config
```
$ litellm --config /path/to/config.yaml  

```

**Expected Logs:**
Look for this line in your console logs to confirm the config.yaml was loaded in correctly.
```
LiteLLM: Proxy initialized with Config, Set models:  

```

### Embedding Models - Use Sagemaker, Bedrock, Azure, OpenAI, XInference​
See supported Embedding Providers & Models here
  * Bedrock Completion/Chat
  * Sagemaker, Bedrock Embeddings
  * Hugging Face Embeddings
  * Azure OpenAI Embeddings
  * OpenAI Embeddings
  * XInference
  * OpenAI Compatible Embeddings


```
model_list:  
 - model_name: bedrock-cohere  
  litellm_params:  
   model: "bedrock/cohere.command-text-v14"  
   aws_region_name: "us-west-2"  
 - model_name: bedrock-cohere  
  litellm_params:  
   model: "bedrock/cohere.command-text-v14"  
   aws_region_name: "us-east-2"  
 - model_name: bedrock-cohere  
  litellm_params:  
   model: "bedrock/cohere.command-text-v14"  
   aws_region_name: "us-east-1"  
  

```

Here's how to route between GPT-J embedding (sagemaker endpoint), Amazon Titan embedding (Bedrock) and Azure OpenAI embedding on the proxy server: 
```
model_list:  
 - model_name: sagemaker-embeddings  
  litellm_params:   
   model: "sagemaker/berri-benchmarking-gpt-j-6b-fp16"  
 - model_name: amazon-embeddings  
  litellm_params:  
   model: "bedrock/amazon.titan-embed-text-v1"  
 - model_name: azure-embeddings  
  litellm_params:   
   model: "azure/azure-embedding-model"  
   api_base: "os.environ/AZURE_API_BASE" # os.getenv("AZURE_API_BASE")  
   api_key: "os.environ/AZURE_API_KEY" # os.getenv("AZURE_API_KEY")  
   api_version: "2023-07-01-preview"  
  
general_settings:  
 master_key: sk-1234 # [OPTIONAL] if set all calls to proxy will require either this key or a valid generated token  

```

LiteLLM Proxy supports all Feature-Extraction Embedding models.
```
model_list:  
 - model_name: deployed-codebert-base  
  litellm_params:   
   # send request to deployed hugging face inference endpoint  
   model: huggingface/microsoft/codebert-base # add huggingface prefix so it routes to hugging face  
   api_key: hf_LdS              # api key for hugging face inference endpoint  
   api_base: https://uysneno1wv2wd4lw.us-east-1.aws.endpoints.huggingface.cloud # your hf inference endpoint   
 - model_name: codebert-base  
  litellm_params:   
   # no api_base set, sends request to hugging face free inference api https://api-inference.huggingface.co/models/  
   model: huggingface/microsoft/codebert-base # add huggingface prefix so it routes to hugging face  
   api_key: hf_LdS              # api key for hugging face             
  

```

```
model_list:  
 - model_name: azure-embedding-model # model group  
  litellm_params:  
   model: azure/azure-embedding-model # model name for litellm.embedding(model=azure/azure-embedding-model) call  
   api_base: your-azure-api-base  
   api_key: your-api-key  
   api_version: 2023-07-01-preview  

```

```
model_list:  
- model_name: text-embedding-ada-002 # model group  
 litellm_params:  
  model: text-embedding-ada-002 # model name for litellm.embedding(model=text-embedding-ada-002)   
  api_key: your-api-key-1  
- model_name: text-embedding-ada-002   
 litellm_params:  
  model: text-embedding-ada-002  
  api_key: your-api-key-2  

```

https://docs.litellm.ai/docs/providers/xinference
**Note add`xinference/` prefix to `litellm_params`: `model` so litellm knows to route to OpenAI**
```
model_list:  
- model_name: embedding-model # model group  
 litellm_params:  
  model: xinference/bge-base-en  # model name for litellm.embedding(model=xinference/bge-base-en)   
  api_base: http://0.0.0.0:9997/v1  

```

Use this for calling /embedding endpoints on OpenAI Compatible Servers.
**Note add`openai/` prefix to `litellm_params`: `model` so litellm knows to route to OpenAI**
```
model_list:  
- model_name: text-embedding-ada-002 # model group  
 litellm_params:  
  model: openai/<your-model-name>  # model name for litellm.embedding(model=text-embedding-ada-002)   
  api_base: <model-api-base>  

```

#### Start Proxy​
```
litellm --config config.yaml  

```

#### Make Request​
Sends Request to `bedrock-cohere`
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
 --header 'Content-Type: application/json' \  
 --data ' {  
 "model": "bedrock-cohere",  
 "messages": [  
   {  
   "role": "user",  
   "content": "gm"  
   }  
 ]  
}'  

```

### Multiple OpenAI Organizations​
Add all openai models across all OpenAI organizations with just 1 model definition 
```
 - model_name: *  
  litellm_params:  
   model: openai/*  
   api_key: os.environ/OPENAI_API_KEY  
   organization:  
    - org-1   
    - org-2   
    - org-3  

```

LiteLLM will automatically create separate deployments for each org.
Confirm this via 
```
curl --location 'http://0.0.0.0:4000/v1/model/info' \  
--header 'Authorization: Bearer ${LITELLM_KEY}' \  
--data ''  

```

### Load Balancing​
info
For more on this, go to this page
Use this to call multiple instances of the same model and configure things like routing strategy.
For optimal performance:
  * Set `tpm/rpm` per model deployment. Weighted picks are then based on the established tpm/rpm.
  * Select your optimal routing strategy in `router_settings:routing_strategy`.


LiteLLM supports
```
["simple-shuffle", "least-busy", "usage-based-routing","latency-based-routing"], default="simple-shuffle"`  

```

When `tpm/rpm` is set + `routing_strategy==simple-shuffle` litellm will use a weighted pick based on set tpm/rpm. **In our load tests setting tpm/rpm for all deployments +`routing_strategy==simple-shuffle` maximized throughput**
  * When using multiple LiteLLM Servers / Kubernetes set redis settings `router_settings:redis_host` etc


```
model_list:  
 - model_name: zephyr-beta  
  litellm_params:  
    model: huggingface/HuggingFaceH4/zephyr-7b-beta  
    api_base: http://0.0.0.0:8001  
    rpm: 60   # Optional[int]: When rpm/tpm set - litellm uses weighted pick for load balancing. rpm = Rate limit for this deployment: in requests per minute (rpm).  
    tpm: 1000  # Optional[int]: tpm = Tokens Per Minute   
 - model_name: zephyr-beta  
  litellm_params:  
    model: huggingface/HuggingFaceH4/zephyr-7b-beta  
    api_base: http://0.0.0.0:8002  
    rpm: 600     
 - model_name: zephyr-beta  
  litellm_params:  
    model: huggingface/HuggingFaceH4/zephyr-7b-beta  
    api_base: http://0.0.0.0:8003  
    rpm: 60000     
 - model_name: gpt-3.5-turbo  
  litellm_params:  
    model: gpt-3.5-turbo  
    api_key: <my-openai-key>  
    rpm: 200     
 - model_name: gpt-3.5-turbo-16k  
  litellm_params:  
    model: gpt-3.5-turbo-16k  
    api_key: <my-openai-key>  
    rpm: 100     
  
litellm_settings:  
 num_retries: 3 # retry call 3 times on each model_name (e.g. zephyr-beta)  
 request_timeout: 10 # raise Timeout error if call takes longer than 10s. Sets litellm.request_timeout   
 fallbacks: [{"zephyr-beta": ["gpt-3.5-turbo"]}] # fallback to gpt-3.5-turbo if call fails num_retries   
 context_window_fallbacks: [{"zephyr-beta": ["gpt-3.5-turbo-16k"]}, {"gpt-3.5-turbo": ["gpt-3.5-turbo-16k"]}] # fallback to gpt-3.5-turbo-16k if context window error  
 allowed_fails: 3 # cooldown model if it fails > 1 call in a minute.   
  
router_settings: # router_settings are optional  
 routing_strategy: simple-shuffle # Literal["simple-shuffle", "least-busy", "usage-based-routing","latency-based-routing"], default="simple-shuffle"  
 model_group_alias: {"gpt-4": "gpt-3.5-turbo"} # all requests with `gpt-4` will be routed to models with `gpt-3.5-turbo`  
 num_retries: 2  
 timeout: 30                 # 30 seconds  
 redis_host: <your redis host>        # set this when using multiple litellm proxy deployments, load balancing state stored in redis  
 redis_password: <your redis password>  
 redis_port: 1992  

```

You can view your cost once you set up Virtual keys or custom_callbacks
### Load API Keys / config values from Environment​
If you have secrets saved in your environment, and don't want to expose them in the config.yaml, here's how to load model-specific keys from the environment. **This works for ANY value on the config.yaml**
```
os.environ/<YOUR-ENV-VAR> # runs os.getenv("YOUR-ENV-VAR")  

```

```
model_list:  
 - model_name: gpt-4-team1  
  litellm_params: # params for litellm.completion() - https://docs.litellm.ai/docs/completion/input#input---request-body  
   model: azure/chatgpt-v-2  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
   api_version: "2023-05-15"  
   api_key: os.environ/AZURE_NORTH_AMERICA_API_KEY # 👈 KEY CHANGE  

```

**See Code**
s/o to @David Manouchehri for helping with this. 
### Centralized Credential Management​
Define credentials once and reuse them across multiple models. This helps with:
  * Secret rotation
  * Reducing config duplication


```
model_list:  
 - model_name: gpt-4o  
  litellm_params:  
   model: azure/gpt-4o  
   litellm_credential_name: default_azure_credential # Reference credential below  
  
credential_list:  
 - credential_name: default_azure_credential  
  credential_values:  
   api_key: os.environ/AZURE_API_KEY # Load from environment  
   api_base: os.environ/AZURE_API_BASE  
   api_version: "2023-05-15"  
  credential_info:  
   description: "Production credentials for EU region"  

```

#### Key Parameters​
  * `credential_name`: Unique identifier for the credential set
  * `credential_values`: Key-value pairs of credentials/secrets (supports `os.environ/` syntax)
  * `credential_info`: Key-value pairs of user provided credentials information. No key-value pairs are required, but the dictionary must exist.


### Load API Keys from Secret Managers (Azure Vault, etc)​
**Using Secret Managers with LiteLLM Proxy**
### Set Supported Environments for a model - `production`, `staging`, `development`​
Use this if you want to control which model is exposed on a specific litellm environment
Supported Environments:
  * `production`
  * `staging`
  * `development`


  1. Set `LITELLM_ENVIRONMENT="<environment>"` in your environment. Can be one of `production`, `staging` or `development`


  1. For each model set the list of supported environments in `model_info.supported_environments`


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: openai/gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   supported_environments: ["development", "production", "staging"]  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/gpt-4  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   supported_environments: ["production", "staging"]  
 - model_name: gpt-4o  
  litellm_params:  
   model: openai/gpt-4o  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   supported_environments: ["production"]  

```

### Set Custom Prompt Templates​
LiteLLM by default checks if a model has a prompt template and applies it (e.g. if a huggingface model has a saved chat template in it's tokenizer_config.json). However, you can also set a custom prompt template on your proxy in the `config.yaml`: 
**Step 1** : Save your prompt template in a `config.yaml`
```
# Model-specific parameters  
model_list:  
 - model_name: mistral-7b # model alias  
  litellm_params: # actual params for litellm.completion()  
   model: "huggingface/mistralai/Mistral-7B-Instruct-v0.1"   
   api_base: "<your-api-base>"  
   api_key: "<your-api-key>" # [OPTIONAL] for hf inference endpoints  
   initial_prompt_value: "\n"  
   roles: {"system":{"pre_message":"<|im_start|>system\n", "post_message":"<|im_end|>"}, "assistant":{"pre_message":"<|im_start|>assistant\n","post_message":"<|im_end|>"}, "user":{"pre_message":"<|im_start|>user\n","post_message":"<|im_end|>"}}  
   final_prompt_value: "\n"  
   bos_token: " "  
   eos_token: " "  
   max_tokens: 4096  

```

**Step 2** : Start server with config
```
$ litellm --config /path/to/config.yaml  

```

### Set custom tokenizer​
If you're using the `/utils/token_counter` endpoint, and want to set a custom huggingface tokenizer for a model, you can do so in the `config.yaml`
```
model_list:  
 - model_name: openai-deepseek  
  litellm_params:  
   model: deepseek/deepseek-chat  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   access_groups: ["restricted-models"]  
   custom_tokenizer:   
    identifier: deepseek-ai/DeepSeek-V3-Base  
    revision: main  
    auth_token: os.environ/HUGGINGFACE_API_KEY  

```

**Spec**
```
custom_tokenizer:   
 identifier: str # huggingface model identifier  
 revision: str # huggingface model revision (usually 'main')  
 auth_token: Optional[str] # huggingface auth token   

```

## General Settings `general_settings` (DB Connection, etc)​
### Configure DB Pool Limits + Connection Timeouts​
```
general_settings:   
 database_connection_pool_limit: 100 # sets connection pool for prisma client to postgres db at 100  
 database_connection_timeout: 60 # sets a 60s timeout for any connection call to the db   

```

## Extras​
### Disable Swagger UI​
To disable the Swagger docs from the base url, set 
```
NO_DOCS="True"  

```

in your environment, and restart the proxy. 
### Use CONFIG_FILE_PATH for proxy (Easier Azure container deployment)​
  1. Setup config.yaml


```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  

```

  1. Store filepath as env var 


```
CONFIG_FILE_PATH="/path/to/config.yaml"  

```

  1. Start Proxy


```
$ litellm   
  
# RUNNING on http://0.0.0.0:4000  

```

### Providing LiteLLM config.yaml file as a s3, GCS Bucket Object/url​
Use this if you cannot mount a config file on your deployment service (example - AWS Fargate, Railway etc)
LiteLLM Proxy will read your config.yaml from an s3 Bucket or GCS Bucket 
  * GCS Bucket
  * s3


Set the following .env vars 
```
LITELLM_CONFIG_BUCKET_TYPE = "gcs"               # set this to "gcs"       
LITELLM_CONFIG_BUCKET_NAME = "litellm-proxy"          # your bucket name on GCS  
LITELLM_CONFIG_BUCKET_OBJECT_KEY = "proxy_config.yaml"     # object key on GCS  

```

Start litellm proxy with these env vars - litellm will read your config from GCS 
```
docker run --name litellm-proxy \  
  -e DATABASE_URL=<database_url> \  
  -e LITELLM_CONFIG_BUCKET_NAME=<bucket_name> \  
  -e LITELLM_CONFIG_BUCKET_OBJECT_KEY="<object_key>> \  
  -e LITELLM_CONFIG_BUCKET_TYPE="gcs" \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm-database:main-latest --detailed_debug  

```

Set the following .env vars 
```
LITELLM_CONFIG_BUCKET_NAME = "litellm-proxy"          # your bucket name on s3   
LITELLM_CONFIG_BUCKET_OBJECT_KEY = "litellm_proxy_config.yaml" # object key on s3  

```

Start litellm proxy with these env vars - litellm will read your config from s3 
```
docker run --name litellm-proxy \  
  -e DATABASE_URL=<database_url> \  
  -e LITELLM_CONFIG_BUCKET_NAME=<bucket_name> \  
  -e LITELLM_CONFIG_BUCKET_OBJECT_KEY="<object_key>> \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm-database:main-latest  

```

Previous
Getting Started - E2E Tutorial
Next
File Management
  * Quick Start
  * LLM configs `model_list`
    * Model-specific params (API Base, Keys, Temperature, Max Tokens, Organization, Headers etc.)
    * Embedding Models - Use Sagemaker, Bedrock, Azure, OpenAI, XInference
    * Multiple OpenAI Organizations
    * Load Balancing
    * Load API Keys / config values from Environment
    * Centralized Credential Management
    * Load API Keys from Secret Managers (Azure Vault, etc)
    * Set Supported Environments for a model - `production`, `staging`, `development`
    * Set Custom Prompt Templates
    * Set custom tokenizer
  * General Settings `general_settings` (DB Connection, etc)
    * Configure DB Pool Limits + Connection Timeouts
  * Extras
    * Disable Swagger UI
    * Use CONFIG_FILE_PATH for proxy (Easier Azure container deployment)
    * Providing LiteLLM config.yaml file as a s3, GCS Bucket Object/url


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
      * 💸 Spend Tracking
      * Custom LLM Pricing
      * Billing
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Spend Tracking
  * 💸 Spend Tracking


On this page
# 💸 Spend Tracking
Track spend for keys, users, and teams across 100+ LLMs.
LiteLLM automatically tracks spend for all known models. See our model cost map
### How to Track Spend with LiteLLM​
**Step 1**
👉 Setup LiteLLM with a Database
**Step2** Send `/chat/completions` request
  * OpenAI Python v1.0.0+
  * Curl Request
  * Langchain


```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="llama3",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  user="palantir", # OPTIONAL: pass user to track spend by user  
  extra_body={   
    "metadata": {  
      "tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"] # ENTERPRISE: pass tags to track spend by tags  
    }  
  }  
)  
  
print(response)  

```

Pass `metadata` as part of the request body
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --header 'Authorization: Bearer sk-1234' \  
  --data '{  
  "model": "llama3",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "user": "palantir", # OPTIONAL: pass user to track spend by user  
  "metadata": {  
    "tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"] # ENTERPRISE: pass tags to track spend by tags  
  }  
}'  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
import os  
  
os.environ["OPENAI_API_KEY"] = "sk-1234"  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "llama3",  
  user="palantir",  
  extra_body={  
    "metadata": {  
      "tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"] # ENTERPRISE: pass tags to track spend by tags  
    }  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

**Step3 - Verify Spend Tracked** That's IT. Now Verify your spend was tracked
  * Response Headers
  * DB + UI


Expect to see `x-litellm-response-cost` in the response headers with calculated cost
The following spend gets tracked in Table `LiteLLM_SpendLogs`
```
{  
 "api_key": "fe6b0cab4ff5a5a8df823196cc8a450*****",              # Hash of API Key used  
 "user": "default_user",                            # Internal User (LiteLLM_UserTable) that owns `api_key=sk-1234`.   
 "team_id": "e8d1460f-846c-45d7-9b43-55f3cc52ac32",              # Team (LiteLLM_TeamTable) that owns `api_key=sk-1234`  
 "request_tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"],# Tags sent in request  
 "end_user": "palantir",                            # Customer - the `user` sent in the request  
 "model_group": "llama3",                           # "model" passed to LiteLLM  
 "api_base": "https://api.groq.com/openai/v1/",                # "api_base" of model used by LiteLLM  
 "spend": 0.000002,                              # Spend in $  
 "total_tokens": 100,  
 "completion_tokens": 80,  
 "prompt_tokens": 20,  
  
}  

```

Navigate to the Usage Tab on the LiteLLM UI (found on https://your-proxy-endpoint/ui) and verify you see spend tracked under `Usage`
### Allowing Non-Proxy Admins to access `/spend` endpoints​
Use this when you want non-proxy admins to access `/spend` endpoints
info
Schedule a meeting with us to get your Enterprise License
##### Create Key​
Create Key with with `permissions={"get_spend_routes": true}`
```
curl --location 'http://0.0.0.0:4000/key/generate' \  
    --header 'Authorization: Bearer sk-1234' \  
    --header 'Content-Type: application/json' \  
    --data '{  
      "permissions": {"get_spend_routes": true}  
  }'  

```

##### Use generated key on `/spend` endpoints​
Access spend Routes with newly generate keys
```
curl -X GET 'http://localhost:4000/global/spend/report?start_date=2024-04-01&end_date=2024-06-30' \  
 -H 'Authorization: Bearer sk-H16BKvrSNConSsBYLGc_7A'  

```

#### Reset Team, API Key Spend - MASTER KEY ONLY​
Use `/global/spend/reset` if you want to:
  * Reset the Spend for all API Keys, Teams. The `spend` for ALL Teams and Keys in `LiteLLM_TeamTable` and `LiteLLM_VerificationToken` will be set to `spend=0`
  * LiteLLM will maintain all the logs in `LiteLLMSpendLogs` for Auditing Purposes


##### Request​
Only the `LITELLM_MASTER_KEY` you set can access this route
```
curl -X POST \  
 'http://localhost:4000/global/spend/reset' \  
 -H 'Authorization: Bearer sk-1234' \  
 -H 'Content-Type: application/json'  

```

##### Expected Responses​
```
{"message":"Spend for all API Keys and Teams reset successfully","status":"success"}  

```

## Daily Spend Breakdown API​
Retrieve granular daily usage data for a user (by model, provider, and API key) with a single endpoint.
Example Request:
Daily Spend Breakdown API
```
curl -L -X GET 'http://localhost:4000/user/daily/activity?start_date=2025-03-20&end_date=2025-03-27' \  
-H 'Authorization: Bearer sk-...'  

```

Daily Spend Breakdown API Response
```
{  
  "results": [  
    {  
      "date": "2025-03-27",  
      "metrics": {  
        "spend": 0.0177072,  
        "prompt_tokens": 111,  
        "completion_tokens": 1711,  
        "total_tokens": 1822,  
        "api_requests": 11  
      },  
      "breakdown": {  
        "models": {  
          "gpt-4o-mini": {  
            "spend": 1.095e-05,  
            "prompt_tokens": 37,  
            "completion_tokens": 9,  
            "total_tokens": 46,  
            "api_requests": 1  
        },  
        "providers": { "openai": { ... }, "azure_ai": { ... } },  
        "api_keys": { "3126b6eaf1...": { ... } }  
      }  
    }  
  ],  
  "metadata": {  
    "total_spend": 0.7274667,  
    "total_prompt_tokens": 280990,  
    "total_completion_tokens": 376674,  
    "total_api_requests": 14  
  }  
}  

```

### API Reference​
See our Swagger API for more details on the `/user/daily/activity` endpoint
## ✨ (Enterprise) Generate Spend Reports​
Use this to charge other teams, customers, users
Use the `/global/spend/report` endpoint to get spend reports
  * Spend Per Team
  * Spend Per Customer
  * Spend for Specific API Key
  * Spend for Internal User (Key Owner)


#### Example Request​
👉 Key Change: Specify `group_by=team`
```
curl -X GET 'http://localhost:4000/global/spend/report?start_date=2024-04-01&end_date=2024-06-30&group_by=team' \  
 -H 'Authorization: Bearer sk-1234'  

```

#### Example Response​
  * Expected Response
  * Script to Parse Response (Python)


```
[  
  {  
    "group_by_day": "2024-04-30T00:00:00+00:00",  
    "teams": [  
      {  
        "team_name": "Prod Team",  
        "total_spend": 0.0015265,  
        "metadata": [ # see the spend by unique(key + model)  
          {  
            "model": "gpt-4",  
            "spend": 0.00123,  
            "total_tokens": 28,  
            "api_key": "88dc28.." # the hashed api key  
          },  
          {  
            "model": "gpt-4",  
            "spend": 0.00123,  
            "total_tokens": 28,  
            "api_key": "a73dc2.." # the hashed api key  
          },  
          {  
            "model": "chatgpt-v-2",  
            "spend": 0.000214,  
            "total_tokens": 122,  
            "api_key": "898c28.." # the hashed api key  
          },  
          {  
            "model": "gpt-3.5-turbo",  
            "spend": 0.0000825,  
            "total_tokens": 85,  
            "api_key": "84dc28.." # the hashed api key  
          }  
        ]  
      }  
    ]  
  }  
]  

```

```
import requests  
url = 'http://localhost:4000/global/spend/report'  
params = {  
  'start_date': '2023-04-01',  
  'end_date': '2024-06-30'  
}  
  
headers = {  
  'Authorization': 'Bearer sk-1234'  
}  
  
# Make the GET request  
response = requests.get(url, headers=headers, params=params)  
spend_report = response.json()  
  
for row in spend_report:  
 date = row["group_by_day"]  
 teams = row["teams"]  
 for team in teams:  
   team_name = team["team_name"]  
   total_spend = team["total_spend"]  
   metadata = team["metadata"]  
  
   print(f"Date: {date}")  
   print(f"Team: {team_name}")  
   print(f"Total Spend: {total_spend}")  
   print("Metadata: ", metadata)  
   print()  

```

Output from script
```
# Date: 2024-05-11T00:00:00+00:00  
# Team: local_test_team  
# Total Spend: 0.003675099999999999  
# Metadata: [{'model': 'gpt-3.5-turbo', 'spend': 0.003675099999999999, 'api_key': 'b94d5e0bc3a71a573917fe1335dc0c14728c7016337451af9714924ff3a729db', 'total_tokens': 3105}]  
  
# Date: 2024-05-13T00:00:00+00:00  
# Team: Unassigned Team  
# Total Spend: 3.4e-05  
# Metadata: [{'model': 'gpt-3.5-turbo', 'spend': 3.4e-05, 'api_key': '9569d13c9777dba68096dea49b0b03e0aaf4d2b65d4030eda9e8a2733c3cd6e0', 'total_tokens': 50}]  
  
# Date: 2024-05-13T00:00:00+00:00  
# Team: central  
# Total Spend: 0.000684  
# Metadata: [{'model': 'gpt-3.5-turbo', 'spend': 0.000684, 'api_key': '0323facdf3af551594017b9ef162434a9b9a8ca1bbd9ccbd9d6ce173b1015605', 'total_tokens': 498}]  
  
# Date: 2024-05-13T00:00:00+00:00  
# Team: local_test_team  
# Total Spend: 0.0005715000000000001  
# Metadata: [{'model': 'gpt-3.5-turbo', 'spend': 0.0005715000000000001, 'api_key': 'b94d5e0bc3a71a573917fe1335dc0c14728c7016337451af9714924ff3a729db', 'total_tokens': 423}]  

```

info
Customer this is `user` passed to `/chat/completions` request
  * LiteLLM API key


#### Example Request​
👉 Key Change: Specify `group_by=customer`
```
curl -X GET 'http://localhost:4000/global/spend/report?start_date=2024-04-01&end_date=2024-06-30&group_by=customer' \  
 -H 'Authorization: Bearer sk-1234'  

```

#### Example Response​
```
[  
  {  
    "group_by_day": "2024-04-30T00:00:00+00:00",  
    "customers": [  
      {  
        "customer": "palantir",  
        "total_spend": 0.0015265,  
        "metadata": [ # see the spend by unique(key + model)  
          {  
            "model": "gpt-4",  
            "spend": 0.00123,  
            "total_tokens": 28,  
            "api_key": "88dc28.." # the hashed api key  
          },  
          {  
            "model": "gpt-4",  
            "spend": 0.00123,  
            "total_tokens": 28,  
            "api_key": "a73dc2.." # the hashed api key  
          },  
          {  
            "model": "chatgpt-v-2",  
            "spend": 0.000214,  
            "total_tokens": 122,  
            "api_key": "898c28.." # the hashed api key  
          },  
          {  
            "model": "gpt-3.5-turbo",  
            "spend": 0.0000825,  
            "total_tokens": 85,  
            "api_key": "84dc28.." # the hashed api key  
          }  
        ]  
      }  
    ]  
  }  
]  

```

👉 Key Change: Specify `api_key=sk-1234`
```
curl -X GET 'http://localhost:4000/global/spend/report?start_date=2024-04-01&end_date=2024-06-30&api_key=sk-1234' \  
 -H 'Authorization: Bearer sk-1234'  

```

#### Example Response​
```
[  
 {  
  "api_key": "88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b",  
  "total_cost": 0.3201286305151999,  
  "total_input_tokens": 36.0,  
  "total_output_tokens": 1593.0,  
  "model_details": [  
   {  
    "model": "dall-e-3",  
    "total_cost": 0.31999939051519993,  
    "total_input_tokens": 0,  
    "total_output_tokens": 0  
   },  
   {  
    "model": "llama3-8b-8192",  
    "total_cost": 0.00012924,  
    "total_input_tokens": 36,  
    "total_output_tokens": 1593  
   }  
  ]  
 }  
]  

```

info
Internal User (Key Owner): This is the value of `user_id` passed when calling `/key/generate`
👉 Key Change: Specify `internal_user_id=ishaan`
```
curl -X GET 'http://localhost:4000/global/spend/report?start_date=2024-04-01&end_date=2024-12-30&internal_user_id=ishaan' \  
 -H 'Authorization: Bearer sk-1234'  

```

#### Example Response​
```
[  
 {  
  "api_key": "88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b",  
  "total_cost": 0.00013132,  
  "total_input_tokens": 105.0,  
  "total_output_tokens": 872.0,  
  "model_details": [  
   {  
    "model": "gpt-3.5-turbo-instruct",  
    "total_cost": 5.85e-05,  
    "total_input_tokens": 15,  
    "total_output_tokens": 18  
   },  
   {  
    "model": "llama3-8b-8192",  
    "total_cost": 7.282000000000001e-05,  
    "total_input_tokens": 90,  
    "total_output_tokens": 854  
   }  
  ]  
 },  
 {  
  "api_key": "151e85e46ab8c9c7fad090793e3fe87940213f6ae665b543ca633b0b85ba6dc6",  
  "total_cost": 5.2699999999999993e-05,  
  "total_input_tokens": 26.0,  
  "total_output_tokens": 27.0,  
  "model_details": [  
   {  
    "model": "gpt-3.5-turbo",  
    "total_cost": 5.2499999999999995e-05,  
    "total_input_tokens": 24,  
    "total_output_tokens": 27  
   },  
   {  
    "model": "text-embedding-ada-002",  
    "total_cost": 2e-07,  
    "total_input_tokens": 2,  
    "total_output_tokens": 0  
   }  
  ]  
 },  
 {  
  "api_key": "60cb83a2dcbf13531bd27a25f83546ecdb25a1a6deebe62d007999dc00e1e32a",  
  "total_cost": 9.42e-06,  
  "total_input_tokens": 30.0,  
  "total_output_tokens": 99.0,  
  "model_details": [  
   {  
    "model": "llama3-8b-8192",  
    "total_cost": 9.42e-06,  
    "total_input_tokens": 30,  
    "total_output_tokens": 99  
   }  
  ]  
 }  
]  

```

## ✨ Custom Spend Log metadata​
Log specific key,value pairs as part of the metadata for a spend log
info
Logging specific key,value pairs in spend logs metadata is an enterprise feature. See here
## ✨ Custom Tags​
info
Tracking spend with Custom tags is an enterprise feature. See here
Previous
Session Logs
Next
Custom LLM Pricing
  * How to Track Spend with LiteLLM
  * Allowing Non-Proxy Admins to access `/spend` endpoints
  * Daily Spend Breakdown API
    * API Reference
  * ✨ (Enterprise) Generate Spend Reports
  * ✨ Custom Spend Log metadata
  * ✨ Custom Tags


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
      * Virtual Keys
      * OIDC - JWT-based Auth
      * [Beta] Service Accounts
      * Role-based Access Controls (RBAC)
      * Custom Auth
      * IP Address Filtering
      * Email Notifications
      * Attribute Management changes to Users
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Authentication
  * Custom Auth


On this page
# Custom Auth
You can now override the default api key auth.
Here's how: 
#### 1. Create a custom auth file.​
Make sure the response type follows the `UserAPIKeyAuth` pydantic object. This is used by for logging usage specific to that user key.
```
from litellm.proxy._types import UserAPIKeyAuth  
  
async def user_api_key_auth(request: Request, api_key: str) -> UserAPIKeyAuth:   
  try:   
    modified_master_key = "sk-my-master-key"  
    if api_key == modified_master_key:  
      return UserAPIKeyAuth(api_key=api_key)  
    raise Exception  
  except:   
    raise Exception  

```

#### 2. Pass the filepath (relative to the config.yaml)​
Pass the filepath to the config.yaml 
e.g. if they're both in the same dir - `./config.yaml` and `./custom_auth.py`, this is what it looks like:
```
model_list:   
 - model_name: "openai-model"  
  litellm_params:   
   model: "gpt-3.5-turbo"  
  
litellm_settings:  
 drop_params: True  
 set_verbose: True  
  
general_settings:  
 custom_auth: custom_auth.user_api_key_auth  

```

**Implementation Code**
#### 3. Start the proxy​
```
$ litellm --config /path/to/config.yaml   

```

Previous
Role-based Access Controls (RBAC)
Next
IP Address Filtering
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
      * 💸 Spend Tracking
      * Custom LLM Pricing
      * Billing
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Spend Tracking
  * Custom LLM Pricing


On this page
# Custom LLM Pricing
Use this to register custom pricing for models. 
There's 2 ways to track cost: 
  * cost per token
  * cost per second


By default, the response cost is accessible in the logging object via `kwargs["response_cost"]` on success (sync + async). **Learn More**
info
LiteLLM already has pricing for any model in our model cost map. 
## Cost Per Second (e.g. Sagemaker)​
### Usage with LiteLLM Proxy Server​
**Step 1: Add pricing to config.yaml**
```
model_list:  
 - model_name: sagemaker-completion-model  
  litellm_params:  
   model: sagemaker/berri-benchmarking-Llama-2-70b-chat-hf-4  
  model_info:  
   input_cost_per_second: 0.000420  
 - model_name: sagemaker-embedding-model  
  litellm_params:  
   model: sagemaker/berri-benchmarking-gpt-j-6b-fp16  
  model_info:  
   input_cost_per_second: 0.000420   

```

**Step 2: Start proxy**
```
litellm /path/to/config.yaml  

```

**Step 3: View Spend Logs**
## Cost Per Token (e.g. Azure)​
### Usage with LiteLLM Proxy Server​
```
model_list:  
 - model_name: azure-model  
  litellm_params:  
   model: azure/<your_deployment_name>  
   api_key: os.environ/AZURE_API_KEY  
   api_base: os.environ/AZURE_API_BASE  
   api_version: os.environ/AZURE_API_VERSION  
  model_info:  
   input_cost_per_token: 0.000421 # 👈 ONLY to track cost per token  
   output_cost_per_token: 0.000520 # 👈 ONLY to track cost per token  

```

## Override Model Cost Map​
You can override our model cost map with your own custom pricing for a mapped model.
Just add a `model_info` key to your model in the config, and override the desired keys.
Example: Override Anthropic's model cost map for the `prod/claude-3-5-sonnet-20241022` model.
```
model_list:  
 - model_name: "prod/claude-3-5-sonnet-20241022"  
  litellm_params:  
   model: "anthropic/claude-3-5-sonnet-20241022"  
   api_key: os.environ/ANTHROPIC_PROD_API_KEY  
  model_info:  
   input_cost_per_token: 0.000006  
   output_cost_per_token: 0.00003  
   cache_creation_input_token_cost: 0.0000075  
   cache_read_input_token_cost: 0.0000006  

```

## Set 'base_model' for Cost Tracking (e.g. Azure deployments)​
**Problem** : Azure returns `gpt-4` in the response when `azure/gpt-4-1106-preview` is used. This leads to inaccurate cost tracking
**Solution** ✅ : Set `base_model` on your config so litellm uses the correct model for calculating azure cost
Get the base model name from here
Example config with `base_model`
```
model_list:  
 - model_name: azure-gpt-3.5  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: "2023-07-01-preview"  
  model_info:  
   base_model: azure/gpt-4-1106-preview  

```

## Debugging​
If you're custom pricing is not being used or you're seeing errors, please check the following:
  1. Run the proxy with `LITELLM_LOG="DEBUG"` or the `--detailed_debug` cli flag


```
litellm --config /path/to/config.yaml --detailed_debug  

```

  1. Check logs for this line: 


```
LiteLLM:DEBUG: utils.py:263 - litellm.acompletion  

```

  1. Check if 'input_cost_per_token' and 'output_cost_per_token' are top-level keys in the acompletion function. 


```
acompletion(  
 ...,  
 input_cost_per_token: my-custom-price,   
 output_cost_per_token: my-custom-price,  
)  

```

If these keys are not present, LiteLLM will not use your custom pricing. 
If the problem persists, please file an issue on GitHub.
Previous
💸 Spend Tracking
Next
Billing
  * Cost Per Second (e.g. Sagemaker)
    * Usage with LiteLLM Proxy Server
  * Cost Per Token (e.g. Azure)
    * Usage with LiteLLM Proxy Server
  * Override Model Cost Map
  * Set 'base_model' for Cost Tracking (e.g. Azure deployments)
  * Debugging


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
    * Prompt Management
    * Custom Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * [Beta] Prompt Management
  * Custom Prompt Management


On this page
# Custom Prompt Management
Connect LiteLLM to your prompt management system with custom hooks.
## Overview​
## How it works​
## Quick Start​
### 1. Create Your Custom Prompt Manager​
Create a class that inherits from `CustomPromptManagement` to handle prompt retrieval and formatting:
**Example Implementation**
Create a new file called `custom_prompt.py` and add this code. The key method here is `get_chat_completion_prompt` you can implement custom logic to retrieve and format prompts based on the `prompt_id` and `prompt_variables`.
```
from typing import List, Tuple, Optional  
from litellm.integrations.custom_prompt_management import CustomPromptManagement  
from litellm.types.llms.openai import AllMessageValues  
from litellm.types.utils import StandardCallbackDynamicParams  
  
class MyCustomPromptManagement(CustomPromptManagement):  
  def get_chat_completion_prompt(  
    self,  
    model: str,  
    messages: List[AllMessageValues],  
    non_default_params: dict,  
    prompt_id: str,  
    prompt_variables: Optional[dict],  
    dynamic_callback_params: StandardCallbackDynamicParams,  
  ) -> Tuple[str, List[AllMessageValues], dict]:  
    """  
    Retrieve and format prompts based on prompt_id.  
      
    Returns:  
      - model: The model to use  
      - messages: The formatted messages  
      - non_default_params: Optional parameters like temperature  
    """  
    # Example matching the diagram: Add system message for prompt_id "1234"  
    if prompt_id == "1234":  
      # Prepend system message while preserving existing messages  
      new_messages = [  
        {"role": "system", "content": "Be a good Bot!"},  
      ] + messages  
      return model, new_messages, non_default_params  
      
    # Default: Return original messages if no prompt_id match  
    return model, messages, non_default_params  
  
prompt_management = MyCustomPromptManagement()  

```

### 2. Configure Your Prompt Manager in LiteLLM `config.yaml`​
```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/gpt-4  
   api_key: os.environ/OPENAI_API_KEY  
  
litellm_settings:  
 callbacks: custom_prompt.prompt_management # sets litellm.callbacks = [prompt_management]  

```

### 3. Start LiteLLM Gateway​
  * Docker Run
  * litellm pip


Mount your `custom_logger.py` on the LiteLLM Docker container.
```
docker run -d \  
 -p 4000:4000 \  
 -e OPENAI_API_KEY=$OPENAI_API_KEY \  
 --name my-app \  
 -v $(pwd)/my_config.yaml:/app/config.yaml \  
 -v $(pwd)/custom_logger.py:/app/custom_logger.py \  
 my-app:latest \  
 --config /app/config.yaml \  
 --port 4000 \  
 --detailed_debug \  

```

```
litellm --config config.yaml --detailed_debug  

```

### 4. Test Your Custom Prompt Manager​
When you pass `prompt_id="1234"`, the custom prompt manager will add a system message "Be a good Bot!" to your conversation:
  * OpenAI Python v1.0.0+
  * Langchain
  * Curl


```
from openai import OpenAI  
  
client = OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="gemini-1.5-pro",  
  messages=[{"role": "user", "content": "hi"}],  
  prompt_id="1234"  
)  
  
print(response.choices[0].message.content)  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.schema import HumanMessage  
  
chat = ChatOpenAI(  
  model="gpt-4",  
  openai_api_key="sk-1234",  
  openai_api_base="http://0.0.0.0:4000",  
  extra_body={  
    "prompt_id": "1234"  
  }  
)  
  
messages = []  
response = chat(messages)  
  
print(response.content)  

```

```
curl -X POST http://0.0.0.0:4000/v1/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer sk-1234" \  
-d '{  
  "model": "gemini-1.5-pro",  
  "messages": [{"role": "user", "content": "hi"}],  
  "prompt_id": "1234"  
}'  

```

The request will be transformed from:
```
{  
  "model": "gemini-1.5-pro",  
  "messages": [{"role": "user", "content": "hi"}],  
  "prompt_id": "1234"  
}  

```

To:
```
{  
  "model": "gemini-1.5-pro",  
  "messages": [  
    {"role": "system", "content": "Be a good Bot!"},  
    {"role": "user", "content": "hi"}  
  ]  
}  

```

Previous
Prompt Management
Next
Benchmarks
  * Overview
  * How it works
  * Quick Start
    * 1. Create Your Custom Prompt Manager
    * 2. Configure Your Prompt Manager in LiteLLM `config.yaml`
    * 3. Start LiteLLM Gateway
    * 4. Test Your Custom Prompt Manager


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
    * Data Privacy and Security
    * Data Retention Policy
    * Migration Policy
    * ❤️ 🚅 Projects built on LiteLLM
    * PII Masking - LiteLLM Gateway (Deprecated Version)
    * Code Quality
    * Rules
    * [DEPRECATED] Team-based Routing
    * [DEPRECATED] Region-based Routing
    * [OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
  * Support & Talk with founders


  *   * Extras
  * [DEPRECATED] Region-based Routing


On this page
# [DEPRECATED] Region-based Routing
info
This is deprecated, please use Tag Based Routing instead
Route specific customers to eu-only models.
By specifying 'allowed_model_region' for a customer, LiteLLM will filter-out any models in a model group which is not in the allowed region (i.e. 'eu').
**See Code**
### 1. Create customer with region-specification​
Use the litellm 'end-user' object for this. 
End-users can be tracked / id'ed by passing the 'user' param to litellm in an openai chat completion/embedding call.
```
curl -X POST --location 'http://0.0.0.0:4000/end_user/new' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{  
  "user_id" : "ishaan-jaff-45",  
  "allowed_model_region": "eu", # 👈 SPECIFY ALLOWED REGION='eu'  
}'  

```

### 2. Add eu models to model-group​
Add eu models to a model group. Use the 'region_name' param to specify the region for each model.
Supported regions are 'eu' and 'us'.
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/gpt-35-turbo # 👈 EU azure model  
   api_base: https://my-endpoint-europe-berri-992.openai.azure.com/  
   api_key: os.environ/AZURE_EUROPE_API_KEY  
   region_name: "eu"  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/chatgpt-v-2  
   api_base: https://openai-gpt-4-test-v-1.openai.azure.com/  
   api_version: "2023-05-15"  
   api_key: os.environ/AZURE_API_KEY  
   region_name: "us"  
  
router_settings:  
 enable_pre_call_checks: true # 👈 IMPORTANT  

```

Start the proxy
```
litellm --config /path/to/config.yaml  

```

### 3. Test it!​
Make a simple chat completions call to the proxy. In the response headers, you should see the returned api base. 
```
curl -X POST --location 'http://localhost:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer sk-1234' \  
--data '{  
  "model": "gpt-3.5-turbo",   
  "messages": [  
    {  
    "role": "user",  
    "content": "what is the meaning of the universe? 1234"  
  }],  
  "user": "ishaan-jaff-45" # 👈 USER ID  
}  
'  

```

Expected API Base in response headers 
```
x-litellm-api-base: "https://my-endpoint-europe-berri-992.openai.azure.com/"  
x-litellm-model-region: "eu" # 👈 CONFIRMS REGION-BASED ROUTING WORKED  

```

### FAQ​
**What happens if there are no available models for that region?**
Since the router filters out models not in the specified region, it will return back as an error to the user, if no models in that region are available.
Previous
[DEPRECATED] Team-based Routing
Next
[OLD PROXY 👉 [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server
  * 1. Create customer with region-specification
  * 2. Add eu models to model-group
  * 3. Test it!
  * FAQ


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
# Page Not Found
We could not find what you were looking for.
Please contact the owner of the site that linked you to the original URL and let them know their link is broken.
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
      * Life of a Request
      * What is stored in the DB
      * High Availability Setup (Resolve DB Deadlocks)
      * Router Architecture (Fallbacks / Retries)
      * User Management Hierarchy
      * Control Model Access with OIDC (Azure AD/Keycloak/etc.)
      * Image URL Handling
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Architecture
  * High Availability Setup (Resolve DB Deadlocks)


On this page
# High Availability Setup (Resolve DB Deadlocks)
Resolve any Database Deadlocks you see in high traffic by using this setup
## What causes the problem?​
LiteLLM writes `UPDATE` and `UPSERT` queries to the DB. When using 10+ instances of LiteLLM, these queries can cause deadlocks since each instance could simultaneously attempt to update the same `user_id`, `team_id`, `key` etc. 
## How the high availability setup fixes the problem​
  * All instances will write to a Redis queue instead of the DB. 
  * A single instance will acquire a lock on the DB and flush the redis queue to the DB. 


## How it works​
### Stage 1. Each instance writes updates to redis​
Each instance will accumulate the spend updates for a key, user, team, etc and write the updates to a redis queue. 
Each instance writes updates to redis
### Stage 2. A single instance flushes the redis queue to the DB​
A single instance will acquire a lock on the DB and flush all elements in the redis queue to the DB. 
  * 1 instance will attempt to acquire the lock for the DB update job 
  * The status of the lock is stored in redis
  * If the instance acquires the lock to write to DB
    * It will read all updates from redis
    * Aggregate all updates into 1 transaction
    * Write updates to DB
    * Release the lock
  * Note: Only 1 instance can acquire the lock at a time, this limits the number of instances that can write to the DB at once


A single instance flushes the redis queue to the DB
## Usage​
### Required components​
  * Redis
  * Postgres


### Setup on LiteLLM config​
You can enable using the redis buffer by setting `use_redis_transaction_buffer: true` in the `general_settings` section of your `proxy_config.yaml` file. 
Note: This setup requires litellm to be connected to a redis instance. 
litellm proxy_config.yaml
```
general_settings:  
 use_redis_transaction_buffer: true  
  
litellm_settings:  
 cache: True  
 cache_params:  
  type: redis  
  supported_call_types: [] # Optional: Set cache for proxy, but not on the actual llm api call  

```

## Monitoring​
LiteLLM emits the following prometheus metrics to monitor the health/status of the in memory buffer and redis buffer. 
Metric Name| Description| Storage Type  
---|---|---  
`litellm_pod_lock_manager_size`| Indicates which pod has the lock to write updates to the database.| Redis  
`litellm_in_memory_daily_spend_update_queue_size`| Number of items in the in-memory daily spend update queue. These are the aggregate spend logs for each user.| In-Memory  
`litellm_redis_daily_spend_update_queue_size`| Number of items in the Redis daily spend update queue. These are the aggregate spend logs for each user.| Redis  
`litellm_in_memory_spend_update_queue_size`| In-memory aggregate spend values for keys, users, teams, team members, etc.| In-Memory  
`litellm_redis_spend_update_queue_size`| Redis aggregate spend values for keys, users, teams, etc.| Redis  
Previous
What is stored in the DB
Next
Router Architecture (Fallbacks / Retries)
  * What causes the problem?
  * How the high availability setup fixes the problem
  * How it works
    * Stage 1. Each instance writes updates to redis
    * Stage 2. A single instance flushes the redis queue to the DB
  * Usage
    * Required components
    * Setup on LiteLLM config
  * Monitoring


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Demo App


# Demo App
Here is a demo of the proxy. To log in pass in:
  * Username: admin
  * Password: sk-1234


Demo UI
Previous
Rotating Master Key
Next
Life of a Request
Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
      * 💰 Budgets, Rate Limits
      * ✨ Temporary Budget Increase
      * ✨ Budget / Rate Limit Tiers
      * 💰 Setting Team Budgets
      * 🙋‍♂️ Customers / End-User Budgets
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Budgets + Rate Limits
  * 🙋‍♂️ Customers / End-User Budgets


On this page
# 🙋‍♂️ Customers / End-User Budgets
Track spend, set budgets for your customers.
## Tracking Customer Spend​
### 1. Make LLM API call w/ Customer ID​
Make a /chat/completions call, pass 'user' - First call Works
```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \  
    --header 'Content-Type: application/json' \  
    --header 'Authorization: Bearer sk-1234' \ # 👈 YOUR PROXY KEY  
    --data ' {  
    "model": "azure-gpt-3.5",  
    "user": "ishaan3", # 👈 CUSTOMER ID  
    "messages": [  
      {  
      "role": "user",  
      "content": "what time is it"  
      }  
    ]  
    }'  

```

The customer_id will be upserted into the DB with the new spend.
If the customer_id already exists, spend will be incremented.
### 2. Get Customer Spend​
  * All-up spend
  * Event Webhook


Call `/customer/info` to get a customer's all up spend
```
curl -X GET 'http://0.0.0.0:4000/customer/info?end_user_id=ishaan3' \ # 👈 CUSTOMER ID  
    -H 'Authorization: Bearer sk-1234' \ # 👈 YOUR PROXY KEY  

```

Expected Response:
```
{  
  "user_id": "ishaan3",  
  "blocked": false,  
  "alias": null,  
  "spend": 0.001413,  
  "allowed_model_region": null,  
  "default_model": null,  
  "litellm_budget_table": null  
}  

```

To update spend in your client-side DB, point the proxy to your webhook. 
E.g. if your server is `https://webhook.site` and your listening on `6ab090e8-c55f-4a23-b075-3209f5c57906`
  1. Add webhook url to your proxy environment: 


```
export WEBHOOK_URL="https://webhook.site/6ab090e8-c55f-4a23-b075-3209f5c57906"  

```

  1. Add 'webhook' to config.yaml


```
general_settings:   
 alerting: ["webhook"] # 👈 KEY CHANGE  

```

  1. Test it! 


```
curl -X POST 'http://localhost:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "model": "mistral",  
  "messages": [  
    {  
    "role": "user",  
    "content": "What's the weather like in Boston today?"  
    }  
  ],  
  "user": "krrish12"  
}  
'  

```

Expected Response 
```
{  
 "spend": 0.0011120000000000001, # 👈 SPEND  
 "max_budget": null,  
 "token": "88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b",  
 "customer_id": "krrish12", # 👈 CUSTOMER ID  
 "user_id": null,  
 "team_id": null,  
 "user_email": null,  
 "key_alias": null,  
 "projected_exceeded_date": null,  
 "projected_spend": null,  
 "event": "spend_tracked",  
 "event_group": "customer",  
 "event_message": "Customer spend tracked. Customer=krrish12, spend=0.0011120000000000001"  
}  

```

See Webhook Spec
## Setting Customer Budgets​
Set customer budgets (e.g. monthly budgets, tpm/rpm limits) on LiteLLM Proxy 
### Quick Start​
Create / Update a customer with budget
**Create New Customer w/ budget**
```
curl -X POST 'http://0.0.0.0:4000/customer/new'       
  -H 'Authorization: Bearer sk-1234'       
  -H 'Content-Type: application/json'       
  -D '{  
    "user_id" : "my-customer-id",  
    "max_budget": "0", # 👈 CAN BE FLOAT  
  }'  

```

**Test it!**
```
curl -X POST 'http://localhost:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "model": "mistral",  
  "messages": [  
    {  
    "role": "user",  
    "content": "What'\''s the weather like in Boston today?"  
    }  
  ],  
  "user": "ishaan-jaff-48"  
}  

```

### Assign Pricing Tiers​
Create and assign customers to pricing tiers.
#### 1. Create a budget​
  * UI
  * API


  * Go to the 'Budgets' tab on the UI. 
  * Click on '+ Create Budget'.
  * Create your pricing tier (e.g. 'my-free-tier' with budget $4). This means each user on this pricing tier will have a max budget of $4. 


Use the `/budget/new` endpoint for creating a new budget. API Reference
```
curl -X POST 'http://localhost:4000/budget/new' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "budget_id": "my-free-tier",   
  "max_budget": 4   
}  

```

#### 2. Assign Budget to Customer​
In your application code, assign budget when creating a new customer. 
Just use the `budget_id` used when creating the budget. In our example, this is `my-free-tier`.
```
curl -X POST 'http://localhost:4000/customer/new' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "user_id": "my-customer-id",  
  "budget_id": "my-free-tier" # 👈 KEY CHANGE  
}  

```

#### 3. Test it!​
  * curl
  * OpenAI


```
curl -X POST 'http://localhost:4000/customer/new' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
  "user_id": "my-customer-id",  
  "budget_id": "my-free-tier" # 👈 KEY CHANGE  
}  

```

```
from openai import OpenAI  
client = OpenAI(  
 base_url="<your_proxy_base_url>",  
 api_key="<your_proxy_key>"  
)  
  
completion = client.chat.completions.create(  
 model="gpt-3.5-turbo",  
 messages=[  
  {"role": "system", "content": "You are a helpful assistant."},  
  {"role": "user", "content": "Hello!"}  
 ],  
 user="my-customer-id"  
)  
  
print(completion.choices[0].message)  

```

Previous
💰 Setting Team Budgets
Next
Logging
  * Tracking Customer Spend
    * 1. Make LLM API call w/ Customer ID
    * 2. Get Customer Spend
  * Setting Customer Budgets
    * Quick Start
    * Assign Pricing Tiers


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
      * Docker, Deployment
      * ⚡ Best Practices for Production
      * CLI Arguments
      * Release Cycle
      * Model Management
      * Health Checks
      * Debugging
      * Using at Scale (1M+ rows in DB)
      * Rotating Master Key
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Setup & Deployment
  * Docker, Deployment


On this page
# Docker, Deployment
You can find the Dockerfile to build litellm proxy here
## Quick Start​
To start using Litellm, run the following commands in a shell:
```
# Get the code  
git clone https://github.com/BerriAI/litellm  
  
# Go to folder  
cd litellm  
  
# Add the master key - you can change this after setup  
echo 'LITELLM_MASTER_KEY="sk-1234"' > .env  
  
# Add the litellm salt key - you cannot change this after adding a model  
# It is used to encrypt / decrypt your LLM API Key credentials  
# We recommend - https://1password.com/password-generator/   
# password generator to get a random hash for litellm salt key  
echo 'LITELLM_SALT_KEY="sk-1234"' >> .env  
  
source .env  
  
# Start  
docker-compose up  

```

### Docker Run​
#### Step 1. CREATE config.yaml​
Example `litellm_config.yaml`
```
model_list:  
 - model_name: azure-gpt-3.5  
  litellm_params:  
   model: azure/<your-azure-model-deployment>  
   api_base: os.environ/AZURE_API_BASE # runs os.getenv("AZURE_API_BASE")  
   api_key: os.environ/AZURE_API_KEY # runs os.getenv("AZURE_API_KEY")  
   api_version: "2023-07-01-preview"  

```

#### Step 2. RUN Docker Image​
```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e AZURE_API_KEY=d6*********** \  
  -e AZURE_API_BASE=https://openai-***********/ \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm:main-latest \  
  --config /app/config.yaml --detailed_debug  

```

Get Latest Image 👉 here
#### Step 3. TEST Request​
Pass `model=azure-gpt-3.5` this was set on step 1
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "azure-gpt-3.5",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ]  
}'  

```

### Docker Run - CLI Args​
See all supported CLI args here: 
Here's how you can run the docker image and pass your config to `litellm`
```
docker run ghcr.io/berriai/litellm:main-latest --config your_config.yaml  

```

Here's how you can run the docker image and start litellm on port 8002 with `num_workers=8`
```
docker run ghcr.io/berriai/litellm:main-latest --port 8002 --num_workers 8  

```

### Use litellm as a base image​
```
# Use the provided base image  
FROM ghcr.io/berriai/litellm:main-latest  
  
# Set the working directory to /app  
WORKDIR /app  
  
# Copy the configuration file into the container at /app  
COPY config.yaml .  
  
# Make sure your docker/entrypoint.sh is executable  
RUN chmod +x ./docker/entrypoint.sh  
  
# Expose the necessary port  
EXPOSE 4000/tcp  
  
# Override the CMD instruction with your desired command and arguments  
# WARNING: FOR PROD DO NOT USE `--detailed_debug` it slows down response times, instead use the following CMD  
# CMD ["--port", "4000", "--config", "config.yaml"]  
  
CMD ["--port", "4000", "--config", "config.yaml", "--detailed_debug"]  

```

### Build from litellm `pip` package​
Follow these instructions to build a docker container from the litellm pip package. If your company has a strict requirement around security / building images you can follow these steps.
Dockerfile 
```
FROM cgr.dev/chainguard/python:latest-dev  
  
USER root  
WORKDIR /app  
  
ENV HOME=/home/litellm  
ENV PATH="${HOME}/venv/bin:$PATH"  
  
# Install runtime dependencies  
RUN apk update && \  
  apk add --no-cache gcc python3-dev openssl openssl-dev  
  
RUN python -m venv ${HOME}/venv  
RUN ${HOME}/venv/bin/pip install --no-cache-dir --upgrade pip  
  
COPY requirements.txt .  
RUN --mount=type=cache,target=${HOME}/.cache/pip \  
  ${HOME}/venv/bin/pip install -r requirements.txt  
  
EXPOSE 4000/tcp  
  
ENTRYPOINT ["litellm"]  
CMD ["--port", "4000"]  

```

Example `requirements.txt`
```
litellm[proxy]==1.57.3 # Specify the litellm version you want to use  
prometheus_client  
langfuse  
prisma  

```

Build the docker image
```
docker build \  
 -f Dockerfile.build_from_pip \  
 -t litellm-proxy-with-pip-5 .  

```

Run the docker image
```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e OPENAI_API_KEY="sk-1222" \  
  -e DATABASE_URL="postgresql://xxxxxxxxx \  
  -p 4000:4000 \  
  litellm-proxy-with-pip-5 \  
  --config /app/config.yaml --detailed_debug  

```

### Terraform​
s/o Nicholas Cecere for his LiteLLM User Management Terraform
👉 Go here for Terraform
### Kubernetes​
Deploying a config file based litellm instance just requires a simple deployment that loads the config.yaml file via a config map. Also it would be a good practice to use the env var declaration for api keys, and attach the env vars with the api key values as an opaque secret.
```
apiVersion: v1  
kind: ConfigMap  
metadata:  
 name: litellm-config-file  
data:  
 config.yaml: |  
   model_list:   
    - model_name: gpt-3.5-turbo  
     litellm_params:  
      model: azure/gpt-turbo-small-ca  
      api_base: https://my-endpoint-canada-berri992.openai.azure.com/  
      api_key: os.environ/CA_AZURE_OPENAI_API_KEY  
---  
apiVersion: v1  
kind: Secret  
type: Opaque  
metadata:  
 name: litellm-secrets  
data:  
 CA_AZURE_OPENAI_API_KEY: bWVvd19pbV9hX2NhdA== # your api key in base64  
---  
apiVersion: apps/v1  
kind: Deployment  
metadata:  
 name: litellm-deployment  
 labels:  
  app: litellm  
spec:  
 selector:  
  matchLabels:  
   app: litellm  
 template:  
  metadata:  
   labels:  
    app: litellm  
  spec:  
   containers:  
   - name: litellm  
    image: ghcr.io/berriai/litellm:main-latest # it is recommended to fix a version generally  
    ports:  
    - containerPort: 4000  
    volumeMounts:  
    - name: config-volume  
     mountPath: /app/proxy_server_config.yaml  
     subPath: config.yaml  
    envFrom:  
    - secretRef:  
      name: litellm-secrets  
   volumes:  
    - name: config-volume  
     configMap:  
      name: litellm-config-file  

```

info
To avoid issues with predictability, difficulties in rollback, and inconsistent environments, use versioning or SHA digests (for example, `litellm:main-v1.30.3` or `litellm@sha256:12345abcdef...`) instead of `litellm:main-latest`.
### Helm Chart​
info
[BETA] Helm Chart is BETA. If you run into an issues/have feedback please let us know https://github.com/BerriAI/litellm/issues
Use this when you want to use litellm helm chart as a dependency for other charts. The `litellm-helm` OCI is hosted here https://github.com/BerriAI/litellm/pkgs/container/litellm-helm
#### Step 1. Pull the litellm helm chart​
```
helm pull oci://ghcr.io/berriai/litellm-helm  
  
# Pulled: ghcr.io/berriai/litellm-helm:0.1.2  
# Digest: sha256:7d3ded1c99c1597f9ad4dc49d84327cf1db6e0faa0eeea0c614be5526ae94e2a  

```

#### Step 2. Unzip litellm helm​
Unzip the specific version that was pulled in Step 1
```
tar -zxvf litellm-helm-0.1.2.tgz  

```

#### Step 3. Install litellm helm​
```
helm install lite-helm ./litellm-helm  

```

#### Step 4. Expose the service to localhost​
```
kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT  

```

Your LiteLLM Proxy Server is now running on `http://127.0.0.1:4000`.
**That's it ! That's the quick start to deploy litellm**
#### Make LLM API Requests​
info
💡 Go here 👉 to make your first LLM API Request
LiteLLM is compatible with several SDKs - including OpenAI SDK, Anthropic SDK, Mistral SDK, LLamaIndex, Langchain (Js, Python)
## Deployment Options​
Docs| When to Use  
---|---  
Quick Start| call 100+ LLMs + Load Balancing  
Deploy with Database| + use Virtual Keys + Track Spend (Note: When deploying with a database providing a `DATABASE_URL` and `LITELLM_MASTER_KEY` are required in your env )  
LiteLLM container + Redis| + load balance across multiple litellm containers  
LiteLLM Database container + PostgresDB + Redis| + use Virtual Keys + Track Spend + load balance across multiple litellm containers  
### Deploy with Database​
##### Docker, Kubernetes, Helm Chart​
Requirements:
  * Need a postgres database (e.g. Supabase, Neon, etc) Set `DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname>` in your env 
  * Set a `LITELLM_MASTER_KEY`, this is your Proxy Admin key - you can use this to create other keys (🚨 must start with `sk-`)


  * Dockerfile
  * Kubernetes
  * Helm
  * Helm OCI Registry (GHCR)


We maintain a separate Dockerfile for reducing build time when running LiteLLM proxy with a connected Postgres Database 
```
docker pull ghcr.io/berriai/litellm-database:main-latest  

```

```
docker run \  
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  
  -e LITELLM_MASTER_KEY=sk-1234 \  
  -e DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname> \  
  -e AZURE_API_KEY=d6*********** \  
  -e AZURE_API_BASE=https://openai-***********/ \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm-database:main-latest \  
  --config /app/config.yaml --detailed_debug  

```

Your LiteLLM Proxy Server is now running on `http://0.0.0.0:4000`.
#### Step 1. Create deployment.yaml​
```
apiVersion: apps/v1  
kind: Deployment  
metadata:  
 name: litellm-deployment  
spec:  
 replicas: 3  
 selector:  
  matchLabels:  
   app: litellm  
 template:  
  metadata:  
   labels:  
    app: litellm  
  spec:  
   containers:  
    - name: litellm-container  
     image: ghcr.io/berriai/litellm:main-latest  
     imagePullPolicy: Always  
     env:  
      - name: AZURE_API_KEY  
       value: "d6******"  
      - name: AZURE_API_BASE  
       value: "https://ope******"  
      - name: LITELLM_MASTER_KEY  
       value: "sk-1234"  
      - name: DATABASE_URL  
       value: "po**********"  
     args:  
      - "--config"  
      - "/app/proxy_config.yaml" # Update the path to mount the config file  
     volumeMounts:         # Define volume mount for proxy_config.yaml  
      - name: config-volume  
       mountPath: /app  
       readOnly: true  
     livenessProbe:  
      httpGet:  
       path: /health/liveliness  
       port: 4000  
      initialDelaySeconds: 120  
      periodSeconds: 15  
      successThreshold: 1  
      failureThreshold: 3  
      timeoutSeconds: 10  
     readinessProbe:  
      httpGet:  
       path: /health/readiness  
       port: 4000  
      initialDelaySeconds: 120  
      periodSeconds: 15  
      successThreshold: 1  
      failureThreshold: 3  
      timeoutSeconds: 10  
   volumes: # Define volume to mount proxy_config.yaml  
    - name: config-volume  
     configMap:  
      name: litellm-config   
  

```

```
kubectl apply -f /path/to/deployment.yaml  

```

#### Step 2. Create service.yaml​
```
apiVersion: v1  
kind: Service  
metadata:  
 name: litellm-service  
spec:  
 selector:  
  app: litellm  
 ports:  
  - protocol: TCP  
   port: 4000  
   targetPort: 4000  
 type: NodePort  

```

```
kubectl apply -f /path/to/service.yaml  

```

#### Step 3. Start server​
```
kubectl port-forward service/litellm-service 4000:4000  

```

Your LiteLLM Proxy Server is now running on `http://0.0.0.0:4000`.
info
[BETA] Helm Chart is BETA. If you run into an issues/have feedback please let us know https://github.com/BerriAI/litellm/issues
Use this to deploy litellm using a helm chart. Link to the LiteLLM Helm Chart
#### Step 1. Clone the repository​
```
git clone https://github.com/BerriAI/litellm.git  

```

#### Step 2. Deploy with Helm​
Run the following command in the root of your `litellm` repo. This will set the litellm proxy master key as `sk-1234`
```
helm install \  
 --set masterkey=sk-1234 \  
 mydeploy \  
 deploy/charts/litellm-helm  

```

#### Step 3. Expose the service to localhost​
```
kubectl \  
 port-forward \  
 service/mydeploy-litellm-helm \  
 4000:4000  

```

Your LiteLLM Proxy Server is now running on `http://127.0.0.1:4000`.
If you need to set your litellm proxy config.yaml, you can find this in values.yaml
info
[BETA] Helm Chart is BETA. If you run into an issues/have feedback please let us know https://github.com/BerriAI/litellm/issues
Use this when you want to use litellm helm chart as a dependency for other charts. The `litellm-helm` OCI is hosted here https://github.com/BerriAI/litellm/pkgs/container/litellm-helm
#### Step 1. Pull the litellm helm chart​
```
helm pull oci://ghcr.io/berriai/litellm-helm  
  
# Pulled: ghcr.io/berriai/litellm-helm:0.1.2  
# Digest: sha256:7d3ded1c99c1597f9ad4dc49d84327cf1db6e0faa0eeea0c614be5526ae94e2a  

```

#### Step 2. Unzip litellm helm​
Unzip the specific version that was pulled in Step 1
```
tar -zxvf litellm-helm-0.1.2.tgz  

```

#### Step 3. Install litellm helm​
```
helm install lite-helm ./litellm-helm  

```

#### Step 4. Expose the service to localhost​
```
kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT  

```

Your LiteLLM Proxy Server is now running on `http://127.0.0.1:4000`.
### Deploy with Redis​
Use Redis when you need litellm to load balance across multiple litellm containers
The only change required is setting Redis on your `config.yaml` LiteLLM Proxy supports sharing rpm/tpm shared across multiple litellm instances, pass `redis_host`, `redis_password` and `redis_port` to enable this. (LiteLLM will use Redis to track rpm/tpm usage )
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/<your-deployment-name>  
   api_base: <your-azure-endpoint>  
   api_key: <your-azure-api-key>  
   rpm: 6   # Rate limit for this deployment: in requests per minute (rpm)  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/gpt-turbo-small-ca  
   api_base: https://my-endpoint-canada-berri992.openai.azure.com/  
   api_key: <your-azure-api-key>  
   rpm: 6  
router_settings:  
 redis_host: <your redis host>  
 redis_password: <your redis password>  
 redis_port: 1992  

```

Start docker container with config
```
docker run ghcr.io/berriai/litellm:main-latest --config your_config.yaml  

```

### Deploy with Database + Redis​
The only change required is setting Redis on your `config.yaml` LiteLLM Proxy supports sharing rpm/tpm shared across multiple litellm instances, pass `redis_host`, `redis_password` and `redis_port` to enable this. (LiteLLM will use Redis to track rpm/tpm usage )
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/<your-deployment-name>  
   api_base: <your-azure-endpoint>  
   api_key: <your-azure-api-key>  
   rpm: 6   # Rate limit for this deployment: in requests per minute (rpm)  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: azure/gpt-turbo-small-ca  
   api_base: https://my-endpoint-canada-berri992.openai.azure.com/  
   api_key: <your-azure-api-key>  
   rpm: 6  
router_settings:  
 redis_host: <your redis host>  
 redis_password: <your redis password>  
 redis_port: 1992  

```

Start `litellm-database`docker container with config
```
docker run --name litellm-proxy \  
-e DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname> \  
-p 4000:4000 \  
ghcr.io/berriai/litellm-database:main-latest --config your_config.yaml  

```

### (Non Root) - without Internet Connection​
By default `prisma generate` downloads prisma's engine binaries. This might cause errors when running without internet connection. 
Use this docker image to deploy litellm with pre-generated prisma binaries.
```
docker pull ghcr.io/berriai/litellm-non_root:main-stable  

```

Published Docker Image link
## Advanced Deployment Settings​
### 1. Custom server root path (Proxy base url)​
💥 Use this when you want to serve LiteLLM on a custom base url path like `https://localhost:4000/api/v1`
info
In a Kubernetes deployment, it's possible to utilize a shared DNS to host multiple applications by modifying the virtual service
Customize the root path to eliminate the need for employing multiple DNS configurations during deployment.
Step 1. 👉 Set `SERVER_ROOT_PATH` in your .env and this will be set as your server root path
```
export SERVER_ROOT_PATH="/api/v1"  

```

**Step 2** (If you want the Proxy Admin UI to work with your root path you need to use this dockerfile)
  * Use the dockerfile below (it uses litellm as a base image)
  * 👉 Set `UI_BASE_PATH=$SERVER_ROOT_PATH/ui` in the Dockerfile, example `UI_BASE_PATH=/api/v1/ui`


Dockerfile
```
# Use the provided base image  
FROM ghcr.io/berriai/litellm:main-latest  
  
# Set the working directory to /app  
WORKDIR /app  
  
# Install Node.js and npm (adjust version as needed)  
RUN apt-get update && apt-get install -y nodejs npm  
  
# Copy the UI source into the container  
COPY ./ui/litellm-dashboard /app/ui/litellm-dashboard  
  
# Set an environment variable for UI_BASE_PATH  
# This can be overridden at build time  
# set UI_BASE_PATH to "<your server root path>/ui"  
# 👇👇 Enter your UI_BASE_PATH here  
ENV UI_BASE_PATH="/api/v1/ui"   
  
# Build the UI with the specified UI_BASE_PATH  
WORKDIR /app/ui/litellm-dashboard  
RUN npm install  
RUN UI_BASE_PATH=$UI_BASE_PATH npm run build  
  
# Create the destination directory  
RUN mkdir -p /app/litellm/proxy/_experimental/out  
  
# Move the built files to the appropriate location  
# Assuming the build output is in ./out directory  
RUN rm -rf /app/litellm/proxy/_experimental/out/* && \  
  mv ./out/* /app/litellm/proxy/_experimental/out/  
  
# Switch back to the main app directory  
WORKDIR /app  
  
# Make sure your entrypoint.sh is executable  
RUN chmod +x ./docker/entrypoint.sh  
  
# Expose the necessary port  
EXPOSE 4000/tcp  
  
# Override the CMD instruction with your desired command and arguments  
# only use --detailed_debug for debugging  
CMD ["--port", "4000", "--config", "config.yaml"]  

```

**Step 3** build this Dockerfile
```
docker build -f Dockerfile -t litellm-prod-build . --progress=plain  

```

**Step 4. Run Proxy with`SERVER_ROOT_PATH` set in your env **
```
docker run \  
  -v $(pwd)/proxy_config.yaml:/app/config.yaml \  
  -p 4000:4000 \  
  -e LITELLM_LOG="DEBUG"\  
  -e SERVER_ROOT_PATH="/api/v1"\  
  -e DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname> \  
  -e LITELLM_MASTER_KEY="sk-1234"\  
  litellm-prod-build \  
  --config /app/config.yaml  

```

After running the proxy you can access it on `http://0.0.0.0:4000/api/v1/` (since we set `SERVER_ROOT_PATH="/api/v1"`)
**Step 5. Verify Running on correct path**
**That's it** , that's all you need to run the proxy on a custom root path
### 2. SSL Certification​
Use this, If you need to set ssl certificates for your on prem litellm proxy
Pass `ssl_keyfile_path` (Path to the SSL keyfile) and `ssl_certfile_path` (Path to the SSL certfile) when starting litellm proxy 
```
docker run ghcr.io/berriai/litellm:main-latest \  
  --ssl_keyfile_path ssl_test/keyfile.key \  
  --ssl_certfile_path ssl_test/certfile.crt  

```

Provide an ssl certificate when starting litellm proxy server 
### 3. Http/2 with Hypercorn​
Use this if you want to run the proxy with hypercorn to support http/2
Step 1. Build your custom docker image with hypercorn
```
# Use the provided base image  
FROM ghcr.io/berriai/litellm:main-latest  
  
# Set the working directory to /app  
WORKDIR /app  
  
# Copy the configuration file into the container at /app  
COPY config.yaml .  
  
# Make sure your docker/entrypoint.sh is executable  
RUN chmod +x ./docker/entrypoint.sh  
  
# Expose the necessary port  
EXPOSE 4000/tcp  
  
# 👉 Key Change: Install hypercorn  
RUN pip install hypercorn  
  
# Override the CMD instruction with your desired command and arguments  
# WARNING: FOR PROD DO NOT USE `--detailed_debug` it slows down response times, instead use the following CMD  
# CMD ["--port", "4000", "--config", "config.yaml"]  
  
CMD ["--port", "4000", "--config", "config.yaml", "--detailed_debug"]  

```

Step 2. Pass the `--run_hypercorn` flag when starting the proxy
```
docker run \  
  -v $(pwd)/proxy_config.yaml:/app/config.yaml \  
  -p 4000:4000 \  
  -e LITELLM_LOG="DEBUG"\  
  -e SERVER_ROOT_PATH="/api/v1"\  
  -e DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname> \  
  -e LITELLM_MASTER_KEY="sk-1234"\  
  your_custom_docker_image \  
  --config /app/config.yaml  
  --run_hypercorn  

```

### 4. config.yaml file on s3, GCS Bucket Object/url​
Use this if you cannot mount a config file on your deployment service (example - AWS Fargate, Railway etc)
LiteLLM Proxy will read your config.yaml from an s3 Bucket or GCS Bucket 
  * GCS Bucket
  * s3


Set the following .env vars 
```
LITELLM_CONFIG_BUCKET_TYPE = "gcs"               # set this to "gcs"       
LITELLM_CONFIG_BUCKET_NAME = "litellm-proxy"          # your bucket name on GCS  
LITELLM_CONFIG_BUCKET_OBJECT_KEY = "proxy_config.yaml"     # object key on GCS  

```

Start litellm proxy with these env vars - litellm will read your config from GCS 
```
docker run --name litellm-proxy \  
  -e DATABASE_URL=<database_url> \  
  -e LITELLM_CONFIG_BUCKET_NAME=<bucket_name> \  
  -e LITELLM_CONFIG_BUCKET_OBJECT_KEY="<object_key>> \  
  -e LITELLM_CONFIG_BUCKET_TYPE="gcs" \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm-database:main-latest --detailed_debug  

```

Set the following .env vars 
```
LITELLM_CONFIG_BUCKET_NAME = "litellm-proxy"          # your bucket name on s3   
LITELLM_CONFIG_BUCKET_OBJECT_KEY = "litellm_proxy_config.yaml" # object key on s3  

```

Start litellm proxy with these env vars - litellm will read your config from s3 
```
docker run --name litellm-proxy \  
  -e DATABASE_URL=<database_url> \  
  -e LITELLM_CONFIG_BUCKET_NAME=<bucket_name> \  
  -e LITELLM_CONFIG_BUCKET_OBJECT_KEY="<object_key>> \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm-database:main-latest  

```

## Platform-specific Guide​
  * AWS EKS - Kubernetes
  * AWS Cloud Formation Stack
  * Google Cloud Run
  * Render deploy
  * Railway


### Kubernetes (AWS EKS)​
Step1. Create an EKS Cluster with the following spec
```
eksctl create cluster --name=litellm-cluster --region=us-west-2 --node-type=t2.small  

```

Step 2. Mount litellm proxy config on kub cluster 
This will mount your local file called `proxy_config.yaml` on kubernetes cluster
```
kubectl create configmap litellm-config --from-file=proxy_config.yaml  

```

Step 3. Apply `kub.yaml` and `service.yaml` Clone the following `kub.yaml` and `service.yaml` files and apply locally
  * Use this `kub.yaml` file - litellm kub.yaml
  * Use this `service.yaml` file - litellm service.yaml


Apply `kub.yaml`
```
kubectl apply -f kub.yaml  

```

Apply `service.yaml` - creates an AWS load balancer to expose the proxy
```
kubectl apply -f service.yaml  
  
# service/litellm-service created  

```

Step 4. Get Proxy Base URL
```
kubectl get services  
  
# litellm-service  LoadBalancer  10.100.6.31  a472dc7c273fd47fd******.us-west-2.elb.amazonaws.com  4000:30374/TCP  63m  

```

Proxy Base URL = `a472dc7c273fd47fd******.us-west-2.elb.amazonaws.com:4000`
That's it, now you can start using LiteLLM Proxy
### AWS Cloud Formation Stack​
LiteLLM AWS Cloudformation Stack - **Get the best LiteLLM AutoScaling Policy and Provision the DB for LiteLLM Proxy**
This will provision:
  * LiteLLMServer - EC2 Instance
  * LiteLLMServerAutoScalingGroup
  * LiteLLMServerScalingPolicy (autoscaling policy)
  * LiteLLMDB - RDS::DBInstance


#### Using AWS Cloud Formation Stack​
**LiteLLM Cloudformation stack is locatedhere - litellm.yaml**
#### 1. Create the CloudFormation Stack:​
In the AWS Management Console, navigate to the CloudFormation service, and click on "Create Stack."
On the "Create Stack" page, select "Upload a template file" and choose the litellm.yaml file 
Now monitor the stack was created successfully. 
#### 2. Get the Database URL:​
Once the stack is created, get the DatabaseURL of the Database resource, copy this value 
#### 3. Connect to the EC2 Instance and deploy litellm on the EC2 container​
From the EC2 console, connect to the instance created by the stack (e.g., using SSH).
Run the following command, replacing `<database_url>` with the value you copied in step 2
```
docker run --name litellm-proxy \  
  -e DATABASE_URL=<database_url> \  
  -p 4000:4000 \  
  ghcr.io/berriai/litellm-database:main-latest  

```

#### 4. Access the Application:​
Once the container is running, you can access the application by going to `http://<ec2-public-ip>:4000` in your browser.
### Google Cloud Run​
  1. Fork this repo - github.com/BerriAI/example_litellm_gcp_cloud_run
  2. Edit the `litellm_config.yaml` file in the repo to include your model settings 
  3. Deploy your forked github repo on Google Cloud Run


#### Testing your deployed proxy​
**Assuming the required keys are set as Environment Variables**
https://litellm-7yjrj3ha2q-uc.a.run.app is our example proxy, substitute it with your deployed cloud run app
```
curl https://litellm-7yjrj3ha2q-uc.a.run.app/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -d '{  
   "model": "gpt-3.5-turbo",  
   "messages": [{"role": "user", "content": "Say this is a test!"}],  
   "temperature": 0.7  
  }'  

```

### Render​
https://render.com/
### Railway​
https://railway.app
**Step 1: Click the button** to deploy to Railway
![Deploy on Railway](https://railway.app/button.svg)
**Step 2:** Set `PORT` = 4000 on Railway Environment Variables
## Extras​
### Docker compose​
**Step 1**
  * (Recommended) Use the example file `docker-compose.yml` given in the project root. e.g. https://github.com/BerriAI/litellm/blob/main/docker-compose.yml


Here's an example `docker-compose.yml` file
```
version: "3.9"  
services:  
 litellm:  
  build:  
   context: .  
   args:  
    target: runtime  
  image: ghcr.io/berriai/litellm:main-latest  
  ports:  
   - "4000:4000" # Map the container port to the host, change the host port if necessary  
  volumes:  
   - ./litellm-config.yaml:/app/config.yaml # Mount the local configuration file  
  # You can change the port or number of workers as per your requirements or pass any new supported CLI argument. Make sure the port passed here matches with the container port defined above in `ports` value  
  command: [ "--config", "/app/config.yaml", "--port", "4000", "--num_workers", "8" ]  
  
# ...rest of your docker-compose config if any  

```

**Step 2**
Create a `litellm-config.yaml` file with your LiteLLM config relative to your `docker-compose.yml` file.
Check the config doc here
**Step 3**
Run the command `docker-compose up` or `docker compose up` as per your docker installation.
> Use `-d` flag to run the container in detached mode (background) e.g. `docker compose up -d`
Your LiteLLM container should be running now on the defined port e.g. `4000`.
### IAM-based Auth for RDS DB​
  1. Set AWS env var 


```
export AWS_WEB_IDENTITY_TOKEN='/path/to/token'  
export AWS_ROLE_NAME='arn:aws:iam::123456789012:role/MyRole'  
export AWS_SESSION_NAME='MySession'  

```

**See all Auth options**
  1. Add RDS credentials to env


```
export DATABASE_USER="db-user"  
export DATABASE_PORT="5432"  
export DATABASE_HOST="database-1-instance-1.cs1ksmwz2xt3.us-west-2.rds.amazonaws.com"  
export DATABASE_NAME="database-1-instance-1"  
export DATABASE_SCHEMA="schema-name" # skip to use the default "public" schema  

```

  1. Run proxy with iam+rds


```
litellm --config /path/to/config.yaml --iam_token_db_auth  

```

Previous
All settings
Next
⚡ Best Practices for Production
  * Quick Start
    * Docker Run
    * Docker Run - CLI Args
    * Use litellm as a base image
    * Build from litellm `pip` package
    * Terraform
    * Kubernetes
    * Helm Chart
  * Deployment Options
    * Deploy with Database
    * Deploy with Redis
    * Deploy with Database + Redis
    * (Non Root) - without Internet Connection
  * Advanced Deployment Settings
    * 1. Custom server root path (Proxy base url)
    * 2. SSL Certification
    * 3. Http/2 with Hypercorn
    * 4. config.yaml file on s3, GCS Bucket Object/url
  * Platform-specific Guide
    * Kubernetes (AWS EKS)
    * AWS Cloud Formation Stack
    * Google Cloud Run
    * Render
    * Railway
  * Extras
    * Docker compose
    * IAM-based Auth for RDS DB


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
      * Virtual Keys
      * OIDC - JWT-based Auth
      * [Beta] Service Accounts
      * Role-based Access Controls (RBAC)
      * Custom Auth
      * IP Address Filtering
      * Email Notifications
      * Attribute Management changes to Users
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Authentication
  * Email Notifications


On this page
# Email Notifications
Send an Email to your users when:
  * A Proxy API Key is created for them 
  * Their API Key crosses it's Budget 
  * All Team members of a LiteLLM Team -> when the team crosses it's budget


## Quick Start​
Get SMTP credentials to set this up Add the following to your proxy env
```
SMTP_HOST="smtp.resend.com"  
SMTP_USERNAME="resend"  
SMTP_PASSWORD="*******"  
SMTP_SENDER_EMAIL="support@alerts.litellm.ai" # email to send alerts from: `support@alerts.litellm.ai`  

```

Add `email` to your proxy config.yaml under `general_settings`
```
general_settings:  
 master_key: sk-1234  
 alerting: ["email"]  

```

That's it ! start your proxy
## Customizing Email Branding​
info
Customizing Email Branding is an Enterprise Feature Get in touch with us for a Free Trial
LiteLLM allows you to customize the:
  * Logo on the Email
  * Email support contact 


Set the following in your env to customize your emails
```
EMAIL_LOGO_URL="https://litellm-listing.s3.amazonaws.com/litellm_logo.png" # public url to your logo  
EMAIL_SUPPORT_CONTACT="support@berri.ai"                  # Your company support email  

```

Previous
IP Address Filtering
Next
Attribute Management changes to Users
  * Quick Start
  * Customizing Email Branding


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * ✨ Enterprise Features


On this page
# ✨ Enterprise Features
tip
To get a license, get in touch with us here
Features: 
  * **Security**
    * ✅ SSO for Admin UI
    * ✅ Audit Logs with retention policy
    * ✅ JWT-Auth
    * ✅ Control available public, private routes (Restrict certain endpoints on proxy)
    * ✅ Control available public, private routes
    * ✅ Secret Managers - AWS Key Manager, Google Secret Manager, Azure Key, Hashicorp Vault
    * ✅ [BETA] AWS Key Manager v2 - Key Decryption
    * ✅ IP address‑based access control lists
    * ✅ Track Request IP Address
    * ✅ Use LiteLLM keys/authentication on Pass Through Endpoints
    * ✅ Set Max Request Size / File Size on Requests
    * ✅ Enforce Required Params for LLM Requests (ex. Reject requests missing ["metadata"]["generation_name"])
    * ✅ Key Rotations
  * **Customize Logging, Guardrails, Caching per project**
    * ✅ Team Based Logging - Allow each team to use their own Langfuse Project / custom callbacks
    * ✅ Disable Logging for a Team - Switch off all logging for a team/project (GDPR Compliance)
  * **Spend Tracking & Data Exports**
    * ✅ Tracking Spend for Custom Tags
    * ✅ Set USD Budgets Spend for Custom Tags
    * ✅ Set Model budgets for Virtual Keys
    * ✅ Exporting LLM Logs to GCS Bucket, Azure Blob Storage
    * ✅ `/spend/report` API endpoint
  * **Prometheus Metrics**
    * ✅ Prometheus Metrics - Num Requests, failures, LLM Provider Outages
    * ✅ `x-ratelimit-remaining-requests`, `x-ratelimit-remaining-tokens` for LLM APIs on Prometheus
  * **Control Guardrails per API Key**
  * **Custom Branding**
    * ✅ Custom Branding + Routes on Swagger Docs
    * ✅ Public Model Hub
    * ✅ Custom Email Branding


## Audit Logs​
Store Audit logs for **Create, Update Delete Operations** done on `Teams` and `Virtual Keys`
**Step 1** Switch on audit Logs 
```
litellm_settings:  
 store_audit_logs: true  

```

Start the litellm proxy with this config
**Step 2** Test it - Create a Team
```
curl --location 'http://0.0.0.0:4000/team/new' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "max_budget": 2  
  }'  

```

**Step 3** Expected Log
```
{  
 "id": "e1760e10-4264-4499-82cd-c08c86c8d05b",  
 "updated_at": "2024-06-06T02:10:40.836420+00:00",  
 "changed_by": "109010464461339474872",  
 "action": "created",  
 "table_name": "LiteLLM_TeamTable",  
 "object_id": "82e725b5-053f-459d-9a52-867191635446",  
 "before_value": null,  
 "updated_values": {  
  "team_id": "82e725b5-053f-459d-9a52-867191635446",  
  "admins": [],  
  "members": [],  
  "members_with_roles": [  
   {  
    "role": "admin",  
    "user_id": "109010464461339474872"  
   }  
  ],  
  "max_budget": 2.0,  
  "models": [],  
  "blocked": false  
 }  
}  

```

## Tracking Spend for Custom Tags​
Requirements: 
  * Virtual Keys & a database should be set up, see virtual keys


#### Usage - /chat/completions requests with request tags​
  * Set on Key
  * Set on Team
  * OpenAI Python v1.0.0+
  * OpenAI JS
  * Curl Request
  * Langchain


```
curl -L -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "metadata": {  
    "tags": ["tag1", "tag2", "tag3"]  
  }  
}  
  
'  

```

```
curl -L -X POST 'http://0.0.0.0:4000/team/new' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "metadata": {  
    "tags": ["tag1", "tag2", "tag3"]  
  }  
}  
  
'  

```

Set `extra_body={"metadata": { }}` to `metadata` you want to pass
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
    "metadata": {  
      "tags": ["model-anthropic-claude-v2.1", "app-ishaan-prod"] # 👈 Key Change  
    }  
  }  
)  
  
print(response)  

```

```
const openai = require('openai');  
  
async function runOpenAI() {  
 const client = new openai.OpenAI({  
  apiKey: 'sk-1234',  
  baseURL: 'http://0.0.0.0:4000'  
 });  
  
 try {  
  const response = await client.chat.completions.create({  
   model: 'gpt-3.5-turbo',  
   messages: [  
    {  
     role: 'user',  
     content: "this is a test request, write a short poem"  
    },  
   ],  
   metadata: {  
    tags: ["model-anthropic-claude-v2.1", "app-ishaan-prod"] // 👈 Key Change  
   }  
  });  
  console.log(response);  
 } catch (error) {  
  console.log("got this exception from server");  
  console.error(error);  
 }  
}  
  
// Call the asynchronous function  
runOpenAI();  

```

Pass `metadata` as part of the request body
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "metadata": {"tags": ["model-anthropic-claude-v2.1", "app-ishaan-prod"]}  
}'  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "gpt-3.5-turbo",  
  temperature=0.1,  
  extra_body={  
    "metadata": {  
      "tags": ["model-anthropic-claude-v2.1", "app-ishaan-prod"]  
    }  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

#### Viewing Spend per tag​
#### `/spend/tags` Request Format​
```
curl -X GET "http://0.0.0.0:4000/spend/tags" \  
-H "Authorization: Bearer sk-1234"  

```

#### `/spend/tags`Response Format​
```
[  
 {  
  "individual_request_tag": "model-anthropic-claude-v2.1",  
  "log_count": 6,  
  "total_spend": 0.000672  
 },  
 {  
  "individual_request_tag": "app-ishaan-local",  
  "log_count": 4,  
  "total_spend": 0.000448  
 },  
 {  
  "individual_request_tag": "app-ishaan-prod",  
  "log_count": 2,  
  "total_spend": 0.000224  
 }  
]  
  

```

## Tracking Spend with custom metadata​
Requirements: 
  * Virtual Keys & a database should be set up, see virtual keys


#### Usage - /chat/completions requests with special spend logs metadata​
  * Set on Key
  * Set on Team
  * OpenAI Python v1.0.0+
  * OpenAI JS
  * Curl Request
  * Langchain


```
curl -L -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "metadata": {  
   "spend_logs_metadata": {  
     "hello": "world"  
   }  
  }  
}  
  
'  

```

```
curl -L -X POST 'http://0.0.0.0:4000/team/new' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "metadata": {  
   "spend_logs_metadata": {  
     "hello": "world"  
   }  
  }  
}  
  
'  

```

Set `extra_body={"metadata": { }}` to `metadata` you want to pass
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
    "metadata": {  
      "spend_logs_metadata": {  
        "hello": "world"  
      }  
    }  
  }  
)  
  
print(response)  

```

```
const openai = require('openai');  
  
async function runOpenAI() {  
 const client = new openai.OpenAI({  
  apiKey: 'sk-1234',  
  baseURL: 'http://0.0.0.0:4000'  
 });  
  
 try {  
  const response = await client.chat.completions.create({  
   model: 'gpt-3.5-turbo',  
   messages: [  
    {  
     role: 'user',  
     content: "this is a test request, write a short poem"  
    },  
   ],  
   metadata: {  
    spend_logs_metadata: { // 👈 Key Change  
      hello: "world"  
    }  
   }  
  });  
  console.log(response);  
 } catch (error) {  
  console.log("got this exception from server");  
  console.error(error);  
 }  
}  
  
// Call the asynchronous function  
runOpenAI();  

```

Pass `metadata` as part of the request body
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "metadata": {  
    "spend_logs_metadata": {  
      "hello": "world"  
    }  
  }  
}'  

```

```
from langchain.chat_models import ChatOpenAI  
from langchain.prompts.chat import (  
  ChatPromptTemplate,  
  HumanMessagePromptTemplate,  
  SystemMessagePromptTemplate,  
)  
from langchain.schema import HumanMessage, SystemMessage  
  
chat = ChatOpenAI(  
  openai_api_base="http://0.0.0.0:4000",  
  model = "gpt-3.5-turbo",  
  temperature=0.1,  
  extra_body={  
    "metadata": {  
      "spend_logs_metadata": {  
        "hello": "world"  
      }  
    }  
  }  
)  
  
messages = [  
  SystemMessage(  
    content="You are a helpful assistant that im using to make a test request to."  
  ),  
  HumanMessage(  
    content="test from litellm. tell me why it's amazing in 1 sentence"  
  ),  
]  
response = chat(messages)  
  
print(response)  

```

#### Viewing Spend w/ custom metadata​
#### `/spend/logs` Request Format​
```
curl -X GET "http://0.0.0.0:4000/spend/logs?request_id=<your-call-id" \ # e.g.: chatcmpl-9ZKMURhVYSi9D6r6PJ9vLcayIK0Vm  
-H "Authorization: Bearer sk-1234"  

```

#### `/spend/logs` Response Format​
```
[  
  {  
    "request_id": "chatcmpl-9ZKMURhVYSi9D6r6PJ9vLcayIK0Vm",  
    "call_type": "acompletion",  
    "metadata": {  
      "user_api_key": "88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b",  
      "user_api_key_alias": null,  
      "spend_logs_metadata": { # 👈 LOGGED CUSTOM METADATA  
        "hello": "world"  
      },  
      "user_api_key_team_id": null,  
      "user_api_key_user_id": "116544810872468347480",  
      "user_api_key_team_alias": null  
    },  
  }  
]  

```

## Enforce Required Params for LLM Requests​
Use this when you want to enforce all requests to include certain params. Example you need all requests to include the `user` and `["metadata]["generation_name"]` params.
  * Set on Config
  * Set on Key


**Step 1** Define all Params you want to enforce on config.yaml
This means `["user"]` and `["metadata]["generation_name"]` are required in all LLM Requests to LiteLLM
```
general_settings:  
 master_key: sk-1234  
 enforced_params:   
  - user  
  - metadata.generation_name  

```

```
curl -L -X POST 'http://0.0.0.0:4000/key/generate' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-d '{  
  "enforced_params": ["user", "metadata.generation_name"]  
}'  

```

**Step 2 Verify if this works**
  * Invalid Request (No `user` passed)
  * Invalid Request (No `metadata` passed)
  * Valid Request


```
curl --location 'http://localhost:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-5fmYeaUEbAMpwBNT-QpxyA' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "hi"  
    }  
  ]  
}'  

```

Expected Response 
```
{"error":{"message":"Authentication Error, BadRequest please pass param=user in request body. This is a required param","type":"auth_error","param":"None","code":401}}%   

```

```
curl --location 'http://localhost:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-5fmYeaUEbAMpwBNT-QpxyA' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "user": "gm",  
  "messages": [  
    {  
    "role": "user",  
    "content": "hi"  
    }  
  ],  
  "metadata": {}  
}'  

```

Expected Response 
```
{"error":{"message":"Authentication Error, BadRequest please pass param=[metadata][generation_name] in request body. This is a required param","type":"auth_error","param":"None","code":401}}%   

```

```
curl --location 'http://localhost:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-5fmYeaUEbAMpwBNT-QpxyA' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "user": "gm",  
  "messages": [  
    {  
    "role": "user",  
    "content": "hi"  
    }  
  ],  
  "metadata": {"generation_name": "prod-app"}  
}'  

```

Expected Response
```
{"id":"chatcmpl-9XALnHqkCBMBKrOx7Abg0hURHqYtY","choices":[{"finish_reason":"stop","index":0,"message":{"content":"Hello! How can I assist you today?","role":"assistant"}}],"created":1717691639,"model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":null,"usage":{"completion_tokens":9,"prompt_tokens":8,"total_tokens":17}}%   

```

## Control available public, private routes​
**Restrict certain endpoints of proxy**
info
❓ Use this when you want to:
  * make an existing private route -> public
  * set certain routes as admin_only routes 


#### Usage - Define public, admin only routes​
**Step 1** - Set on config.yaml 
Route Type| Optional| Requires Virtual Key Auth| Admin Can Access| All Roles Can Access| Description  
---|---|---|---|---|---  
`public_routes`| ✅| ❌| ✅| ✅| Routes that can be accessed without any authentication  
`admin_only_routes`| ✅| ✅| ✅| ❌| Routes that can only be accessed by Proxy Admin  
`allowed_routes`| ✅| ✅| ✅| ✅| Routes are exposed on the proxy. If not set then all routes exposed.  
`LiteLLMRoutes.public_routes` is an ENUM corresponding to the default public routes on LiteLLM. You can see this here
```
general_settings:  
 master_key: sk-1234  
 public_routes: ["LiteLLMRoutes.public_routes", "/spend/calculate"]   # routes that can be accessed without any auth  
 admin_only_routes: ["/key/generate"] # Optional - routes that can only be accessed by Proxy Admin  
 allowed_routes: ["/chat/completions", "/spend/calculate", "LiteLLMRoutes.public_routes"] # Optional - routes that can be accessed by anyone after Authentication  

```

**Step 2** - start proxy 
```
litellm --config config.yaml  

```

**Step 3** - Test it 
  * Test `public_routes`
  * Test `admin_only_routes`
  * Test `allowed_routes`


```
curl --request POST \  
 --url 'http://localhost:4000/spend/calculate' \  
 --header 'Content-Type: application/json' \  
 --data '{  
  "model": "gpt-4",  
  "messages": [{"role": "user", "content": "Hey, how'\''s it going?"}]  
 }'  

```

🎉 Expect this endpoint to work without an `Authorization / Bearer Token`
**Successful Request**
```
curl --location 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <your-master-key>' \  
--header 'Content-Type: application/json' \  
--data '{}'  

```

**Un-successfull Request**
```
 curl --location 'http://0.0.0.0:4000/key/generate' \  
--header 'Authorization: Bearer <virtual-key-from-non-admin>' \  
--header 'Content-Type: application/json' \  
--data '{"user_role": "internal_user"}'  

```

**Expected Response**
```
{  
 "error": {  
  "message": "user not allowed to access this route. Route=/key/generate is an admin only route",  
  "type": "auth_error",  
  "param": "None",  
  "code": "403"  
 }  
}  

```

**Successful Request**
```
curl http://localhost:4000/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer sk-1234" \  
-d '{  
"model": "fake-openai-endpoint",  
"messages": [  
  {"role": "user", "content": "Hello, Claude"}  
]  
}'  

```

**Un-successfull Request**
```
curl --location 'http://0.0.0.0:4000/embeddings' \  
--header 'Content-Type: application/json' \  
-H "Authorization: Bearer sk-1234" \  
--data ' {  
"model": "text-embedding-ada-002",  
"input": ["write a litellm poem"]  
}'  

```

**Expected Response**
```
{  
 "error": {  
  "message": "Route /embeddings not allowed",  
  "type": "auth_error",  
  "param": "None",  
  "code": "403"  
 }  
}  

```

## Guardrails - Secret Detection/Redaction​
❓ Use this to REDACT API Keys, Secrets sent in requests to an LLM. 
Example if you want to redact the value of `OPENAI_API_KEY` in the following request
#### Incoming Request​
```
{  
  "messages": [  
    {  
      "role": "user",  
      "content": "Hey, how's it going, API_KEY = 'sk_1234567890abcdef'",  
    }  
  ]  
}  

```

#### Request after Moderation​
```
{  
  "messages": [  
    {  
      "role": "user",  
      "content": "Hey, how's it going, API_KEY = '[REDACTED]'",  
    }  
  ]  
}  

```

**Usage**
**Step 1** Add this to your config.yaml 
```
litellm_settings:  
 callbacks: ["hide_secrets"]  

```

**Step 2** Run litellm proxy with `--detailed_debug` to see the server logs
```
litellm --config config.yaml --detailed_debug  

```

**Step 3** Test it with request
Send this request
```
curl --location 'http://localhost:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "llama3",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what is the value of my open ai key? openai_api_key=sk-1234998222"  
    }  
  ]  
}'  

```

Expect to see the following warning on your litellm server logs
```
LiteLLM Proxy:WARNING: secret_detection.py:88 - Detected and redacted secrets in message: ['Secret Keyword']  

```

You can also see the raw request sent from litellm to the API Provider
```
POST Request Sent from LiteLLM:  
curl -X POST \  
https://api.groq.com/openai/v1/ \  
-H 'Authorization: Bearer gsk_mySVchjY********************************************' \  
-d {  
 "model": "llama3-8b-8192",  
 "messages": [  
  {  
   "role": "user",  
   "content": "what is the time today, openai_api_key=[REDACTED]"  
  }  
 ],  
 "stream": false,  
 "extra_body": {}  
}  

```

### Secret Detection On/Off per API Key​
❓ Use this when you need to switch guardrails on/off per API Key
**Step 1** Create Key with `hide_secrets` Off 
👉 Set `"permissions": {"hide_secrets": false}` with either `/key/generate` or `/key/update`
This means the `hide_secrets` guardrail is off for all requests from this API Key
  * /key/generate
  * /key/update


```
curl --location 'http://0.0.0.0:4000/key/generate' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "permissions": {"hide_secrets": false}  
}'  

```

```
# {"permissions":{"hide_secrets":false},"key":"sk-jNm1Zar7XfNdZXp49Z1kSQ"}   

```

```
curl --location 'http://0.0.0.0:4000/key/update' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "key": "sk-jNm1Zar7XfNdZXp49Z1kSQ",  
    "permissions": {"hide_secrets": false}  
}'  

```

```
# {"permissions":{"hide_secrets":false},"key":"sk-jNm1Zar7XfNdZXp49Z1kSQ"}   

```

**Step 2** Test it with new key
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-jNm1Zar7XfNdZXp49Z1kSQ' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "llama3",  
  "messages": [  
    {  
    "role": "user",  
    "content": "does my openai key look well formatted OpenAI_API_KEY=sk-1234777"  
    }  
  ]  
}'  

```

Expect to see `sk-1234777` in your server logs on your callback. 
info
The `hide_secrets` guardrail check did not run on this request because api key=sk-jNm1Zar7XfNdZXp49Z1kSQ has `"permissions": {"hide_secrets": false}`
## Content Moderation​
### Content Moderation with LLM Guard​
Set the LLM Guard API Base in your environment 
```
LLM_GUARD_API_BASE = "http://0.0.0.0:8192" # deployed llm guard api  

```

Add `llmguard_moderations` as a callback 
```
litellm_settings:  
  callbacks: ["llmguard_moderations"]  

```

Now you can easily test it
  * Make a regular /chat/completion call 
  * Check your proxy logs for any statement with `LLM Guard:`


Expected results: 
```
LLM Guard: Received response - {"sanitized_prompt": "hello world", "is_valid": true, "scanners": { "Regex": 0.0 }}  

```

#### Turn on/off per key​
**1. Update config**
```
litellm_settings:  
  callbacks: ["llmguard_moderations"]  
  llm_guard_mode: "key-specific"  

```

**2. Create new key**
```
curl --location 'http://localhost:4000/key/generate' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{  
  "models": ["fake-openai-endpoint"],  
  "permissions": {  
    "enable_llm_guard_check": true # 👈 KEY CHANGE  
  }  
}'  
  
# Returns {..'key': 'my-new-key'}  

```

**3. Test it!**
```
curl --location 'http://0.0.0.0:4000/v1/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer my-new-key' \ # 👈 TEST KEY  
--data '{"model": "fake-openai-endpoint", "messages": [  
    {"role": "system", "content": "Be helpful"},  
    {"role": "user", "content": "What do you know?"}  
  ]  
  }'  

```

#### Turn on/off per request​
**1. Update config**
```
litellm_settings:  
  callbacks: ["llmguard_moderations"]  
  llm_guard_mode: "request-specific"  

```

**2. Create new key**
```
curl --location 'http://localhost:4000/key/generate' \  
--header 'Authorization: Bearer sk-1234' \  
--header 'Content-Type: application/json' \  
--data '{  
  "models": ["fake-openai-endpoint"],  
}'  
  
# Returns {..'key': 'my-new-key'}  

```

**3. Test it!**
  * OpenAI Python v1.0.0+
  * Curl Request


```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={ # pass in any provider-specific param, if not supported by openai, https://docs.litellm.ai/docs/completion/input#provider-specific-params  
    "metadata": {  
      "permissions": {  
        "enable_llm_guard_check": True # 👈 KEY CHANGE  
      },  
    }  
  }  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/v1/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer my-new-key' \ # 👈 TEST KEY  
--data '{"model": "fake-openai-endpoint", "messages": [  
    {"role": "system", "content": "Be helpful"},  
    {"role": "user", "content": "What do you know?"}  
  ]  
  }'  

```

### Content Moderation with LlamaGuard​
Currently works with Sagemaker's LlamaGuard endpoint. 
How to enable this in your config.yaml: 
```
litellm_settings:  
  callbacks: ["llamaguard_moderations"]  
  llamaguard_model_name: "sagemaker/jumpstart-dft-meta-textgeneration-llama-guard-7b"  

```

Make sure you have the relevant keys in your environment, eg.: 
```
os.environ["AWS_ACCESS_KEY_ID"] = ""  
os.environ["AWS_SECRET_ACCESS_KEY"] = ""  
os.environ["AWS_REGION_NAME"] = ""  

```

#### Customize LlamaGuard prompt​
To modify the unsafe categories llama guard evaluates against, just create your own version of this category list
Point your proxy to it
```
callbacks: ["llamaguard_moderations"]  
 llamaguard_model_name: "sagemaker/jumpstart-dft-meta-textgeneration-llama-guard-7b"  
 llamaguard_unsafe_content_categories: /path/to/llamaguard_prompt.txt  

```

### Content Moderation with Google Text Moderation​
Requires your GOOGLE_APPLICATION_CREDENTIALS to be set in your .env (same as VertexAI).
How to enable this in your config.yaml: 
```
litellm_settings:  
  callbacks: ["google_text_moderation"]  

```

#### Set custom confidence thresholds​
Google Moderations checks the test against several categories. Source
#### Set global default confidence threshold​
By default this is set to 0.8. But you can override this in your config.yaml.
```
litellm_settings:   
  google_moderation_confidence_threshold: 0.4   

```

#### Set category-specific confidence threshold​
Set a category specific confidence threshold in your config.yaml. If none set, the global default will be used. 
```
litellm_settings:   
  toxic_confidence_threshold: 0.1  

```

Here are the category specific values: 
Category| Setting  
---|---  
"toxic"| toxic_confidence_threshold: 0.1  
"insult"| insult_confidence_threshold: 0.1  
"profanity"| profanity_confidence_threshold: 0.1  
"derogatory"| derogatory_confidence_threshold: 0.1  
"sexual"| sexual_confidence_threshold: 0.1  
"death_harm_and_tragedy"| death_harm_and_tragedy_threshold: 0.1  
"violent"| violent_threshold: 0.1  
"firearms_and_weapons"| firearms_and_weapons_threshold: 0.1  
"public_safety"| public_safety_threshold: 0.1  
"health"| health_threshold: 0.1  
"religion_and_belief"| religion_and_belief_threshold: 0.1  
"illicit_drugs"| illicit_drugs_threshold: 0.1  
"war_and_conflict"| war_and_conflict_threshold: 0.1  
"politics"| politics_threshold: 0.1  
"finance"| finance_threshold: 0.1  
"legal"| legal_threshold: 0.1  
## Swagger Docs - Custom Routes + Branding​
info
Requires a LiteLLM Enterprise key to use. Get a free 2-week license here
Set LiteLLM Key in your environment
```
LITELLM_LICENSE=""  

```

#### Customize Title + Description​
In your environment, set: 
```
DOCS_TITLE="TotalGPT"  
DOCS_DESCRIPTION="Sample Company Description"  

```

#### Customize Routes​
Hide admin routes from users. 
In your environment, set: 
```
DOCS_FILTERED="True" # only shows openai routes to user  

```

## Enable Blocked User Lists​
If any call is made to proxy with this user id, it'll be rejected - use this if you want to let users opt-out of ai features 
```
litellm_settings:   
   callbacks: ["blocked_user_check"]   
   blocked_user_list: ["user_id_1", "user_id_2", ...] # can also be a .txt filepath e.g. `/relative/path/blocked_list.txt`   

```

### How to test​
  * OpenAI Python v1.0.0+
  * Curl Request


Set `user=<user_id>` to the user id of the user who might have opted out.
```
import openai  
client = openai.OpenAI(  
  api_key="sk-1234",  
  base_url="http://0.0.0.0:4000"  
)  
  
# request sent to model set on litellm proxy, `litellm --model`  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  user="user_id_1"  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "what llm are you"  
    }  
   ],  
   "user": "user_id_1" # this is also an openai supported param   
  }  
'  

```

info
Suggest a way to improve this
### Using via API​
**Block all calls for a customer id**
```
curl -X POST "http://0.0.0.0:4000/customer/block" \  
-H "Authorization: Bearer sk-1234" \   
-D '{  
"user_ids": [<user_id>, ...]   
}'  

```

**Unblock calls for a user id**
```
curl -X POST "http://0.0.0.0:4000/user/unblock" \  
-H "Authorization: Bearer sk-1234" \   
-D '{  
"user_ids": [<user_id>, ...]   
}'  

```

## Enable Banned Keywords List​
```
litellm_settings:   
   callbacks: ["banned_keywords"]  
   banned_keywords_list: ["hello"] # can also be a .txt file - e.g.: `/relative/path/keywords.txt`  

```

### Test this​
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--data ' {  
   "model": "gpt-3.5-turbo",  
   "messages": [  
    {  
     "role": "user",  
     "content": "Hello world!"  
    }  
   ]  
  }  
'  

```

## Public Model Hub​
Share a public page of available models for users
## [BETA] AWS Key Manager - Key Decryption​
This is a beta feature, and subject to changes.
**Step 1.** Add `USE_AWS_KMS` to env
```
USE_AWS_KMS="True"  

```

**Step 2.** Add `LITELLM_SECRET_AWS_KMS_` to encrypted keys in env 
```
LITELLM_SECRET_AWS_KMS_DATABASE_URL="AQICAH.."  

```

LiteLLM will find this and use the decrypted `DATABASE_URL="postgres://.."` value in runtime.
**Step 3.** Start proxy 
```
$ litellm  

```

How it works? 
  * Key Decryption runs before server starts up. **Code**
  * It adds the decrypted value to the `os.environ` for the python process. 


**Note:** Setting an environment variable within a Python script using os.environ will not make that variable accessible via SSH sessions or any other new processes that are started independently of the Python script. Environment variables set this way only affect the current process and its child processes.
## Set Max Request / Response Size on LiteLLM Proxy​
Use this if you want to set a maximum request / response size for your proxy server. If a request size is above the size it gets rejected + slack alert triggered
#### Usage​
**Step 1.** Set `max_request_size_mb` and `max_response_size_mb`
For this example we set a very low limit on `max_request_size_mb` and expect it to get rejected 
info
In production we recommend setting a `max_request_size_mb` / `max_response_size_mb` around `32 MB`
```
model_list:  
 - model_name: fake-openai-endpoint  
  litellm_params:  
   model: openai/fake  
   api_key: fake-key  
   api_base: https://exampleopenaiendpoint-production.up.railway.app/  
general_settings:   
 master_key: sk-1234  
  
 # Security controls  
 max_request_size_mb: 0.000000001 # 👈 Key Change - Max Request Size in MB. Set this very low for testing   
 max_response_size_mb: 100 # 👈 Key Change - Max Response Size in MB  

```

**Step 2.** Test it with `/chat/completions` request
```
curl http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-1234" \  
 -d '{  
  "model": "fake-openai-endpoint",  
  "messages": [  
   {"role": "user", "content": "Hello, Claude!"}  
  ]  
 }'  

```

**Expected Response from request** We expect this to fail since the request size is over `max_request_size_mb`
```
{"error":{"message":"Request size is too large. Request size is 0.0001125335693359375 MB. Max size is 1e-09 MB","type":"bad_request_error","param":"content-length","code":400}}  

```

Previous
Image URL Handling
Next
Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples
  * Audit Logs
  * Tracking Spend for Custom Tags
  * Tracking Spend with custom metadata
  * Enforce Required Params for LLM Requests
  * Control available public, private routes
  * Guardrails - Secret Detection/Redaction
    * Secret Detection On/Off per API Key
  * Content Moderation
    * Content Moderation with LLM Guard
    * Content Moderation with LlamaGuard
    * Content Moderation with Google Text Moderation
  * Swagger Docs - Custom Routes + Branding
  * Enable Blocked User Lists
    * How to test
    * Using via API
  * Enable Banned Keywords List
    * Test this
  * Public Model Hub
  * BETA AWS Key Manager - Key Decryption
  * Set Max Request / Response Size on LiteLLM Proxy


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
      * Guardrails - Quick Start
      * Aim Security
      * Aporia
      * Bedrock
      * Custom Guardrail
      * Guardrails.ai
      * Lakera AI
      * PII Masking - Presidio
      * In-memory Prompt Injection Detection
      * ✨ Secret Detection/Redaction (Enterprise-only)
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * [Beta] Guardrails
  * Aim Security


On this page
# Aim Security
## Quick Start​
### 1. Create a new Aim Guard​
Go to Aim Application and create a new guard.
When prompted, select API option, and name your guard.
note
In case you want to host your guard on-premise, you can enable this option by installing Aim Outpost prior to creating the guard.
### 2. Configure your Aim Guard policies​
In the newly created guard's page, you can find a reference to the prompt policy center of this guard.
You can decide which detections will be enabled, and set the threshold for each detection.
info
When using LiteLLM with virtual keys, key-specific policies can be set directly in Aim's guards page by specifying the virtual key alias when creating the guard.
Only the aliases of your virtual keys (and not the actual key secrets) will be sent to Aim.
### 3. Add Aim Guardrail on your LiteLLM config.yaml​
Define your guardrails under the `guardrails` section
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: openai/gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  
  
guardrails:  
 - guardrail_name: aim-protected-app  
  litellm_params:  
   guardrail: aim  
   mode: [pre_call, post_call] # "During_call" is also available  
   api_key: os.environ/AIM_API_KEY  
   api_base: os.environ/AIM_API_BASE # Optional, use only when using a self-hosted Aim Outpost  

```

Under the `api_key`, insert the API key you were issued. The key can be found in the guard's page. You can also set `AIM_API_KEY` as an environment variable.
By default, the `api_base` is set to `https://api.aim.security`. If you are using a self-hosted Aim Outpost, you can set the `api_base` to your Outpost's URL.
### 4. Start LiteLLM Gateway​
```
litellm --config config.yaml  

```

### 5. Make your first request​
note
The following example depends on enabling _PII_ detection in your guard. You can adjust the request content to match different guard's policies.
  * Successfully blocked request
  * Successfully permitted request


note
When using LiteLLM with virtual keys, an `Authorization` header with the virtual key is required.
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi my email is ishaan@berri.ai"}  
  ],  
  "guardrails": ["aim-protected-app"]  
 }'  

```

If configured correctly, since `ishaan@berri.ai` would be detected by the Aim Guard as PII, you'll receive a response similar to the following with a `400 Bad Request` status code:
```
{  
 "error": {  
  "message": "\"ishaan@berri.ai\" detected as email",  
  "type": "None",  
  "param": "None",  
  "code": "400"  
 }  
}  

```

note
When using LiteLLM with virtual keys, an `Authorization` header with the virtual key is required.
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi what is the weather"}  
  ],  
  "guardrails": ["aim-protected-app"]  
 }'  

```

The above request should not be blocked, and you should receive a regular LLM response (simplified for brevity):
```
{  
 "model": "gpt-3.5-turbo-0125",  
 "choices": [  
  {  
   "finish_reason": "stop",  
   "index": 0,  
   "message": {  
    "content": "I can’t provide live weather updates without the internet. Let me know if you’d like general weather trends for a location and season instead!",  
    "role": "assistant"  
   }  
  }  
 ]  
}  

```

## Advanced​
Aim Guard provides user-specific Guardrail policies, enabling you to apply tailored policies to individual users. To utilize this feature, include the end-user's email in the request payload by setting the `x-aim-user-email` header of your request.
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "x-aim-user-email: ishaan@berri.ai" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi what is the weather"}  
  ],  
  "guardrails": ["aim-protected-app"]  
 }'  

```

Previous
Guardrails - Quick Start
Next
Aporia
  * Quick Start
    * 1. Create a new Aim Guard
    * 2. Configure your Aim Guard policies
    * 3. Add Aim Guardrail on your LiteLLM config.yaml
    * 4. Start LiteLLM Gateway
    * 5. Make your first request
  * Advanced


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
      * Docker, Deployment
      * ⚡ Best Practices for Production
      * CLI Arguments
      * Release Cycle
      * Model Management
      * Health Checks
      * Debugging
      * Using at Scale (1M+ rows in DB)
      * Rotating Master Key
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * Setup & Deployment
  * Health Checks


On this page
# Health Checks
Use this to health check all LLMs defined in your config.yaml
## Summary​
The proxy exposes: 
  * a /health endpoint which returns the health of the LLM APIs 
  * a /health/readiness endpoint for returning if the proxy is ready to accept requests 
  * a /health/liveliness endpoint for returning if the proxy is alive 


## `/health`​
#### Request​
Make a GET Request to `/health` on the proxy 
info
**This endpoint makes an LLM API call to each model to check if it is healthy.**
```
curl --location 'http://0.0.0.0:4000/health' -H "Authorization: Bearer sk-1234"  

```

You can also run `litellm -health` it makes a `get` request to `http://0.0.0.0:4000/health` for you
```
litellm --health  

```

#### Response​
```
{  
  "healthy_endpoints": [  
    {  
      "model": "azure/gpt-35-turbo",  
      "api_base": "https://my-endpoint-canada-berri992.openai.azure.com/"  
    },  
    {  
      "model": "azure/gpt-35-turbo",  
      "api_base": "https://my-endpoint-europe-berri-992.openai.azure.com/"  
    }  
  ],  
  "unhealthy_endpoints": [  
    {  
      "model": "azure/gpt-35-turbo",  
      "api_base": "https://openai-france-1234.openai.azure.com/"  
    }  
  ]  
}  

```

### Embedding Models​
To run embedding health checks, specify the mode as "embedding" in your config for the relevant model.
```
model_list:  
 - model_name: azure-embedding-model  
  litellm_params:  
   model: azure/azure-embedding-model  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: "2023-07-01-preview"  
  model_info:  
   mode: embedding # 👈 ADD THIS  

```

### Image Generation Models​
To run image generation health checks, specify the mode as "image_generation" in your config for the relevant model.
```
model_list:  
 - model_name: dall-e-3  
  litellm_params:  
   model: azure/dall-e-3  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: "2023-07-01-preview"  
  model_info:  
   mode: image_generation # 👈 ADD THIS  

```

### Text Completion Models​
To run `/completions` health checks, specify the mode as "completion" in your config for the relevant model.
```
model_list:  
 - model_name: azure-text-completion  
  litellm_params:  
   model: azure/text-davinci-003  
   api_base: os.environ/AZURE_API_BASE  
   api_key: os.environ/AZURE_API_KEY  
   api_version: "2023-07-01-preview"  
  model_info:  
   mode: completion # 👈 ADD THIS  

```

### Speech to Text Models​
```
model_list:  
 - model_name: whisper  
  litellm_params:  
   model: whisper-1  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   mode: audio_transcription  

```

### Text to Speech Models​
```
# OpenAI Text to Speech Models  
 - model_name: tts  
  litellm_params:  
   model: openai/tts-1  
   api_key: "os.environ/OPENAI_API_KEY"  
  model_info:  
   mode: audio_speech  

```

### Rerank Models​
To run rerank health checks, specify the mode as "rerank" in your config for the relevant model.
```
model_list:  
 - model_name: rerank-english-v3.0  
  litellm_params:  
   model: cohere/rerank-english-v3.0  
   api_key: os.environ/COHERE_API_KEY  
  model_info:  
   mode: rerank  

```

### Batch Models (Azure Only)​
For Azure models deployed as 'batch' models, set `mode: batch`. 
```
model_list:  
 - model_name: "batch-gpt-4o-mini"  
  litellm_params:  
   model: "azure/batch-gpt-4o-mini"  
   api_key: os.environ/AZURE_API_KEY  
   api_base: os.environ/AZURE_API_BASE  
  model_info:  
   mode: batch  

```

Expected Response 
```
{  
  "healthy_endpoints": [  
    {  
      "api_base": "https://...",  
      "model": "azure/gpt-4o-mini",  
      "x-ms-region": "East US"  
    }  
  ],  
  "unhealthy_endpoints": [],  
  "healthy_count": 1,  
  "unhealthy_count": 0  
}  

```

### Realtime Models​
To run realtime health checks, specify the mode as "realtime" in your config for the relevant model.
```
model_list:  
 - model_name: openai/gpt-4o-realtime-audio  
  litellm_params:  
   model: openai/gpt-4o-realtime-audio  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   mode: realtime  

```

### Wildcard Routes​
For wildcard routes, you can specify a `health_check_model` in your config.yaml. This model will be used for health checks for that wildcard route.
In this example, when running a health check for `openai/*`, the health check will make a `/chat/completions` request to `openai/gpt-4o-mini`.
```
model_list:  
 - model_name: openai/*  
  litellm_params:  
   model: openai/*  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   health_check_model: openai/gpt-4o-mini  
 - model_name: anthropic/*  
  litellm_params:  
   model: anthropic/*  
   api_key: os.environ/ANTHROPIC_API_KEY  
  model_info:  
   health_check_model: anthropic/claude-3-5-sonnet-20240620  

```

## Background Health Checks​
You can enable model health checks being run in the background, to prevent each model from being queried too frequently via `/health`. 
info
**This makes an LLM API call to each model to check if it is healthy.**
Here's how to use it: 
  1. in the config.yaml add:


```
general_settings:   
 background_health_checks: True # enable background health checks  
 health_check_interval: 300 # frequency of background health checks  

```

  1. Start server 


```
$ litellm /path/to/config.yaml  

```

  1. Query health endpoint: 


```
curl --location 'http://0.0.0.0:4000/health'  

```

### Hide details​
The health check response contains details like endpoint URLs, error messages, and other LiteLLM params. While this is useful for debugging, it can be problematic when exposing the proxy server to a broad audience.
You can hide these details by setting the `health_check_details` setting to `False`.
```
general_settings:   
 health_check_details: False  

```

## Health Check Timeout​
The health check timeout is set in `litellm/constants.py` and defaults to 60 seconds.
This can be overridden in the config.yaml by setting `health_check_timeout` in the model_info section.
```
model_list:  
 - model_name: openai/gpt-4o  
  litellm_params:  
   model: openai/gpt-4o  
   api_key: os.environ/OPENAI_API_KEY  
  model_info:  
   health_check_timeout: 10 # 👈 OVERRIDE HEALTH CHECK TIMEOUT  

```

## `/health/readiness`​
Unprotected endpoint for checking if proxy is ready to accept requests
Example Request: 
```
curl http://0.0.0.0:4000/health/readiness  

```

Example Response: 
```
{  
 "status": "connected",  
 "db": "connected",  
 "cache": null,  
 "litellm_version": "1.40.21",  
 "success_callbacks": [  
  "langfuse",  
  "_PROXY_track_cost_callback",  
  "response_taking_too_long_callback",  
  "_PROXY_MaxParallelRequestsHandler",  
  "_PROXY_MaxBudgetLimiter",  
  "_PROXY_CacheControlCheck",  
  "ServiceLogging"  
 ],  
 "last_updated": "2024-07-10T18:59:10.616968"  
}  

```

If the proxy is not connected to a database, then the `"db"` field will be `"Not connected"` instead of `"connected"` and the `"last_updated"` field will not be present.
## `/health/liveliness`​
Unprotected endpoint for checking if proxy is alive
Example Request: 
```
curl -X 'GET' \  
 'http://0.0.0.0:4000/health/liveliness' \  
 -H 'accept: application/json'  

```

Example Response: 
```
"I'm alive!"  

```

## `/health/services`​
Use this admin-only endpoint to check if a connected service (datadog/slack/langfuse/etc.) is healthy.
```
curl -L -X GET 'http://0.0.0.0:4000/health/services?service=datadog'   -H 'Authorization: Bearer sk-1234'  

```

**API Reference**
## Advanced - Call specific models​
To check health of specific models, here's how to call them: 
### 1. Get model id via `/model/info`​
```
curl -X GET 'http://0.0.0.0:4000/v1/model/info' \  
--header 'Authorization: Bearer sk-1234' \  

```

**Expected Response**
```
{  
  "model_name": "bedrock-anthropic-claude-3",  
  "litellm_params": {  
    "model": "anthropic.claude-3-sonnet-20240229-v1:0"  
  },  
  "model_info": {  
    "id": "634b87c444..", # 👈 UNIQUE MODEL ID  
}  

```

### 2. Call specific model via `/chat/completions`​
```
curl -X POST 'http://localhost:4000/chat/completions' \  
-H 'Content-Type: application/json' \  
-H 'Authorization: Bearer sk-1234' \  
-D '{  
 "model": "634b87c444.." # 👈 UNIQUE MODEL ID  
 "messages": [  
  {  
   "role": "user",  
   "content": "ping"  
  }  
 ],  
}  
'  

```

Previous
Model Management
Next
Debugging
  * Summary
  * `/health`
    * Embedding Models
    * Image Generation Models
    * Text Completion Models
    * Speech to Text Models
    * Text to Speech Models
    * Rerank Models
    * Batch Models (Azure Only)
    * Realtime Models
    * Wildcard Routes
  * Background Health Checks
    * Hide details
  * Health Check Timeout
  * `/health/readiness`
  * `/health/liveliness`
  * `/health/services`
  * Advanced - Call specific models
    * 1. Get model id via `/model/info`
    * 2. Call specific model via `/chat/completions`


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
      * Guardrails - Quick Start
      * Aim Security
      * Aporia
      * Bedrock
      * Custom Guardrail
      * Guardrails.ai
      * Lakera AI
      * PII Masking - Presidio
      * In-memory Prompt Injection Detection
      * ✨ Secret Detection/Redaction (Enterprise-only)
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * [Beta] Guardrails
  * Guardrails - Quick Start


On this page
# Guardrails - Quick Start
Setup Prompt Injection Detection, PII Masking on LiteLLM Proxy (AI Gateway)
## 1. Define guardrails on your LiteLLM config.yaml​
Set your guardrails under the `guardrails` section
```
model_list:  
 - model_name: gpt-3.5-turbo  
  litellm_params:  
   model: openai/gpt-3.5-turbo  
   api_key: os.environ/OPENAI_API_KEY  
  
guardrails:  
 - guardrail_name: general-guard  
  litellm_params:  
   guardrail: aim  
   mode: [pre_call, post_call]  
   api_key: os.environ/AIM_API_KEY  
   api_base: os.environ/AIM_API_BASE  
   default_on: true # Optional  
   
 - guardrail_name: "aporia-pre-guard"  
  litellm_params:  
   guardrail: aporia # supported values: "aporia", "lakera"  
   mode: "during_call"  
   api_key: os.environ/APORIA_API_KEY_1  
   api_base: os.environ/APORIA_API_BASE_1  
 - guardrail_name: "aporia-post-guard"  
  litellm_params:  
   guardrail: aporia # supported values: "aporia", "lakera"  
   mode: "post_call"  
   api_key: os.environ/APORIA_API_KEY_2  
   api_base: os.environ/APORIA_API_BASE_2  
  guardrail_info: # Optional field, info is returned on GET /guardrails/list  
   # you can enter any fields under info for consumers of your guardrail  
   params:  
    - name: "toxicity_score"  
     type: "float"  
     description: "Score between 0-1 indicating content toxicity level"  
    - name: "pii_detection"  
     type: "boolean"  

```

### Supported values for `mode` (Event Hooks)​
  * `pre_call` Run **before** LLM call, on **input**
  * `post_call` Run **after** LLM call, on **input & output**
  * `during_call` Run **during** LLM call, on **input** Same as `pre_call` but runs in parallel as LLM call. Response not returned until guardrail check completes
  * A list of the above values to run multiple modes, e.g. `mode: [pre_call, post_call]`


## 2. Start LiteLLM Gateway​
```
litellm --config config.yaml --detailed_debug  

```

## 3. Test request​
**Langchain, OpenAI SDK Usage Examples**
  * Unsuccessful call
  * Successful Call 


Expect this to fail since since `ishaan@berri.ai` in the request is PII
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi my email is ishaan@berri.ai"}  
  ],  
  "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
 }'  

```

Expected response on failure
```
{  
 "error": {  
  "message": {  
   "error": "Violated guardrail policy",  
   "aporia_ai_response": {  
    "action": "block",  
    "revised_prompt": null,  
    "revised_response": "Aporia detected and blocked PII",  
    "explain_log": null  
   }  
  },  
  "type": "None",  
  "param": "None",  
  "code": "400"  
 }  
}  
  

```

```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi what is the weather"}  
  ],  
  "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
 }'  

```

## **Default On Guardrails**​
Set `default_on: true` in your guardrail config to run the guardrail on every request. This is useful if you want to run a guardrail on every request without the user having to specify it.
**Note:** These will run even if user specifies a different guardrail or empty guardrails array.
```
guardrails:  
 - guardrail_name: "aporia-pre-guard"  
  litellm_params:  
   guardrail: aporia  
   mode: "pre_call"  
   default_on: true  

```

**Test Request**
In this request, the guardrail `aporia-pre-guard` will run on every request because `default_on: true` is set.
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi my email is ishaan@berri.ai"}  
  ]  
 }'  

```

**Expected response**
Your response headers will include `x-litellm-applied-guardrails` with the guardrail applied 
```
x-litellm-applied-guardrails: aporia-pre-guard  

```

## **Using Guardrails Client Side**​
### Test yourself **(OSS)**​
Pass `guardrails` to your request body to test it
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi my email is ishaan@berri.ai"}  
  ],  
  "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
 }'  

```

### Expose to your users **(Enterprise)**​
Follow this simple workflow to implement and tune guardrails:
### 1. ✨ View Available Guardrails​
info
✨ This is an Enterprise only feature Get a free trial
First, check what guardrails are available and their parameters:
Call `/guardrails/list` to view available guardrails and the guardrail info (supported parameters, description, etc)
```
curl -X GET 'http://0.0.0.0:4000/guardrails/list'  

```

Expected response
```
{  
  "guardrails": [  
    {  
    "guardrail_name": "aporia-post-guard",  
    "guardrail_info": {  
      "params": [  
      {  
        "name": "toxicity_score",  
        "type": "float",  
        "description": "Score between 0-1 indicating content toxicity level"  
      },  
      {  
        "name": "pii_detection",  
        "type": "boolean"  
      }  
      ]  
    }  
    }  
  ]  
}  

```

> This config will return the `/guardrails/list` response above. The `guardrail_info` field is optional and you can add any fields under info for consumers of your guardrail
> ```
- guardrail_name: "aporia-post-guard"  
>   litellm_params:  
>    guardrail: aporia # supported values: "aporia", "lakera"  
>    mode: "post_call"  
>    api_key: os.environ/APORIA_API_KEY_2  
>    api_base: os.environ/APORIA_API_BASE_2  
>   guardrail_info: # Optional field, info is returned on GET /guardrails/list  
>    # you can enter any fields under info for consumers of your guardrail  
>    params:  
>     - name: "toxicity_score"  
>      type: "float"  
>      description: "Score between 0-1 indicating content toxicity level"  
>     - name: "pii_detection"  
>      type: "boolean"  
> 
```

### 2. Apply Guardrails​
Add selected guardrails to your chat completion request:
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [{"role": "user", "content": "your message"}],  
  "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
 }'  

```

### 3. Test with Mock LLM completions​
Send `mock_response` to test guardrails without making an LLM call. More info on `mock_response` here
```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi my email is ishaan@berri.ai"}  
  ],  
  "mock_response": "This is a mock response",  
  "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
 }'  

```

### 4. ✨ Pass Dynamic Parameters to Guardrail​
info
✨ This is an Enterprise only feature Get a free trial
Use this to pass additional parameters to the guardrail API call. e.g. things like success threshold. **See`guardrails` spec for more details**
  * OpenAI Python v1.0.0+
  * Curl Request


Set `guardrails={"aporia-pre-guard": {"extra_body": {"success_threshold": 0.9}}}` to pass additional parameters to the guardrail
In this example `success_threshold=0.9` is passed to the `aporia-pre-guard` guardrail request body
```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages = [  
    {  
      "role": "user",  
      "content": "this is a test request, write a short poem"  
    }  
  ],  
  extra_body={  
   "guardrails": [  
    "aporia-pre-guard": {  
     "extra_body": {  
      "success_threshold": 0.9  
     }  
    }  
   ]  
  }  
  
)  
  
print(response)  

```

```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "what llm are you"  
    }  
  ],  
  "guardrails": [  
   "aporia-pre-guard": {  
    "extra_body": {  
     "success_threshold": 0.9  
    }  
   }  
  ]  
}'  

```

## **Proxy Admin Controls**​
### ✨ Monitoring Guardrails​
Monitor which guardrails were executed and whether they passed or failed. e.g. guardrail going rogue and failing requests we don't intend to fail
info
✨ This is an Enterprise only feature Get a free trial
#### Setup​
  1. Connect LiteLLM to a supported logging provider
  2. Make a request with a `guardrails` parameter
  3. Check your logging provider for the guardrail trace


#### Traced Guardrail Success​
#### Traced Guardrail Failure​
### ✨ Control Guardrails per API Key​
info
✨ This is an Enterprise only feature Get a free trial
Use this to control what guardrails run per API Key. In this tutorial we only want the following guardrails to run for 1 API Key
  * `guardrails`: ["aporia-pre-guard", "aporia-post-guard"]


**Step 1** Create Key with guardrail settings
  * /key/generate
  * /key/update


```
curl -X POST 'http://0.0.0.0:4000/key/generate' \  
  -H 'Authorization: Bearer sk-1234' \  
  -H 'Content-Type: application/json' \  
  -D '{  
      "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
    }  
  }'  

```

```
curl --location 'http://0.0.0.0:4000/key/update' \  
  --header 'Authorization: Bearer sk-1234' \  
  --header 'Content-Type: application/json' \  
  --data '{  
    "key": "sk-jNm1Zar7XfNdZXp49Z1kSQ",  
    "guardrails": ["aporia-pre-guard", "aporia-post-guard"]  
    }  
}'  

```

**Step 2** Test it with new key
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
  --header 'Authorization: Bearer sk-jNm1Zar7XfNdZXp49Z1kSQ' \  
  --header 'Content-Type: application/json' \  
  --data '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
    "role": "user",  
    "content": "my email is ishaan@berri.ai"  
    }  
  ]  
}'  

```

### ✨ Disable team from turning on/off guardrails​
info
✨ This is an Enterprise only feature Get a free trial
#### 1. Disable team from modifying guardrails​
```
curl -X POST 'http://0.0.0.0:4000/team/update' \  
-H 'Authorization: Bearer sk-1234' \  
-H 'Content-Type: application/json' \  
-D '{  
  "team_id": "4198d93c-d375-4c83-8d5a-71e7c5473e50",  
  "metadata": {"guardrails": {"modify_guardrails": false}}  
}'  

```

#### 2. Try to disable guardrails for a call​
```
curl --location 'http://0.0.0.0:4000/chat/completions' \  
--header 'Content-Type: application/json' \  
--header 'Authorization: Bearer $LITELLM_VIRTUAL_KEY' \  
--data '{  
"model": "gpt-3.5-turbo",  
  "messages": [  
   {  
    "role": "user",  
    "content": "Think of 10 random colors."  
   }  
  ],  
  "metadata": {"guardrails": {"hide_secrets": false}}  
}'  

```

#### 3. Get 403 Error​
```
{  
  "error": {  
    "message": {  
      "error": "Your team does not have permission to modify guardrails."  
    },  
    "type": "auth_error",  
    "param": "None",  
    "code": 403  
  }  
}  

```

Expect to NOT see `+1 412-612-9992` in your server logs on your callback. 
info
The `pii_masking` guardrail ran on this request because api key=sk-jNm1Zar7XfNdZXp49Z1kSQ has `"permissions": {"pii_masking": true}`
## Specification​
### `guardrails` Configuration on YAML​
```
guardrails:  
 - guardrail_name: string   # Required: Name of the guardrail  
  litellm_params:      # Required: Configuration parameters  
   guardrail: string    # Required: One of "aporia", "bedrock", "guardrails_ai", "lakera", "presidio", "hide-secrets"  
   mode: Union[string, List[string]]       # Required: One or more of "pre_call", "post_call", "during_call", "logging_only"  
   api_key: string     # Required: API key for the guardrail service  
   api_base: string     # Optional: Base URL for the guardrail service  
   default_on: boolean   # Optional: Default False. When set to True, will run on every request, does not need client to specify guardrail in request  
  guardrail_info:      # Optional[Dict]: Additional information about the guardrail  
     

```

### `guardrails` Request Parameter​
The `guardrails` parameter can be passed to any LiteLLM Proxy endpoint (`/chat/completions`, `/completions`, `/embeddings`).
#### Format Options​
  1. Simple List Format:


```
"guardrails": [  
  "aporia-pre-guard",  
  "aporia-post-guard"  
]  

```

  1. Advanced Dictionary Format:


In this format the dictionary key is `guardrail_name` you want to run
```
"guardrails": {  
  "aporia-pre-guard": {  
    "extra_body": {  
      "success_threshold": 0.9,  
      "other_param": "value"  
    }  
  }  
}  

```

#### Type Definition​
```
guardrails: Union[  
  List[str],               # Simple list of guardrail names  
  Dict[str, DynamicGuardrailParams]    # Advanced configuration  
]  
  
class DynamicGuardrailParams:  
  extra_body: Dict[str, Any]       # Additional parameters for the guardrail  

```

Previous
PagerDuty Alerting
Next
Aim Security
  * 1. Define guardrails on your LiteLLM config.yaml
    * Supported values for `mode` (Event Hooks)
  * 2. Start LiteLLM Gateway
  * 3. Test request
  * **Default On Guardrails**
  * **Using Guardrails Client Side**
    * Test yourself **(OSS)**
    * Expose to your users **(Enterprise)**
    * 1. ✨ View Available Guardrails
    * 2. Apply Guardrails
    * 3. Test with Mock LLM completions
    * 4. ✨ Pass Dynamic Parameters to Guardrail
  * **Proxy Admin Controls**
    * ✨ Monitoring Guardrails
    * ✨ Control Guardrails per API Key
    * ✨ Disable team from turning on/off guardrails
  * Specification
    * `guardrails` Configuration on YAML
    * `guardrails` Request Parameter


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM


---

Skip to main content
**🚅 LiteLLM**DocsEnterpriseHostedRelease Notes
💸 LLM Model Cost MapGitHubDiscord
Search
  * LiteLLM - Getting Started
  * LiteLLM Proxy Server
    * Getting Started - E2E Tutorial
    * Config.yaml
    * Setup & Deployment
    * Demo App
    * Architecture
    * All Endpoints (Swagger)
    * ✨ Enterprise Features
    * Making LLM Requests
    * Authentication
    * Model Access
    * Admin UI
    * Spend Tracking
    * Budgets + Rate Limits
    * Load Balancing, Routing, Fallbacks
    * Logging, Alerting, Metrics
    * [Beta] Guardrails
      * Guardrails - Quick Start
      * Aim Security
      * Aporia
      * Bedrock
      * Custom Guardrail
      * Guardrails.ai
      * Lakera AI
      * PII Masking - Presidio
      * In-memory Prompt Injection Detection
      * ✨ Secret Detection/Redaction (Enterprise-only)
    * Secret Managers
    * Create Custom Plugins
    * Caching
  * Supported Models & Providers
  * Guides
  * Supported Endpoints
  * Routing, Loadbalancing & Fallbacks
  * LiteLLM Python SDK
  * [Beta] Prompt Management
  * Load Testing
  * Logging & Observability
  * Tutorials
  * Contributing
  * Extras
  * Support & Talk with founders


  *   * LiteLLM Proxy Server
  * [Beta] Guardrails
  * Custom Guardrail


On this page
# Custom Guardrail
Use this is you want to write code to run a custom guardrail
## Quick Start​
### 1. Write a `CustomGuardrail` Class​
A CustomGuardrail has 4 methods to enforce guardrails 
  * `async_pre_call_hook` - (Optional) modify input or reject request before making LLM API call
  * `async_moderation_hook` - (Optional) reject request, runs while making LLM API call (help to lower latency)
  * `async_post_call_success_hook`- (Optional) apply guardrail on input/output, runs after making LLM API call
  * `async_post_call_streaming_iterator_hook` - (Optional) pass the entire stream to the guardrail


**See detailed spec of methods here**
**Example`CustomGuardrail` Class**
Create a new file called `custom_guardrail.py` and add this code to it
```
from typing import Any, Dict, List, Literal, Optional, Union  
  
import litellm  
from litellm._logging import verbose_proxy_logger  
from litellm.caching.caching import DualCache  
from litellm.integrations.custom_guardrail import CustomGuardrail  
from litellm.proxy._types import UserAPIKeyAuth  
from litellm.proxy.guardrails.guardrail_helpers import should_proceed_based_on_metadata  
from litellm.types.guardrails import GuardrailEventHooks  
  
  
class myCustomGuardrail(CustomGuardrail):  
  def __init__(  
    self,  
    **kwargs,  
  ):  
    # store kwargs as optional_params  
    self.optional_params = kwargs  
  
    super().__init__(**kwargs)  
  
  async def async_pre_call_hook(  
    self,  
    user_api_key_dict: UserAPIKeyAuth,  
    cache: DualCache,  
    data: dict,  
    call_type: Literal[  
      "completion",  
      "text_completion",  
      "embeddings",  
      "image_generation",  
      "moderation",  
      "audio_transcription",  
      "pass_through_endpoint",  
      "rerank"  
    ],  
  ) -> Optional[Union[Exception, str, dict]]:  
    """  
    Runs before the LLM API call  
    Runs on only Input  
    Use this if you want to MODIFY the input  
    """  
  
    # In this guardrail, if a user inputs `litellm` we will mask it and then send it to the LLM  
    _messages = data.get("messages")  
    if _messages:  
      for message in _messages:  
        _content = message.get("content")  
        if isinstance(_content, str):  
          if "litellm" in _content.lower():  
            _content = _content.replace("litellm", "********")  
            message["content"] = _content  
  
    verbose_proxy_logger.debug(  
      "async_pre_call_hook: Message after masking %s", _messages  
    )  
  
    return data  
  
  async def async_moderation_hook(  
    self,  
    data: dict,  
    user_api_key_dict: UserAPIKeyAuth,  
    call_type: Literal["completion", "embeddings", "image_generation", "moderation", "audio_transcription"],  
  ):  
    """  
    Runs in parallel to LLM API call  
    Runs on only Input  
  
    This can NOT modify the input, only used to reject or accept a call before going to LLM API  
    """  
  
    # this works the same as async_pre_call_hook, but just runs in parallel as the LLM API Call  
    # In this guardrail, if a user inputs `litellm` we will mask it.  
    _messages = data.get("messages")  
    if _messages:  
      for message in _messages:  
        _content = message.get("content")  
        if isinstance(_content, str):  
          if "litellm" in _content.lower():  
            raise ValueError("Guardrail failed words - `litellm` detected")  
  
  async def async_post_call_success_hook(  
    self,  
    data: dict,  
    user_api_key_dict: UserAPIKeyAuth,  
    response,  
  ):  
    """  
    Runs on response from LLM API call  
  
    It can be used to reject a response  
  
    If a response contains the word "coffee" -> we will raise an exception  
    """  
    verbose_proxy_logger.debug("async_pre_call_hook response: %s", response)  
    if isinstance(response, litellm.ModelResponse):  
      for choice in response.choices:  
        if isinstance(choice, litellm.Choices):  
          verbose_proxy_logger.debug("async_pre_call_hook choice: %s", choice)  
          if (  
            choice.message.content  
            and isinstance(choice.message.content, str)  
            and "coffee" in choice.message.content  
          ):  
            raise ValueError("Guardrail failed Coffee Detected")  
  
  async def async_post_call_streaming_iterator_hook(  
    self,  
    user_api_key_dict: UserAPIKeyAuth,  
    response: Any,  
    request_data: dict,  
  ) -> AsyncGenerator[ModelResponseStream, None]:  
    """  
    Passes the entire stream to the guardrail  
  
    This is useful for guardrails that need to see the entire response, such as PII masking.  
  
    See Aim guardrail implementation for an example - https://github.com/BerriAI/litellm/blob/d0e022cfacb8e9ebc5409bb652059b6fd97b45c0/litellm/proxy/guardrails/guardrail_hooks/aim.py#L168  
  
    Triggered by mode: 'post_call'  
    """  
    async for item in response:  
      yield item  
  

```

### 2. Pass your custom guardrail class in LiteLLM `config.yaml`​
In the config below, we point the guardrail to our custom guardrail by setting `guardrail: custom_guardrail.myCustomGuardrail`
  * Python Filename: `custom_guardrail.py`
  * Guardrail class name : `myCustomGuardrail`. This is defined in Step 1


`guardrail: custom_guardrail.myCustomGuardrail`
```
model_list:  
 - model_name: gpt-4  
  litellm_params:  
   model: openai/gpt-4o  
   api_key: os.environ/OPENAI_API_KEY  
  
guardrails:  
 - guardrail_name: "custom-pre-guard"  
  litellm_params:  
   guardrail: custom_guardrail.myCustomGuardrail # 👈 Key change  
   mode: "pre_call"         # runs async_pre_call_hook  
 - guardrail_name: "custom-during-guard"  
  litellm_params:  
   guardrail: custom_guardrail.myCustomGuardrail   
   mode: "during_call"        # runs async_moderation_hook  
 - guardrail_name: "custom-post-guard"  
  litellm_params:  
   guardrail: custom_guardrail.myCustomGuardrail  
   mode: "post_call"         # runs async_post_call_success_hook  

```

### 3. Start LiteLLM Gateway​
  * Docker Run
  * litellm pip


Mount your `custom_guardrail.py` on the LiteLLM Docker container
This mounts your `custom_guardrail.py` file from your local directory to the `/app` directory in the Docker container, making it accessible to the LiteLLM Gateway.
```
docker run -d \  
 -p 4000:4000 \  
 -e OPENAI_API_KEY=$OPENAI_API_KEY \  
 --name my-app \  
 -v $(pwd)/my_config.yaml:/app/config.yaml \  
 -v $(pwd)/custom_guardrail.py:/app/custom_guardrail.py \  
 my-app:latest \  
 --config /app/config.yaml \  
 --port 4000 \  
 --detailed_debug \  

```

```
litellm --config config.yaml --detailed_debug  

```

### 4. Test it​
#### Test `"custom-pre-guard"`​
**Langchain, OpenAI SDK Usage Examples**
  * Modify input
  * Successful Call 


Expect this to mask the word `litellm` before sending the request to the LLM API. This runs the `async_pre_call_hook`
```
curl -i -X POST http://localhost:4000/v1/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer sk-1234" \  
-d '{  
  "model": "gpt-4",  
  "messages": [  
    {  
      "role": "user",  
      "content": "say the word - `litellm`"  
    }  
  ],  
  "guardrails": ["custom-pre-guard"]  
}'  

```

Expected response after pre-guard
```
{  
 "id": "chatcmpl-9zREDkBIG20RJB4pMlyutmi1hXQWc",  
 "choices": [  
  {  
   "finish_reason": "stop",  
   "index": 0,  
   "message": {  
    "content": "It looks like you've chosen a string of asterisks. This could be a way to censor or hide certain text. However, without more context, I can't provide a specific word or phrase. If there's something specific you'd like me to say or if you need help with a topic, feel free to let me know!",  
    "role": "assistant",  
    "tool_calls": null,  
    "function_call": null  
   }  
  }  
 ],  
 "created": 1724429701,  
 "model": "gpt-4o-2024-05-13",  
 "object": "chat.completion",  
 "system_fingerprint": "fp_3aa7262c27",  
 "usage": {  
  "completion_tokens": 65,  
  "prompt_tokens": 14,  
  "total_tokens": 79  
 },  
 "service_tier": null  
}  
  

```

```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi what is the weather"}  
  ],  
  "guardrails": ["custom-pre-guard"]  
 }'  

```

#### Test `"custom-during-guard"`​
**Langchain, OpenAI SDK Usage Examples**
  * Unsuccessful call
  * Successful Call 


Expect this to fail since since `litellm` is in the message content. This runs the `async_moderation_hook`
```
curl -i -X POST http://localhost:4000/v1/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer sk-1234" \  
-d '{  
  "model": "gpt-4",  
  "messages": [  
    {  
      "role": "user",  
      "content": "say the word - `litellm`"  
    }  
  ],  
  "guardrails": ["custom-during-guard"]  
}'  

```

Expected response after running during-guard
```
{  
 "error": {  
  "message": "Guardrail failed words - `litellm` detected",  
  "type": "None",  
  "param": "None",  
  "code": "500"  
 }  
}  

```

```
curl -i http://localhost:4000/v1/chat/completions \  
 -H "Content-Type: application/json" \  
 -H "Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ" \  
 -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
   {"role": "user", "content": "hi what is the weather"}  
  ],  
  "guardrails": ["custom-during-guard"]  
 }'  

```

#### Test `"custom-post-guard"`​
**Langchain, OpenAI SDK Usage Examples**
  * Unsuccessful call
  * Successful Call 


Expect this to fail since since `coffee` will be in the response content. This runs the `async_post_call_success_hook`
```
curl -i -X POST http://localhost:4000/v1/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer sk-1234" \  
-d '{  
  "model": "gpt-4",  
  "messages": [  
    {  
      "role": "user",  
      "content": "what is coffee"  
    }  
  ],  
  "guardrails": ["custom-post-guard"]  
}'  

```

Expected response after running during-guard
```
{  
 "error": {  
  "message": "Guardrail failed Coffee Detected",  
  "type": "None",  
  "param": "None",  
  "code": "500"  
 }  
}  

```

```
 curl -i -X POST http://localhost:4000/v1/chat/completions \  
-H "Content-Type: application/json" \  
-H "Authorization: Bearer sk-1234" \  
-d '{  
  "model": "gpt-4",  
  "messages": [  
    {  
      "role": "user",  
      "content": "what is tea"  
    }  
  ],  
  "guardrails": ["custom-post-guard"]  
}'  

```

## ✨ Pass additional parameters to guardrail​
info
✨ This is an Enterprise only feature Contact us to get a free trial
Use this to pass additional parameters to the guardrail API call. e.g. things like success threshold
  1. Use `get_guardrail_dynamic_request_body_params`


`get_guardrail_dynamic_request_body_params` is a method of the `litellm.integrations.custom_guardrail.CustomGuardrail` class that fetches the dynamic guardrail params passed in the request body.
```
from typing import Any, Dict, List, Literal, Optional, Union  
import litellm  
from litellm._logging import verbose_proxy_logger  
from litellm.caching.caching import DualCache  
from litellm.integrations.custom_guardrail import CustomGuardrail  
from litellm.proxy._types import UserAPIKeyAuth  
  
class myCustomGuardrail(CustomGuardrail):  
  def __init__(self, **kwargs):  
    super().__init__(**kwargs)  
  
  async def async_pre_call_hook(  
    self,  
    user_api_key_dict: UserAPIKeyAuth,  
    cache: DualCache,  
    data: dict,  
    call_type: Literal[  
      "completion",  
      "text_completion",  
      "embeddings",  
      "image_generation",  
      "moderation",  
      "audio_transcription",  
      "pass_through_endpoint",  
      "rerank"  
    ],  
  ) -> Optional[Union[Exception, str, dict]]:  
    # Get dynamic params from request body  
    params = self.get_guardrail_dynamic_request_body_params(request_data=data)  
    # params will contain: {"success_threshold": 0.9}  
    verbose_proxy_logger.debug("Guardrail params: %s", params)  
    return data  

```

  1. Pass parameters in your API requests:


LiteLLM Proxy allows you to pass `guardrails` in the request body, following the `guardrails` spec.
  * OpenAI Python
  * Curl


```
import openai  
client = openai.OpenAI(  
  api_key="anything",  
  base_url="http://0.0.0.0:4000"  
)  
  
response = client.chat.completions.create(  
  model="gpt-3.5-turbo",  
  messages=[{"role": "user", "content": "Write a short poem"}],  
  extra_body={  
    "guardrails": [  
      "custom-pre-guard": {  
        "extra_body": {  
          "success_threshold": 0.9  
        }  
      }  
    ]  
  }  
)  

```

```
curl 'http://0.0.0.0:4000/chat/completions' \  
  -H 'Content-Type: application/json' \  
  -d '{  
  "model": "gpt-3.5-turbo",  
  "messages": [  
    {  
      "role": "user",  
      "content": "Write a short poem"  
    }  
  ],  
  "guardrails": [  
    "custom-pre-guard": {  
      "extra_body": {  
        "success_threshold": 0.9  
      }  
    }  
  ]  
}'  

```

The `get_guardrail_dynamic_request_body_params` method will return:
```
{  
  "success_threshold": 0.9  
}  

```

## **CustomGuardrail methods**​
Component| Description| Optional| Checked Data| Can Modify Input| Can Modify Output| Can Fail Call  
---|---|---|---|---|---|---  
`async_pre_call_hook`| A hook that runs before the LLM API call| ✅| INPUT| ✅| ❌| ✅  
`async_moderation_hook`| A hook that runs during the LLM API call| ✅| INPUT| ❌| ❌| ✅  
`async_post_call_success_hook`| A hook that runs after a successful LLM API call| ✅| INPUT, OUTPUT| ❌| ✅| ✅  
Previous
Bedrock
Next
Guardrails.ai
  * Quick Start
    * 1. Write a `CustomGuardrail` Class
    * 2. Pass your custom guardrail class in LiteLLM `config.yaml`
    * 3. Start LiteLLM Gateway
    * 4. Test it
  * ✨ Pass additional parameters to guardrail
  * **CustomGuardrail methods**


Docs
  * Getting Started


Community
  * Discord
  * Twitter


More
  * GitHub


Copyright © 2025 liteLLM
