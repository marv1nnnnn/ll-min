  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch documentation
  * Edit on GitHub


Shortcuts 
# PyTorch documentation
PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
Features described in this documentation are classified by release status:
> _Stable:_ These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).
> _Beta:_ These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.
> _Prototype:_ These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.
Community
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings
  * C++
  * Javadoc
  * torch::deploy


Python API
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


# Indices and tables
  * Index
  * Module Index


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg)
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch documentation
  * Indices and tables


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch documentation
  * Edit on GitHub


Shortcuts 
# PyTorch documentation
PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
Features described in this documentation are classified by release status:
> _Stable:_ These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).
> _Beta:_ These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.
> _Prototype:_ These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.
Community
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings
  * C++
  * Javadoc
  * torch::deploy


Python API
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


# Indices and tables
  * Index
  * Module Index


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg)
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch documentation
  * Indices and tables


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.accelerator
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.accelerator
This package introduces support for the current accelerator in python.
`device_count` | Return the number of current accelerator available.  
---|---  
`is_available` | Check if the current accelerator is available at runtime: it was build, all the required drivers are available and at least one device is visible.  
`current_accelerator` | Return the device of the accelerator available at compilation time.  
`set_device_index` | Set the current device index to a given device.  
`set_device_idx` | Set the current device index to a given device.  
`current_device_index` | Return the index of a currently selected device for the current accelerator.  
`current_device_idx` | Return the index of a currently selected device for the current accelerator.  
`set_stream` | Set the current stream to a given stream.  
`current_stream` | Return the currently selected stream for a given device.  
`synchronize` | Wait for all kernels in all streams on the given device to complete.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.accelerator


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Automatic Mixed Precision package - torch.amp
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Automatic Mixed Precision package - torch.amp
`torch.amp` provides convenience methods for mixed precision, where some operations use the `torch.float32` (`float`) datatype and other operations use lower precision floating point datatype (`lower_precision_fp`): `torch.float16` (`half`) or `torch.bfloat16`. Some ops, like linear layers and convolutions, are much faster in `lower_precision_fp`. Other ops, like reductions, often require the dynamic range of `float32`. Mixed precision tries to match each op to its appropriate datatype.
Ordinarily, “automatic mixed precision training” with datatype of `torch.float16` uses `torch.autocast` and `torch.amp.GradScaler` together, as shown in the Automatic Mixed Precision examples and Automatic Mixed Precision recipe. However, `torch.autocast` and `torch.GradScaler` are modular, and may be used separately if desired. As shown in the CPU example section of `torch.autocast`, “automatic mixed precision training/inference” on CPU with datatype of `torch.bfloat16` only uses `torch.autocast`.
Warning
`torch.cuda.amp.autocast(args...)` and `torch.cpu.amp.autocast(args...)` will be deprecated. Please use `torch.autocast("cuda", args...)` or `torch.autocast("cpu", args...)` instead. `torch.cuda.amp.GradScaler(args...)` and `torch.cpu.amp.GradScaler(args...)` will be deprecated. Please use `torch.GradScaler("cuda", args...)` or `torch.GradScaler("cpu", args...)` instead.
`torch.autocast` and `torch.cpu.amp.autocast` are new in version 1.10.
  * Autocasting
  * Gradient Scaling
  * Autocast Op Reference
    * Op Eligibility
    * CUDA Op-Specific Behavior
      * CUDA Ops that can autocast to `float16`
      * CUDA Ops that can autocast to `float32`
      * CUDA Ops that promote to the widest input type
      * Prefer `binary_cross_entropy_with_logits` over `binary_cross_entropy`
    * XPU Op-Specific Behavior (Experimental)
      * XPU Ops that can autocast to `float16`
      * XPU Ops that can autocast to `float32`
      * XPU Ops that promote to the widest input type
    * CPU Op-Specific Behavior
      * CPU Ops that can autocast to `bfloat16`
      * CPU Ops that can autocast to `float32`
      * CPU Ops that promote to the widest input type


## Autocasting 

torch.amp.autocast_mode.is_autocast_available(_device_type_)[source][source]
    
Return a bool indicating if autocast is available on `device_type`. 

Parameters
    
**device_type** (_str_) – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘xpu’ and so on. The type is the same as the type attribute of a `torch.device`. Thus, you may obtain the device type of a tensor using Tensor.device.type. 

Return type
    
bool 

_class_ torch.autocast(_device_type_ , _dtype =None_, _enabled =True_, _cache_enabled =None_)[source][source]
    
Instances of `autocast` serve as context managers or decorators that allow regions of your script to run in mixed precision.
In these regions, ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details.
When entering an autocast-enabled region, Tensors may be any type. You should not call `half()` or `bfloat16()` on your model(s) or inputs when using autocasting.
`autocast` should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.
Example for CUDA Devices:
```
# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)
for input, target in data:
  optimizer.zero_grad()
  # Enables autocasting for the forward pass (model + loss)
  with torch.autocast(device_type="cuda"):
    output = model(input)
    loss = loss_fn(output, target)
  # Exits the context manager before backward()
  loss.backward()
  optimizer.step()

```
Copy to clipboard
See the Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).
`autocast` can also be used as a decorator, e.g., on the `forward` method of your model:
```
class AutocastModel(nn.Module):
  ...
  @torch.autocast(device_type="cuda")
  def forward(self, input):
    ...

```
Copy to clipboard
Floating-point Tensors produced in an autocast-enabled region may be `float16`. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to `float32` (or other dtype if desired). If a Tensor from the autocast region is already `float32`, the cast is a no-op, and incurs no additional overhead. CUDA Example:
```
# Creates some tensors in default dtype (here assumed to be float32)
a_float32 = torch.rand((8, 8), device="cuda")
b_float32 = torch.rand((8, 8), device="cuda")
c_float32 = torch.rand((8, 8), device="cuda")
d_float32 = torch.rand((8, 8), device="cuda")
with torch.autocast(device_type="cuda"):
  # torch.mm is on autocast's list of ops that should run in float16.
  # Inputs are float32, but the op runs in float16 and produces float16 output.
  # No manual casts are required.
  e_float16 = torch.mm(a_float32, b_float32)
  # Also handles mixed input types
  f_float16 = torch.mm(d_float32, e_float16)
# After exiting autocast, calls f_float16.float() to use with d_float32
g_float32 = torch.mm(d_float32, f_float16.float())

```
Copy to clipboard
CPU Training Example:
```
# Creates model and optimizer in default precision
model = Net()
optimizer = optim.SGD(model.parameters(), ...)
for epoch in epochs:
  for input, target in data:
    optimizer.zero_grad()
    # Runs the forward pass with autocasting.
    with torch.autocast(device_type="cpu", dtype=torch.bfloat16):
      output = model(input)
      loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()

```
Copy to clipboard
CPU Inference Example:
```
# Creates model in default precision
model = Net().eval()
with torch.autocast(device_type="cpu", dtype=torch.bfloat16):
  for input in data:
    # Runs the forward pass with autocasting.
    output = model(input)

```
Copy to clipboard
CPU Inference Example with Jit Trace:
```
class TestModel(nn.Module):
  def __init__(self, input_size, num_classes):
    super().__init__()
    self.fc1 = nn.Linear(input_size, num_classes)
  def forward(self, x):
    return self.fc1(x)
input_size = 2
num_classes = 2
model = TestModel(input_size, num_classes).eval()
# For now, we suggest to disable the Jit Autocast Pass,
# As the issue: https://github.com/pytorch/pytorch/issues/75956
torch._C._jit_set_autocast_mode(False)
with torch.cpu.amp.autocast(cache_enabled=False):
  model = torch.jit.trace(model, torch.randn(1, input_size))
model = torch.jit.freeze(model)
# Models Run
for _ in range(3):
  model(torch.randn(1, input_size))

```
Copy to clipboard
Type mismatch errors _in_ an autocast-enabled region are a bug; if this is what you observe, please file an issue.
`autocast(enabled=False)` subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular `dtype`. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to `dtype` before use:
```
# Creates some tensors in default dtype (here assumed to be float32)
a_float32 = torch.rand((8, 8), device="cuda")
b_float32 = torch.rand((8, 8), device="cuda")
c_float32 = torch.rand((8, 8), device="cuda")
d_float32 = torch.rand((8, 8), device="cuda")
with torch.autocast(device_type="cuda"):
  e_float16 = torch.mm(a_float32, b_float32)
  with torch.autocast(device_type="cuda", enabled=False):
    # Calls e_float16.float() to ensure float32 execution
    # (necessary because e_float16 was created in an autocasted region)
    f_float32 = torch.mm(c_float32, e_float16.float())
  # No manual casts are required when re-entering the autocast-enabled region.
  # torch.mm again runs in float16 and produces float16 output, regardless of input types.
  g_float16 = torch.mm(d_float32, f_float32)

```
Copy to clipboard
The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects `torch.nn.DataParallel` and `torch.nn.parallel.DistributedDataParallel` when used with more than one GPU per process (see Working with Multiple GPUs). 

Parameters
    
  * **device_type** (_str_ _,__required_) – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘xpu’, and ‘hpu’. The type is the same as the type attribute of a `torch.device`. Thus, you may obtain the device type of a tensor using Tensor.device.type.
  * **enabled** (_bool_ _,__optional_) – Whether autocasting should be enabled in the region. Default: `True`
  * **dtype** (_torch_dtype_ _,__optional_) – Data type for ops run in autocast. It uses the default value (`torch.float16` for CUDA and `torch.bfloat16` for CPU), given by `get_autocast_dtype()`, if `dtype` is `None`. Default: `None`
  * **cache_enabled** (_bool_ _,__optional_) – Whether the weight cache inside autocast should be enabled. Default: `True`



torch.amp.custom_fwd(_fwd =None_, _*_ , _device_type_ , _cast_inputs =None_)[source][source]
    
Create a helper decorator for `forward` methods of custom autograd functions.
Autograd functions are subclasses of `torch.autograd.Function`. See the example page for more detail. 

Parameters
    
  * **device_type** (_str_) – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘xpu’ and so on. The type is the same as the type attribute of a `torch.device`. Thus, you may obtain the device type of a tensor using Tensor.device.type.
  * **cast_inputs** (`torch.dtype` or None, optional, default=None) – If not `None`, when `forward` runs in an autocast-enabled region, casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors are not affected), then executes `forward` with autocast disabled. If `None`, `forward`’s internal ops execute with the current autocast state.


Note
If the decorated `forward` is called outside an autocast-enabled region, `custom_fwd` is a no-op and `cast_inputs` has no effect. 

torch.amp.custom_bwd(_bwd =None_, _*_ , _device_type_)[source][source]
    
Create a helper decorator for backward methods of custom autograd functions.
Autograd functions are subclasses of `torch.autograd.Function`. Ensures that `backward` executes with the same autocast state as `forward`. See the example page for more detail. 

Parameters
    
**device_type** (_str_) – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘xpu’ and so on. The type is the same as the type attribute of a `torch.device`. Thus, you may obtain the device type of a tensor using Tensor.device.type. 

_class_ torch.cuda.amp.autocast(_enabled =True_, _dtype =torch.float16_, _cache_enabled =True_)[source][source]
    
See `torch.autocast`.
`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast("cuda", args...)` instead. 

torch.cuda.amp.custom_fwd(_fwd =None_, _*_ , _cast_inputs =None_)[source][source]
    
`torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead. 

torch.cuda.amp.custom_bwd(_bwd_)[source][source]
    
`torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead. 

_class_ torch.cpu.amp.autocast(_enabled =True_, _dtype =torch.bfloat16_, _cache_enabled =True_)[source][source]
    
See `torch.autocast`. `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast("cpu", args...)` instead.
## Gradient Scaling
If the forward pass for a particular op has `float16` inputs, the backward pass for that op will produce `float16` gradients. Gradient values with small magnitudes may not be representable in `float16`. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.
To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.
Each parameter’s gradient (`.grad` attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.
Note
AMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in the fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In this case, the scale factor may decrease under 1 as an attempt to bring gradients to a number representable in the fp16 dynamic range. While one may expect the scale to always be above 1, our GradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss or gradients when running with AMP/fp16, verify your model is compatible. 

_class_ torch.cuda.amp.GradScaler(_init_scale =65536.0_, _growth_factor =2.0_, _backoff_factor =0.5_, _growth_interval =2000_, _enabled =True_)[source][source]
    
See `torch.amp.GradScaler`. `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler("cuda", args...)` instead. 

_class_ torch.cpu.amp.GradScaler(_init_scale =65536.0_, _growth_factor =2.0_, _backoff_factor =0.5_, _growth_interval =2000_, _enabled =True_)[source][source]
    
See `torch.amp.GradScaler`. `torch.cpu.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler("cpu", args...)` instead.
## Autocast Op Reference
### Op Eligibility
Ops that run in `float64` or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.
Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an `out=...` Tensor are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region `a.addmm(b, c)` can autocast, but `a.addmm_(b, c)` and `a.addmm(b, c, out=d)` cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.
Ops called with an explicit `dtype=...` argument are not eligible, and will produce output that respects the `dtype` argument.
### CUDA Op-Specific Behavior
The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a `torch.nn.Module`, as a function, or as a `torch.Tensor` method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.
Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.
If an op is unlisted, we assume it’s numerically stable in `float16`. If you believe an unlisted op is numerically unstable in `float16`, please file an issue.
#### CUDA Ops that can autocast to `float16`
`__matmul__`, `addbmm`, `addmm`, `addmv`, `addr`, `baddbmm`, `bmm`, `chain_matmul`, `multi_dot`, `conv1d`, `conv2d`, `conv3d`, `conv_transpose1d`, `conv_transpose2d`, `conv_transpose3d`, `GRUCell`, `linear`, `LSTMCell`, `matmul`, `mm`, `mv`, `prelu`, `RNNCell`
#### CUDA Ops that can autocast to `float32`
`__pow__`, `__rdiv__`, `__rpow__`, `__rtruediv__`, `acos`, `asin`, `binary_cross_entropy_with_logits`, `cosh`, `cosine_embedding_loss`, `cdist`, `cosine_similarity`, `cross_entropy`, `cumprod`, `cumsum`, `dist`, `erfinv`, `exp`, `expm1`, `group_norm`, `hinge_embedding_loss`, `kl_div`, `l1_loss`, `layer_norm`, `log`, `log_softmax`, `log10`, `log1p`, `log2`, `margin_ranking_loss`, `mse_loss`, `multilabel_margin_loss`, `multi_margin_loss`, `nll_loss`, `norm`, `normalize`, `pdist`, `poisson_nll_loss`, `pow`, `prod`, `reciprocal`, `rsqrt`, `sinh`, `smooth_l1_loss`, `soft_margin_loss`, `softmax`, `softmin`, `softplus`, `sum`, `renorm`, `tan`, `triplet_margin_loss`
#### CUDA Ops that promote to the widest input type
These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are `float16`, the op runs in `float16`. If any of the inputs is `float32`, autocast casts all inputs to `float32` and runs the op in `float32`.
`addcdiv`, `addcmul`, `atan2`, `bilinear`, `cross`, `dot`, `grid_sample`, `index_put`, `scatter_add`, `tensordot`
Some ops not listed here (e.g., binary ops like `add`) natively promote inputs without autocasting’s intervention. If inputs are a mixture of `float16` and `float32`, these ops run in `float32` and produce `float32` output, regardless of whether autocast is enabled.
#### Prefer `binary_cross_entropy_with_logits` over `binary_cross_entropy`
The backward passes of `torch.nn.functional.binary_cross_entropy()` (and `torch.nn.BCELoss`, which wraps it) can produce gradients that aren’t representable in `float16`. In autocast-enabled regions, the forward input may be `float16`, which means the backward gradient must be representable in `float16` (autocasting `float16` forward inputs to `float32` doesn’t help, because that cast must be reversed in backward). Therefore, `binary_cross_entropy` and `BCELoss` raise an error in autocast-enabled regions.
Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using `torch.nn.functional.binary_cross_entropy_with_logits()` or `torch.nn.BCEWithLogitsLoss`. `binary_cross_entropy_with_logits` and `BCEWithLogits` are safe to autocast.
### XPU Op-Specific Behavior (Experimental)
The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a `torch.nn.Module`, as a function, or as a `torch.Tensor` method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.
Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.
If an op is unlisted, we assume it’s numerically stable in `float16`. If you believe an unlisted op is numerically unstable in `float16`, please file an issue.
#### XPU Ops that can autocast to `float16`
`addbmm`, `addmm`, `addmv`, `addr`, `baddbmm`, `bmm`, `chain_matmul`, `multi_dot`, `conv1d`, `conv2d`, `conv3d`, `conv_transpose1d`, `conv_transpose2d`, `conv_transpose3d`, `GRUCell`, `linear`, `LSTMCell`, `matmul`, `mm`, `mv`, `RNNCell`
#### XPU Ops that can autocast to `float32`
`__pow__`, `__rdiv__`, `__rpow__`, `__rtruediv__`, `binary_cross_entropy_with_logits`, `cosine_embedding_loss`, `cosine_similarity`, `cumsum`, `dist`, `exp`, `group_norm`, `hinge_embedding_loss`, `kl_div`, `l1_loss`, `layer_norm`, `log`, `log_softmax`, `margin_ranking_loss`, `nll_loss`, `normalize`, `poisson_nll_loss`, `pow`, `reciprocal`, `rsqrt`, `soft_margin_loss`, `softmax`, `softmin`, `sum`, `triplet_margin_loss`
#### XPU Ops that promote to the widest input type
These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are `float16`, the op runs in `float16`. If any of the inputs is `float32`, autocast casts all inputs to `float32` and runs the op in `float32`.
`bilinear`, `cross`, `grid_sample`, `index_put`, `scatter_add`, `tensordot`
Some ops not listed here (e.g., binary ops like `add`) natively promote inputs without autocasting’s intervention. If inputs are a mixture of `float16` and `float32`, these ops run in `float32` and produce `float32` output, regardless of whether autocast is enabled.
### CPU Op-Specific Behavior
The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a `torch.nn.Module`, as a function, or as a `torch.Tensor` method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.
Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.
If an op is unlisted, we assume it’s numerically stable in `bfloat16`. If you believe an unlisted op is numerically unstable in `bfloat16`, please file an issue. `float16` shares the lists of `bfloat16`.
#### CPU Ops that can autocast to `bfloat16`
`conv1d`, `conv2d`, `conv3d`, `bmm`, `mm`, `linalg_vecdot`, `baddbmm`, `addmm`, `addbmm`, `linear`, `matmul`, `_convolution`, `conv_tbc`, `mkldnn_rnn_layer`, `conv_transpose1d`, `conv_transpose2d`, `conv_transpose3d`, `prelu`, `scaled_dot_product_attention`, `_native_multi_head_attention`
#### CPU Ops that can autocast to `float32`
`avg_pool3d`, `binary_cross_entropy`, `grid_sampler`, `grid_sampler_2d`, `_grid_sampler_2d_cpu_fallback`, `grid_sampler_3d`, `polar`, `prod`, `quantile`, `nanquantile`, `stft`, `cdist`, `trace`, `view_as_complex`, `cholesky`, `cholesky_inverse`, `cholesky_solve`, `inverse`, `lu_solve`, `orgqr`, `inverse`, `ormqr`, `pinverse`, `max_pool3d`, `max_unpool2d`, `max_unpool3d`, `adaptive_avg_pool3d`, `reflection_pad1d`, `reflection_pad2d`, `replication_pad1d`, `replication_pad2d`, `replication_pad3d`, `mse_loss`, `cosine_embedding_loss`, `nll_loss`, `nll_loss2d`, `hinge_embedding_loss`, `poisson_nll_loss`, `cross_entropy_loss`, `l1_loss`, `huber_loss`, `margin_ranking_loss`, `soft_margin_loss`, `triplet_margin_loss`, `multi_margin_loss`, `ctc_loss`, `kl_div`, `multilabel_margin_loss`, `binary_cross_entropy_with_logits`, `fft_fft`, `fft_ifft`, `fft_fft2`, `fft_ifft2`, `fft_fftn`, `fft_ifftn`, `fft_rfft`, `fft_irfft`, `fft_rfft2`, `fft_irfft2`, `fft_rfftn`, `fft_irfftn`, `fft_hfft`, `fft_ihfft`, `linalg_cond`, `linalg_matrix_rank`, `linalg_solve`, `linalg_cholesky`, `linalg_svdvals`, `linalg_eigvals`, `linalg_eigvalsh`, `linalg_inv`, `linalg_householder_product`, `linalg_tensorinv`, `linalg_tensorsolve`, `fake_quantize_per_tensor_affine`, `geqrf`, `_lu_with_info`, `qr`, `svd`, `triangular_solve`, `fractional_max_pool2d`, `fractional_max_pool3d`, `adaptive_max_pool3d`, `multilabel_margin_loss_forward`, `linalg_qr`, `linalg_cholesky_ex`, `linalg_svd`, `linalg_eig`, `linalg_eigh`, `linalg_lstsq`, `linalg_inv_ex`
#### CPU Ops that promote to the widest input type
These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are `bfloat16`, the op runs in `bfloat16`. If any of the inputs is `float32`, autocast casts all inputs to `float32` and runs the op in `float32`.
`cat`, `stack`, `index_copy`
Some ops not listed here (e.g., binary ops like `add`) natively promote inputs without autocasting’s intervention. If inputs are a mixture of `bfloat16` and `float32`, these ops run in `float32` and produce `float32` output, regardless of whether autocast is enabled.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Automatic Mixed Precision package - torch.amp
    * Autocasting
      * `is_autocast_available()`
      * `autocast`
      * `custom_fwd()`
      * `custom_bwd()`
      * `autocast`
      * `custom_fwd()`
      * `custom_bwd()`
      * `autocast`
    * Gradient Scaling
      * `GradScaler`
      * `GradScaler`
    * Autocast Op Reference
      * Op Eligibility
      * CUDA Op-Specific Behavior
        * CUDA Ops that can autocast to `float16`
        * CUDA Ops that can autocast to `float32`
        * CUDA Ops that promote to the widest input type
        * Prefer `binary_cross_entropy_with_logits` over `binary_cross_entropy`
      * XPU Op-Specific Behavior (Experimental)
        * XPU Ops that can autocast to `float16`
        * XPU Ops that can autocast to `float32`
        * XPU Ops that promote to the widest input type
      * CPU Op-Specific Behavior
        * CPU Ops that can autocast to `bfloat16`
        * CPU Ops that can autocast to `float32`
        * CPU Ops that promote to the widest input type


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Automatic differentiation package - torch.autograd
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Automatic differentiation package - torch.autograd
`torch.autograd` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.
It requires minimal changes to the existing code - you only need to declare `Tensor` s for which gradients should be computed with the `requires_grad=True` keyword. As of now, we only support autograd for floating point `Tensor` types ( half, float, double and bfloat16) and complex `Tensor` types (cfloat, cdouble).
`backward` | Compute the sum of gradients of given tensors with respect to graph leaves.  
---|---  
`grad` | Compute and return the sum of gradients of outputs with respect to the inputs.  
## Forward-mode Automatic Differentiation
Warning
This API is in beta. Even though the function signatures are very unlikely to change, improved operator coverage is planned before we consider this stable.
Please see the forward-mode AD tutorial for detailed steps on how to use this API.
`forward_ad.dual_level` | Context-manager for forward AD, where all forward AD computation must occur within the `dual_level` context.  
---|---  
`forward_ad.make_dual` | Associate a tensor value with its tangent to create a "dual tensor" for forward AD gradient computation.  
`forward_ad.unpack_dual` | Unpack a "dual tensor" to get both its Tensor value and its forward AD gradient.  
`forward_ad.enter_dual_level` | Enter a new forward grad level.  
`forward_ad.exit_dual_level` | Exit a forward grad level.  
`forward_ad.UnpackedDualTensor` | Namedtuple returned by `unpack_dual()` containing the primal and tangent components of the dual tensor.  
## Functional higher level API
Warning
This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.
This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.
This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set, you can use a lambda to capture them. For example, for a function `f` that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as `f(input, constant, flag=flag)` you can use it as `functional.jacobian(lambda x: f(x, constant, flag=flag), input)`.
`functional.jacobian` | Compute the Jacobian of a given function.  
---|---  
`functional.hessian` | Compute the Hessian of a given scalar function.  
`functional.vjp` | Compute the dot product between a vector `v` and the Jacobian of the given function at the point given by the inputs.  
`functional.jvp` | Compute the dot product between the Jacobian of the given function at the point given by the inputs and a vector `v`.  
`functional.vhp` | Compute the dot product between vector `v` and Hessian of a given scalar function at a specified point.  
`functional.hvp` | Compute the dot product between the scalar function's Hessian and a vector `v` at a specified point.  
## Locally disabling gradient computation
See Locally disabling gradient computation for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two. Also see Locally disabling gradient computation for a list of functions that can be used to locally disable gradients.
## Default gradient layouts
When a non-sparse `param` receives a non-sparse gradient during `torch.autograd.backward()` or `torch.Tensor.backward()` `param.grad` is accumulated as follows.
If `param.grad` is initially `None`:
  1. If `param`’s memory is non-overlapping and dense, `.grad` is created with strides matching `param` (thus matching `param`’s layout).
  2. Otherwise, `.grad` is created with rowmajor-contiguous strides.


If `param` already has a non-sparse `.grad` attribute:
  1. If `create_graph=False`, `backward()` accumulates into `.grad` in-place, which preserves its strides.
  2. If `create_graph=True`, `backward()` replaces `.grad` with a new tensor `.grad + new grad`, which attempts (but does not guarantee) matching the preexisting `.grad`’s strides.


The default behavior (letting `.grad`s be `None` before the first `backward()`, such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to `model.zero_grad()` or `optimizer.zero_grad()` will not affect `.grad` layouts.
In fact, resetting all `.grad`s to `None` before each accumulation phase, e.g.:
```
for iterations...
  ...
  for param in model.parameters():
    param.grad = None
  loss.backward()

```
Copy to clipboard
such that they’re recreated according to 1 or 2 every time, is a valid alternative to `model.zero_grad()` or `optimizer.zero_grad()` that may improve performance for some networks.
### Manual gradient layouts
If you need manual control over `.grad`’s strides, assign `param.grad =` a zeroed tensor with desired strides before the first `backward()`, and never reset it to `None`. 3 guarantees your layout is preserved as long as `create_graph=False`. 4 indicates your layout is _likely_ preserved even if `create_graph=True`.
## In-place operations on Tensors
Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.
### In-place correctness checks
All `Tensor` s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.
## Variable (deprecated)
Warning
The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with `requires_grad` set to `True`. Below please find a quick guide on what has changed:
  * `Variable(tensor)` and `Variable(tensor, requires_grad)` still work as expected, but they return Tensors instead of Variables.
  * `var.data` is the same thing as `tensor.data`.
  * Methods such as `var.backward(), var.detach(), var.register_hook()` now work on tensors with the same method names.


In addition, one can now create tensors with `requires_grad=True` using factory methods such as `torch.randn()`, `torch.zeros()`, `torch.ones()`, and others like the following:
`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`
## Tensor autograd functions
`torch.Tensor.grad` | This attribute is `None` by default and becomes a Tensor the first time a call to `backward()` computes gradients for `self`.  
---|---  
`torch.Tensor.requires_grad` | Is `True` if gradients need to be computed for this Tensor, `False` otherwise.  
`torch.Tensor.is_leaf` | All Tensors that have `requires_grad` which is `False` will be leaf Tensors by convention.  
`torch.Tensor.backward`([gradient, ...]) | Computes the gradient of current tensor wrt graph leaves.  
`torch.Tensor.detach` | Returns a new Tensor, detached from the current graph.  
`torch.Tensor.detach_` | Detaches the Tensor from the graph that created it, making it a leaf.  
`torch.Tensor.register_hook`(hook) | Registers a backward hook.  
`torch.Tensor.register_post_accumulate_grad_hook`(hook) | Registers a backward hook that runs after grad accumulation.  
`torch.Tensor.retain_grad`() | Enables this Tensor to have their `grad` populated during `backward()`.  
## Function 

_class_ torch.autograd.Function(_* args_, _** kwargs_)[source][source]
    
Base class to create custom autograd.Function.
To create a custom autograd.Function, subclass this class and implement the `forward()` and `backward()` static methods. Then, to use your custom op in the forward pass, call the class method `apply`. Do not call `forward()` directly.
To ensure correctness and best performance, make sure you are calling the correct methods on `ctx` and validating your backward function using `torch.autograd.gradcheck()`.
See Extending torch.autograd for more details on how to use this class.
Examples:
```
>>> class Exp(Function):
>>>   @staticmethod
>>>   def forward(ctx, i):
>>>     result = i.exp()
>>>     ctx.save_for_backward(result)
>>>     return result
>>>
>>>   @staticmethod
>>>   def backward(ctx, grad_output):
>>>     result, = ctx.saved_tensors
>>>     return grad_output * result
>>>
>>> # Use it by calling the apply method:
>>> output = Exp.apply(input)

```
Copy to clipboard
`Function.forward` | Define the forward of the custom autograd Function.  
---|---  
`Function.backward` | Define a formula for differentiating the operation with backward mode automatic differentiation.  
`Function.jvp` | Define a formula for differentiating the operation with forward mode automatic differentiation.  
`Function.vmap` | Define the behavior for this autograd.Function underneath `torch.vmap()`.  
## Context method mixins
When creating a new `Function`, the following methods are available to ctx.
`function.FunctionCtx.mark_dirty` | Mark given tensors as modified in an in-place operation.  
---|---  
`function.FunctionCtx.mark_non_differentiable` | Mark outputs as non-differentiable.  
`function.FunctionCtx.save_for_backward` | Save given tensors for a future call to `backward()`.  
`function.FunctionCtx.set_materialize_grads` | Set whether to materialize grad tensors.  
## Custom Function utilities
Decorator for backward method.
`function.once_differentiable` |   
---|---  
Base custom `Function` used to build PyTorch utilities
`function.BackwardCFunction` | This class is used for internal autograd work.  
---|---  
`function.InplaceFunction` | This class is here only for backward compatibility reasons.  
`function.NestedIOFunction` | This class is here only for backward compatibility reasons.  
## Numerical gradient checking
`gradcheck` | Check gradients computed via small finite differences against analytical gradients wrt tensors in `inputs` that are of floating point or complex type and with `requires_grad=True`.  
---|---  
`gradgradcheck` | Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with `requires_grad=True`.  
`GradcheckError` | Error raised by `gradcheck()` and `gradgradcheck()`.  
## Profiler
Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are three modes implemented at the moment - CPU-only using `profile`. nvprof based (registers both CPU and GPU activity) using `emit_nvtx`. and vtune profiler based using `emit_itt`. 

_class_ torch.autograd.profiler.profile(_enabled =True_, _*_ , _use_cuda =False_, _use_device =None_, _record_shapes =False_, _with_flops =False_, _profile_memory =False_, _with_stack =False_, _with_modules =False_, _use_kineto =False_, _use_cpu =True_, _experimental_config =None_, _acc_events =False_, _custom_trace_id_callback =None_)[source][source]
    
Context manager that manages autograd profiler state and holds a summary of results.
Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks 

Parameters
    
  * **enabled** (_bool_ _,__optional_) – Setting this to False makes this context manager a no-op.
  * **use_cuda** (_bool_ _,__optional_) – Enables timing of CUDA events as well using the cudaEvent API. (will be deprecated)
  * **use_device** (_str_ _,__optional_) – Enables timing of device events. Adds approximately 4us of overhead to each tensor operation when use cuda. The valid devices options are ‘cuda’, ‘xpu’, ‘mtia’ and ‘privateuseone’.
  * **record_shapes** (_bool_ _,__optional_) – If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection.
  * **with_flops** (_bool_ _,__optional_) – If with_flops is set, the profiler will estimate the FLOPs (floating point operations) value using the operator’s input shape. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators.
  * **profile_memory** (_bool_ _,__optional_) – track tensor memory allocation/deallocation.
  * **with_stack** (_bool_ _,__optional_) – record source information (file and line number) for the ops.
  * **with_modules** (_bool_) – record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module A’s forward call’s module B’s forward which contains an aten::add op, then aten::add’s module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models.
  * **use_kineto** (_bool_ _,__optional_) – experimental, enable profiling with Kineto profiler.
  * **use_cpu** (_bool_ _,__optional_) – profile CPU events; setting to `False` requires `use_kineto=True` and can be used to lower the overhead for GPU-only profiling.
  * **experimental_config** (__ExperimentalConfig_) – A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.
  * **acc_events** (_bool_) – Enable the accumulation of FunctionEvents across multiple profiling cycles


Example
```
>>> x = torch.randn((1, 1), requires_grad=True)
>>> with torch.autograd.profiler.profile() as prof:
>>>   for _ in range(100): # any normal python code, really!
>>>     y = x ** 2
>>>     y.backward()
>>> # NOTE: some columns were removed for brevity
>>> print(prof.key_averages().table(sort_by="self_cpu_time_total"))
----------------------------------- --------------- --------------- ---------------
Name                 Self CPU total  CPU time avg   Number of Calls
----------------------------------- --------------- --------------- ---------------
mul                 32.048ms     32.048ms     200
pow                 27.041ms     27.041ms     200
PowBackward0             9.727ms     55.483ms     100
torch::autograd::AccumulateGrad   9.148ms     9.148ms     100
torch::autograd::GraphRoot      691.816us    691.816us    100
----------------------------------- --------------- --------------- ---------------

```
Copy to clipboard
`profiler.profile.export_chrome_trace` | Export an EventList as a Chrome tracing tools file.  
---|---  
`profiler.profile.key_averages` | Averages all function events over their keys.  
`profiler.profile.self_cpu_time_total` | Returns total time spent on CPU.  
`profiler.profile.total_average` | Averages all events.  
`profiler.parse_nvprof_trace` |   
`profiler.EnforceUnique` | Raises an error if a key is seen more than once.  
`profiler.KinetoStepTracker` | Provides an abstraction for incrementing the step count globally.  
`profiler.record_function` | Context manager/function decorator that adds a label to a code block/function when running autograd profiler.  
`profiler_util.Interval` |   
`profiler_util.Kernel` |   
`profiler_util.MemRecordsAcc` | Acceleration structure for accessing mem_records in interval.  
`profiler_util.StringTable` |  

_class_ torch.autograd.profiler.emit_nvtx(_enabled =True_, _record_shapes =False_)[source][source]
      
Context manager that makes every autograd operation emit an NVTX range.
It is useful when running the program under nvprof:
```
nvprof --profile-from-start off -o trace_name.prof -- <regular command here>

```
Copy to clipboard
Unfortunately, there’s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or `torch.autograd.profiler.load_nvprof()` can load the results for inspection e.g. in Python REPL. 

Parameters
    
  * **enabled** (_bool_ _,__optional_) – Setting `enabled=False` makes this context manager a no-op. Default: `True`.
  * **record_shapes** (_bool_ _,__optional_) – If `record_shapes=True`, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: `[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]` Non-tensor arguments will be represented by `[]`. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation. Default: `False`


Example
```
>>> with torch.cuda.profiler.profile():
...   model(x) # Warmup CUDA memory allocator and profiler
...   with torch.autograd.profiler.emit_nvtx():
...     model(x)

```
Copy to clipboard
**Forward-backward correlation**
When viewing a profile created using `emit_nvtx` in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, `emit_nvtx` appends sequence number information to the ranges it generates.
During the forward pass, each function range is decorated with `seq=<N>`. `seq` is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the `seq=<N>` annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function’s `apply()` call is decorated with `stashed seq=<M>`. `M` is the sequence number that the backward object was created with. By comparing `stashed seq` numbers in backward with `seq` numbers in forward, you can track down which forward op created each backward Function.
Any functions executed during the backward pass are also decorated with `seq=<N>`. During default backward (with `create_graph=False`) this information is irrelevant, and in fact, `N` may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects’ `apply()` methods are useful, as a way to correlate these Function objects with the earlier forward pass.
**Double-backward**
If, on the other hand, a backward pass with `create_graph=True` is underway (in other words, if you are setting up for a double-backward), each function’s execution during backward is given a nonzero, useful `seq=<N>`. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects’ `apply()` ranges are still tagged with `stashed seq` numbers, which can be compared to seq numbers from the backward pass. 

_class_ torch.autograd.profiler.emit_itt(_enabled =True_, _record_shapes =False_)[source][source]
    
Context manager that makes every autograd operation emit an ITT range.
It is useful when running the program under Intel(R) VTune Profiler:
```
vtune <--vtune-flags> <regular command here>

```
Copy to clipboard
The Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI. 

Parameters
    
  * **enabled** (_bool_ _,__optional_) – Setting `enabled=False` makes this context manager a no-op. Default: `True`.
  * **record_shapes** (_bool_ _,__optional_) – If `record_shapes=True`, the itt range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: `[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]` Non-tensor arguments will be represented by `[]`. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of itt range creation. Default: `False`


Example
```
>>> with torch.autograd.profiler.emit_itt():
...   model(x)

```
Copy to clipboard
`profiler.load_nvprof` | Open an nvprof trace file and parses autograd annotations.  
---|---  
## Debugging and anomaly detection 

_class_ torch.autograd.detect_anomaly(_check_nan =True_)[source][source]
    
Context-manager that enable anomaly detection for the autograd engine.
This does two things:
  * Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.
  * If `check_nan` is `True`, any backward computation that generate “nan” value will raise an error. Default `True`.


Warning
This mode should be enabled only for debugging as the different tests will slow down your program execution.
Example
```
>>> import torch
>>> from torch import autograd
>>> class MyFunc(autograd.Function):
...   @staticmethod
...   def forward(ctx, inp):
...     return inp.clone()
...   @staticmethod
...   def backward(ctx, gO):
...     # Error during the backward pass
...     raise RuntimeError("Some error in backward")
...     return gO.clone()
>>> def run_fn(a):
...   out = MyFunc.apply(a)
...   return out.sum()
>>> inp = torch.rand(10, 10, requires_grad=True)
>>> out = run_fn(inp)
>>> out.backward()
  Traceback (most recent call last):
   File "<stdin>", line 1, in <module>
   File "/your/pytorch/install/torch/_tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
   File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True) # allow_unreachable flag
   File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
    return self._forward_cls.backward(self, *args)
   File "<stdin>", line 8, in backward
  RuntimeError: Some error in backward
>>> with autograd.detect_anomaly():
...   inp = torch.rand(10, 10, requires_grad=True)
...   out = run_fn(inp)
...   out.backward()
  Traceback of forward call that caused the error:
   File "tmp.py", line 53, in <module>
    out = run_fn(inp)
   File "tmp.py", line 44, in run_fn
    out = MyFunc.apply(a)
  Traceback (most recent call last):
   File "<stdin>", line 4, in <module>
   File "/your/pytorch/install/torch/_tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
   File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True) # allow_unreachable flag
   File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
    return self._forward_cls.backward(self, *args)
   File "<stdin>", line 8, in backward
  RuntimeError: Some error in backward

```
Copy to clipboard 

_class_ torch.autograd.set_detect_anomaly(_mode_ , _check_nan =True_)[source][source]
    
Context-manager that sets the anomaly detection for the autograd engine on or off.
`set_detect_anomaly` will enable or disable the autograd anomaly detection based on its argument `mode`. It can be used as a context-manager or as a function.
See `detect_anomaly` above for details of the anomaly detection behaviour. 

Parameters
    
  * **mode** (_bool_) – Flag whether to enable anomaly detection (`True`), or disable (`False`).
  * **check_nan** (_bool_) – Flag whether to raise an error when the backward generate “nan”


`grad_mode.set_multithreading_enabled` | Context-manager that sets multithreaded backwards on or off.  
---|---  
## Autograd graph
Autograd exposes methods that allow one to inspect the graph and interpose behavior during the backward pass.
The `grad_fn` attribute of a `torch.Tensor` holds a `torch.autograd.graph.Node` if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is enabled and at least one of the inputs required gradients), or `None` otherwise.
`graph.Node.name` | Return the name.  
---|---  
`graph.Node.metadata` | Return the metadata.  
`graph.Node.next_functions` |   
`graph.Node.register_hook` | Register a backward hook.  
`graph.Node.register_prehook` | Register a backward pre-hook.  
`graph.increment_version` | Update autograd metadata tracking whether the given Tensor was modified in place.  
Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass. These intermediary results are saved as attributes on the `grad_fn` and can be accessed. For example:
```
>>> a = torch.tensor([0., 0., 0.], requires_grad=True)
>>> b = a.exp()
>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))
True
>>> print(dir(b.grad_fn))
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']
>>> print(torch.allclose(b.grad_fn._saved_result, b))
True

```
Copy to clipboard
You can also define how these saved tensors should be packed / unpacked using hooks. A common application is to trade compute for memory by saving those intermediary results to disk or to CPU instead of leaving them on the GPU. This is especially useful if you notice your model fits on GPU during evaluation, but not training. Also see Hooks for saved tensors. 

_class_ torch.autograd.graph.saved_tensors_hooks(_pack_hook_ , _unpack_hook_)[source][source]
    
Context-manager that sets a pair of pack / unpack hooks for saved tensors.
Use this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval.
In that context, the `pack_hook` function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using `save_for_backward()` but also those recorded by a PyTorch-defined operation). The output of `pack_hook` is then stored in the computation graph instead of the original tensor.
The `unpack_hook` is called when the saved tensor needs to be accessed, namely when executing `torch.Tensor.backward()` or `torch.autograd.grad()`. It takes as argument the _packed_ object returned by `pack_hook` and should return a tensor which has the same content as the original tensor (passed as input to the corresponding `pack_hook`).
The hooks should have the following signatures:
> pack_hook(tensor: Tensor) -> Any
> unpack_hook(Any) -> Tensor
where the return value of `pack_hook` is a valid input to `unpack_hook`.
In general, you want `unpack_hook(pack_hook(t))` to be equal to `t` in terms of value, size, dtype and device.
Example:
```
>>> def pack_hook(x):
...   print("Packing", x)
...   return x
>>>
>>> def unpack_hook(x):
...   print("Unpacking", x)
...   return x
>>>
>>> a = torch.ones(5, requires_grad=True)
>>> b = torch.ones(5, requires_grad=True) * 2
>>> with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
...   y = a * b
Packing tensor([1., 1., 1., 1., 1.], requires_grad=True)
Packing tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)
>>> y.sum().backward()
Unpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)
Unpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)

```
Copy to clipboard
Warning
Performing an inplace operation on the input to either hooks may lead to undefined behavior.
Warning
Only one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied. 

_class_ torch.autograd.graph.save_on_cpu(_pin_memory =False_, _device_type ='cuda'_)[source][source]
    
Context manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.
When performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.
Use this context-manager to trade compute for GPU memory usage (e.g. when your model doesn’t fit in GPU memory during training). 

Parameters
    
**pin_memory** (_bool_) – If `True` tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to `False`. Also see Use pinned memory buffers.
Example:
```
>>> a = torch.randn(5, requires_grad=True, device="cuda")
>>> b = torch.randn(5, requires_grad=True, device="cuda")
>>> c = torch.randn(5, requires_grad=True, device="cuda")
>>>
>>> def f(a, b, c):
...   prod_1 = a * b      # a and b are saved on GPU
...   with torch.autograd.graph.save_on_cpu():
...     prod_2 = prod_1 * c # prod_1 and c are saved on CPU
...   y = prod_2 * a      # prod_2 and a are saved on GPU
...   return y
>>>
>>> y = f(a, b, c)
>>> del a, b, c # for illustration only
>>> # the content of a, b, and prod_2 are still alive on GPU
>>> # the content of prod_1 and c only live on CPU
>>> y.sum().backward() # all CPU tensors are moved back to GPU, for backward
>>> # all intermediary tensors are released (deleted) after the call to backward

```
Copy to clipboard 

_class_ torch.autograd.graph.disable_saved_tensors_hooks(_error_message_)[source][source]
    
Context-manager that disables the saved tensors default hooks feature.
Useful for if you are creating a feature that does not work with saved tensors default hooks. 

Parameters
    
**error_message** (_str_) – When saved tensors default hooks are used when they have been are disabled, a RuntimeError with this error message gets raised. 

Return type
    
_Generator_[None, None, None]
Example:
```
>>> message = "saved tensors default hooks are disabled"
>>> with torch.autograd.graph.disable_saved_tensors_hooks(message):
...   # Raises RuntimeError: saved tensors default hooks are disabled
...   with torch.autograd.graph.save_on_cpu():
...     pass

```
Copy to clipboard 

_class_ torch.autograd.graph.register_multi_grad_hook(_tensors_ , _fn_ , _*_ , _mode ='all'_)[source][source]
    
Register a multi-grad backward hook.
There are two supported modes: `"all"` and `"any"`.
Under the `"all"` mode, the hook will be called after gradients with respect to every tensor in `tensors` have been computed. If a tensor is in `tensors` but is not part of the graph, or if a tensor is not needed to compute the gradients for any `inputs` specified for the current `.backward()` or `.grad()` call, this tensor will be ignored and the hook will not wait for its gradient to be computed.
After every non-ignored tensor’s gradient has been computed, `fn` will be called with those gradients. `None` will be passed for tensors that did not have their gradients computed.
Under the `"any"` mode, the hook will be called after the first gradient with respect to a tensor in `tensors` has been computed. The hook will be called with that gradient as its argument.
The hook should not modify its arguments.
This function returns a handle with a method `handle.remove()` that removes the hook.
Note
See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.
Example:
```
>>> import torch
>>>
>>> a = torch.rand(2, 3, requires_grad=True)
>>> b = torch.rand(2, 3, requires_grad=True)
>>> c = a * b
>>> d = a * b
>>>
>>> def fn(grads):
...   print([g is not None for g in grads])
...
>>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)
>>>
>>> c.sum().backward(retain_graph=True)
[True, True, True, False]
>>> c.sum().backward(inputs=(a,), retain_graph=True)
[True, False, True, False]
>>>

```
Copy to clipboard 

Return type
    
_RemovableHandle_ 

_class_ torch.autograd.graph.allow_mutation_on_saved_tensors[source][source]
    
Context manager under which mutating tensors saved for backward is allowed.
Under this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when it’s used during backward.
To ensure the correct behavior, both the forward and backward should be run under the same context manager. 

Returns
    
An _AllowMutationOnSavedContext object storing the state managed by this context manager. This object can be useful for debugging purposes. The state managed by the context manager is automatically cleared upon exiting. 

Return type
    
_Generator_[__AllowMutationOnSavedContext_ , None, None]
Example:
```
>>> import torch
>>> with torch.autograd.graph.allow_mutation_on_saved_tensors():
...   # forward
...   a = torch.ones(2, 3, requires_grad=True)
...   b = a.clone()
...   out = (b**2).sum()
...   b.sin_()
...   # backward
...   out.sum().backward()
...
tensor([[0.8415, 0.8415, 0.8415],
    [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)

```
Copy to clipboard 

_class_ torch.autograd.graph.GradientEdge(_node_ , _output_nr_)[source][source]
    
Object representing a given gradient edge within the autograd graph.
To get the gradient edge where a given Tensor gradient will be computed, you can do `edge = autograd.graph.get_gradient_edge(tensor)`. 

torch.autograd.graph.get_gradient_edge(_tensor_)[source][source]
    
Get the gradient edge for computing the gradient of the given Tensor.
In particular, it is equivalent to call `g = autograd.grad(loss, input)` and `g = autograd.grad(loss, get_gradient_edge(input))`. 

Return type
    
_GradientEdge_
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Automatic differentiation package - torch.autograd
    * Forward-mode Automatic Differentiation
    * Functional higher level API
    * Locally disabling gradient computation
    * Default gradient layouts
      * Manual gradient layouts
    * In-place operations on Tensors
      * In-place correctness checks
    * Variable (deprecated)
    * Tensor autograd functions
    * Function
      * `Function`
    * Context method mixins
    * Custom Function utilities
    * Numerical gradient checking
    * Profiler
      * `profile`
      * `emit_nvtx`
      * `emit_itt`
    * Debugging and anomaly detection
      * `detect_anomaly`
      * `set_detect_anomaly`
    * Autograd graph
      * `saved_tensors_hooks`
      * `save_on_cpu`
      * `disable_saved_tensors_hooks`
      * `register_multi_grad_hook`
      * `allow_mutation_on_saved_tensors`
      * `GradientEdge`
      * `get_gradient_edge()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Benchmark Utils - torch.utils.benchmark
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Benchmark Utils - torch.utils.benchmark 

_class_ torch.utils.benchmark.Timer(_stmt='pass'_ , _setup='pass'_ , _global_setup=''_ , _timer= <built-in function perf_counter>_, _globals=None_ , _label=None_ , _sub_label=None_ , _description=None_ , _env=None_ , _num_threads=1_ , _language=Language.PYTHON_)[source][source]
    
Helper class for measuring execution time of PyTorch statements.
For a full tutorial on how to use this class, see: https://pytorch.org/tutorials/recipes/recipes/benchmark.html
The PyTorch Timer is based on timeit.Timer (and in fact uses timeit.Timer internally), but with several key differences:
  1. 

Runtime aware:
    
Timer will perform warmups (important as some elements of PyTorch are lazily initialized), set threadpool size so that comparisons are apples-to-apples, and synchronize asynchronous CUDA functions when necessary.
  2. 

Focus on replicates:
    
When measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the timeit API by conceptually merging timeit.Timer.repeat and timeit.Timer.autorange. (Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not desired.
  3. 

Optional metadata:
    
When defining a Timer, one can optionally specify label, sub_label, description, and env. (Defined later) These fields are included in the representation of result object and by the Compare class to group and display results for comparison.
  4. 

Instruction counts
    
In addition to wall times, Timer can run a statement under Callgrind and report instructions executed.


Directly analogous to timeit.Timer constructor arguments:
> stmt, setup, timer, globals
PyTorch Timer specific constructor arguments:
> label, sub_label, description, env, num_threads 

Parameters
    
  * **stmt** (_str_) – Code snippet to be run in a loop and timed.
  * **setup** (_str_) – Optional setup code. Used to define variables used in stmt
  * **global_setup** (_str_) – (C++ only) Code which is placed at the top level of the file for things like #include statements.
  * **timer** (_Callable_ _[__[__]__,__float_ _]_) – Callable which returns the current time. If PyTorch was built without CUDA or there is no GPU present, this defaults to timeit.default_timer; otherwise it will synchronize CUDA before measuring the time.
  * **globals** (_Optional_ _[__dict_ _[__str_ _,__Any_ _]__]_) – A dict which defines the global variables when stmt is being executed. This is the other method for providing variables which stmt needs.
  * **label** (_Optional_ _[__str_ _]_) – String which summarizes stmt. For instance, if stmt is “torch.nn.functional.relu(torch.add(x, 1, out=out))” one might set label to “ReLU(x + 1)” to improve readability.
  * **sub_label** (_Optional_ _[__str_ _]_) – 
Provide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be “float” or “int”, so that it is easy to differentiate: “ReLU(x + 1): (float)”
”ReLU(x + 1): (int)” when printing Measurements or summarizing using Compare.
  * **description** (_Optional_ _[__str_ _]_) – 
String to distinguish measurements with identical label and sub_label. The principal use of description is to signal to Compare the columns of data. For instance one might set it based on the input size to create a table of the form:
```
            | n=1 | n=4 | ...
            ------------- ...
ReLU(x + 1): (float)  | ... | ... | ...
ReLU(x + 1): (int)   | ... | ... | ...

```
Copy to clipboard
using Compare. It is also included when printing a Measurement.
  * **env** (_Optional_ _[__str_ _]_) – This tag indicates that otherwise identical tasks were run in different environments, and are therefore not equivalent, for instance when A/B testing a change to a kernel. Compare will treat Measurements with different env specification as distinct when merging replicate runs.
  * **num_threads** (_int_) – The size of the PyTorch threadpool when executing stmt. Single threaded performance is important as both a key inference workload and a good indicator of intrinsic algorithmic efficiency, so the default is set to one. This is in contrast to the default PyTorch threadpool size which tries to utilize all cores.



adaptive_autorange(_threshold =0.1_, _*_ , _min_run_time =0.01_, _max_run_time =10.0_, _callback =None_)[source][source]
    
Similar to blocked_autorange but also checks for variablility in measurements and repeats until iqr/median is smaller than threshold or max_run_time is reached.
At a high level, adaptive_autorange executes the following pseudo-code:
```
`setup`
times = []
while times.sum < max_run_time
  start = timer()
  for _ in range(block_size):
    `stmt`
  times.append(timer() - start)
  enough_data = len(times)>3 and times.sum > min_run_time
  small_iqr=times.iqr/times.mean<threshold
  if enough_data and small_iqr:
    break

```
Copy to clipboard 

Parameters
    
  * **threshold** (_float_) – value of iqr/median threshold for stopping
  * **min_run_time** (_float_) – total runtime needed before checking threshold
  * **max_run_time** (_float_) – total runtime for all measurements regardless of threshold



Returns
    
A Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.) 

Return type
    
_Measurement_ 

blocked_autorange(_callback =None_, _min_run_time =0.2_)[source][source]
    
Measure many replicates while keeping timer overhead to a minimum.
At a high level, blocked_autorange executes the following pseudo-code:
```
`setup`
total_time = 0
while total_time < min_run_time
  start = timer()
  for _ in range(block_size):
    `stmt`
  total_time += (timer() - start)

```
Copy to clipboard
Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:
>   1. A small block size results in more replicates and generally better statistics.
>   2. A large block size better amortizes the cost of timer invocation, and results in a less biased measurement. This is important because CUDA synchronization time is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement.
> 

blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop. 

Returns
    
A Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.) 

Return type
    
_Measurement_ 

collect_callgrind(_number :int_, _*_ , _repeats :None_, _collect_baseline :bool_, _retain_out_file :bool_) → CallgrindStats[source][source]


collect_callgrind(_number :int_, _*_ , _repeats :int_, _collect_baseline :bool_, _retain_out_file :bool_) → tuple[torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats,...]
    
Collect instruction counts using Callgrind.
Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, however this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.
In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed.
Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules’s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly.
By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt. 

Returns
    
A CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results. 

timeit(_number =1000000_)[source][source]
    
Mirrors the semantics of timeit.Timer.timeit().
Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit 

Return type
    
_Measurement_ 

_class_ torch.utils.benchmark.Measurement(_number_per_run_ , _raw_times_ , _task_spec_ , _metadata =None_)[source][source]
    
The result of a Timer measurement.
This class stores one or more measurements of a given statement. It is serializable and provides several convenience methods (including a detailed __repr__) for downstream consumers. 

_static_ merge(_measurements_)[source][source]
    
Convenience method for merging replicates.
Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates) 

Return type
    
list[‘Measurement’] 

_property_ significant_figures _: int_
    
Approximate significant figure estimate.
This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t.
The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare. 

_class_ torch.utils.benchmark.CallgrindStats(_task_spec_ , _number_per_run_ , _built_with_debug_symbols_ , _baseline_inclusive_stats_ , _baseline_exclusive_stats_ , _stmt_inclusive_stats_ , _stmt_exclusive_stats_ , _stmt_callgrind_out_)[source][source]
    
Top level container for Callgrind results collected by Timer.
Manipulation is generally done using the FunctionCounts class, which is obtained by calling CallgrindStats.stats(…). Several convenience methods are provided as well; the most significant is CallgrindStats.as_standardized(). 

as_standardized()[source][source]
    
Strip library names and some prefixes from function strings.
When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:
```
23234231 /tmp/first_build_dir/thing.c:foo(...)
 9823794 /tmp/first_build_dir/thing.c:bar(...)
 ...
  53453 .../aten/src/Aten/...:function_that_actually_changed(...)
 ...
 -9823794 /tmp/second_build_dir/thing.c:bar(...)
-23234231 /tmp/second_build_dir/thing.c:foo(...)

```
Copy to clipboard
Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivalent call sites when diffing. 

Return type
    
_CallgrindStats_ 

counts(_*_ , _denoise =False_)[source][source]
    
Returns the total number of instructions executed.
See FunctionCounts.denoise() for an explanation of the denoise arg. 

Return type
    
int 

delta(_other_ , _inclusive =False_)[source][source]
    
Diff two sets of counts.
One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is “why”. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis. 

Return type
    
_FunctionCounts_ 

stats(_inclusive =False_)[source][source]
    
Returns detailed function counts.
Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples.
inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(…) for more details) 

Return type
    
_FunctionCounts_ 

_class_ torch.utils.benchmark.FunctionCounts(__data_ , _inclusive_ , _truncate_rows =True_, __linewidth =None_)[source][source]
    
Container for manipulating Callgrind results. 

It supports:
    
  1. Addition and subtraction to combine or diff results.
  2. Tuple-like indexing.
  3. A denoise function which strips CPython calls which are known to be non-deterministic and quite noisy.
  4. Two higher order methods (filter and transform) for custom manipulation.



denoise()[source][source]
    
Remove known noisy instructions.
Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception. 

Return type
    
_FunctionCounts_ 

filter(_filter_fn_)[source][source]
    
Keep only the elements where filter_fn applied to function name returns True. 

Return type
    
_FunctionCounts_ 

transform(_map_fn_)[source][source]
    
Apply map_fn to all of the function names.
This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc. 

Return type
    
_FunctionCounts_ 

_class_ torch.utils.benchmark.Compare(_results_)[source][source]
    
Helper class for displaying the results of many measurements in a formatted table.
The table format is based on the information fields provided in `torch.utils.benchmark.Timer` (description, label, sub_label, num_threads, etc).
The table can be directly printed using `print()` or casted as a str.
For a full tutorial on how to use this class, see: https://pytorch.org/tutorials/recipes/recipes/benchmark.html 

Parameters
    
**results** (_list_ _[__torch.utils.benchmark.utils.common.Measurement_ _]_) – List of Measurment to display. 

colorize(_rowwise =False_)[source][source]
    
Colorize formatted table.
Colorize columnwise by default. 

extend_results(_results_)[source][source]
    
Append results to already stored ones.
All added results must be instances of `Measurement`. 

highlight_warnings()[source][source]
    
Enables warning highlighting when building formatted table. 

print()[source][source]
    
Print formatted table 

trim_significant_figures()[source][source]
    
Enables trimming of significant figures when building the formatted table.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Benchmark Utils - torch.utils.benchmark
    * `Timer`
      * `Timer.adaptive_autorange()`
      * `Timer.blocked_autorange()`
      * `Timer.collect_callgrind()`
      * `Timer.timeit()`
    * `Measurement`
      * `Measurement.merge()`
      * `Measurement.significant_figures`
    * `CallgrindStats`
      * `CallgrindStats.as_standardized()`
      * `CallgrindStats.counts()`
      * `CallgrindStats.delta()`
      * `CallgrindStats.stats()`
    * `FunctionCounts`
      * `FunctionCounts.denoise()`
      * `FunctionCounts.filter()`
      * `FunctionCounts.transform()`
    * `Compare`
      * `Compare.colorize()`
      * `Compare.extend_results()`
      * `Compare.highlight_warnings()`
      * `Compare.print()`
      * `Compare.trim_significant_figures()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.bottleneck
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.bottleneck
torch.utils.bottleneck is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch’s autograd profiler.
Run it on the command line with
```
python -m torch.utils.bottleneck /path/to/source/script.py [args]

```
Copy to clipboard
where [args] are any number of arguments to script.py, or run `python -m torch.utils.bottleneck -h` for more usage instructions.
Warning
Because your script will be profiled, please ensure that it exits in a finite amount of time.
Warning
Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.
Note
To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (“CPU total time is much greater than CUDA total time”). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.
Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you’re evaluating. If the profiler outputs don’t help, you could try looking at the result of `torch.autograd.profiler.emit_nvtx()` with `nvprof`. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Similarly, `Intel® VTune™ Profiler` helps to analyze performance on Intel platforms further with `torch.autograd.profiler.emit_itt()`.
Warning
If you are profiling CUDA code, the first profiler that `bottleneck` runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.
For more complicated uses of the profilers (like in a multi-GPU case), please see https://docs.python.org/3/library/profile.html or `torch.autograd.profiler.profile()` for more information.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.bottleneck


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.backends
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.backends
torch.backends controls the behavior of various backends that PyTorch supports.
These backends include:
  * `torch.backends.cpu`
  * `torch.backends.cuda`
  * `torch.backends.cudnn`
  * `torch.backends.cusparselt`
  * `torch.backends.mha`
  * `torch.backends.mps`
  * `torch.backends.mkl`
  * `torch.backends.mkldnn`
  * `torch.backends.nnpack`
  * `torch.backends.openmp`
  * `torch.backends.opt_einsum`
  * `torch.backends.xeon`


## torch.backends.cpu 

torch.backends.cpu.get_cpu_capability()[source][source]
    
Return cpu capability as a string value.
Possible values: - “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512” - “SVE256” 

Return type
    
str
## torch.backends.cuda 

torch.backends.cuda.is_built()[source][source]
    
Return whether PyTorch is built with CUDA support.
Note that this doesn’t necessarily mean CUDA is available; just that if this PyTorch binary were run on a machine with working CUDA drivers and devices, we would be able to use it. 

torch.backends.cuda.matmul.allow_tf32
    
A `bool` that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices. 

torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction
    
A `bool` that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs. 

torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction
    
A `bool` that controls whether reduced precision reductions are allowed with bf16 GEMMs. 

torch.backends.cuda.cufft_plan_cache
    
`cufft_plan_cache` contains the cuFFT plan caches for each CUDA device. Query a specific device i’s cache via torch.backends.cuda.cufft_plan_cache[i]. 

torch.backends.cuda.cufft_plan_cache.size
    
A readonly `int` that shows the number of plans currently in a cuFFT plan cache. 

torch.backends.cuda.cufft_plan_cache.max_size
    
A `int` that controls the capacity of a cuFFT plan cache. 

torch.backends.cuda.cufft_plan_cache.clear()
    
Clears a cuFFT plan cache. 

torch.backends.cuda.preferred_blas_library(_backend =None_)[source][source]
    
Override the library PyTorch uses for BLAS operations. Choose between cuBLAS, cuBLASLt, and CK [ROCm-only].
Warning
This flag is experimental and subject to change.
When PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available. For PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance. This flag (a `str`) allows overriding which BLAS library to use.
  * If “cublas” is set then cuBLAS will be used wherever possible.
  * If “cublaslt” is set then cuBLASLt will be used wherever possible.
  * If “ck” is set then CK will be used wherever possible.
  * If “default” (the default) is set then heuristics will be used to pick between the other options.
  * When no input is given, this function returns the currently preferred library.
  * User may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.


Note: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s library selection is incorrect for your application’s inputs. 

Return type
    
__BlasBackend_ 

torch.backends.cuda.preferred_rocm_fa_library(_backend =None_)[source][source]
    
[ROCm-only] Override the backend PyTorch uses in ROCm environments for Flash Attention. Choose between AOTriton and CK
Warning
This flag is experimeental and subject to change.
When Flash Attention is enabled and desired, PyTorch defaults to using AOTriton as the backend. This flag (a `str`) allows users to override this backend to use composable_kernel
  * If “default” is set then the default backend will be used wherever possible. Currently AOTriton.
  * If “aotriton” is set then AOTriton will be used wherever possible.
  * If “ck” is set then CK will be used wherever possible.
  * When no input is given, this function returns the currently preferred library.
  * User may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK globally.


Note: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s library selection is incorrect for your application’s inputs. 

Return type
    
__ROCmFABackend_ 

torch.backends.cuda.preferred_linalg_library(_backend =None_)[source][source]
    
Override the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.
Warning
This flag is experimental and subject to change.
When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a `str`) allows overriding those heuristics.
  * If “cusolver” is set then cuSOLVER will be used wherever possible.
  * If “magma” is set then MAGMA will be used wherever possible.
  * If “default” (the default) is set then heuristics will be used to pick between cuSOLVER and MAGMA if both are available.
  * When no input is given, this function returns the currently preferred library.
  * User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.


Note: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect for your application’s inputs.
Currently supported linalg operators:
  * `torch.linalg.inv()`
  * `torch.linalg.inv_ex()`
  * `torch.linalg.cholesky()`
  * `torch.linalg.cholesky_ex()`
  * `torch.cholesky_solve()`
  * `torch.cholesky_inverse()`
  * `torch.linalg.lu_factor()`
  * `torch.linalg.lu()`
  * `torch.linalg.lu_solve()`
  * `torch.linalg.qr()`
  * `torch.linalg.eigh()`
  * `torch.linalg.eighvals()`
  * `torch.linalg.svd()`
  * `torch.linalg.svdvals()`



Return type
    
__LinalgBackend_ 

_class_ torch.backends.cuda.SDPAParams


torch.backends.cuda.flash_sdp_enabled()[source][source]
    
Warning
This flag is beta and subject to change.
Returns whether flash scaled dot product attention is enabled or not. 

torch.backends.cuda.enable_mem_efficient_sdp(_enabled_)[source][source]
    
Warning
This flag is beta and subject to change.
Enables or disables memory efficient scaled dot product attention. 

torch.backends.cuda.mem_efficient_sdp_enabled()[source][source]
    
Warning
This flag is beta and subject to change.
Returns whether memory efficient scaled dot product attention is enabled or not. 

torch.backends.cuda.enable_flash_sdp(_enabled_)[source][source]
    
Warning
This flag is beta and subject to change.
Enables or disables flash scaled dot product attention. 

torch.backends.cuda.math_sdp_enabled()[source][source]
    
Warning
This flag is beta and subject to change.
Returns whether math scaled dot product attention is enabled or not. 

torch.backends.cuda.enable_math_sdp(_enabled_)[source][source]
    
Warning
This flag is beta and subject to change.
Enables or disables math scaled dot product attention. 

torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed()[source][source]
    
Warning
This flag is beta and subject to change.
Returns whether fp16/bf16 reduction in math scaled dot product attention is enabled or not. 

torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(_enabled_)[source][source]
    
Warning
This flag is beta and subject to change.
Enables or disables fp16/bf16 reduction in math scaled dot product attention. 

torch.backends.cuda.cudnn_sdp_enabled()[source][source]
    
Warning
This flag is beta and subject to change.
Returns whether cuDNN scaled dot product attention is enabled or not. 

torch.backends.cuda.enable_cudnn_sdp(_enabled_)[source][source]
    
Warning
This flag is beta and subject to change.
Enables or disables cuDNN scaled dot product attention. 

torch.backends.cuda.is_flash_attention_available()[source][source]
    
Check if PyTorch was built with FlashAttention for scaled_dot_product_attention. 

Returns
    
True if FlashAttention is built and available; otherwise, False. 

Return type
    
bool
Note
This function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments. 

torch.backends.cuda.can_use_flash_attention(_params_ , _debug =False_)[source][source]
    
Check if FlashAttention can be utilized in scaled_dot_product_attention. 

Parameters
    
  * **params** (__SDPAParams_) – An instance of SDPAParams containing the tensors for query, key, value, an optional attention mask, dropout rate, and a flag indicating if the attention is causal.
  * **debug** (_bool_) – Whether to logging.warn debug information as to why FlashAttention could not be run. Defaults to False.



Returns
    
True if FlashAttention can be used with the given parameters; otherwise, False. 

Return type
    
bool
Note
This function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments. 

torch.backends.cuda.can_use_efficient_attention(_params_ , _debug =False_)[source][source]
    
Check if efficient_attention can be utilized in scaled_dot_product_attention. 

Parameters
    
  * **params** (__SDPAParams_) – An instance of SDPAParams containing the tensors for query, key, value, an optional attention mask, dropout rate, and a flag indicating if the attention is causal.
  * **debug** (_bool_) – Whether to logging.warn with information as to why efficient_attention could not be run. Defaults to False.



Returns
    
True if efficient_attention can be used with the given parameters; otherwise, False. 

Return type
    
bool
Note
This function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments. 

torch.backends.cuda.can_use_cudnn_attention(_params_ , _debug =False_)[source][source]
    
Check if cudnn_attention can be utilized in scaled_dot_product_attention. 

Parameters
    
  * **params** (__SDPAParams_) – An instance of SDPAParams containing the tensors for query, key, value, an optional attention mask, dropout rate, and a flag indicating if the attention is causal.
  * **debug** (_bool_) – Whether to logging.warn with information as to why cuDNN attention could not be run. Defaults to False.



Returns
    
True if cuDNN can be used with the given parameters; otherwise, False. 

Return type
    
bool
Note
This function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments. 

torch.backends.cuda.sdp_kernel(_enable_flash =True_, _enable_math =True_, _enable_mem_efficient =True_, _enable_cudnn =True_)[source][source]
    
Warning
This flag is beta and subject to change.
This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.
## torch.backends.cudnn 

torch.backends.cudnn.version()[source][source]
    
Return the version of cuDNN. 

torch.backends.cudnn.is_available()[source][source]
    
Return a bool indicating if CUDNN is currently available. 

torch.backends.cudnn.enabled
    
A `bool` that controls whether cuDNN is enabled. 

torch.backends.cudnn.allow_tf32
    
A `bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices. 

torch.backends.cudnn.deterministic
    
A `bool` that, if True, causes cuDNN to only use deterministic convolution algorithms. See also `torch.are_deterministic_algorithms_enabled()` and `torch.use_deterministic_algorithms()`. 

torch.backends.cudnn.benchmark
    
A `bool` that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest. 

torch.backends.cudnn.benchmark_limit
    
A `int` that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API.
## torch.backends.cusparselt 

torch.backends.cusparselt.version()[source][source]
    
Return the version of cuSPARSELt 

Return type
    
_Optional_[int] 

torch.backends.cusparselt.is_available()[source][source]
    
Return a bool indicating if cuSPARSELt is currently available. 

Return type
    
bool
## torch.backends.mha 

torch.backends.mha.get_fastpath_enabled()[source][source]
    
Returns whether fast path for TransformerEncoder and MultiHeadAttention is enabled, or `True` if jit is scripting.
Note
The fastpath might not be run even if `get_fastpath_enabled` returns `True` unless all conditions on inputs are met. 

Return type
    
bool 

torch.backends.mha.set_fastpath_enabled(_value_)[source][source]
    
Sets whether fast path is enabled
## torch.backends.mps 

torch.backends.mps.is_available()[source][source]
    
Return a bool indicating if MPS is currently available. 

Return type
    
bool 

torch.backends.mps.is_built()[source][source]
    
Return whether PyTorch is built with MPS support.
Note that this doesn’t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it. 

Return type
    
bool
## torch.backends.mkl 

torch.backends.mkl.is_available()[source][source]
    
Return whether PyTorch is built with MKL support. 

_class_ torch.backends.mkl.verbose(_enable_)[source][source]
    
On-demand oneMKL verbosing functionality.
To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.
```
import torch
model(data)
with torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):
  model(data)

```
Copy to clipboard 

Parameters
    
**level** – Verbose level - `VERBOSE_OFF`: Disable verbosing - `VERBOSE_ON`: Enable verbosing
## torch.backends.mkldnn 

torch.backends.mkldnn.is_available()[source][source]
    
Return whether PyTorch is built with MKL-DNN support. 

_class_ torch.backends.mkldnn.verbose(_level_)[source][source]
    
On-demand oneDNN (former MKL-DNN) verbosing functionality.
To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.
```
import torch
model(data)
with torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):
  model(data)

```
Copy to clipboard 

Parameters
    
**level** – Verbose level - `VERBOSE_OFF`: Disable verbosing - `VERBOSE_ON`: Enable verbosing - `VERBOSE_ON_CREATION`: Enable verbosing, including oneDNN kernel creation
## torch.backends.nnpack 

torch.backends.nnpack.is_available()[source][source]
    
Return whether PyTorch is built with NNPACK support. 

torch.backends.nnpack.flags(_enabled =False_)[source][source]
    
Context manager for setting if nnpack is enabled globally 

torch.backends.nnpack.set_flags(__enabled_)[source][source]
    
Set if nnpack is enabled globally
## torch.backends.openmp 

torch.backends.openmp.is_available()[source][source]
    
Return whether PyTorch is built with OpenMP support.
## torch.backends.opt_einsum 

torch.backends.opt_einsum.is_available()[source][source]
    
Return a bool indicating if opt_einsum is currently available.
You must install opt-einsum in order for torch to automatically optimize einsum. To make opt-einsum available, you can install it along with torch: `pip install torch[opt-einsum]` or by itself: `pip install opt-einsum`. If the package is installed, torch will import it automatically and use it accordingly. Use this function to check whether opt-einsum was installed and properly imported by torch. 

Return type
    
bool 

torch.backends.opt_einsum.get_opt_einsum()[source][source]
    
Return the opt_einsum package if opt_einsum is currently available, else None. 

Return type
    
_Any_ 

torch.backends.opt_einsum.enabled
    
A `bool` that controls whether opt_einsum is enabled (`True` by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance.
If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right. 

torch.backends.opt_einsum.strategy
    
A `str` that specifies which strategies to try when `torch.backends.opt_einsum.enabled` is `True`. By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal” strategies are also supported. Note that the “optimal” strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum’s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).
## torch.backends.xeon
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.backends
    * torch.backends.cpu
      * `get_cpu_capability()`
    * torch.backends.cuda
      * `is_built()`
      * `allow_tf32`
      * `allow_fp16_reduced_precision_reduction`
      * `allow_bf16_reduced_precision_reduction`
      * `cufft_plan_cache`
      * `size`
      * `max_size`
      * `clear()`
      * `preferred_blas_library()`
      * `preferred_rocm_fa_library()`
      * `preferred_linalg_library()`
      * `SDPAParams`
      * `flash_sdp_enabled()`
      * `enable_mem_efficient_sdp()`
      * `mem_efficient_sdp_enabled()`
      * `enable_flash_sdp()`
      * `math_sdp_enabled()`
      * `enable_math_sdp()`
      * `fp16_bf16_reduction_math_sdp_allowed()`
      * `allow_fp16_bf16_reduction_math_sdp()`
      * `cudnn_sdp_enabled()`
      * `enable_cudnn_sdp()`
      * `is_flash_attention_available()`
      * `can_use_flash_attention()`
      * `can_use_efficient_attention()`
      * `can_use_cudnn_attention()`
      * `sdp_kernel()`
    * torch.backends.cudnn
      * `version()`
      * `is_available()`
      * `enabled`
      * `allow_tf32`
      * `deterministic`
      * `benchmark`
      * `benchmark_limit`
    * torch.backends.cusparselt
      * `version()`
      * `is_available()`
    * torch.backends.mha
      * `get_fastpath_enabled()`
      * `set_fastpath_enabled()`
    * torch.backends.mps
      * `is_available()`
      * `is_built()`
    * torch.backends.mkl
      * `is_available()`
      * `verbose`
    * torch.backends.mkldnn
      * `is_available()`
      * `verbose`
    * torch.backends.nnpack
      * `is_available()`
      * `flags()`
      * `set_flags()`
    * torch.backends.openmp
      * `is_available()`
    * torch.backends.opt_einsum
      * `is_available()`
      * `get_opt_einsum()`
      * `enabled`
      * `strategy`
    * torch.backends.xeon


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.checkpoint
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.checkpoint
Note
Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward propagation. This can cause persistent states like the RNG state to be more advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply `preserve_rng_state=False` to `checkpoint` or `checkpoint_sequential` to omit stashing and restoring the RNG state during each checkpoint.
The stashing logic saves and restores the RNG state for CPU and another device type (infer the device type from Tensor arguments excluding CPU tensors by `_infer_device_type`) to the `run_fn`. If there are multiple device, device state will only be saved for devices of a single device type, and the remaining devices will be ignored. Consequently, if any checkpointed functions involve randomness, this may result in incorrect gradients. (Note that if CUDA devices are among the devices detected, it will be prioritized; otherwise, the first device encountered will be selected.) If there are no CPU-tensors, the default device type state (default value is cuda, and it could be set to other device by `DefaultDeviceType`) will be saved and restored. However, the logic has no way to anticipate if the user will move Tensors to a new device within the `run_fn` itself. Therefore, if you move Tensors to a new device (“new” meaning not belonging to the set of [current device + devices of Tensor arguments]) within `run_fn`, deterministic output compared to non-checkpointed passes is never guaranteed. 

torch.utils.checkpoint.checkpoint(_function_ , _*args_ , _use_reentrant=None_ , _context_fn= <function noop_context_fn>_, _determinism_check='default'_ , _debug=False_ , _**kwargs_)[source][source]
    
Checkpoint a model or part of the model.
Activation checkpointing is a technique that trades compute for memory. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, forward computation in checkpointed regions omits saving tensors for backward and recomputes them during the backward pass. Activation checkpointing can be applied to any part of a model.
There are currently two checkpointing implementations available, determined by the `use_reentrant` parameter. It is recommended that you use `use_reentrant=False`. Please refer the note below for a discussion of their differences.
Warning
If the `function` invocation during the backward pass differs from the forward pass, e.g., due to a global variable, the checkpointed version may not be equivalent, potentially causing an error being raised or leading to silently incorrect gradients.
Warning
The `use_reentrant` parameter should be passed explicitly. In version 2.4 we will raise an exception if `use_reentrant` is not passed. If you are using the `use_reentrant=True` variant, please refer to the note below for important considerations and potential limitations.
Note
The reentrant variant of checkpoint (`use_reentrant=True`) and the non-reentrant variant of checkpoint (`use_reentrant=False`) differ in the following ways:
  * Non-reentrant checkpoint stops recomputation as soon as all needed intermediate activations have been recomputed. This feature is enabled by default, but can be disabled with `set_checkpoint_early_stop()`. Reentrant checkpoint always recomputes `function` in its entirety during the backward pass.
  * The reentrant variant does not record the autograd graph during the forward pass, as it runs with the forward pass under `torch.no_grad()`. The non-reentrant version does record the autograd graph, allowing one to perform backward on the graph within checkpointed regions.
  * The reentrant checkpoint only supports the `torch.autograd.backward()` API for the backward pass without its inputs argument, while the non-reentrant version supports all ways of performing the backward pass.
  * At least one input and output must have `requires_grad=True` for the reentrant variant. If this condition is unmet, the checkpointed part of the model will not have gradients. The non-reentrant version does not have this requirement.
  * The reentrant version does not consider tensors in nested structures (e.g., custom objects, lists, dicts, etc) as participating in autograd, while the non-reentrant version does.
  * The reentrant checkpoint does not support checkpointed regions with detached tensors from the computational graph, whereas the non-reentrant version does. For the reentrant variant, if the checkpointed segment contains tensors detached using `detach()` or with `torch.no_grad()`, the backward pass will raise an error. This is because `checkpoint` makes all the outputs require gradients and this causes issues when a tensor is defined to have no gradient in the model. To avoid this, detach the tensors outside of the `checkpoint` function.



Parameters
    
  * **function** – describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes `(activation, hidden)`, `function` should correctly use the first input as `activation` and the second input as `hidden`
  * **preserve_rng_state** (_bool_ _,__optional_) – Omit stashing and restoring the RNG state during each checkpoint. Note that under torch.compile, this flag doesn’t take effect and we always preserve RNG state. Default: `True`
  * **use_reentrant** (_bool_) – specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. In version 2.5 we will raise an exception if `use_reentrant` is not passed. If `use_reentrant=False`, `checkpoint` will use an implementation that does not require reentrant autograd. This allows `checkpoint` to support additional functionality, such as working as expected with `torch.autograd.grad` and support for keyword arguments input into the checkpointed function.
  * **context_fn** (_Callable_ _,__optional_) – A callable returning a tuple of two context managers. The function and its recomputation will be run under the first and second context managers respectively. This argument is only supported if `use_reentrant=False`.
  * **determinism_check** (_str_ _,__optional_) – A string specifying the determinism check to perform. By default it is set to `"default"` which compares the shapes, dtypes, and devices of the recomputed tensors against those the saved tensors. To turn off this check, specify `"none"`. Currently these are the only two supported values. Please open an issue if you would like to see more determinism checks. This argument is only supported if `use_reentrant=False`, if `use_reentrant=True`, the determinism check is always disabled.
  * **debug** (_bool_ _,__optional_) – If `True`, error messages will also include a trace of the operators ran during the original forward computation as well as the recomputation. This argument is only supported if `use_reentrant=False`.
  * **args** – tuple containing inputs to the `function`



Returns
    
Output of running `function` on `*args` 

torch.utils.checkpoint.checkpoint_sequential(_functions_ , _segments_ , _input_ , _use_reentrant =None_, _** kwargs_)[source][source]
    
Checkpoint a sequential model to save memory.
Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will not store the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.
Warning
The `use_reentrant` parameter should be passed explicitly. In version 2.4 we will raise an exception if `use_reentrant` is not passed. If you are using the `use_reentrant=True` variant, please see :func:`~torch.utils.checkpoint.checkpoint` for the important considerations and limitations of this variant. It is recommended that you use ``use_reentrant=False`. 

Parameters
    
  * **functions** – A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.
  * **segments** – Number of chunks to create in the model
  * **input** – A Tensor that is input to `functions`
  * **preserve_rng_state** (_bool_ _,__optional_) – Omit stashing and restoring the RNG state during each checkpoint. Default: `True`
  * **use_reentrant** (_bool_) – specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. In version 2.5 we will raise an exception if `use_reentrant` is not passed. If `use_reentrant=False`, `checkpoint` will use an implementation that does not require reentrant autograd. This allows `checkpoint` to support additional functionality, such as working as expected with `torch.autograd.grad` and support for keyword arguments input into the checkpointed function.



Returns
    
Output of running `functions` sequentially on `*inputs`
Example
```
>>> model = nn.Sequential(...)
>>> input_var = checkpoint_sequential(model, chunks, input_var)

```
Copy to clipboard 

torch.utils.checkpoint.set_checkpoint_debug_enabled(_enabled_)[source][source]
    
Context manager that sets whether checkpoint should print additional debug information when running. See the `debug` flag for `checkpoint()` for more information. Note that when set, this context manager overrides the value of `debug` passed to checkpoint. To defer to the local setting, pass `None` to this context. 

Parameters
    
**enabled** (_bool_) – Whether checkpoint should print debug information. Default is ‘None’. 

_class_ torch.utils.checkpoint.CheckpointPolicy(_value_)[source][source]
    
Enum for specifying the policy for checkpointing during backpropagation.
The following policies are supported:
  * `{MUST,PREFER}_SAVE`: The operation’s output will be saved during the forward pass and will not be recomputed during the backward pass
  * `{MUST,PREFER}_RECOMPUTE`: The operation’s output will not be saved during the forward pass and will be recomputed during the backward pass


Use `MUST_*` over `PREFER_*` to indicate that the policy should not be overridden by other subsystems like torch.compile.
Note
A policy function that always returns `PREFER_RECOMPUTE` is equivalent to vanilla checkpointing.
A policy function that returns `PREFER_SAVE` every op is NOT equivalent to not using checkpointing. Using such a policy would save additional tensors not limited to ones that are actually needed for gradient computation. 

_class_ torch.utils.checkpoint.SelectiveCheckpointContext(_*_ , _is_recompute_)[source][source]
    
Context passed to policy function during selective checkpointing.
This class is used to pass relevant metadata to the policy function during selective checkpointing. The metadata includes whether the current invocation of the policy function is during recomputation or not.
Example
```
>>>
>>> def policy_fn(ctx, op, *args, **kwargs):
>>>   print(ctx.is_recompute)
>>>
>>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)
>>>
>>> out = torch.utils.checkpoint.checkpoint(
>>>   fn, x, y,
>>>   use_reentrant=False,
>>>   context_fn=context_fn,
>>> )

```
Copy to clipboard 

torch.utils.checkpoint.create_selective_checkpoint_contexts(_policy_fn_or_list_ , _allow_cache_entry_mutation =False_)[source][source]
    
Helper to avoid recomputing certain ops during activation checkpointing.
Use this with torch.utils.checkpoint.checkpoint to control which operations are recomputed during the backward pass. 

Parameters
    
  * **policy_fn_or_list** (_Callable_ _or_ _List_) – 
    * If a policy function is provided, it should accept a `SelectiveCheckpointContext`, the `OpOverload`, args and kwargs to the op, and return a `CheckpointPolicy` enum value indicating whether the execution of the op should be recomputed or not.
    * If a list of operations is provided, it is equivalent to a policy returning CheckpointPolicy.MUST_SAVE for the specified operations and CheckpointPolicy.PREFER_RECOMPUTE for all other operations.
  * **allow_cache_entry_mutation** (_bool_ _,__optional_) – By default, an error is raised if any tensors cached by selective activation checkpoint are mutated in order to ensure correctness. If set to True, this check is disabled.



Returns
    
A tuple of two context managers.
Example
```
>>> import functools
>>>
>>> x = torch.rand(10, 10, requires_grad=True)
>>> y = torch.rand(10, 10, requires_grad=True)
>>>
>>> ops_to_save = [
>>>   torch.ops.aten.mm.default,
>>> ]
>>>
>>> def policy_fn(ctx, op, *args, **kwargs):
>>>   if op in ops_to_save:
>>>     return CheckpointPolicy.MUST_SAVE
>>>   else:
>>>     return CheckpointPolicy.PREFER_RECOMPUTE
>>>
>>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)
>>>
>>> # or equivalently
>>> context_fn = functools.partial(create_selective_checkpoint_contexts, ops_to_save)
>>>
>>> def fn(x, y):
>>>   return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y
>>>
>>> out = torch.utils.checkpoint.checkpoint(
>>>   fn, x, y,
>>>   use_reentrant=False,
>>>   context_fn=context_fn,
>>> )

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.checkpoint
    * `checkpoint()`
    * `checkpoint_sequential()`
    * `set_checkpoint_debug_enabled()`
    * `CheckpointPolicy`
    * `SelectiveCheckpointContext`
    * `create_selective_checkpoint_contexts()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch Governance | Build + CI
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# PyTorch Governance | Build + CI
## How to Add a New Maintainer
For the person to be a maintainer, a person needs to:
  * Land at least six commits to the related part of the PyTorch repository
  * At least one of these commits must be submitted in the last six months


To add a qualified person to the maintainers’ list, please create a PR that adds a person to the persons of interests page and merge_rules files. Current maintainers will cast their votes of support. Decision criteria for approving the PR:
  * Not earlier than two business days passed before merging (ensure the majority of the contributors have seen it)
  * PR has the correct label (module: ci)
  * There are no objections from the current maintainers
  * There are at least three net _thumbs up_ from current maintainers (or all maintainers vote _thumbs up_ when the module has less than 3 maintainers).


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch Governance | Build + CI
    * How to Add a New Maintainer


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch Design Philosophy
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# PyTorch Design Philosophy
This document is designed to help contributors and module maintainers understand the high-level design principles that have developed over time in PyTorch. These are not meant to be hard-and-fast rules, but to serve as a guide to help trade off different concerns and to resolve disagreements that may come up while developing PyTorch. For more information on contributing, module maintainership, and how to escalate a disagreement to the Core Maintainers, please see PyTorch Governance.
## Design Principles
### Principle 1: Usability over Performance
This principle may be surprising! As one Hacker News poster wrote: _PyTorch is amazing! […] Although I’m confused. How can a ML framework be not obsessed with speed/performance?_ See Hacker News discussion on PyTorch.
Soumith’s blog post on Growing the PyTorch Community goes into this in some depth, but at a high-level:
  * PyTorch’s primary goal is usability
  * A secondary goal is to have _reasonable_ performance


We believe the ability to maintain our flexibility to support researchers who are building on top of our abstractions remains critical. We can’t see what the future of what workloads will be, but we know we want them to be built first on PyTorch and that requires flexibility.
In more concrete terms, we operate in a _usability-first_ manner and try to avoid jumping to _restriction-first_ regimes (for example, static shapes, graph-mode only) without a clear-eyed view of the tradeoffs. Often there is a temptation to impose strict user restrictions upfront because it can simplify implementation, but this comes with risks:
  * The performance may not be worth the user friction, either because the performance benefit is not compelling enough or it only applies to a relatively narrow set of subproblems.
  * Even if the performance benefit is compelling, the restrictions can fragment the ecosystem into different sets of limitations that can quickly become incomprehensible to users.


We want users to be able to seamlessly move their PyTorch code to different hardware and software platforms, to interoperate with different libraries and frameworks, and to experience the full richness of the PyTorch user experience, not a least common denominator subset.
### Principle 2: Simple Over Easy
Here, we borrow from The Zen of Python:
  * _Explicit is better than implicit_
  * _Simple is better than complex_


A more concise way of describing these two goals is Simple Over Easy. Let’s start with an example because _simple_ and _easy_ are often used interchangeably in everyday English. Consider how one may model devices in PyTorch:
  * **Simple / Explicit (to understand, debug):** every tensor is associated with a device. The user explicitly specifies tensor device movement. Operations that require cross-device movement result in an error.
  * **Easy / Implicit (to use):** the user does not have to worry about devices; the system figures out the globally optimal device placement.


In this specific case, and as a general design philosophy, PyTorch favors exposing simple and explicit building blocks rather than APIs that are easy-to-use by practitioners. The simple version is immediately understandable and debuggable by a new PyTorch user: you get a clear error if you call an operator requiring cross-device movement at the point in the program where the operator is actually invoked. The easy solution may let a new user move faster initially, but debugging such a system can be complex: How did the system make its determination? What is the API for plugging into such a system and how are objects represented in its IR?
Some classic arguments in favor of this sort of design come from A Note on Distributed Computation (TLDR: Do not model resources with very different performance characteristics uniformly, the details will leak) and the End-to-End Principle (TLDR: building smarts into the lower-layers of the stack can prevent building performant features at higher layers in the stack, and often doesn’t work anyway). For example, we could build operator-level or global device movement rules, but the precise choices aren’t obvious and building an extensible mechanism has unavoidable complexity and latency costs.
A caveat here is that this does not mean that higher-level “easy” APIs are not valuable; certainly there is a value in, for example, higher-levels in the stack to support efficient tensor computations across heterogeneous compute in a large cluster. Instead, what we mean is that focusing on simple lower-level building blocks helps inform the easy API while still maintaining a good experience when users need to leave the beaten path. It also allows space for innovation and the growth of more opinionated tools at a rate we cannot support in the PyTorch core library, but ultimately benefit from, as evidenced by our rich ecosystem. In other words, not automating at the start allows us to potentially reach levels of good automation faster.
### Principle 3: Python First with Best In Class Language Interoperability
This principle began as **Python First** :
> PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use NumPy, SciPy, scikit-learn, or other Python libraries. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython and Numba. Our goal is to not reinvent the wheel where appropriate.
One thing PyTorch has needed to deal with over the years is Python overhead: we first rewrote the autograd engine in C++, then the majority of operator definitions, then developed TorchScript and the C++ frontend.
Still, working in Python provides easily the best experience for our users: it is flexible, familiar, and perhaps most importantly, has a huge ecosystem of scientific computing libraries and extensions available for use. This fact motivates a few of our most recent contributions, which attempt to hit a Pareto optimal point close to the Python usability end of the curve:
  * TorchDynamo, a Python frame evaluation tool capable of speeding up existing eager-mode PyTorch programs with minimal user intervention.
  * torch_function and torch_dispatch extension points, which have enabled Python-first functionality to be built on-top of C++ internals, such as the torch.fx tracer and functorch respectively.


These design principles are not hard-and-fast rules, but hard won choices and anchor how we built PyTorch to be the debuggable, hackable and flexible framework it is today. As we have more contributors and maintainers, we look forward to applying these core principles with you across our libraries and ecosystem. We are also open to evolving them as we learn new things and the AI space evolves, as we know it will.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch Design Philosophy
    * Design Principles
      * Principle 1: Usability over Performance
      * Principle 2: Simple Over Easy
      * Principle 3: Python First with Best In Class Language Interoperability


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.cpu
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.cpu
This package implements abstractions found in `torch.cuda` to facilitate writing device-agnostic code.
`current_device` | Returns current device for cpu.  
---|---  
`current_stream` | Returns the currently selected `Stream` for a given device.  
`is_available` | Returns a bool indicating if CPU is currently available.  
`synchronize` | Waits for all kernels in all streams on the CPU device to complete.  
`stream` | Wrapper around the Context-manager StreamContext that selects a given stream.  
`set_device` | Sets the current device, in CPU we do nothing.  
`device_count` | Returns number of CPU devices (not cores).  
`StreamContext` | Context-manager that selects a given stream.  
## Streams and events
`Stream` | N.B.  
---|---  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.cpu
    * Streams and events


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch Contribution Guide
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
Note
This page has been deprecated. Please refer to the Contribution Guide on the PyTorch Wiki.
# PyTorch Contribution Guide
PyTorch is a GPU-accelerated Python tensor computation package for building deep neural networks using a tape-based autograd systems.
## Contribution Process
The PyTorch organization is governed by PyTorch Governance and the technical guide to contributing can be found in CONTRIBUTING.md.
The PyTorch development process involves a healthy amount of open discussions between the core development team and the community.
PyTorch operates similarly to most open source projects on GitHub. However, if you’ve never contributed to an open source project before, here is the basic process.
  * **Figure out what you’re going to work on.** The majority of open source contributions come from people scratching their own itches. However, if you don’t know what you want to work on, or are just looking to get more acquainted with the project, here are some tips for how to find appropriate tasks:
    * Look through the issue tracker and see if there are any issues you know how to fix. Issues that are confirmed by other contributors tend to be better to investigate. We also maintain some labels for issues that are likely to be good for new people, e.g., **bootcamp** and **1hr** , although these labels are less well maintained.
    * Join us on dev discuss and let us know you’re interested in getting to know PyTorch. We’re very happy to help out researchers and partners get up to speed with the codebase.
  * **Figure out the scope of your change and reach out for design comments on a GitHub issue if it’s large.** The majority of pull requests are small; in that case, no need to let us know about what you want to do, just get cracking. But if the change is going to be large, it’s usually a good idea to get some design comments about it first by submitting an RFC.
    * If you don’t know how big a change is going to be, we can help you figure it out! Just post about it on issues or dev discuss.
    * Some feature additions are very standardized; for example, lots of people add new operators or optimizers to PyTorch. Design discussion in these cases boils down mostly to, “Do we want this operator/optimizer?” Giving evidence for its utility, e.g., usage in peer reviewed papers, or existence in other frameworks, helps a bit when making this case.
      * **Adding operators / algorithms from recently-released research** is generally not accepted unless there is overwhelming evidence that this newly published work has ground-breaking results and will eventually become a standard in the field. If you are not sure where your method falls, open an issue first before implementing a PR.
    * Core changes and refactors can be quite difficult to coordinate since the pace of development on the PyTorch main branch is quite fast. Definitely reach out about fundamental or cross-cutting changes; we can often give guidance about how to stage such changes into more easily reviewable pieces.
  * **Code it out!**
    * See the CONTRIBUTING.md file for advice for working with PyTorch in a technical form.
  * **Open a pull request.**
    * If you are not ready for the pull request to be reviewed, create a draft pull request first - you can later convert it to a full PR by pressing “Ready for review” button. You can also prepend the title of the PR with “[WIP]” (“work in progress”) while it’s still in draft. We will ignore draft PRs when doing review passes. If you are working on a complex change, it’s good to start things off as a draft, because you will need to spend time looking at CI results to see if things worked out or not.
    * Find an appropriate reviewer for your change. We have some folks who regularly go through the PR queue and try to review everything, but if you happen to know who the maintainer for a given subsystem affected by your patch is, feel free to include them directly on the pull request. You can learn more about Persons of Interest that could review your code.
  * **Iterate on the pull request until it’s accepted!**
    * We’ll try our best to minimize the number of review round trips and block PRs only when there are major issues. For the most common issues in pull requests, take a look at Common Mistakes.
    * Once a pull request is accepted and CI is passing, there is nothing else you need to do; we will merge the PR for you.


## Getting Started
### Proposing New Features
New feature ideas are best discussed on a specific issue. Please include as much information as you can, any accompanying data, and your proposed solution. The PyTorch team and community frequently review new issues and comments where they think they can help. If you feel confident in your solution, go ahead and implement it.
### Reporting Issues
If you’ve identified an issue, first search through the list of existing issues on the repo. If you are unable to find a similar issue, then create a new one. Supply as much information you can to reproduce the problematic behavior. Also, include any additional insights like the behavior you expect.
### Implementing Features or Fixing Bugs
If you want to fix a specific issue, it’s best to comment on the individual issue with your intent. However, we do not lock or assign issues except in cases where we have worked with the developer before. It’s best to strike up a conversation on the issue and discuss your proposed solution. The PyTorch team can provide guidance that saves you time.
Issues that are labeled first-new-issue, low, or medium priority provide the best entrance points and are great places to start.
### Adding Tutorials
A great deal of the tutorials on pytorch.org come from the community itself and we welcome additional contributions. To learn more about how to contribute a new tutorial you can learn more here: PyTorch.org Tutorial Contribution Guide on GitHub
### Improving Documentation & Tutorials
We aim to produce high quality documentation and tutorials. On rare occasions that content includes typos or bugs. If you find something you can fix, send us a pull request for consideration.
Take a look at the Documentation section to learn how our system works.
### Participating in Online Discussions
You can find active discussions happening on the PyTorch Discussion Forums for users as well as the PyTorch Dev Discussion Forums for developers and maintainers.
### Submitting Pull Requests to Fix Open Issues
You can view a list of all open issues here. Commenting on an issue is a great way to get the attention of the team. From here you can share your ideas and how you plan to resolve the issue.
For more challenging issues, the team will provide feedback and direction for how to best solve the issue.
If you’re not able to fix the issue yourself, commenting and sharing whether you can reproduce the issue can help the team identify problem areas.
### Reviewing Open Pull Requests
We appreciate your help reviewing and commenting on pull requests. Our team strives to keep the number of open pull requests at a manageable size, we respond quickly for more information if we need it, and we merge PRs that we think are useful. However, due to the high level of interest, additional eyes on the pull requests are always appreciated.
### Improving Code Readability
Improving code readability helps everyone. It is often better to submit a small number of pull requests that touch a few files versus a large pull request that touches many files. Starting a discussion in the PyTorch forum here or on an issue related to your improvement is the best way to get started.
### Adding Test Cases to Make the Codebase More Robust
Additional test coverage is appreciated.
### Promoting PyTorch
Your use of PyTorch in your projects, research papers, write ups, blogs, or general discussions around the internet helps to raise awareness for PyTorch and our growing community. Please reach out to marketing@pytorch.org for marketing support.
### Triaging Issues
If you feel that an issue could benefit from a particular tag or level of complexity, comment on the issue and share your opinion. If you feel an issue isn’t categorized properly, comment and let the team know.
## About Open Source Development
If this is your first time contributing to an open source project, some aspects of the development process may seem unusual to you.
  * **There is no way to “claim” issues.** People often want to “claim” an issue when they decide to work on it, to ensure that there isn’t wasted work when someone else ends up working on it. This doesn’t really work too well in open source, since someone may decide to work on something, and end up not having time to do it. Feel free to give information in an advisory fashion, but at the end of the day, we will take running code and rough consensus to move forward quickly.
  * **There is a high bar for new functionality.** Unlike in a corporate environment, where the person who wrote code implicitly “owns” it and can be expected to take care of it for the code’s lifetime, once a pull request is merged into an open source project, it immediately becomes the collective responsibility of all maintainers on the project. When we merge code, we are saying that we, the maintainers, can review subsequent changes and make a bugfix to the code. This naturally leads to a higher standard of contribution.


## Common Mistakes To Avoid
  * **Did you add tests?** (Or if the change is hard to test, did you describe how you tested your change?)
    * We have a few motivations for why we ask for tests:
      1. to help us tell if we break it later
      2. to help us tell if the patch is correct in the first place (yes, we did review it, but as Knuth says, “beware of the following code, for I have not run it, merely proven it correct”)
    * When is it OK not to add a test? Sometimes a change can’t be conveniently tested, or the change is so obviously correct (and unlikely to be broken) that it’s OK not to test it. On the contrary, if a change seems likely (or is known to be likely) to be accidentally broken, it’s important to put in the time to work out a testing strategy.
  * **Is your PR too long?**
    * It’s easier for us to review and merge small PRs. The difficulty of reviewing a PR scales nonlinearly with its size.
    * When is it OK to submit a large PR? It helps a lot if there was a corresponding design discussion in an issue, with sign off from the people who are going to review your diff. We can also help give advice about how to split up a large change into individually shippable parts. Similarly, it helps if there is a complete description of the contents of the PR: it’s easier to review code if we know what’s inside!
  * **Comments for subtle things?** In cases where the behavior of your code is nuanced, please include extra comments and documentation to allow us to better understand the intention of your code.
  * **Did you add a hack?** Sometimes, the right answer is a hack. But usually, we will have to discuss it.
  * **Do you want to touch a very core component?** To prevent major regressions, pull requests that touch core components receive extra scrutiny. Make sure you’ve discussed your changes with the team before undertaking major changes.
  * **Want to add a new feature?** If you want to add new features, comment your intention on the related issue. Our team tries to comment on and provide feedback to the community. It’s better to have an open discussion with the team and the rest of the community before building new features. This helps us stay aware of what you’re working on and increases the chance that it’ll be merged.
  * **Did you touch code unrelated to the PR?** To aid in code review, please only include files in your pull request that are directly related to your changes.


## Frequently Asked Questions
  * **How can I contribute as a reviewer?** There is lots of value if community developers reproduce issues, try out new functionality, or otherwise help us identify or troubleshoot issues. Commenting on tasks or pull requests with your environment details is helpful and appreciated.
  * **CI tests failed, what does it mean?** Maybe your PR is based off a broken main branch? You can try to rebase your change on top of the latest main branch. You can also see the current status of main branch’s CI at https://hud.pytorch.org/.
  * **What are the most high risk changes?** Anything that touches build configuration is a risky area. Please avoid changing these unless you’ve had a discussion with the team beforehand.
  * **Hey, a commit showed up on my branch, what’s up with that?** Sometimes another community member will provide a patch or fix to your pull request or branch. This is often needed for getting CI tests to pass.


## On Documentation
### Python Docs
PyTorch documentation is generated from python source using Sphinx. Generated HTML is copied to the docs folder in the main branch of pytorch.github.io, and is served via GitHub pages.
  * Site: https://pytorch.org/docs
  * GitHub: https://github.com/pytorch/pytorch/tree/main/docs
  * Served from: https://github.com/pytorch/pytorch.github.io/tree/master/docs


### C++ Docs
For C++ code we use Doxygen to generate the content files. The C++ docs are built on a special server and the resulting files are copied to the https://github.com/pytorch/cppdocs repo, and are served from GitHub pages.
  * Site: https://pytorch.org/cppdocs
  * GitHub: https://github.com/pytorch/pytorch/tree/main/docs/cpp
  * Served from: https://github.com/pytorch/cppdocs


## Tutorials
PyTorch tutorials are documents used to help understand using PyTorch to accomplish specific tasks or to understand more holistic concepts. Tutorials are built using Sphinx-Gallery from executable python source files, or from restructured-text (rst) files.
  * Site: https://pytorch.org/tutorials
  * GitHub: https://github.com/pytorch/tutorials


### Tutorials Build Overview
For tutorials, pull requests trigger a rebuild of the entire site using CircleCI to test the effects of the change. This build is sharded into 9 worker builds and takes around 40 minutes total. At the same time, we do a Netlify build using _make html-noplot_ , which builds the site without rendering the notebook output into pages for quick review.
After a PR is accepted, the site is rebuilt and deployed using GitHub Actions.
### Contributing a New Tutorial
See PyTorch.org Tutorial Contribution Guide.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch Contribution Guide
    * Contribution Process
    * Getting Started
      * Proposing New Features
      * Reporting Issues
      * Implementing Features or Fixing Bugs
      * Adding Tutorials
      * Improving Documentation & Tutorials
      * Participating in Online Discussions
      * Submitting Pull Requests to Fix Open Issues
      * Reviewing Open Pull Requests
      * Improving Code Readability
      * Adding Test Cases to Make the Codebase More Robust
      * Promoting PyTorch
      * Triaging Issues
    * About Open Source Development
    * Common Mistakes To Avoid
    * Frequently Asked Questions
    * On Documentation
      * Python Docs
      * C++ Docs
    * Tutorials
      * Tutorials Build Overview
      * Contributing a New Tutorial


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.__config__
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.__config__ 

torch.__config__.show()[source][source]
    
Return a human-readable string with descriptions of the configuration of PyTorch. 

Return type
    
str 

torch.__config__.parallel_info()[source][source]
    
Returns detailed string with parallelization settings 

Return type
    
str
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.__config__
    * `show()`
    * `parallel_info()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch Governance | Maintainers
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# PyTorch Governance | Maintainers
## Responsibilities
  * Triage and fix high priority issues assigned to the module or library
  * Triage, review, and land high priority pull requests assigned to the module or library
  * Answer module or library questions on discuss.pytorch.org and dev-discuss.pytorch.org
  * Maintain public user and development documentation
  * Run meetings and share minutes plus roadmap on a half or quarterly basis


## Lead Core Maintainer (BDFL)
  * Soumith Chintala (soumith)


## Core Maintainers
  * Soumith Chintala (soumith)
  * Edward Yang (ezyang)
  * Greg Chanan (gchanan)
  * Dmytro Dzhulgakov (dzhulgakov)
  * Nikita Shulga (malfet)
  * Alban Desmaison (albanD)
  * Piotr Bialecki (ptrblck)


## Module-level maintainers
### NN APIs (torch.nn)
  * Mikayla Gawarecki (mikaylagawarecki)
  * Alban Desmaison (albanD)
  * Joel Schlosser (jbschlosser)
  * (emeritus) Greg Chanan (gchanan)
  * (emeritus) Soumith Chintala (soumith)
  * (emeritus) Sam Gross (colesbury)
  * (emeritus) Adam Paszke (apaszke)


### Optimizers (torch.optim)
  * Jane Xu (janeyx99)
  * Alban Desmaison (albanD)
  * Joel Schlosser (jbschlosser)
  * (emeritus) Soumith Chintala (soumith)
  * (emeritus) Ilqar Ramazanli (iramazanli)
  * (emeritus) Vincent Quenneville-Belair (vincentqb)


### Autograd (torch.autograd)
  * Jeffrey Wan (soulitzer)
  * Alban Desmaison (alband)
  * Edward Yang (ezyang)
  * (emeritus) Adam Paszke (apaszke)


### TorchDynamo
  * Animesh Jain (anijain2305)
  * Jason Ansel (jansel)
  * Edward Yang (ezyang)


### TorchInductor
  * Elias Ellison (eellison)
  * Horace He (Chillee)
  * Shunting Zhang (shunting314)
  * Jason Ansel (jansel)
  * Jiong Gong (jgong5)


### Cudagraph Tree
  * Elias Ellison (eellison)


### PT2 Dispatcher
  * Brian Hirsh (bdhirsh)
  * Richard Zou (zou3519)
  * Horace He (Chillee)
  * Edward Yang (ezyang)


### PT2 Export (torch.export)
  * Avik Chaudhuri (avikchaudhuri)
  * Yanan Cao (gmagogsfm)


### AOT Inductor (AOTI) & AOTI Runtime
  * Bin Bao (desertfire)
  * Angela Yi (angelayi)
  * Yang Chen (chenyang78)


### Compilers (JIT / TorchScript / Package / Deploy)
  * (emeritus) Elias Ellison (eellison)
  * (emeritus) Michael Suo (suo)
  * (emeritus) Yanan Cao (gmagogsfm)
  * (emeritus) James Reed (jamesr66a)
  * (emeritus) Jason Ansel (jansel)
  * (emeritus) Jiong Gong (jgong5)
  * (emeritus) Zach Devito (zdevito)


### Distributions & RNG
  * Fritz Obermeyer (fritzo)
  * Neeraj Pradhan (neerajprad)
  * Alican Bozkurt (alicanb)
  * (emeritus) Vishwak Srinivasan (vishwakftw)


### Distributed
  * Will Constable (wconstab)
  * Howard Huang (H-Huang)
  * Wanchao Liang (wanchaol)
  * Ke Wen (kwen2501)
  * Chien-Chin Huang (fegin)
  * Tristan Rice (d4l3k)
  * (emeritus) Shen Li (mrshenli)
  * (emeritus) Pritam Damania (pritamdamania87)
  * (emeritus) Yanli Zhao (zhaojuanmao)
  * (emeritus) Rohan Varma (rohan-varma)
  * (emeritus) Junjie Wang (fduwjj)
  * (emeritus) Alisson Azzolini (aazzolini)
  * (emeritus) James Reed (jamesr66a)
  * (emeritus) Kiuk Chung (kiukchung)
  * (emeritus) Pieter Noordhuis (pietern)
  * (emeritus) Mingzhe Li (mingzhe09088)
  * (emeritus) Omkar Salpekar (osalpekar)


### Multiprocessing
  * (emeritus) Simon Wang (SsnL)
  * (emeritus) Vitaly Fedyunin (VitalyFedyunin)
  * (emeritus) Adam Paszke (apaszke)


### Linear Algebra (torch.linalg)
  * Mario Lezcano (lezcano)
  * (emeritus) Mike Ruberry (mruberry)
  * (emeritus) Ivan Yashchuk (IvanYashchuk)
  * (emeritus) Vishwak Srinivasan (vishwakftw)
  * (emeritus) Nikita Vedeneev (nikitaved)


### Sparse (torch.sparse)
  * (emeritus) Pearu Peterson (pearu)
  * (emeritus) Nikita Vedeneev (nikitaved)
  * (emeritus) Ivan Yashchuk (IvanYashchuk)
  * (emeritus) Christian Puhrsch (cpuhrsch)
  * (emeritus) Andrew James (amjames)


### NestedTensor (torch.nested)
  * Joel Schlosser (jbschlosser)
  * Christian Puhrsch (cpuhrsch)
  * Driss Guessous (drisspg)
  * Mikayla Gawarecki (mikaylagawarecki)
  * Alban Desmaison (albanD)
  * (emeritus) Natalia Gimelshein (ngimel)


### MaskedTensor (torch.masked)
  * Christian Puhrsch (cpuhrsch)
  * (emeritus) George Qi (george-qi)


### Fast Fourier Transform (torch.fft)
  * (emeritus) Mike Ruberry (mruberry)
  * (emeritus) Peter Bell (peterbell10)


### MKLDNN
  * Xiaobing Zhang (XiaobingSuper)
  * Mingfei Ma (mingfeima)
  * Jiong Gong (jgong5)
  * (emeritus) Xiaoqiang Zheng (zheng-xq)
  * (emeritus) Sam Gross (colesbury)
  * (emeritus) Christian Puhrsch (cpuhrsch)
  * (emeritus) Ilia Cherniavskii (ilia-cher)
  * (emeritus) Junjie Bai (bddppq)
  * (emeritus) Yinghai Lu (yinghai)
  * (emeritus) Vitaly Fedyunin (VitalyFedyunin)
  * (emeritus) Jianhui Li (Jianhui-Li)


### CUDA
  * Natalia Gimelshein (ngimel)
  * Edward Yang (ezyang)
  * Piotr Bialecki (ptrblck)
  * Christian Sarofeen (csarofeen)
  * (emeritus) Andrew Tulloch (ajtulloch)
  * (emeritus) Xiaoqiang Zheng (zheng-xq)


### AMD/ROCm/HIP
  * Jeff Daily (jeffdaily)
  * Jithun Nair (jithunnair-amd)
  * (emeritus) Junjie Bai (bddppq)


### Build + CI
  * Nikita Shulga (malfet)
  * Eli Uriegas (seemethere)
  * Alban Desmaison (alband)
  * Andrey Talman (atalman)
  * Zain Rizvi (ZainRizvi)
  * (emeritus) Mikey Dagitses (dagitses)
  * (emeritus) Omkar Salpekar (osalpekar)
  * (emeritus) Nirav Mehta (mehtanirav)
  * (emeritus) Zhuojie Zhou (zhouzhuojie)
  * (emeritus) Edward Yang (ezyang)
  * (emeritus) Karl Ostmo (kostmo)


### Performance Tools
  * Taylor Robie (robieta)
  * Xu Zhao (xuzhao9)
  * (emeritus) Victor Bittorf (bitfort)
  * (emeritus) Gisle Dankel (gdankel)
  * (emeritus) Natalia Gimelshein (ngimel)
  * (emeritus) Mingzhe Li (mingzhe09088)


### C++ API
  * (emeritus) Joel Schlosser (jbschlosser)
  * (emeritus) Will Feng (yf225)


### C10 utils and operator dispatch
  * Brian Hirsh (bdhirsh)
  * Edward Yang (ezyang)
  * (emeritus) Dmytro Dzhulgakov (dzhulgakov)
  * (emeritus) Sebastian Messmer (smessmer)


### ONNX exporter
  * Shubham Bhokare (shubhambhokare1)
  * Justin Chu (justinchuby)
  * Xavier Dupré (xadupre)
  * Titai Wang (titaiwangms)
  * (emeritus) Bowen Bao (BowenBao)
  * (emeritus) Thiago Crepaldi (thiagocrepaldi)
  * (emeritus) Aaron Bockover (abock)
  * (emeritus) Gary Miguel (garymm)
  * (emeritus) Lara Haidar (lara-hdr)
  * (emeritus) Lu Fang (houseroad)
  * (emeritus) Negin Raoof (neginraoof)
  * (emeritus) Spandan Tiwari (spandantiwari)


### LiteInterpreter
  * (emeritus) David Reiss (dreiss)
  * (emeritus) Raziel Guevara (raziel)
  * (emeritus) Linbin Yu (linbinyu)
  * (emeritus) Ivan Kobzarev (IvanKobzarev)
  * (emeritus) Tao Xu (xta0)


### Quantization (torch/ao)
  * Mark Saroufim (msaroufim)
  * Vasiliy Kuznetsov (vkuzo)
  * Jerry Zhang (jerryzh168)
  * (emeritus) Zafar Takhirov (z-a-f)
  * (emeritus) Raghuraman Krishnamoorthi (raghuramank100)


### Windows
  * (emeritus) Guoliang Hua (nbcsm)
  * (emeritus) Teng Gao (gaoteng-git)
  * (emeritus) Peter Johnson (peterjc123)


### Apple M1/MPS/Metal
  * Kulin Seth (kulinseth)
  * Alban Desmaison (alband)
  * Nikita Shulga (malfet)
  * (emeritus) Ramin Azarmehr (razarmehr)


### PowerPC
  * (emeritus) Alfredo Mendoza (avmgithub)


### x86 CPU
  * Mingfei Ma (mingfeima)
  * Jiong Gong (jgong5)


### AArch64 CPU
  * Sunita Nadampalli (snadampal)


### Docs / Tutorials
  * Svetlana Karslioglu (svekars)


## Library-level maintainers
### XLA
  * Jack Cao (JackCaoG)
  * Daniel Sohn (jysohn23)
  * Zach Cain (zcain117)
  * Brian Hirsh (bdhirsh)
  * Gregory Chanan (gchanan)
  * (emeritus) Ailing Zhang (ailzhang)
  * (emeritus) Davide Libenzi (dlibenzi)
  * (emeritus) Alex Suhan (asuhan)


### TorchServe
  * Li Ning (lxning)
  * Ankith Gunapal (agunapal)
  * Hamid Shojanazeri (HamidShojanazeri)
  * (emeritus) Mark Saroufim (msaroufIm)
  * (emeritus) Manoj Rao (mycpuorg)
  * (emeritus) Vamshi Dantu (vdantu)
  * (emeritus) Dhanasekar Karuppasamy (dhanainme)


### TorchVision
  * Nicolas Hug (NicolasHug)
  * Philip Meier (pmeier)
  * Victor Fomin (vfdev-5)
  * (emeritus) Francisco Massa (fmassa)
  * (emeritus) Vasilis Vryniotis (datumbox)
  * (emeritus) Yosua Michael Maranatha (YosuaMichael)
  * (emeritus) Joao Gomes (jdsgomes)


### TorchText
  * (emeritus) Nayef Ahmed (Nayef211)
  * (emeritus) Parmeet Singh Bhatia (parmeet)
  * (emeritus) Guanheng George Zhang (zhangguanheng66)
  * (emeritus) Christian Puhrsch (cpuhrsch)


### TorchAudio
  * Moto Hira (mthrok)
  * (emeritus) Jeff Hwang (hwangjeff)
  * (emeritus) Caroline Chen (carolineechen)
  * (emeritus) Xiaohui Zhang (xiaohui-zhang)
  * (emeritus) Zhaoheng Ni (nateanl)
  * (emeritus) Christian Puhrsch (cpuhrsch)
  * (emeritus) Vincent QB (vincentqb)


### TorchRec
  * Colin Taylor (colin2328)
  * Paul Zhang (PaulZhang12)
  * (emeritus) Dmytro Ivchenko (divchenko)


### TorchX
  * (emeritus) Tristan Rice (d4l3k)
  * (emeritus) Kiuk Chung (kiukchung)


### TorchData
  * Andrew Ho (andrewkho)
  * Divyansh Khanna (divyanshk)


### TorchArrow
  * (emeritus) Wenlei Xie (wenleix)
  * (emeritus) Vitaly Fedyunin (VitalyFedyunin)


### ExecuTorch (Edge, Mobile)
  * Mergen Nachin (mergennachin)
  * Kimish Patel (kimishpatel)
  * Dave Bort (dbort)
  * Martin Yuan (iseeyuan)


### TorchTune
  * Kartikay Khandelwal (kartikayk)
  * Evan Smothers (ebsmothers)
  * Joe Cummings (joecummings)


### TorchChat
  * Jack Khuu (Jack-Khuu)
  * Jesse White (byjlw)
  * (emeritus) Michael Gschwind (mikekgfb)


### TorchCodec
  * Nicolas Hug (nicolashug)
  * Ahmad Sharif (ahmadsharif1)
  * Scott Schneider (scotts)


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch Governance | Maintainers
    * Responsibilities
    * Lead Core Maintainer (BDFL)
    * Core Maintainers
    * Module-level maintainers
      * NN APIs (torch.nn)
      * Optimizers (torch.optim)
      * Autograd (torch.autograd)
      * TorchDynamo
      * TorchInductor
      * Cudagraph Tree
      * PT2 Dispatcher
      * PT2 Export (torch.export)
      * AOT Inductor (AOTI) & AOTI Runtime
      * Compilers (JIT / TorchScript / Package / Deploy)
      * Distributions & RNG
      * Distributed
      * Multiprocessing
      * Linear Algebra (torch.linalg)
      * Sparse (torch.sparse)
      * NestedTensor (torch.nested)
      * MaskedTensor (torch.masked)
      * Fast Fourier Transform (torch.fft)
      * MKLDNN
      * CUDA
      * AMD/ROCm/HIP
      * Build + CI
      * Performance Tools
      * C++ API
      * C10 utils and operator dispatch
      * ONNX exporter
      * LiteInterpreter
      * Quantization (torch/ao)
      * Windows
      * Apple M1/MPS/Metal
      * PowerPC
      * x86 CPU
      * AArch64 CPU
      * Docs / Tutorials
    * Library-level maintainers
      * XLA
      * TorchServe
      * TorchVision
      * TorchText
      * TorchAudio
      * TorchRec
      * TorchX
      * TorchData
      * TorchArrow
      * ExecuTorch (Edge, Mobile)
      * TorchTune
      * TorchChat
      * TorchCodec


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * C++
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# C++
Note
If you are looking for the PyTorch C++ API docs, directly go here.
PyTorch provides several features for working with C++, and it’s best to choose from them based on your needs. At a high level, the following support is available:
## TorchScript C++ API
TorchScript allows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in the Loading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including:
  * Loading serialized TorchScript models saved from Python
  * Doing simple model modifications if needed (e.g. pulling out submodules)
  * Constructing the input and doing preprocessing using C++ Tensor API


## Extending PyTorch and TorchScript with C++ Extensions
TorchScript can be augmented with user-supplied code through custom operators and custom classes. Once registered with TorchScript, these operators and classes can be invoked in TorchScript code run from Python or from C++ as part of a serialized TorchScript model. The Extending TorchScript with Custom C++ Operators tutorial walks through interfacing TorchScript with OpenCV. In addition to wrapping a function call with a custom operator, C++ classes and structs can be bound into TorchScript through a pybind11-like interface which is explained in the Extending TorchScript with Custom C++ Classes tutorial.
## Tensor and Autograd in C++
Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include:
  * `torch::Tensor` methods such as `add` / `reshape` / `clone`. For the full list of methods available, please see: https://pytorch.org/cppdocs/api/classat_1_1_tensor.html
  * C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see: https://pytorch.org/cppdocs/notes/tensor_indexing.html
  * The tensor autograd APIs and the `torch::autograd` package that are crucial for building dynamic neural networks in C++ frontend. For more details, please see: https://pytorch.org/tutorials/advanced/cpp_autograd.html


## Authoring Models in C++
The “author in TorchScript, infer in C++” workflow requires model authoring to be done in TorchScript. However, there might be cases where the model has to be authored in C++ (e.g. in workflows where a Python component is undesirable). To serve such use cases, we provide the full capability of authoring and training a neural net model purely in C++, with familiar components such as `torch::nn` / `torch::nn::functional` / `torch::optim` that closely resemble the Python API.
  * For an overview of the PyTorch C++ model authoring and training API, please see: https://pytorch.org/cppdocs/frontend.html
  * For a detailed tutorial on how to use the API, please see: https://pytorch.org/tutorials/advanced/cpp_frontend.html
  * Docs for components such as `torch::nn` / `torch::nn::functional` / `torch::optim` can be found at: https://pytorch.org/cppdocs/api/library_root.html


## Packaging for C++
For guidance on how to install and link with libtorch (the library that contains all of the above C++ APIs), please see: https://pytorch.org/cppdocs/installing.html. Note that on Linux there are two types of libtorch binaries provided: one compiled with GCC pre-cxx11 ABI and the other with GCC cxx11 ABI, and you should make the selection based on the GCC ABI your system is using.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * C++
    * TorchScript C++ API
    * Extending PyTorch and TorchScript with C++ Extensions
    * Tensor and Autograd in C++
    * Authoring Models in C++
    * Packaging for C++


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.cpp_extension
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.cpp_extension 

torch.utils.cpp_extension.CppExtension(_name_ , _sources_ , _* args_, _** kwargs_)[source][source]
    
Create a `setuptools.Extension` for C++.
Convenience method that creates a `setuptools.Extension` with the bare minimum (but often sufficient) arguments to build a C++ extension.
All arguments are forwarded to the `setuptools.Extension` constructor. Full list arguments can be found at https://setuptools.pypa.io/en/latest/userguide/ext_modules.html#extension-api-reference
Warning
The PyTorch python API (as provided in libtorch_python) cannot be built with the flag `py_limited_api=True`. When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.
Contrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in `setup`, PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.
Example
```
>>> from setuptools import setup
>>> from torch.utils.cpp_extension import BuildExtension, CppExtension
>>> setup(
...   name='extension',
...   ext_modules=[
...     CppExtension(
...       name='extension',
...       sources=['extension.cpp'],
...       extra_compile_args=['-g'],
...       extra_link_args=['-Wl,--no-as-needed', '-lm'])
...   ],
...   cmdclass={
...     'build_ext': BuildExtension
...   })

```
Copy to clipboard 

torch.utils.cpp_extension.CUDAExtension(_name_ , _sources_ , _* args_, _** kwargs_)[source][source]
    
Create a `setuptools.Extension` for CUDA/C++.
Convenience method that creates a `setuptools.Extension` with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.
All arguments are forwarded to the `setuptools.Extension` constructor. Full list arguments can be found at https://setuptools.pypa.io/en/latest/userguide/ext_modules.html#extension-api-reference
Warning
The PyTorch python API (as provided in libtorch_python) cannot be built with the flag `py_limited_api=True`. When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.
Contrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in `setup`, PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.
Example
```
>>> from setuptools import setup
>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension
>>> setup(
...   name='cuda_extension',
...   ext_modules=[
...     CUDAExtension(
...         name='cuda_extension',
...         sources=['extension.cpp', 'extension_kernel.cu'],
...         extra_compile_args={'cxx': ['-g'],
...                   'nvcc': ['-O2']},
...         extra_link_args=['-Wl,--no-as-needed', '-lcuda'])
...   ],
...   cmdclass={
...     'build_ext': BuildExtension
...   })

```
Copy to clipboard
Compute capabilities:
By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, PyTorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).
You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support:
`TORCH_CUDA_ARCH_LIST="6.1 8.6" python build_my_extension.py` `TORCH_CUDA_ARCH_LIST="5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX" python build_my_extension.py`
The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary’s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, “8.0+PTX” would work functionally because it includes PTX that can runtime-compile for 8.6, but “8.0 8.6” would be better.
Note that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.
Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows. To workaround the issue, move python binding logic to pure C++ file. 

Example use:
    
#include <ATen/ATen.h> at::Tensor SigmoidAlphaBlendForwardCuda(….) 

Instead of:
    
#include <torch/extension.h> torch::Tensor SigmoidAlphaBlendForwardCuda(…)
Currently open issue for nvcc bug: https://github.com/pytorch/pytorch/issues/69460 Complete workaround code example: https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48
Relocatable device code linking:
If you want to reference device symbols across compilation units (across object files), the object files need to be built with relocatable device code (-rdc=true or -dc). An exception to this rule is “dynamic parallelism” (nested kernel launches) which is not used a lot anymore. Relocatable device code is less optimized so it needs to be used only on object files that need it. Using -dlto (Device Link Time Optimization) at the device code compilation step and dlink step helps reduce the protentional perf degradation of -rdc. Note that it needs to be used at both steps to be useful.
If you have rdc objects you need to have an extra -dlink (device linking) step before the CPU symbol linking step. There is also a case where -dlink is used without -rdc: when an extension is linked against a static lib containing rdc-compiled objects like the [NVSHMEM library](https://developer.nvidia.com/nvshmem).
Note: Ninja is required to build a CUDA Extension with RDC linking.
Example
```
>>> CUDAExtension(
...     name='cuda_extension',
...     sources=['extension.cpp', 'extension_kernel.cu'],
...     dlink=True,
...     dlink_libraries=["dlink_lib"],
...     extra_compile_args={'cxx': ['-g'],
...               'nvcc': ['-O2', '-rdc=true']})

```
Copy to clipboard 

torch.utils.cpp_extension.SyclExtension(_name_ , _sources_ , _* args_, _** kwargs_)[source][source]
    
Creates a `setuptools.Extension` for SYCL/C++.
Convenience method that creates a `setuptools.Extension` with the bare minimum (but often sufficient) arguments to build a SYCL/C++ extension.
All arguments are forwarded to the `setuptools.Extension` constructor.
Warning
The PyTorch python API (as provided in libtorch_python) cannot be built with the flag `py_limited_api=True`. When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.
Contrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in `setup`, PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.
Example
```
>>> from torch.utils.cpp_extension import BuildExtension, SyclExtension
>>> setup(
...   name='xpu_extension',
...   ext_modules=[
...   SyclExtension(
...         name='xpu_extension',
...         sources=['extension.cpp', 'extension_kernel.cpp'],
...         extra_compile_args={'cxx': ['-g', '-std=c++20', '-fPIC']})
...   ],
...   cmdclass={
...     'build_ext': BuildExtension
...   })

```
Copy to clipboard
By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension. If down the road a new card is installed the extension may need to be recompiled. You can override the default behavior using TORCH_XPU_ARCH_LIST to explicitly specify which device architectures you want the extension to support:
`TORCH_XPU_ARCH_LIST="pvc,xe-lpg" python build_my_extension.py`
Note that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.
Note: Ninja is required to build SyclExtension. 

torch.utils.cpp_extension.BuildExtension(_* args_, _** kwargs_)[source][source]
    
A custom `setuptools` build extension .
This `setuptools.build_ext` subclass takes care of passing the minimum required compiler flags (e.g. `-std=c++17`) as well as mixed C++/CUDA/SYCL compilation (and support for CUDA/SYCL files in general).
When using `BuildExtension`, it is allowed to supply a dictionary for `extra_compile_args` (rather than the usual list) that maps from languages/compilers (the only expected values are `cxx`, `nvcc` or `sycl`) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++, CUDA and SYCL compiler during mixed compilation.
`use_ninja` (bool): If `use_ninja` is `True` (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard `setuptools.build_ext`. Fallbacks to the standard distutils backend if Ninja is not available.
Note
By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number. 

torch.utils.cpp_extension.load(_name_ , _sources_ , _extra_cflags =None_, _extra_cuda_cflags =None_, _extra_sycl_cflags =None_, _extra_ldflags =None_, _extra_include_paths =None_, _build_directory =None_, _verbose =False_, _with_cuda =None_, _with_sycl =None_, _is_python_module =True_, _is_standalone =False_, _keep_intermediates =True_)[source][source]
    
Load a PyTorch C++ extension just-in-time (JIT).
To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.
By default, the directory to which the build file is emitted and the resulting library compiled to is `<tmp>/torch_extensions/<name>`, where `<tmp>` is the temporary folder on the current platform and `<name>` the name of the extension. This location can be overridden in two ways. First, if the `TORCH_EXTENSIONS_DIR` environment variable is set, it replaces `<tmp>/torch_extensions` and all extensions will be compiled into subfolders of this directory. Second, if the `build_directory` argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.
To compile the sources, the default system compiler (`c++`) is used, which can be overridden by setting the `CXX` environment variable. To pass additional arguments to the compilation process, `extra_cflags` or `extra_ldflags` can be provided. For example, to compile your extension with optimizations, pass `extra_cflags=['-O3']`. You can also use `extra_cflags` to pass further include directories.
CUDA support with mixed compilation is provided. Simply pass CUDA source files (`.cu` or `.cuh`) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking `cudart`. You can pass additional flags to nvcc via `extra_cuda_cflags`, just like with `extra_cflags` for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the `CUDA_HOME` environment variable is the safest option.
SYCL support with mixed compilation is provided. Simply pass SYCL source files (`.sycl`) along with other sources. Such files will be detected and compiled with SYCL compiler (such as Intel DPC++ Compiler) rather than the C++ compiler. You can pass additional flags to SYCL compiler via `extra_sycl_cflags`, just like with `extra_cflags` for C++. SYCL compiler is expected to be found via system PATH environment variable. 

Parameters
    
  * **name** – The name of the extension to build. This MUST be the same as the name of the pybind11 module!
  * **sources** (_Union_ _[__str_ _,__list_ _[__str_ _]__]_) – A list of relative or absolute paths to C++ source files.
  * **extra_cflags** – optional list of compiler flags to forward to the build.
  * **extra_cuda_cflags** – optional list of compiler flags to forward to nvcc when building CUDA sources.
  * **extra_sycl_cflags** – optional list of compiler flags to forward to SYCL compiler when building SYCL sources.
  * **extra_ldflags** – optional list of linker flags to forward to the build.
  * **extra_include_paths** – optional list of include directories to forward to the build.
  * **build_directory** – optional path to use as build workspace.
  * **verbose** – If `True`, turns on verbose logging of load steps.
  * **with_cuda** (_Optional_ _[__bool_ _]_) – Determines whether CUDA headers and libraries are added to the build. If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`. Set it to True` to force CUDA headers and libraries to be included.
  * **with_sycl** (_Optional_ _[__bool_ _]_) – Determines whether SYCL headers and libraries are added to the build. If set to `None` (default), this value is automatically determined based on the existence of `.sycl` in `sources`. Set it to True` to force SYCL headers and libraries to be included.
  * **is_python_module** – If `True` (default), imports the produced shared library as a Python module. If `False`, behavior depends on `is_standalone`.
  * **is_standalone** – If `False` (default) loads the constructed extension into the process as a plain dynamic library. If `True`, build a standalone executable.



Returns
    
Returns the loaded PyTorch extension as a Python module. 

If `is_python_module` is `False` and `is_standalone` is `False`:
    
Returns nothing. (The shared library is loaded into the process as a side effect.) 

If `is_standalone` is `True`.
    
Return the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.) 

Return type
    
If `is_python_module` is `True`
Example
```
>>> from torch.utils.cpp_extension import load
>>> module = load(
...   name='extension',
...   sources=['extension.cpp', 'extension_kernel.cu'],
...   extra_cflags=['-O2'],
...   verbose=True)

```
Copy to clipboard 

torch.utils.cpp_extension.load_inline(_name_ , _cpp_sources_ , _cuda_sources =None_, _sycl_sources =None_, _functions =None_, _extra_cflags =None_, _extra_cuda_cflags =None_, _extra_sycl_cflags =None_, _extra_ldflags =None_, _extra_include_paths =None_, _build_directory =None_, _verbose =False_, _with_cuda =None_, _with_sycl =None_, _is_python_module =True_, _with_pytorch_error_handling =True_, _keep_intermediates =True_, _use_pch =False_)[source][source]
    
Load a PyTorch C++ extension just-in-time (JIT) from string sources.
This function behaves exactly like `load()`, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of `load_inline()` is identical to `load()`.
See the tests for good examples of using this function.
Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to `cpp_sources` are first concatenated into a single `.cpp` file. This file is then prepended with `#include <torch/extension.h>`.
Furthermore, if the `functions` argument is supplied, bindings will be automatically generated for each function specified. `functions` can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.
The sources in `cuda_sources` are concatenated into a separate `.cu` file and prepended with `torch/types.h`, `cuda.h` and `cuda_runtime.h` includes. The `.cpp` and `.cu` files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in `cuda_sources` per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the `cpp_sources` (and include its name in `functions`).
The sources in `sycl_sources` are concatenated into a separate `.sycl` file and prepended with `torch/types.h`, `sycl/sycl.hpp` includes. The `.cpp` and `.sycl` files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in `sycl_sources` per se. To bind to a SYCL kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the `cpp_sources` (and include its name in `functions`).
See `load()` for a description of arguments omitted below. 

Parameters
    
  * **cpp_sources** – A string, or list of strings, containing C++ source code.
  * **cuda_sources** – A string, or list of strings, containing CUDA source code.
  * **sycl_sources** – A string, or list of strings, containing SYCL source code.
  * **functions** – A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).
  * **with_cuda** – Determines whether CUDA headers and libraries are added to the build. If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided. Set it to `True` to force CUDA headers and libraries to be included.
  * **with_sycl** – Determines whether SYCL headers and libraries are added to the build. If set to `None` (default), this value is automatically determined based on whether `sycl_sources` is provided. Set it to `True` to force SYCL headers and libraries to be included.
  * **with_pytorch_error_handling** – Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function `foo` is called via an intermediary `_safe_foo` function. This redirection might cause issues in obscure cases of cpp. This flag should be set to `False` when this redirect causes issues.


Example
```
>>> from torch.utils.cpp_extension import load_inline
>>> source = """
at::Tensor sin_add(at::Tensor x, at::Tensor y) {
 return x.sin() + y.sin();
}
"""
>>> module = load_inline(name='inline_extension',
...            cpp_sources=[source],
...            functions=['sin_add'])

```
Copy to clipboard
Note
Since load_inline will just-in-time compile the source code, please ensure that you have the right toolchains installed in the runtime. For example, when loading C++, make sure a C++ compiler is available. If you’re loading a CUDA extension, you will need to additionally install the corresponding CUDA toolkit (nvcc and any other dependencies your code has). Compiling toolchains are not included when you install torch and must be additionally installed.
During compiling, by default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number. 

torch.utils.cpp_extension.include_paths(_device_type ='cpu'_)[source][source]
    
Get the include paths required to build a C++ or CUDA or SYCL extension. 

Parameters
    
**device_type** (_str_) – Defaults to “cpu”. 

Returns
    
A list of include path strings. 

Return type
    
list[str] 

torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version(_compiler_)[source][source]
    
Determine if the given compiler is ABI-compatible with PyTorch alongside its version. 

Parameters
    
**compiler** (_str_) – The compiler executable name to check (e.g. `g++`). Must be executable in a shell process. 

Returns
    
A tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch, followed by a TorchVersion string that contains the compiler version separated by dots. 

Return type
    
tuple[bool, torch.torch_version.TorchVersion] 

torch.utils.cpp_extension.verify_ninja_availability()[source][source]
    
Raise `RuntimeError` if ninja build system is not available on the system, does nothing otherwise. 

torch.utils.cpp_extension.is_ninja_available()[source][source]
    
Return `True` if the ninja build system is available on the system, `False` otherwise.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.cpp_extension
    * `CppExtension()`
    * `CUDAExtension()`
    * `SyclExtension()`
    * `BuildExtension()`
    * `load()`
    * `load_inline()`
    * `include_paths()`
    * `get_compiler_abi_compatibility_and_version()`
    * `verify_ninja_availability()`
    * `is_ninja_available()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch Governance | Mechanics
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# PyTorch Governance | Mechanics
## Summary
PyTorch adopts a technical governance structure that is hierarchical.
  * A community of **contributors** who file issues, make pull requests, and contribute to the project.
  * A small set of **module maintainers** drive each module of the PyTorch project.
  * They are overseen by **core maintainers** , who drive the overall project direction.
  * The core maintainers have a **lead core maintainer** who is the catch-all decision maker.


All maintainers are expected to have a strong bias towards PyTorch’s design philosophy.
Beyond the maintainers, the community is encouraged to contribute, file issues, make proposals, review pull requests and be present in the community. Given contributions and willingness to invest, anyone can be accepted as a maintainer and provided write access or ownership of parts of the codebase.
Technical governance is strictly separated from business governance. Separating technical from business governance ensures that there is no way for any person or company to “buy their way into” the technical guidance of the project. Additionally, membership in the technical governance process is for **individuals** , not companies. That is, there are no seats reserved for specific companies, and membership is associated with the person rather than the company employing that person.
## Module Maintainers
Modules are defined as GitHub repositories within the PyTorch org, or as directories within the core repository pytorch/pytorch. Each module will have its own maintainer group. Maintainer groups are responsible for reviewing and approving commits, improving design, and changing the scope of the module. Each maintainer group may adopt its own rules and procedures for making decisions (majority vote being default). Module maintainers have the right to dispute decisions made by other module maintainers – especially if it affects them. When disputes are made, the module maintainer group should provide a reasonable and public explanation of the dispute, the relevant arguments, and the resolution. In the exceptional cases where module maintainers cannot come to a conclusion themselves, they will escalate to core maintainers for review. The escalations are resolved by the core maintainers in accordance with their rules and procedures.
Each maintainer group should publish publicly available communication for their module (a vision, rough roadmap, design docs, any disputes and dispute resolutions) so that contributors and other interested parties understand the future direction of the project and can participate in discussion.
Responsibilities of the maintainer includes:
  * Triaging high priority issues of the module
  * Triaging and reviewing and landing high priority pull requests of the module
  * Supporting public documentation related to the module
  * Running public developer meetings


## Core Maintainers
The core maintainers are expected to have a deep understanding of the PyTorch code base and design philosophies. Their responsibilities include:
  * Articulating a cohesive long-term vision for the project
  * Negotiating and resolving contentious issues in ways acceptable to all parties involved
  * Receiving broad requests for changes from stakeholders of PyTorch and evaluating / accepting them (small module-level requests are handled by module maintainers)


The core maintainers as a group have the power to veto any decision made at a Module maintainer level. The core maintainers have power to resolve disputes as they see fit. The core maintainers should publicly articulate their decision-making, and give a clear reasoning for their decisions, vetoes and dispute resolution.
The core maintainers are admins of the PyTorch GitHub Org and are listed in Maintainers.
## Lead Core Maintainer (BDFL)
There may be decisions in which the core maintainers cannot come to a consensus. To make such difficult decisions, the core maintainers have an assigned and publicly declared Lead Core Maintainer amongst them, also commonly known in open-source governance models as a BDFL.
The Lead Core Maintainer should publicly articulate their decision-making, and give a clear reasoning for their decisions. The Lead Core Maintainer is also responsible for confirming or removing core maintainers.
## Nominating, Confirming and Removing Maintainers
### The Principles
  * Membership in module maintainer groups is given to **individuals** on **merit basis** after they demonstrated strong expertise of the component through contributions, reviews and discussions and are aligned with how the component fits in overall PyTorch direction.
  * For membership in the maintainer group the individual has to demonstrate strong and continued alignment with the overall PyTorch principles.
  * No term limits for module maintainers or core maintainers
  * Light criteria of moving module maintenance to ‘emeritus’ status if they don’t actively participate over long periods of time. Each module maintainer group may define the inactive period that’s appropriate for that module.
  * The membership is for an individual, not a company.


### The Process for Nomination
  * Each module has its own process. Please contact module maintainers for more information. However, if there is no process identified, you can file a request to the core maintainers by submitting this form. Core maintainers are meeting every three months.
  * If you are submitting a request to the core maintainers, the information in your request must include the following items:
    * The nominees depth and breadth of code, review and design contributions on the module
    * Testimonials (positive and negative) of the nominee’s interactions with the maintainers, users, and the community
    * General testimonials of support from the maintainers
  * The core maintainers then evaluate all information and make a final decision to Confirm or Decline the nomination. The decision of the core maintainers has to be articulated well and would be public.


### The Process for Removal
  * Similar to the process for nomination, anyone in the community can nominate a person to be removed from a Module maintainer position or a Core maintainer position.
  * A person can also self-nominate to be removed
  * The core maintainers (excluding persons with conflict of interest) will request or put together more information around the following:
    * Their activity (or lack of) on the project
    * Their changing thinking of the space, which results in conflict with the overall direction of the project
    * Other information that makes them unfit to be a maintainer, such as Code of Conduct issues, their activity outside the scope of the project that conflicts with the project’s values
    * **Conflicts of interest** : filial or romantic relationships
  * The core maintainers then evaluate all information and make a final decision to Confirm or Decline the removal. The decision of the core maintainers has to be articulated well and would be public.


### Nominating Core Maintainers
  * Any core or module maintainer can nominate someone to become a core maintainer
  * The lead maintainer (BDFL) is responsible for evaluating the nomination.
  * The lead maintainer requests or puts together more information around the strength of the candidate to be a core maintainer:
    * Letters of support from other core and module maintainers
    * General letters of support from stakeholders within the PyTorch community
    * Any new relevant information that is befitting for the candidacy
  * The lead maintainer evaluates all information and makes a final decision to Confirm or Decline the nomination, with a clear public articulation of their reasoning behind the decision.


### Removing the Lead Core Maintainer and Nominating a New Lead Core Maintainer
  * A super-majority of core maintainers (75%) can choose to remove the Lead Core Maintainer
  * After a removal of the Lead Core Maintainer or in unforeseen circumstances (such as permanent unavailability of the Lead Core Maintainer), the core maintainers follow a Ranked-Choice voting method to elect a new Lead Core Maintainer.


## Add, Remove, and Re-Scope Modules and Projects
The core maintainers together are responsible for taking decisions on adding, removing and re-scoping new modules in the PyTorch org, either as new repositories in the PyTorch GitHub org, or as folders in the pytorch/pytorch repository.
They invite proposals from members in the community (including themselves) for such changes. The proposals are open-ended, but should have some basic ground-work to make a convincing case to make change. The following is an example approach to this process:
  1. Interview researchers / stakeholders, talk to community, gather issues;
  2. Read papers, attend conferences, build example pipelines based on experience;
  3. Create a state of the world - make sure this change is necessary, for example adding a new project or module is worth the maintenance cost; or removing a project or module will not remove too much value from PyTorch;
  4. Create a proposal; the proposal covers the maintainership, development and community plan once the proposal is approved.


The core maintainers take final decisions on the proposal, articulating the reasoning behind the decision publicly.
## Decision Making
### Uncontroversial Changes
Primary work happens through issues and pull requests on GitHub. Maintainers should avoid pushing their changes directly to the PyTorch repository, instead relying on pull requests. Approving a pull request by a core or module maintainer allows it to be merged without further process. Core and module maintainers, as listed on the Maintainers page and within CODEOWNERS ultimately approve these changes.
Notifying relevant experts about an issue or a pull request is important. Reviews from experts in the given interest area are strongly preferred, especially on pull request approvals. Failure to do so might end up with the change being reverted by the relevant expert.
### Controversial Decision Process
Substantial changes in a given interest area require a GitHub issue to be opened for discussion. This includes:
  * Any semantic or syntactic change to the PyTorch framework or library.
  * Backwards-incompatible changes to the Python or C++ API.
  * Additions to the core framework or library, including substantial new functionality within an existing library.
  * Removal of core features or platform support


Core and module maintainers ultimately approve these changes.
### General Project Policies
PyTorch has been established as PyTorch a Series of LF Projects, LLC. Policies applicable to PyTorch and participants in PyTorch, including guidelines on the usage of trademarks, are located at https://www.lfprojects.org/policies/.
PyTorch participants acknowledge that the copyright in all new contributions will be retained by the copyright holder as independent works of authorship and that no contributor or copyright holder will be required to assign copyrights to the project. Except as described below, all code contributions to the project must be made using the 3-Clause-BSD License available here: https://opensource.org/licenses/BSD-3-Clause (the “Project License”). All outbound code will be made available under the Project License. The Maintainers may approve the use of an alternative open license or licenses for inbound or outbound contributions on an exception basis.
## FAQ
**Q: What if I would like to own (or partly own) a part of the project such as a feature area or domain library, for example** Linear Algebra **or** Torch Vision **?** This is absolutely possible. The first step is to start contributing to the existing project area and supporting its health and success. In addition to this, you can make a proposal through a GitHub issue for new functionality or changes to improve the project area.
**Q: What if I am a company looking to use PyTorch internally for development, can I be granted or purchase a board seat to drive the project direction?** No, the PyTorch project is strictly driven by the a maintainer project philosophy and clearly separates technical governance from business governance. However, if you want to be involved in sponsorship and support, you can become involved in the PyTorch Foundation (PTF) and sponsorship through this. You can also have individual engineers look to become maintainers, but this is not guaranteed and is merit-based.
**Q: Does the PyTorch project support grants or ways to support independent developers using or contributing to the project?** No, not at this point. We are however looking at ways to better support the community of independent developers around PyTorch. If you have suggestions or inputs, please reach out on the PyTorch forums to discuss.
**Q: How do I contribute code to the project?** If the change is relatively minor, a pull request on GitHub can be opened up immediately for review and merge by the project committers. For larger changes, please open an issue to make a proposal to discuss prior. Please also see the PyTorch Contributor Wiki for contribution for a walkthrough.
**Q: Can I become a committer on the project?** Unfortunately, the current commit process to PyTorch involves an interaction with Facebook infrastructure that can only be triggered by Facebook employees. We are however looking at ways to expand the committer base to individuals outside of Facebook and will provide an update when the tooling exists to allow this.
**Q: What if I would like to deliver a PyTorch tutorial at a conference or otherwise? Do I need to be ‘officially’ a committer to do this?** No, we encourage community members to showcase their work wherever and whenever they can. Please reach out to marketing@pytorch.org for marketing support.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch Governance | Mechanics
    * Summary
    * Module Maintainers
    * Core Maintainers
    * Lead Core Maintainer (BDFL)
    * Nominating, Confirming and Removing Maintainers
      * The Principles
      * The Process for Nomination
      * The Process for Removal
      * Nominating Core Maintainers
      * Removing the Lead Core Maintainer and Nominating a New Lead Core Maintainer
    * Add, Remove, and Re-Scope Modules and Projects
    * Decision Making
      * Uncontroversial Changes
      * Controversial Decision Process
      * General Project Policies
    * FAQ


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Complex Numbers
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Complex Numbers
Complex numbers are numbers that can be expressed in the form a+bja + bja+bj, where a and b are real numbers, and _j_ is called the imaginary unit, which satisfies the equation j2=−1j^2 = -1j2=−1. Complex numbers frequently occur in mathematics and engineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have handled complex numbers by representing the data in float tensors with shape (...,2)(..., 2)(...,2) where the last dimension contains the real and imaginary values.
Tensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on complex tensors (e.g., `torch.mv()`, `torch.matmul()`) are likely to be faster and more memory efficient than operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized to use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).
Note
Spectral operations in the torch.fft module support native complex tensors.
Warning
Complex tensors is a beta feature and subject to change.
## Creating Complex Tensors
We support two complex dtypes: torch.cfloat and torch.cdouble
```
>>> x = torch.randn(2,2, dtype=torch.cfloat)
>>> x
tensor([[-0.4621-0.0303j, -0.2438-0.5874j],
   [ 0.7706+0.1421j, 1.2110+0.1918j]])

```
Copy to clipboard
Note
The default dtype for complex tensors is determined by the default floating point dtype. If the default floating point dtype is torch.float64 then complex numbers are inferred to have a dtype of torch.complex128, otherwise they are assumed to have a dtype of torch.complex64.
All factory functions apart from `torch.linspace()`, `torch.logspace()`, and `torch.arange()` are supported for complex tensors.
## Transition from the old representation
Users who currently worked around the lack of complex tensors with real tensors of shape (...,2)(..., 2)(...,2) can easily to switch using the complex tensors in their code using `torch.view_as_complex()` and `torch.view_as_real()`. Note that these functions don’t perform any copy and return a view of the input tensor.
```
>>> x = torch.randn(3, 2)
>>> x
tensor([[ 0.6125, -0.1681],
   [-0.3773, 1.3487],
   [-0.0861, -0.7981]])
>>> y = torch.view_as_complex(x)
>>> y
tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])
>>> torch.view_as_real(y)
tensor([[ 0.6125, -0.1681],
   [-0.3773, 1.3487],
   [-0.0861, -0.7981]])

```
Copy to clipboard
## Accessing real and imag
The real and imaginary values of a complex tensor can be accessed using the `real` and `imag`.
Note
Accessing real and imag attributes doesn’t allocate any memory, and in-place updates on the real and imag tensors will update the original complex tensor. Also, the returned real and imag tensors are not contiguous.
```
>>> y.real
tensor([ 0.6125, -0.3773, -0.0861])
>>> y.imag
tensor([-0.1681, 1.3487, -0.7981])
>>> y.real.mul_(2)
tensor([ 1.2250, -0.7546, -0.1722])
>>> y
tensor([ 1.2250-0.1681j, -0.7546+1.3487j, -0.1722-0.7981j])
>>> y.real.stride()
(2,)

```
Copy to clipboard
## Angle and abs
The angle and absolute values of a complex tensor can be computed using `torch.angle()` and `torch.abs()`.
```
>>> x1=torch.tensor([3j, 4+4j])
>>> x1.abs()
tensor([3.0000, 5.6569])
>>> x1.angle()
tensor([1.5708, 0.7854])

```
Copy to clipboard
## Linear Algebra
Many linear algebra operations, like `torch.matmul()`, `torch.linalg.svd()`, `torch.linalg.solve()` etc., support complex numbers. If you’d like to request an operation we don’t currently support, please search if an issue has already been filed and if not, file one.
## Serialization
Complex tensors can be serialized, allowing data to be saved as complex values.
```
>>> torch.save(y, 'complex_tensor.pt')
>>> torch.load('complex_tensor.pt')
tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])

```
Copy to clipboard
## Autograd
PyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative, the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, all the existing optimizers can be implemented to work out of the box with complex parameters. For more details, check out the note Autograd for Complex Numbers.
## Optimizers
Semantically, we define stepping through a PyTorch optimizer with complex parameters as being equivalent to stepping through the same optimizer on the `torch.view_as_real()` equivalent of the complex params. More concretely:
```
>>> params = [torch.rand(2, 3, dtype=torch.complex64) for _ in range(5)]
>>> real_params = [torch.view_as_real(p) for p in params]
>>> complex_optim = torch.optim.AdamW(params)
>>> real_optim = torch.optim.AdamW(real_params)

```
Copy to clipboard
real_optim and complex_optim will compute the same updates on the parameters, though there may be slight numerical discrepancies between the two optimizers, similar to numerical discrepancies between foreach vs forloop optimizers and capturable vs default optimizers. For more details, see https://pytorch.org/docs/stable/notes/numerical_accuracy.html.
Specifically, while you can think of our optimizer’s handling of complex tensors as the same as optimizing over their p.real and p.imag pieces separately, the implementation details are not precisely that. Note that the `torch.view_as_real()` equivalent will convert a complex tensor to a real tensor with shape (...,2)(..., 2)(...,2), whereas splitting a complex tensor into two tensors is 2 tensors of size (...)(...)(...). This distinction has no impact on pointwise optimizers (like AdamW) but will cause slight discrepancy in optimizers that do global reductions (like LBFGS). We currently do not have optimizers that do per-Tensor reductions and thus do not yet define this behavior. Open an issue if you have a use case that requires precisely defining this behavior.
We do not fully support the following subsystems:
  * Quantization
  * JIT
  * Sparse Tensors
  * Distributed


If any of these would help your use case, please search if an issue has already been filed and if not, file one.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Complex Numbers
    * Creating Complex Tensors
    * Transition from the old representation
    * Accessing real and imag
    * Angle and abs
    * Linear Algebra
    * Serialization
    * Autograd
    * Optimizers


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.cuda
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.cuda
This package adds support for CUDA tensor types.
It implements the same function as CPU tensors, but they utilize GPUs for computation.
It is lazily initialized, so you can always import it, and use `is_available()` to determine if your system supports CUDA.
CUDA semantics has more details about working with CUDA.
`StreamContext` | Context-manager that selects a given stream.  
---|---  
`can_device_access_peer` | Check if peer access between two devices is possible.  
`current_blas_handle` | Return cublasHandle_t pointer to current cuBLAS handle  
`current_device` | Return the index of a currently selected device.  
`current_stream` | Return the currently selected `Stream` for a given device.  
`cudart` | Retrieves the CUDA runtime API module.  
`default_stream` | Return the default `Stream` for a given device.  
`device` | Context-manager that changes the selected device.  
`device_count` | Return the number of GPUs available.  
`device_memory_used` | Return used global (device) memory in bytes as given by nvidia-smi or amd-smi.  
`device_of` | Context-manager that changes the current device to that of given object.  
`get_arch_list` | Return list CUDA architectures this library was compiled for.  
`get_device_capability` | Get the cuda capability of a device.  
`get_device_name` | Get the name of a device.  
`get_device_properties` | Get the properties of a device.  
`get_gencode_flags` | Return NVCC gencode flags this library was compiled with.  
`get_stream_from_external` | Return a `Stream` from an externally allocated CUDA stream.  
`get_sync_debug_mode` | Return current value of debug mode for cuda synchronizing operations.  
`init` | Initialize PyTorch's CUDA state.  
`ipc_collect` | Force collects GPU memory after it has been released by CUDA IPC.  
`is_available` | Return a bool indicating if CUDA is currently available.  
`is_initialized` | Return whether PyTorch's CUDA state has been initialized.  
`is_tf32_supported` | Return a bool indicating if the current CUDA/ROCm device supports dtype tf32.  
`memory_usage` | Return the percent of time over the past sample period during which global (device) memory was being read or written as given by nvidia-smi.  
`set_device` | Set the current device.  
`set_stream` | Set the current stream.This is a wrapper API to set the stream.  
`set_sync_debug_mode` | Set the debug mode for cuda synchronizing operations.  
`stream` | Wrap around the Context-manager StreamContext that selects a given stream.  
`synchronize` | Wait for all kernels in all streams on a CUDA device to complete.  
`utilization` | Return the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.  
`temperature` | Return the average temperature of the GPU sensor in Degrees C (Centigrades).  
`power_draw` | Return the average power draw of the GPU sensor in mW (MilliWatts)  
`clock_rate` | Return the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by nvidia-smi.  
`OutOfMemoryError` | Exception raised when device is out of memory  
## Random Number Generator
`get_rng_state` | Return the random number generator state of the specified GPU as a ByteTensor.  
---|---  
`get_rng_state_all` | Return a list of ByteTensor representing the random number states of all devices.  
`set_rng_state` | Set the random number generator state of the specified GPU.  
`set_rng_state_all` | Set the random number generator state of all devices.  
`manual_seed` | Set the seed for generating random numbers for the current GPU.  
`manual_seed_all` | Set the seed for generating random numbers on all GPUs.  
`seed` | Set the seed for generating random numbers to a random number for the current GPU.  
`seed_all` | Set the seed for generating random numbers to a random number on all GPUs.  
`initial_seed` | Return the current random seed of the current GPU.  
## Communication collectives
`comm.broadcast` | Broadcasts a tensor to specified GPU devices.  
---|---  
`comm.broadcast_coalesced` | Broadcast a sequence of tensors to the specified GPUs.  
`comm.reduce_add` | Sum tensors from multiple GPUs.  
`comm.scatter` | Scatters tensor across multiple GPUs.  
`comm.gather` | Gathers tensors from multiple GPU devices.  
## Streams and events
`Stream` | Wrapper around a CUDA stream.  
---|---  
`ExternalStream` | Wrapper around an externally allocated CUDA stream.  
`Event` | Wrapper around a CUDA event.  
## Graphs (beta)
`is_current_stream_capturing` | Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.  
---|---  
`graph_pool_handle` | Return an opaque token representing the id of a graph memory pool.  
`CUDAGraph` | Wrapper around a CUDA graph.  
`graph` | Context-manager that captures CUDA work into a `torch.cuda.CUDAGraph` object for later replay.  
`make_graphed_callables` | Accept callables (functions or `nn.Module`s) and returns graphed versions.  
## Memory management
`empty_cache` | Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  
---|---  
`get_per_process_memory_fraction` | Get memory fraction for a process.  
`list_gpu_processes` | Return a human-readable printout of the running processes and their GPU memory use for a given device.  
`mem_get_info` | Return the global free and total GPU memory for a given device using cudaMemGetInfo.  
`memory_stats` | Return a dictionary of CUDA memory allocator statistics for a given device.  
`host_memory_stats` | Return a dictionary of CUDA memory allocator statistics for a given device.  
`memory_summary` | Return a human-readable printout of the current memory allocator statistics for a given device.  
`memory_snapshot` | Return a snapshot of the CUDA memory allocator state across all devices.  
`memory_allocated` | Return the current GPU memory occupied by tensors in bytes for a given device.  
`max_memory_allocated` | Return the maximum GPU memory occupied by tensors in bytes for a given device.  
`reset_max_memory_allocated` | Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device.  
`memory_reserved` | Return the current GPU memory managed by the caching allocator in bytes for a given device.  
`max_memory_reserved` | Return the maximum GPU memory managed by the caching allocator in bytes for a given device.  
`set_per_process_memory_fraction` | Set memory fraction for a process.  
`memory_cached` | Deprecated; see `memory_reserved()`.  
`max_memory_cached` | Deprecated; see `max_memory_reserved()`.  
`reset_max_memory_cached` | Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.  
`reset_peak_memory_stats` | Reset the "peak" stats tracked by the CUDA memory allocator.  
`reset_peak_host_memory_stats` | Reset the "peak" stats tracked by the host memory allocator.  
`caching_allocator_alloc` | Perform a memory allocation using the CUDA memory allocator.  
`caching_allocator_delete` | Delete memory allocated using the CUDA memory allocator.  
`get_allocator_backend` | Return a string describing the active allocator backend as set by `PYTORCH_CUDA_ALLOC_CONF`.  
`CUDAPluggableAllocator` | CUDA memory allocator loaded from a so file.  
`change_current_allocator` | Change the currently used memory allocator to be the one provided.  
`MemPool` | MemPool represents a pool of memory in a caching allocator.  
`MemPoolContext` | MemPoolContext holds the currently active pool and stashes the previous pool.  
`caching_allocator_enable` | Enable or disable the CUDA memory allocator.  
---|--- 

_class_ torch.cuda.use_mem_pool(_pool_ , _device =None_)[source][source]
      
A context manager that routes allocations to a given pool. 

Parameters
    
  * **pool** (_torch.cuda.MemPool_) – a MemPool object to be made active so that allocations route to this pool.
  * **device** (_torch.device_ _or_ _int_ _,__optional_) – selected device. Uses MemPool on the current device, given by `current_device()`, if `device` is `None` (default).


## NVIDIA Tools Extension (NVTX)
`nvtx.mark` | Describe an instantaneous event that occurred at some point.  
---|---  
`nvtx.range_push` | Push a range onto a stack of nested range span.  
`nvtx.range_pop` | Pop a range off of a stack of nested range spans.  
`nvtx.range` | Context manager / decorator that pushes an NVTX range at the beginning of its scope, and pops it at the end.  
## Jiterator (beta)
`jiterator._create_jit_fn` | Create a jiterator-generated cuda kernel for an elementwise op.  
---|---  
`jiterator._create_multi_output_jit_fn` | Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.  
## TunableOp
Some operations could be implemented using more than one library or more than one technique. For example, a GEMM could be implemented for CUDA or ROCm using either the cublas/cublasLt libraries or hipblas/hipblasLt libraries, respectively. How does one know which implementation is the fastest and should be chosen? That’s what TunableOp provides. Certain operators have been implemented using multiple strategies as Tunable Operators. At runtime, all strategies are profiled and the fastest is selected for all subsequent operations.
See the documentation for information on how to use it.
## Stream Sanitizer (prototype)
CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the documentation for information on how to use it.
## GPUDirect Storage (prototype)
The APIs in `torch.cuda.gds` provide thin wrappers around certain cuFile APIs that allow direct memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See the cufile api documentation for more details.
These APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must ensure that their system is appropriately configured to use GPUDirect Storage per the GPUDirect Storage documentation.
See the docs for `GdsFile` for an example of how to use these.
`gds_register_buffer` | Registers a storage on a CUDA device as a cufile buffer.  
---|---  
`gds_deregister_buffer` | Deregisters a previously registered storage on a CUDA device as a cufile buffer.  
`GdsFile` | Wrapper around cuFile.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.cuda
    * Random Number Generator
    * Communication collectives
    * Streams and events
    * Graphs (beta)
    * Memory management
      * `use_mem_pool`
    * NVIDIA Tools Extension (NVTX)
    * Jiterator (beta)
    * TunableOp
    * Stream Sanitizer (prototype)
    * GPUDirect Storage (prototype)


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.data
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.data
At the heart of PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It represents a Python iterable over a dataset, with support for
  * map-style and iterable-style datasets,
  * customizing data loading order,
  * automatic batching,
  * single- and multi-process data loading,
  * automatic memory pinning.


These options are configured by the constructor arguments of a `DataLoader`, which has signature:
```
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
      batch_sampler=None, num_workers=0, collate_fn=None,
      pin_memory=False, drop_last=False, timeout=0,
      worker_init_fn=None, *, prefetch_factor=2,
      persistent_workers=False)

```
Copy to clipboard
The sections below describe in details the effects and usages of these options.
## Dataset Types
The most important argument of `DataLoader` constructor is `dataset`, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:
  * map-style datasets,
  * iterable-style datasets.


### Map-style datasets
A map-style dataset is one that implements the `__getitem__()` and `__len__()` protocols, and represents a map from (possibly non-integral) indices/keys to data samples.
For example, such a dataset, when accessed with `dataset[idx]`, could read the `idx`-th image and its corresponding label from a folder on the disk.
See `Dataset` for more details.
### Iterable-style datasets
An iterable-style dataset is an instance of a subclass of `IterableDataset` that implements the `__iter__()` protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.
For example, such a dataset, when called `iter(dataset)`, could return a stream of data reading from a database, a remote server, or even logs generated in real time.
See `IterableDataset` for more details.
Note
When using a `IterableDataset` with multi-process data loading. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See `IterableDataset` documentations for how to achieve this.
## Data Loading Order and `Sampler`
For iterable-style datasets, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).
The rest of this section concerns the case with map-style datasets. `torch.utils.data.Sampler` classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a `Sampler` could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.
A sequential or shuffled sampler will be automatically constructed based on the `shuffle` argument to a `DataLoader`. Alternatively, users may use the `sampler` argument to specify a custom `Sampler` object that at each time yields the next index/key to fetch.
A custom `Sampler` that yields a list of batch indices at a time can be passed as the `batch_sampler` argument. Automatic batching can also be enabled via `batch_size` and `drop_last` arguments. See the next section for more details on this.
Note
Neither `sampler` nor `batch_sampler` is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.
## Loading Batched and Non-Batched Data
`DataLoader` supports automatically collating individual fetched data samples into batches via arguments `batch_size`, `drop_last`, `batch_sampler`, and `collate_fn` (which has a default function).
### Automatic batching (default)
This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).
When `batch_size` (default `1`) is not `None`, the data loader yields batched samples instead of individual samples. `batch_size` and `drop_last` arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify `batch_sampler`, which yields a list of keys at a time.
Note
The `batch_size` and `drop_last` arguments essentially are used to construct a `batch_sampler` from `sampler`. For map-style datasets, the `sampler` is either provided by user or constructed based on the `shuffle` argument. For iterable-style datasets, the `sampler` is a dummy infinite one. See this section on more details on samplers.
Note
When fetching from iterable-style datasets with multi-processing, the `drop_last` argument drops the last non-full batch of each worker’s dataset replica.
After fetching a list of samples using the indices from sampler, the function passed as the `collate_fn` argument is used to collate lists of samples into batches.
In this case, loading from a map-style dataset is roughly equivalent with:
```
for indices in batch_sampler:
  yield collate_fn([dataset[i] for i in indices])

```
Copy to clipboard
and loading from an iterable-style dataset is roughly equivalent with:
```
dataset_iter = iter(dataset)
for indices in batch_sampler:
  yield collate_fn([next(dataset_iter) for _ in indices])

```
Copy to clipboard
A custom `collate_fn` can be used to customize collation, e.g., padding sequential data to max length of a batch. See this section on more about `collate_fn`.
### Disable automatic batching
In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it’s likely better to not use automatic batching (where `collate_fn` is used to collate the samples), but let the data loader directly return each member of the `dataset` object.
When both `batch_size` and `batch_sampler` are `None` (default value for `batch_sampler` is already `None`), automatic batching is disabled. Each sample obtained from the `dataset` is processed with the function passed as the `collate_fn` argument.
**When automatic batching is disabled** , the default `collate_fn` simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.
In this case, loading from a map-style dataset is roughly equivalent with:
```
for index in sampler:
  yield collate_fn(dataset[index])

```
Copy to clipboard
and loading from an iterable-style dataset is roughly equivalent with:
```
for data in iter(dataset):
  yield collate_fn(data)

```
Copy to clipboard
See this section on more about `collate_fn`.
### Working with `collate_fn`
The use of `collate_fn` is slightly different when automatic batching is enabled or disabled.
**When automatic batching is disabled** , `collate_fn` is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default `collate_fn` simply converts NumPy arrays in PyTorch tensors.
**When automatic batching is enabled** , `collate_fn` is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default `collate_fn` (`default_collate()`).
For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple `(image, class_index)`, the default `collate_fn` collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default `collate_fn` has the following properties:
  * It always prepends a new dimension as the batch dimension.
  * It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors.
  * It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for `list` s, `tuple` s, `namedtuple` s, etc.


Users may use customized `collate_fn` to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types.
If you run into a situation where the outputs of `DataLoader` have dimensions or type that is different from your expectation, you may want to check your `collate_fn`.
## Single- and Multi-process Data Loading
A `DataLoader` uses single-process data loading by default.
Within a Python process, the Global Interpreter Lock (GIL) prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument `num_workers` to a positive integer.
### Single-process data loading (default)
In this mode, data fetching is done in the same process a `DataLoader` is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.
### Multi-process data loading
Setting the argument `num_workers` as a positive integer will turn on multi-process data loading with the specified number of loader worker processes.
Warning
After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is `number of workers * size of parent process`). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out issue #13246 for more details on why this occurs and example code for how to workaround these problems.
In this mode, each time an iterator of a `DataLoader` is created (e.g., when you call `enumerate(dataloader)`), `num_workers` worker processes are created. At this point, the `dataset`, `collate_fn`, and `worker_init_fn` are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including `collate_fn`) runs in the worker process.
`torch.utils.data.get_worker_info()` returns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns `None` in main process. Users may use this function in dataset code and/or `worker_init_fn` to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset.
For map-style datasets, the main process generates the indices using `sampler` and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.
For iterable-style datasets, since each worker process gets a replica of the `dataset` object, naive multi-process loading will often result in duplicated data. Using `torch.utils.data.get_worker_info()` and/or `worker_init_fn`, users may configure each replica independently. (See `IterableDataset` documentations for how to achieve this. ) For similar reasons, in multi-process loading, the `drop_last` argument drops the last non-full batch of each worker’s iterable-style dataset replica.
Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.
Warning
It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see CUDA in multiprocessing). Instead, we recommend using automatic memory pinning (i.e., setting `pin_memory=True`), which enables fast data transfer to CUDA-enabled GPUs.
#### Platform-specific behaviors
Since workers rely on Python `multiprocessing`, worker launch behavior is different on Windows compared to Unix.
  * On Unix, `fork()` is the default `multiprocessing` start method. Using `fork()`, child workers typically can access the `dataset` and Python argument functions directly through the cloned address space.
  * On Windows or MacOS, `spawn()` is the default `multiprocessing` start method. Using `spawn()`, another interpreter is launched which runs your main script, followed by the internal worker function that receives the `dataset`, `collate_fn` and other arguments through `pickle` serialization.


This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:
  * Wrap most of you main script’s code within `if __name__ == '__main__':` block, to make sure it doesn’t run again (most likely generating error) when each worker process is launched. You can place your dataset and `DataLoader` instance creation logic here, as it doesn’t need to be re-executed in workers.
  * Make sure that any custom `collate_fn`, `worker_init_fn` or `dataset` code is declared as top level definitions, outside of the `__main__` check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not `bytecode`.)


#### Randomness in multi-process data loading
By default, each worker will have its PyTorch seed set to `base_seed + worker_id`, where `base_seed` is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified `generator`. However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See this section in FAQ.).
In `worker_init_fn`, you may access the PyTorch seed set for each worker with either `torch.utils.data.get_worker_info().seed` or `torch.initial_seed()`, and use it to seed other libraries before data loading.
## Memory Pinning
Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See Use pinned memory buffers for more details on when and how to use pinned memory generally.
For data loading, passing `pin_memory=True` to a `DataLoader` will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.
The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a `collate_fn` that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a `pin_memory()` method on your custom type(s).
See the example below.
Example:
```
class SimpleCustomBatch:
  def __init__(self, data):
    transposed_data = list(zip(*data))
    self.inp = torch.stack(transposed_data[0], 0)
    self.tgt = torch.stack(transposed_data[1], 0)
  # custom memory pinning method on custom type
  def pin_memory(self):
    self.inp = self.inp.pin_memory()
    self.tgt = self.tgt.pin_memory()
    return self
def collate_wrapper(batch):
  return SimpleCustomBatch(batch)
inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
dataset = TensorDataset(inps, tgts)
loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,
          pin_memory=True)
for batch_ndx, sample in enumerate(loader):
  print(sample.inp.is_pinned())
  print(sample.tgt.is_pinned())

```
Copy to clipboard 

_class_ torch.utils.data.DataLoader(_dataset_ , _batch_size =1_, _shuffle =None_, _sampler =None_, _batch_sampler =None_, _num_workers =0_, _collate_fn =None_, _pin_memory =False_, _drop_last =False_, _timeout =0_, _worker_init_fn =None_, _multiprocessing_context =None_, _generator =None_, _*_ , _prefetch_factor =None_, _persistent_workers =False_, _pin_memory_device =''_, _in_order =True_)[source][source]
    
Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.
The `DataLoader` supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.
See `torch.utils.data` documentation page for more details. 

Parameters
    
  * **dataset** (_Dataset_) – dataset from which to load the data.
  * **batch_size** (_int_ _,__optional_) – how many samples per batch to load (default: `1`).
  * **shuffle** (_bool_ _,__optional_) – set to `True` to have the data reshuffled at every epoch (default: `False`).
  * **sampler** (_Sampler_ _or_ _Iterable_ _,__optional_) – defines the strategy to draw samples from the dataset. Can be any `Iterable` with `__len__` implemented. If specified, `shuffle` must not be specified.
  * **batch_sampler** (_Sampler_ _or_ _Iterable_ _,__optional_) – like `sampler`, but returns a batch of indices at a time. Mutually exclusive with `batch_size`, `shuffle`, `sampler`, and `drop_last`.
  * **num_workers** (_int_ _,__optional_) – how many subprocesses to use for data loading. `0` means that the data will be loaded in the main process. (default: `0`)
  * **collate_fn** (_Callable_ _,__optional_) – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.
  * **pin_memory** (_bool_ _,__optional_) – If `True`, the data loader will copy Tensors into device/CUDA pinned memory before returning them. If your data elements are a custom type, or your `collate_fn` returns a batch that is a custom type, see the example below.
  * **drop_last** (_bool_ _,__optional_) – set to `True` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If `False` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: `False`)
  * **timeout** (_numeric_ _,__optional_) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: `0`)
  * **worker_init_fn** (_Callable_ _,__optional_) – If not `None`, this will be called on each worker subprocess with the worker id (an int in `[0, num_workers - 1]`) as input, after seeding and before data loading. (default: `None`)
  * **multiprocessing_context** (_str_ _or_ _multiprocessing.context.BaseContext_ _,__optional_) – If `None`, the default multiprocessing context of your operating system will be used. (default: `None`)
  * **generator** (_torch.Generator_ _,__optional_) – If not `None`, this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate `base_seed` for workers. (default: `None`)
  * **prefetch_factor** (_int_ _,__optional_ _,__keyword-only arg_) – Number of batches loaded in advance by each worker. `2` means there will be a total of 2 * num_workers batches prefetched across all workers. (default value depends on the set value for num_workers. If value of num_workers=0 default is `None`. Otherwise, if value of `num_workers > 0` default is `2`).
  * **persistent_workers** (_bool_ _,__optional_) – If `True`, the data loader will not shut down the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. (default: `False`)
  * **pin_memory_device** (_str_ _,__optional_) – the device to `pin_memory` on if `pin_memory` is `True`. If not given, the current accelerator will be the default. This argument is discouraged and subject to deprecated.
  * **in_order** (_bool_ _,__optional_) – If `False`, the data loader will not enforce that batches are returned in a first-in, first-out order. Only applies when `num_workers > 0`. (default: `True`)


Warning
If the `spawn` start method is used, `worker_init_fn` cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.
Warning
`len(dataloader)` heuristic is based on the length of the sampler used. When `dataset` is an `IterableDataset`, it instead returns an estimate based on `len(dataset) / batch_size`, with proper rounding depending on `drop_last`, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user `dataset` code in correctly handling multi-process loading to avoid duplicate data.
However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when `drop_last` is set. Unfortunately, PyTorch can not detect such cases in general.
See Dataset Types for more details on these two types of datasets and how `IterableDataset` interacts with Multi-process data loading.
Warning
See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions.
Warning
Setting in_order to False can harm reproducibility and may lead to a skewed data distribution being fed to the trainer in cases with imbalanced data. 

_class_ torch.utils.data.Dataset[source][source]
    
An abstract class representing a `Dataset`.
All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite `__getitem__()`, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite `__len__()`, which is expected to return the size of the dataset by many `Sampler` implementations and the default options of `DataLoader`. Subclasses could also optionally implement `__getitems__()`, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.
Note
`DataLoader` by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided. 

_class_ torch.utils.data.IterableDataset[source][source]
    
An iterable Dataset.
All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.
All subclasses should overwrite `__iter__()`, which would return an iterator of samples in this dataset.
When a subclass is used with `DataLoader`, each item in the dataset will be yielded from the `DataLoader` iterator. When `num_workers > 0`, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. `get_worker_info()`, when called in a worker process, returns information about the worker. It can be used in either the dataset’s `__iter__()` method or the `DataLoader` ‘s `worker_init_fn` option to modify each copy’s behavior.
Example 1: splitting workload across all workers in `__iter__()`:
```
>>> class MyIterableDataset(torch.utils.data.IterableDataset):
...   def __init__(self, start, end):
...     super(MyIterableDataset).__init__()
...     assert end > start, "this example code only works with end >= start"
...     self.start = start
...     self.end = end
...
...   def __iter__(self):
...     worker_info = torch.utils.data.get_worker_info()
...     if worker_info is None: # single-process data loading, return the full iterator
...       iter_start = self.start
...       iter_end = self.end
...     else: # in a worker process
...       # split workload
...       per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))
...       worker_id = worker_info.id
...       iter_start = self.start + worker_id * per_worker
...       iter_end = min(iter_start + per_worker, self.end)
...     return iter(range(iter_start, iter_end))
...
>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
>>> ds = MyIterableDataset(start=3, end=7)
>>> # Single-process loading
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[tensor([3]), tensor([4]), tensor([5]), tensor([6])]
>>> # Multi-process loading with two worker processes
>>> # Worker 0 fetched [3, 4]. Worker 1 fetched [5, 6].
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[tensor([3]), tensor([5]), tensor([4]), tensor([6])]
>>> # With even more workers
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=12)))
[tensor([3]), tensor([5]), tensor([4]), tensor([6])]

```
Copy to clipboard
Example 2: splitting workload across all workers using `worker_init_fn`:
```
>>> class MyIterableDataset(torch.utils.data.IterableDataset):
...   def __init__(self, start, end):
...     super(MyIterableDataset).__init__()
...     assert end > start, "this example code only works with end >= start"
...     self.start = start
...     self.end = end
...
...   def __iter__(self):
...     return iter(range(self.start, self.end))
...
>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
>>> ds = MyIterableDataset(start=3, end=7)
>>> # Single-process loading
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[3, 4, 5, 6]
>>>
>>> # Directly doing multi-process loading yields duplicate data
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[3, 3, 4, 4, 5, 5, 6, 6]
>>> # Define a `worker_init_fn` that configures each dataset copy differently
>>> def worker_init_fn(worker_id):
...   worker_info = torch.utils.data.get_worker_info()
...   dataset = worker_info.dataset # the dataset copy in this worker process
...   overall_start = dataset.start
...   overall_end = dataset.end
...   # configure the dataset to only process the split workload
...   per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))
...   worker_id = worker_info.id
...   dataset.start = overall_start + worker_id * per_worker
...   dataset.end = min(dataset.start + per_worker, overall_end)
...
>>> # Mult-process loading with the custom `worker_init_fn`
>>> # Worker 0 fetched [3, 4]. Worker 1 fetched [5, 6].
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))
[3, 5, 4, 6]
>>> # With even more workers
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))
[3, 4, 5, 6]

```
Copy to clipboard 

_class_ torch.utils.data.TensorDataset(_* tensors_)[source][source]
    
Dataset wrapping tensors.
Each sample will be retrieved by indexing tensors along the first dimension. 

Parameters
    
***tensors** (_Tensor_) – tensors that have the same size of the first dimension. 

_class_ torch.utils.data.StackDataset(_* args_, _** kwargs_)[source][source]
    
Dataset as a stacking of multiple datasets.
This class is useful to assemble different parts of complex input data, given as datasets.
Example
```
>>> images = ImageDataset()
>>> texts = TextDataset()
>>> tuple_stack = StackDataset(images, texts)
>>> tuple_stack[0] == (images[0], texts[0])
>>> dict_stack = StackDataset(image=images, text=texts)
>>> dict_stack[0] == {'image': images[0], 'text': texts[0]}

```
Copy to clipboard 

Parameters
    
  * ***args** (_Dataset_) – Datasets for stacking returned as tuple.
  * ****kwargs** (_Dataset_) – Datasets for stacking returned as dict.



_class_ torch.utils.data.ConcatDataset(_datasets_)[source][source]
    
Dataset as a concatenation of multiple datasets.
This class is useful to assemble different existing datasets. 

Parameters
    
**datasets** (_sequence_) – List of datasets to be concatenated 

_class_ torch.utils.data.ChainDataset(_datasets_)[source][source]
    
Dataset for chaining multiple `IterableDataset` s.
This class is useful to assemble different existing dataset streams. The chaining operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient. 

Parameters
    
**datasets** (_iterable_ _of_ _IterableDataset_) – datasets to be chained together 

_class_ torch.utils.data.Subset(_dataset_ , _indices_)[source][source]
    
Subset of a dataset at specified indices. 

Parameters
    
  * **dataset** (_Dataset_) – The whole Dataset
  * **indices** (_sequence_) – Indices in the whole set selected for subset



torch.utils.data._utils.collate.collate(_batch_ , _*_ , _collate_fn_map =None_)[source][source]
    
General collate function that handles collection type of element within each batch.
The function also opens function registry to deal with specific element types. default_collate_fn_map provides default collate functions for tensors, numpy arrays, numbers and strings. 

Parameters
    
  * **batch** – a single batch to be collated
  * **collate_fn_map** (_Optional_ _[__dict_ _[__Union_ _[__type_ _,__tuple_ _[__type_ _,__...__]__]__,__Callable_ _]__]_) – Optional dictionary mapping from element type to the corresponding collate function. If the element type isn’t present in this dictionary, this function will go through each key of the dictionary in the insertion order to invoke the corresponding collate function if the element type is a subclass of the key.


Examples
```
>>> def collate_tensor_fn(batch, *, collate_fn_map):
...   # Extend this function to handle batch of tensors
...   return torch.stack(batch, 0)
>>> def custom_collate(batch):
...   collate_map = {torch.Tensor: collate_tensor_fn}
...   return collate(batch, collate_fn_map=collate_map)
>>> # Extend `default_collate` by in-place modifying `default_collate_fn_map`
>>> default_collate_fn_map.update({torch.Tensor: collate_tensor_fn})

```
Copy to clipboard
Note
Each collate function requires a positional argument for batch and a keyword argument for the dictionary of collate functions as collate_fn_map. 

torch.utils.data.default_collate(_batch_)[source][source]
    
Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.
The exact output type can be a `torch.Tensor`, a Sequence of `torch.Tensor`, a Collection of `torch.Tensor`, or left unchanged, depending on the input type. This is used as the default function for collation when batch_size or batch_sampler is defined in `DataLoader`.
Here is the general input type (based on the type of the element within the batch) to output type mapping:
>   * `torch.Tensor` -> `torch.Tensor` (with an added outer dimension batch size)
>   * NumPy Arrays -> `torch.Tensor`
>   * float -> `torch.Tensor`
>   * int -> `torch.Tensor`
>   * str -> str (unchanged)
>   * bytes -> bytes (unchanged)
>   * Mapping[K, V_i] -> Mapping[K, default_collate([V_1, V_2, …])]
>   * NamedTuple[V1_i, V2_i, …] -> NamedTuple[default_collate([V1_1, V1_2, …]), default_collate([V2_1, V2_2, …]), …]
>   * Sequence[V1_i, V2_i, …] -> Sequence[default_collate([V1_1, V1_2, …]), default_collate([V2_1, V2_2, …]), …]
> 


Parameters
    
**batch** – a single batch to be collated
Examples
```
>>> # Example with a batch of `int`s:
>>> default_collate([0, 1, 2, 3])
tensor([0, 1, 2, 3])
>>> # Example with a batch of `str`s:
>>> default_collate(['a', 'b', 'c'])
['a', 'b', 'c']
>>> # Example with `Map` inside the batch:
>>> default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}])
{'A': tensor([ 0, 100]), 'B': tensor([ 1, 100])}
>>> # Example with `NamedTuple` inside the batch:
>>> Point = namedtuple('Point', ['x', 'y'])
>>> default_collate([Point(0, 0), Point(1, 1)])
Point(x=tensor([0, 1]), y=tensor([0, 1]))
>>> # Example with `Tuple` inside the batch:
>>> default_collate([(0, 1), (2, 3)])
[tensor([0, 2]), tensor([1, 3])]
>>> # Example with `List` inside the batch:
>>> default_collate([[0, 1], [2, 3]])
[tensor([0, 2]), tensor([1, 3])]
>>> # Two options to extend `default_collate` to handle specific type
>>> # Option 1: Write custom collate function and invoke `default_collate`
>>> def custom_collate(batch):
...   elem = batch[0]
...   if isinstance(elem, CustomType): # Some custom condition
...     return ...
...   else: # Fall back to `default_collate`
...     return default_collate(batch)
>>> # Option 2: In-place modify `default_collate_fn_map`
>>> def collate_customtype_fn(batch, *, collate_fn_map=None):
...   return ...
>>> default_collate_fn_map.update(CustomType, collate_customtype_fn)
>>> default_collate(batch) # Handle `CustomType` automatically

```
Copy to clipboard 

torch.utils.data.default_convert(_data_)[source][source]
    
Convert each NumPy array element into a `torch.Tensor`.
If the input is a Sequence, Collection, or Mapping, it tries to convert each element inside to a `torch.Tensor`. If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both batch_sampler and batch_size are NOT defined in `DataLoader`.
The general input type to output type mapping is similar to that of `default_collate()`. See the description there for more details. 

Parameters
    
**data** – a single data point to be converted
Examples
```
>>> # Example with `int`
>>> default_convert(0)
0
>>> # Example with NumPy array
>>> default_convert(np.array([0, 1]))
tensor([0, 1])
>>> # Example with NamedTuple
>>> Point = namedtuple('Point', ['x', 'y'])
>>> default_convert(Point(0, 0))
Point(x=0, y=0)
>>> default_convert(Point(np.array(0), np.array(0)))
Point(x=tensor(0), y=tensor(0))
>>> # Example with List
>>> default_convert([np.array([0, 1]), np.array([2, 3])])
[tensor([0, 1]), tensor([2, 3])]

```
Copy to clipboard 

torch.utils.data.get_worker_info()[source][source]
    
Returns the information about the current `DataLoader` iterator worker process.
When called in a worker, this returns an object guaranteed to have the following attributes:
  * `id`: the current worker id.
  * `num_workers`: the total number of workers.
  * `seed`: the random seed set for the current worker. This value is determined by main process RNG and the worker id. See `DataLoader`’s documentation for more details.
  * `dataset`: the copy of the dataset object in **this** process. Note that this will be a different object in a different process than the one in the main process.


When called in the main process, this returns `None`.
Note
When used in a `worker_init_fn` passed over to `DataLoader`, this method can be useful to set up each worker process differently, for instance, using `worker_id` to configure the `dataset` object to only read a specific fraction of a sharded dataset, or use `seed` to seed other libraries used in dataset code. 

Return type
    
_Optional_[_WorkerInfo_] 

torch.utils.data.random_split(_dataset_ , _lengths_ , _generator= <torch._C.Generator object>_)[source][source]
    
Randomly split a dataset into non-overlapping new datasets of given lengths.
If a list of fractions that sum up to 1 is given, the lengths will be computed automatically as floor(frac * len(dataset)) for each fraction provided.
After computing the lengths, if there are any remainders, 1 count will be distributed in round-robin fashion to the lengths until there are no remainders left.
Optionally fix the generator for reproducible results, e.g.:
Example
```
>>> generator1 = torch.Generator().manual_seed(42)
>>> generator2 = torch.Generator().manual_seed(42)
>>> random_split(range(10), [3, 7], generator=generator1)
>>> random_split(range(30), [0.3, 0.3, 0.4], generator=generator2)

```
Copy to clipboard 

Parameters
    
  * **dataset** (_Dataset_) – Dataset to be split
  * **lengths** (_sequence_) – lengths or fractions of splits to be produced
  * **generator** (_Generator_) – Generator used for the random permutation.



Return type
    
list[torch.utils.data.dataset.Subset[~_T]] 

_class_ torch.utils.data.Sampler(_data_source =None_)[source][source]
    
Base class for all Samplers.
Every Sampler subclass has to provide an `__iter__()` method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and may provide a `__len__()` method that returns the length of the returned iterators. 

Parameters
    
**data_source** (_Dataset_) – This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it.
Example
```
>>> class AccedingSequenceLengthSampler(Sampler[int]):
>>>   def __init__(self, data: List[str]) -> None:
>>>     self.data = data
>>>
>>>   def __len__(self) -> int:
>>>     return len(self.data)
>>>
>>>   def __iter__(self) -> Iterator[int]:
>>>     sizes = torch.tensor([len(x) for x in self.data])
>>>     yield from torch.argsort(sizes).tolist()
>>>
>>> class AccedingSequenceLengthBatchSampler(Sampler[List[int]]):
>>>   def __init__(self, data: List[str], batch_size: int) -> None:
>>>     self.data = data
>>>     self.batch_size = batch_size
>>>
>>>   def __len__(self) -> int:
>>>     return (len(self.data) + self.batch_size - 1) // self.batch_size
>>>
>>>   def __iter__(self) -> Iterator[List[int]]:
>>>     sizes = torch.tensor([len(x) for x in self.data])
>>>     for batch in torch.chunk(torch.argsort(sizes), len(self)):
>>>       yield batch.tolist()

```
Copy to clipboard
Note
The `__len__()` method isn’t strictly required by `DataLoader`, but is expected in any calculation involving the length of a `DataLoader`. 

_class_ torch.utils.data.SequentialSampler(_data_source_)[source][source]
    
Samples elements sequentially, always in the same order. 

Parameters
    
**data_source** (_Dataset_) – dataset to sample from 

_class_ torch.utils.data.RandomSampler(_data_source_ , _replacement =False_, _num_samples =None_, _generator =None_)[source][source]
    
Samples elements randomly. If without replacement, then sample from a shuffled dataset.
If with replacement, then user can specify `num_samples` to draw. 

Parameters
    
  * **data_source** (_Dataset_) – dataset to sample from
  * **replacement** (_bool_) – samples are drawn on-demand with replacement if `True`, default=``False``
  * **num_samples** (_int_) – number of samples to draw, default=`len(dataset)`.
  * **generator** (_Generator_) – Generator used in sampling.



_class_ torch.utils.data.SubsetRandomSampler(_indices_ , _generator =None_)[source][source]
    
Samples elements randomly from a given list of indices, without replacement. 

Parameters
    
  * **indices** (_sequence_) – a sequence of indices
  * **generator** (_Generator_) – Generator used in sampling.



_class_ torch.utils.data.WeightedRandomSampler(_weights_ , _num_samples_ , _replacement =True_, _generator =None_)[source][source]
    
Samples elements from `[0,..,len(weights)-1]` with given probabilities (weights). 

Parameters
    
  * **weights** (_sequence_) – a sequence of weights, not necessary summing up to one
  * **num_samples** (_int_) – number of samples to draw
  * **replacement** (_bool_) – if `True`, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.
  * **generator** (_Generator_) – Generator used in sampling.


Example
```
>>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))
[4, 4, 1, 4, 5]
>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))
[0, 1, 4, 3, 2]

```
Copy to clipboard 

_class_ torch.utils.data.BatchSampler(_sampler_ , _batch_size_ , _drop_last_)[source][source]
    
Wraps another sampler to yield a mini-batch of indices. 

Parameters
    
  * **sampler** (_Sampler_ _or_ _Iterable_) – Base sampler. Can be any iterable object
  * **batch_size** (_int_) – Size of mini-batch.
  * **drop_last** (_bool_) – If `True`, the sampler will drop the last batch if its size would be less than `batch_size`


Example
```
>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))
[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))
[[0, 1, 2], [3, 4, 5], [6, 7, 8]]

```
Copy to clipboard 

_class_ torch.utils.data.distributed.DistributedSampler(_dataset_ , _num_replicas =None_, _rank =None_, _shuffle =True_, _seed =0_, _drop_last =False_)[source][source]
    
Sampler that restricts data loading to a subset of the dataset.
It is especially useful in conjunction with `torch.nn.parallel.DistributedDataParallel`. In such a case, each process can pass a `DistributedSampler` instance as a `DataLoader` sampler, and load a subset of the original dataset that is exclusive to it.
Note
Dataset is assumed to be of constant size and that any instance of it always returns the same elements in the same order. 

Parameters
    
  * **dataset** (_Dataset_) – Dataset used for sampling.
  * **num_replicas** (_int_ _,__optional_) – Number of processes participating in distributed training. By default, `world_size` is retrieved from the current distributed group.
  * **rank** (_int_ _,__optional_) – Rank of the current process within `num_replicas`. By default, `rank` is retrieved from the current distributed group.
  * **shuffle** (_bool_ _,__optional_) – If `True` (default), sampler will shuffle the indices.
  * **seed** (_int_ _,__optional_) – random seed used to shuffle the sampler if `shuffle=True`. This number should be identical across all processes in the distributed group. Default: `0`.
  * **drop_last** (_bool_ _,__optional_) – if `True`, then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If `False`, the sampler will add extra indices to make the data evenly divisible across the replicas. Default: `False`.


Warning
In distributed mode, calling the `set_epoch()` method at the beginning of each epoch **before** creating the `DataLoader` iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.
Example:
```
>>> sampler = DistributedSampler(dataset) if is_distributed else None
>>> loader = DataLoader(dataset, shuffle=(sampler is None),
...           sampler=sampler)
>>> for epoch in range(start_epoch, n_epochs):
...   if is_distributed:
...     sampler.set_epoch(epoch)
...   train(loader)

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.data
    * Dataset Types
      * Map-style datasets
      * Iterable-style datasets
    * Data Loading Order and `Sampler`
    * Loading Batched and Non-Batched Data
      * Automatic batching (default)
      * Disable automatic batching
      * Working with `collate_fn`
    * Single- and Multi-process Data Loading
      * Single-process data loading (default)
      * Multi-process data loading
        * Platform-specific behaviors
        * Randomness in multi-process data loading
    * Memory Pinning
      * `DataLoader`
      * `Dataset`
      * `IterableDataset`
      * `TensorDataset`
      * `StackDataset`
      * `ConcatDataset`
      * `ChainDataset`
      * `Subset`
      * `collate()`
      * `default_collate()`
      * `default_convert()`
      * `get_worker_info()`
      * `random_split()`
      * `Sampler`
      * `SequentialSampler`
      * `RandomSampler`
      * `SubsetRandomSampler`
      * `WeightedRandomSampler`
      * `BatchSampler`
      * `DistributedSampler`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * DDP Communication Hooks
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# DDP Communication Hooks
DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in DistributedDataParallel. A few built-in communication hooks are provided, and users can easily apply any of these hooks to optimize communication. Besides, the hook interface can also support user-defined communication strategies for more advanced use cases.
## How to Use a Communication Hook?
To use a communication hook, the user just needs to let the DDP model register the hook before the training loop as below.
`torch.nn.parallel.DistributedDataParallel.register_comm_hook()`
## What Does a Communication Hook Operate On?
A communication hook provides a flexible way to allreduce gradients. Therefore, it mainly operates on the gradients on each replica before allreduce, which are bucketized to increase the overlap between communication and computation. Particularly, `torch.distributed.GradBucket` represents a bucket of gradient tensors to be allreduced. 

_class_ torch.distributed.GradBucket
    
This class mainly passes a flattened gradient tensor (returned by `buffer()`) to DDP communication hook. This tensor can be further decomposed into a list of per-parameter tensors within this bucket (returned by `get_per_parameter_tensors()`) to apply layer-wise operations. 

torch.distributed.GradBucket.index(_self :torch._C._distributed_c10d.GradBucket_) → int
    
Warning
Since the buckets are rebuilt after the first iteration, should not rely on the indices at the beginning of training. 

Returns
    
The index of a bucket that stores gradients of a few contiguous layers. All the gradients are bucketized. 

torch.distributed.GradBucket.buffer(_self :torch._C._distributed_c10d.GradBucket_) → torch.Tensor
     

Returns
    
A flattened 1D `torch.Tensor` buffer, which can be further decomposed into a list of per-parameter tensors within this bucket. 

torch.distributed.GradBucket.gradients(_self :torch._C._distributed_c10d.GradBucket_) → list[torch.Tensor]
     

Returns
    
A list of `torch.Tensor`. Each tensor in the list corresponds to a gradient. 

torch.distributed.GradBucket.is_last(_self :torch._C._distributed_c10d.GradBucket_) → bool
     

Returns
    
Whether this bucket is the last bucket to allreduce in an iteration. This also means that this bucket corresponds to the first few layers in the forward pass. 

torch.distributed.GradBucket.set_buffer(_self :torch._C._distributed_c10d.GradBucket_, _buffer :torch.Tensor_) → None
    
Replaces the tensor in the bucket with the input tensor buffer. 

torch.distributed.GradBucket.parameters(_self :torch._C._distributed_c10d.GradBucket_) → list[torch.Tensor]
     

Returns
    
A list of `torch.Tensor`. Each tensor in the list corresponds to a model parameter.
## Default Communication Hooks
Default communication hooks are simple **stateless** hooks, so the input state in `register_comm_hook` is either a process group or `None`. The input `bucket` is a `torch.distributed.GradBucket` object. 

torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(_process_group_ , _bucket_)[source][source]
    
Call `allreduce` using `GradBucket` tensors.
Once gradient tensors are aggregated across all workers, its `then` callback takes the mean and returns the result.
If user registers this DDP communication hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won’t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior. 

Example::
    
```
>>> ddp_model.register_comm_hook(process_group, allreduce_hook)

```
Copy to clipboard 

Return type
    
_Future_[_Tensor_] 

torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(_process_group_ , _bucket_)[source][source]
    
Compress by casting `GradBucket` to `torch.float16` divided by process group size.
This DDP communication hook implements a simple gradient compression approach that casts `GradBucket` tensor to half-precision floating-point format (`torch.float16`) and then divides it by the process group size. It allreduces those `float16` gradient tensors. Once compressed gradient tensors are allreduced, the chained callback `decompress` casts it back to the input data type (such as `float32`). 

Example::
    
```
>>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)

```
Copy to clipboard 

Return type
    
_Future_[_Tensor_] 

torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook(_process_group_ , _bucket_)[source][source]
    
Warning: This API is experimental, and it requires NCCL version later than 2.9.6.
This DDP communication hook implements a simple gradient compression approach that casts `GradBucket` tensor to half-precision Brain floating point format (`torch.bfloat16`) and then divides it by the process group size. It allreduces those `bfloat16` gradient tensors. Once compressed gradient tensors are allreduced, the chained callback `decompress` casts it back to the input data type (such as `float32`). 

Example::
    
```
>>> ddp_model.register_comm_hook(process_group, bf16_compress_hook)

```
Copy to clipboard 

Return type
    
_Future_[_Tensor_]
Additionally, a communication hook wrapper is provided to support `fp16_compress_hook()` or `bf16_compress_hook()` as a wrapper, which can be combined with other communication hooks. 

torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper(_hook_)[source][source]
    
Cast input tensor to `torch.float16`, cast result of hook back to input dtype.
This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision floating point format (`torch.float16`), and casts the resulting tensor of the given hook back to the input data type, such as `float32`. Therefore, `fp16_compress_hook` is equivalent to `fp16_compress_wrapper(allreduce_hook)`. 

Example::
    
```
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)
>>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook))

```
Copy to clipboard 

Return type
    
_Callable_[[_Any_, _GradBucket_], _Future_[_Tensor_]] 

torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper(_hook_)[source][source]
    
Warning: This API is experimental, and it requires NCCL version later than 2.9.6.
This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format> `_ (``torch.bfloat16`), and casts the resulting tensor of the given hook back to the input data type, such as `float32`.
Therefore, `bf16_compress_hook` is equivalent to `bf16_compress_wrapper(allreduce_hook)`. 

Example::
    
```
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)
>>> ddp_model.register_comm_hook(state, bf16_compress_wrapper(powerSGD_hook))

```
Copy to clipboard 

Return type
    
_Callable_[[_Any_, _GradBucket_], _Future_[_Tensor_]]
## PowerSGD Communication Hook
PowerSGD (Vogels et al., NeurIPS 2019) is a gradient compression algorithm, which can provide very high compression rates and accelerate bandwidth-bound distributed training. This algorithm needs to maintain both some hyperparameters and the internal state. Therefore, PowerSGD communication hook is a **stateful** hook, and the user needs to provide a state object defined as below.
### PowerSGD State 

_class_ torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(_process_group_ , _matrix_approximation_rank =1_, _start_powerSGD_iter =1000_, _min_compression_rate =2_, _use_error_feedback =True_, _warm_start =True_, _orthogonalization_epsilon =0_, _random_seed =0_, _compression_stats_logging_frequency =10000_, _batch_tensors_with_same_shape =False_)[source][source]
    
Store both the algorithm’s hyperparameters and internal state for all gradients during training.
Particularly, `matrix_approximation_rank` and `start_powerSGD_iter` are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters `use_error_feedback` and `warm_start` on.
  1. `matrix_approximation_rank` controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.
> 1.1. If `matrix_approximation_rank` is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.
> 1.2. The increase of `matrix_approximation_rank` can substantially increase the computation costs of the compression, and the accuracy may not be further improved beyond a certain `matrix_approximation_rank` threshold.


To tune `matrix_approximation_rank`, we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, …), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.
  1. `start_powerSGD_iter` defers PowerSGD compression until step `start_powerSGD_iter`, and vanilla allreduce runs prior to step `start_powerSGD_iter`. This hybrid scheme of **vanilla allreduce + PowerSGD** can effectively improve the accuracy, even a relatively small `matrix_approximation_rank` is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.


To tune `start_powerSGD_iter`, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training, `start_powerSGD_iter` typically should be no less than the number of warm-up steps.
  1. `min_compression_rate` is the minimum compression rate required when a layer is compressed. Due to the computation overheads incurred by the compression, a tensor is worth compressing only if there can be sufficient saving in bandwidth, where `(num_rows + num_cols) * matrix_approximation_rank * min_compression_rate < num_rows * num_cols`. If the specified compression rate threshold cannot be satisfied, the tensor will be directly allreduced without compression.


Compression statistics are logged every `compression_stats_logging_frequency` iterations once PowerSGD compression starts.
  1. `orthogonalization_epsilon` can be a very small value (e.g., 1e-8) added to every normalized matrix column in orthogonalization step, to prevent div-by-zero error if any column has all 0s. If this can already be prevented (e.g., by batch normalization), an epsilon of 0 is recommended for accuracy.
  2. `batch_tensors_with_same_shape` controls whether to compress and decompress tensors with same shape in a batched operation to achieve higher parallelism. Note that you should also increase the bucket size (i.e., `bucket_cap_mb` arg in DDP constructor) to make more same-shaped tensors appear in the same bucket, however this may reduce the overlap between computation and communication, and increase the memory footprint due to stacking the tensors of the same shape. Set to `True` if the compression / decompression computation is a bottleneck.


Warning
If error feedback or warm-up is enabled, the minimum value of `start_powerSGD_iter` allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process.
### PowerSGD Hooks
Warning
PowerSGD typically requires extra memory of the same size as the model’s gradients to enable error feedback, which can compensate for biased compressed communication and improve accuracy.
Warning
PowerSGD hooks may conflict with Apex automatic mixed precision package. Please use PyTorch native automatic mixed precision package instead. 

torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(_state_ , _bucket_)[source][source]
    
Implement PowerSGD algorithm.
This DDP communication hook implements PowerSGD gradient compression algorithm described in the paper. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:
  1. Views the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups:
> 1.1 The tensors that should be compressed before allreduce, because the compression can give enough saving in bandwidth.
> 1.2 Rest of the tensors will be directly allreduced without compression, including all the vector tensors (for biases).
  2. Handles uncompressed tensors:
> 2.1. Allocate contiguous memory for those uncompressed tensors, and allreduces all the uncompressed tensors as a batch, without compression;
> 2.2. Copies the individual uncompressed tensors from the contiguous memory back to the input tensor.
  3. Handles the tensors that should be compressed by PowerSGD compression:
> 3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;
> 3.2. Computes each P in Ps, which is equal to MQ;
> 3.3. Allreduces Ps as a batch;
> 3.4. Orthogonalizes each P in Ps;
> 3.5. Computes each Q in Qs, which is approximately equal to M^TP;
> 3.6. Allreduces Qs as a batch;
> 3.7. Computes each M among all the compressed tensors, which is approximately equal to PQ^T.


Note that this communication hook enforces vanilla allreduce for the first `state.start_powerSGD_iter` iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers. 

Parameters
    
  * **state** (_PowerSGDState_) – State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune `matrix_approximation_rank`, `start_powerSGD_iter` and `min_compression_rate`.
  * **bucket** (_dist.GradBucket_) – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.



Returns
    
Future handler of the communication, which updates the gradients in place. 

Return type
    
_Future_[_Tensor_] 

Example::
    
```
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1,
             start_powerSGD_iter=10, min_compression_rate=0.5)
>>> ddp_model.register_comm_hook(state, powerSGD_hook)

```
Copy to clipboard 

torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(_state_ , _bucket_)[source][source]
    
Implement simplified PowerSGD algorithm.
This DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the paper. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is **faster** than `powerSGD_hook()`, but usually results in a **much lower accuracy** , unless `matrix_approximation_rank` is 1.
Warning
Increasing `matrix_approximation_rank` here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider `powerSGD_hook()` first, and only consider this variant when a satisfactory accuracy can be achieved when `matrix_approximation_rank` is 1.
Once gradient tensors are aggregated across all workers, this hook applies compression as follows:
  1. Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings;
  2. Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;
  3. Computes P, which is equal to MQ;
  4. Allreduces P;
  5. Orthogonalizes P;
  6. Computes Q, which is approximately equal to M^TP;
  7. Allreduces Q;
  8. Computes M, which is approximately equal to PQ^T.
  9. Truncates the input tensor to the original length.


Note that this communication hook enforces vanilla allreduce for the first `state.start_powerSGD_iter` iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers. 

Parameters
    
  * **state** (_PowerSGDState_) – State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune `matrix_approximation_rank` and `start_powerSGD_iter`.
  * **bucket** (_dist.GradBucket_) – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.



Returns
    
Future handler of the communication, which updates the gradients in place. 

Return type
    
_Future_[_Tensor_] 

Example::
    
```
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)
>>> ddp_model.register_comm_hook(state, batched_powerSGD_hook)

```
Copy to clipboard
## Debugging Communication Hooks
As the name implies, debugging communication hooks are **only** used for debugging and performance optimization purpose.
Warning
Debugging communication hooks do not necessarily output the correct results. 

torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook(___ , _bucket_)[source][source]
    
Return a future that wraps the input, so it is a no-op that does not incur any communication overheads.
This hook should **only** be used for headroom analysis of allreduce optimization, instead of the normal gradient synchronization. For example, if only less than 10% speedup of training time can be observed after this hook is registered, it usually implies that allreduce is not a performance bottleneck for this case. Such instrumentation can be particularly useful if GPU traces cannot be easily retrieved or the trace analysis is complicated some factors such as the overlap between allreduce and computation or the desynchronization across ranks. 

Example::
    
```
>>> ddp_model.register_comm_hook(None, noop_hook)

```
Copy to clipboard 

Return type
    
_Future_[_Tensor_]
## Checkpointing of Communication Hooks
A stateful communication hook can be saved as a part of model checkpointing to enable trainer restarts. To make a hook serializable, `__setstate__` and `__getstate__` should be defined.
Warning
`__getstate__` should exclude non-serializable attributes from a returned dictionary.
Warning
`__setstate__` should properly initialize non-serializable attributes, excluded from a provided `state`.
`PowerSGDState` has `__setstate__` and `__getstate__` implemented and can be used as a reference. 

_class_ torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState[source][source]
     

__getstate__()[source][source]
    
Return a `Dict[str, Any]` which will be pickled and saved.
`process_group` is not serializable and excluded from a returned state. 

__setstate__(_state_)[source][source]
    
Take a provided `state` and set to this `PowerSGDState` instance.
`process_group` is set to default.
Here is a simple, end-to-end example of saving and reloading PowerSGD state and hook.
```
import os
import sys
import tempfile
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel
from torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD
class SimpleModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.fc1 = nn.Linear(24,24)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(24,12)
  def forward(self, x):
    return self.fc2(self.relu(self.fc1(x)))
def setup(rank, world_size):
  os.environ['MASTER_ADDR'] = 'localhost'
  os.environ['MASTER_PORT'] = '12355'
  # initialize the process group
  dist.init_process_group("nccl", rank=rank, world_size=world_size)
def cleanup():
  dist.destroy_process_group()
def run_demo(demo_fn, world_size):
  mp.spawn(
    demo_fn,
    args=(world_size,),
    nprocs=world_size,
    join=True)
def demo_serialization(rank, world_size):
  setup(rank, world_size)
  CHECKPOINT = tempfile.gettempdir() + "/checkpoint.pt"
  model = SimpleModel().to(rank)
  ddp_model = DistributedDataParallel(model, device_ids=[rank])
  powersgd_hook = powerSGD.powerSGD_hook
  powersgd_state = powerSGD.PowerSGDState(process_group=None)
  optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
  ddp_model.register_comm_hook(powersgd_state, powersgd_hook)
  state = {
    'state_dict': ddp_model.state_dict(),
    'comm_hook': powersgd_hook,
    'comm_hook_state': powersgd_state}
  if rank == 0:
    torch.save(state, CHECKPOINT)
  dist.barrier()
  map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
  checkpoint = torch.load(CHECKPOINT, map_location=map_location)
  new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank])
  new_ddp_model.load_state_dict(checkpoint['state_dict'])
  powersgd_hook = checkpoint['comm_hook']
  powersgd_state = checkpoint['comm_hook_state']
  new_ddp_model.register_comm_hook(powersgd_state, powersgd_hook)
  if rank == 0:
    os.remove(CHECKPOINT)
  cleanup()
if __name__ == "__main__":
  n_gpus = torch.cuda.device_count()
  assert n_gpus >= 2, f"Requires at least 2 GPUs to run, but got {n_gpus}"
  world_size = n_gpus
  run_demo(demo_serialization, world_size)

```
Copy to clipboard
## Acknowledgements
Many thanks to PowerSGD paper author **Thijs Vogels** for the code review on PowerSGD communication hook, as well as the comparison experiments, which show that the performance of PowerSGD communication hook is on par with the implementation in the original paper.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * DDP Communication Hooks
    * How to Use a Communication Hook?
    * What Does a Communication Hook Operate On?
      * `GradBucket`
      * `index()`
      * `buffer()`
      * `gradients()`
      * `is_last()`
      * `set_buffer()`
      * `parameters()`
    * Default Communication Hooks
      * `allreduce_hook()`
      * `fp16_compress_hook()`
      * `bf16_compress_hook()`
      * `fp16_compress_wrapper()`
      * `bf16_compress_wrapper()`
    * PowerSGD Communication Hook
      * PowerSGD State
        * `PowerSGDState`
      * PowerSGD Hooks
        * `powerSGD_hook()`
        * `batched_powerSGD_hook()`
    * Debugging Communication Hooks
      * `noop_hook()`
    * Checkpointing of Communication Hooks
      * `PowerSGDState.__getstate__()`
      * `PowerSGDState.__setstate__()`
    * Acknowledgements


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Generic Join Context Manager
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Generic Join Context Manager
The generic join context manager facilitates distributed training on uneven inputs. This page outlines the API of the relevant classes: `Join`, `Joinable`, and `JoinHook`. For a tutorial, see Distributed Training with Uneven Inputs Using the Join Context Manager. 

_class_ torch.distributed.algorithms.Join(_joinables_ , _enable =True_, _throw_on_early_termination =False_, _** kwargs_)[source][source]
    
This class defines the generic join context manager, which allows custom hooks to be called after a process joins.
These hooks should shadow the collective communications of non-joined processes to prevent hanging and erroring and to ensure algorithmic correctness. Refer to `JoinHook` for details about the hook definition.
Warning
The context manager requires each participating `Joinable` to call the method `notify_join_context()` before its own per- iteration collective communications to ensure correctness.
Warning
The context manager requires that all `process_group` attributes in the `JoinHook` objects are the same. If there are multiple `JoinHook` objects, then the `device` of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if `throw_on_early_termination` is enabled, both of which using an all- reduce. 

Parameters
    
  * **joinables** (_List_ _[__Joinable_ _]_) – a list of the participating `Joinable` s; their hooks are iterated over in the given order.
  * **enable** (_bool_) – a flag enabling uneven input detection; setting to `False` disables the context manager’s functionality and should only be set when the user knows the inputs will not be uneven (default: `True`).
  * **throw_on_early_termination** (_bool_) – a flag controlling whether to throw an exception upon detecting uneven inputs (default: `False`).


Example:
```
>>> import os
>>> import torch
>>> import torch.distributed as dist
>>> import torch.multiprocessing as mp
>>> import torch.nn.parallel.DistributedDataParallel as DDP
>>> import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO
>>> from torch.distributed.algorithms.join import Join
>>>
>>> # On each spawned worker
>>> def worker(rank):
>>>   dist.init_process_group("nccl", rank=rank, world_size=2)
>>>   model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])
>>>   optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)
>>>   # Rank 1 gets one more input than rank 0
>>>   inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]
>>>   with Join([model, optim]):
>>>     for input in inputs:
>>>       loss = model(input).sum()
>>>       loss.backward()
>>>       optim.step()
>>>   # All ranks reach here without hanging/erroring

```
Copy to clipboard 

_static_ notify_join_context(_joinable_)[source][source]
    
Notifies the join context manager that the calling process has not yet joined.
Then, if `throw_on_early_termination=True`, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.
This method should be called from a `Joinable` object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in `DistributedDataParallel`.
Only the first `Joinable` object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous. 

Parameters
    
**joinable** (_Joinable_) – the `Joinable` object calling this method. 

Returns
    
An async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if `joinable` is the first one passed into the context manager; `None` otherwise. 

_class_ torch.distributed.algorithms.Joinable[source][source]
    
This defines an abstract base class for joinable classes.
A joinable class (inheriting from `Joinable`) should implement `join_hook()`, which returns a `JoinHook` instance, in addition to `join_device()` and `join_process_group()` that return device and process group information, respectively. 

_abstract property_join_device _: device_
    
Return the device from which to perform collective communications needed by the join context manager. 

_abstract_ join_hook(_** kwargs_)[source][source]
    
Return a `JoinHook` instance for the given `Joinable`. 

Parameters
    
**kwargs** (_dict_) – a `dict` containing any keyword arguments to modify the behavior of the join hook at run time; all `Joinable` instances sharing the same join context manager are forwarded the same value for `kwargs`. 

Return type
    
_JoinHook_ 

_abstract property_join_process_group _: Any_
    
Returns the process group for the collective communications needed by the join context manager itself. 

_class_ torch.distributed.algorithms.JoinHook[source][source]
    
This defines a join hook, which provides two entry points in the join context manager.
Entry points : a main hook, which is called repeatedly while there exists a non-joined process, and a post-hook, which is called once all processes have joined.
To implement a join hook for the generic join context manager, define a class that inherits from `JoinHook` and override `main_hook()` and `post_hook()` as appropriate. 

main_hook()[source][source]
    
Call this hook while there exists a non-joined process to shadow collective communications in a training iteration.
Training iteration i.e., in one forward pass, backward pass, and optimizer step. 

post_hook(_is_last_joiner_)[source][source]
    
Call hook after all processes have joined.
It is passed an additional `bool` argument `is_last_joiner`, which indicates if the rank is one of the last to join. 

Parameters
    
**is_last_joiner** (_bool_) – `True` if the rank is one of the last to join; `False` otherwise.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Generic Join Context Manager
    * `Join`
      * `Join.notify_join_context()`
    * `Joinable`
      * `Joinable.join_device`
      * `Joinable.join_hook()`
      * `Joinable.join_process_group`
    * `JoinHook`
      * `JoinHook.main_hook()`
      * `JoinHook.post_hook()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.distributed.fsdp.fully_shard
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.distributed.fsdp.fully_shard
## PyTorch FSDP2 (`fully_shard`)
PyTorch FSDP2 provides a fully sharded data parallelism (FSDP) implementation targeting performant eager-mode while using per-parameter sharding for improved usability.
  * If you are new to FSDP, we recommend that you start with FSDP2 due to improved usability.
  * If you are currently using FSDP1, consider evaluating the following differences to see if you should switch to FSDP2:


Compared to PyTorch FSDP1 (`FullyShardedDataParallel`):
  * FSDP2 uses `DTensor`-based dim-0 per-parameter sharding for a simpler sharding representation compared to FSDP1’s flat-parameter sharding, while preserving similar throughput performance. More specifically, FSDP2 chunks each parameter on dim-0 across the data parallel workers (using `torch.chunk(dim=0)`), whereas FSDP1 flattens, concatenates, and chunks a group of tensors together, making reasoning about what data is present on each worker and resharding to different parallelisms complex. Per-parameter sharding provides a more intuitive user experience, relaxes constraints around frozen parameters, and allows for communication-free (sharded) state dicts, which otherwise require all-gathers in FSDP1.
  * FSDP2 implements a different memory management approach to handle the multi-stream usages that avoids `torch.Tensor.record_stream`. This ensures deterministic and expected memory usage and does not require blocking the CPU like in FSDP1’s `limit_all_gathers=True`.
  * FSDP2 exposes APIs for manual control over prefetching and collective scheduling, allowing power users more customization. See the methods on `FSDPModule` below for details.
  * FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly support full state dicts. Instead, users can reshard the sharded state dicts containing `DTensor` s to full state dicts themselves using `DTensor` APIs like `DTensor.full_tensor()` or by using higher-level APIs like PyTorch Distributed Checkpoint ‘s distributed state dict APIs. Also, some other args have been removed; see here for details.


If you are onboarding FSDP for the first time or if any of the above appeals to your use case, we recommend that you consider using FSDP2.
See this RFC for details on system design and implementation.
Note
`torch.distributed.fsdp.fully_shard` is currently in prototype state and under development. The core API will likely not change, but we may make some API changes if necessary.
The frontend API is `fully_shard` that can be called on a `module`: 

torch.distributed.fsdp.fully_shard(_module_ , _*_ , _mesh =None_, _reshard_after_forward =True_, _shard_placement_fn =None_, _mp_policy =MixedPrecisionPolicy(param_dtype=None, reduce_dtype=None, output_dtype=None, cast_forward_inputs=True)_, _offload_policy =OffloadPolicy()_, _ignored_params =None_)[source]
    
Apply fully sharded data parallelism (FSDP) to `module`, where FSDP shards module parameters, gradients, and optimizer states across data parallel workers to save memory at the cost of communication.
At initialization, FSDP shards the module’s parameters across the data parallel workers given by `mesh`. Before forward, FSDP all-gathers the sharded parameters across the data-parallel workers to get the unsharded parameters for forward computation. If `reshard_after_forward` is `True`, then FSDP frees the unsharded parameters after forward and re-all-gathers them in backward before gradient computation. After gradient computation, FSDP frees the unsharded parameters and reduce-scatters the unsharded gradients across data-parallel workers.
This implementation represents the sharded parameters as `DTensor` s sharded on dim-0, while the unsharded parameters will be like the original parameters on `module` (e.g. `torch.Tensor` if originally `torch.Tensor`). A module forward pre-hook on `module` all-gathers the parameters, and a module forward hook on `module` frees them (if needed). Similar backward hooks all-gather parameters and later free parameters and reduce-scatter gradients.
Since grouping multiple tensors together for one collective is critical for communication efficiency, this implementation makes this grouping first class. Calling `fully_shard()` on `module` constructs one group that includes the parameters in `module.parameters()` except those already assigned to a group from an earlier call on a submodule. This means that `fully_shard()` should be called bottom-up on your model. Each group’s parameters are all-gathered in one collective, and its gradients are reduce-scattered in one collective. Partitioning the model into multiple groups (“layer by layer”) allows for peak memory savings and communication/computation overlap. Users generally should _not_ call `fully_shard()` only on the topmost root module. 

Parameters
    
  * **module** (_Union_ _[__nn.Module_ _,__List_ _[__nn.Module_ _]_) – The module or modules to shard with FSDP and group together for communication.
  * **mesh** (_Optional_ _[__DeviceMesh_ _]_) – This data parallel mesh defines the sharding and device. If 1D, then parameters are fully sharded across the 1D mesh (FSDP) with `(Shard(0),)` placement. If 2D, then parameters are sharded across the 1st dim and replicated across the 0th dim (HSDP) with `(Replicate(), Shard(0))` placement. The mesh’s device type gives the device type used for communication; if a CUDA or CUDA-like device type, then we use the current device.
  * **reshard_after_forward** (_Union_ _[__bool_ _,__int_ _]_) – 
This controls the parameter behavior after forward and can trade off memory and communication:
    * If `True`, then this reshards parameters after forward and re-all-gathers in backward.
    * If `False`, then this keeps the unsharded parameters in memory after forward and avoids the all-gather in backward.
    * If an `int`, then this represents the world size to reshard to after forward. It should be a non-trivial divisor of the `mesh` shard dim size (i.e. excluding 1 and the dim size itself). A choice may be the intra-node size (e.g. `torch.cuda.device_count()`). This allows the all-gather in backward to be over a smaller world size at the cost of higher memory usage than setting to `True`.
    * The root FSDP state has its value specially set to `False` as a heuristic since its parameters would typically be immediately all-gathered for backward.
    * After forward, the parameters registered to the module depend on to this: The registered parameters are the sharded parameters if `True`; unsharded parameters if `False`; and the paramters resharded to the smaller mesh otherwise. To modify the parameters between forward and backward, the registered parameters must be the sharded parameters. For `False` or an `int`, this can be done by manually resharding via `reshard()`.
  * **shard_placement_fn** (_Optional_ _[__Callable_ _[__[__nn.Parameter_ _]__,__Optional_ _[__Shard_ _]__]__]_) – This callable can be used to override the sharding placement for a parameter to shard a parameter on a dimension other than dim-0. If this callable returns a `Shard` placement (not `None`), then FSDP will shard according to that placement (e.g. `Shard(1)`). If sharding on a nonzero dim, we currently require even sharding, i.e. the tensor dim size on that dim must be divisible by the FSDP shard mesh size.
  * **mp_policy** (_MixedPrecisionPolicy_) – This controls the mixed precision policy, which offers parameter/reduction mixed precision for this module. See `MixedPrecisionPolicy` for details.
  * **offload_policy** (_OffloadPolicy_) – This controls the offloading policy, which offers parameter/gradient/optimizer state offloading. See `OffloadPolicy` and its subclasses for details.
  * **ignored_params** (_Optional_ _[__set_ _[__nn.Parameter_ _]__]_) – Optional(Set[nn.Parameter]): The set of parameters that we don’t want to shard with FSDP.



Returns
    
The module with FSDP applied (in-place). 

Return type
    
FSDPModule
Calling `fully_shard(module)` dynamically constructs a new class that subclasses `type(module)` and an FSDP class `FSDPModule`. For example, if we call `fully_shard(linear)` on a module `linear: nn.Linear`, then FSDP constructs a new class `FSDPLinear` and changes `linear` ‘s type to this. Otherwise, `fully_shard` does not change the module structure and parameter fully-qualified names. The class `FSDPModule` allows providing some FSDP-specific methods on the module. 

_class_ torch.distributed.fsdp.FSDPModule(_* args_, _** kwargs_)
     

reshard()[source][source]
    
Reshards the module’s parameters, freeing the unsharded parameters if they are allocated and registering the sharded parameters to the module. This method is _not_ recursive. 

set_all_reduce_hook(_hook_ , _*_ , _stream =None_)[source][source]
     

Parameters
    
  * **hook** (_Callable_ _[__[__torch.Tensor_ _]__,__None_ _]_) – User-defined all-reduce hook with expected signature `hook(reduce_output: torch.Tensor) -> None` where `reduce_output` is the reduce-scatter output if only using FSDP or the all-reduce output if using native HSDP.
  * **stream** (_Optional_ _[__torch.cuda.Stream_ _]_) – Stream to run the all-reduce hook in. This should only be set if not using native HSDP. If using native HSDP, the hook will run in the internally defined all-reduce stream used by the native HSDP all-reduce.



set_is_last_backward(_is_last_backward_)[source][source]
    
Sets whether the next backward is the last one. On the last backward, FSDP waits on pending gradient reduction and clears internal data data structures for backward prefetching. This can be useful for microbatching. 

set_modules_to_backward_prefetch(_modules_)[source][source]
    
Sets the FSDP modules for which this FSDP module should explicitly prefetch all-gathers in backward. This overrides the default backward pretching implementation that prefetches the next FSDP module based on the reverse post-forward order.
Passing a singleton list containing the previous FSDP module gives the same all-gather overlap behavior as the default overlap behavior. Passing a list with at least length two is required for more aggressive overlap and will use more reserved memory. 

Parameters
    
**modules** (_List_ _[__FSDPModule_ _]_) – FSDP modules to prefetch. 

set_modules_to_forward_prefetch(_modules_)[source][source]
    
Sets the FSDP modules for which this FSDP module should explicitly prefetch all-gathers in forward. The prefetching runs after this module’s all-gather copy-out.
Passing a singleton list containing the next FSDP module gives the same all-gather overlap behavior as the default overlap behavior, except the prefetched all-gather is issued earlier from the CPU. Passing a list with at least length two is required for more aggressive overlap and will use more reserved memory. 

Parameters
    
**modules** (_List_ _[__FSDPModule_ _]_) – FSDP modules to prefetch. 

set_post_optim_event(_event_)[source][source]
    
Sets a post-optimizer-step event for the root FSDP module to wait the all-gather streams on.
By default, the root FSDP module waits the all-gather streams on the current stream to ensure that the optimizer step has finished before all-gathering. However, this may introduce false dependencies if there is unrelated computation after the optimizer step. This API allows the user to provide their own event to wait on. After the root waits on the event, the event is discarded, so this API should be called with a new event each iteration. 

Parameters
    
**event** (_torch.Event_) – Event recorded after the optimizer step to wait all-gather streams on. 

set_reduce_scatter_divide_factor(_factor_)[source][source]
    
Sets a custom divide factor for the reduce-scatter. This becomes a custom reduce op using NCCL’s PreMulSum, which allows multiplying by the factor before reduction. 

Parameters
    
**factor** (_float_) – Custom divide factor. 

set_requires_all_reduce(_requires_all_reduce_ , _*_ , _recurse =True_)[source][source]
    
Sets if the module should all-reduce gradients. This can be used to implement gradient accumulation with only reduce-scatter but not all-reduce for HSDP. 

set_requires_gradient_sync(_requires_gradient_sync_ , _*_ , _recurse =True_)[source][source]
    
Sets if the module should sync gradients. This can be used to implement gradient accumulation _without communication_. For HSDP, this controls both reduce-scatter and all-reduce together. This is the equivalence of no_sync in FSDP1. 

Parameters
    
  * **requires_gradient_sync** (_bool_) – Whether to reduce gradients for the module’s parameters.
  * **recurse** (_bool_) – Whether to set for all FSDP submodules or just the passed-in module.



set_reshard_after_backward(_reshard_after_backward_ , _*_ , _recurse =True_)[source][source]
    
Sets if the module should reshard parameters after backward. This can be used during gradient accumulation to trade off higher memory for reduced communication since the unsharded parameters do not need to be re-all-gathered before the next forward. 

Parameters
    
  * **reshard_after_backward** (_bool_) – Whether to reshard parameters after backward.
  * **recurse** (_bool_) – Whether to set for all FSDP submodules or just the passed-in module.



set_unshard_in_backward(_unshard_in_backward_)[source][source]
    
Sets whether the FSDP module’s parameters need to be unsharded in backward. This can be used in expert cases when the user knows that all parameters in this FSDP module’s parameter group are not needed for backward computation (e.g. embedding). 

unshard(_async_op =False_)[source][source]
    
Unshards the module’s parameters by allocating memory and all-gathering the parameters. This method is _not_ recursive. The unshard follows the `MixedPrecisionPolicy`, so it will all-gather following `param_dtype` if set. 

Parameters
    
**async_op** (_bool_) – If `True`, then returns a `UnshardHandle` that has a `wait()` method to wait on the unshard op. If `False`, then returns `None` and waits on the handle inside this function. 

Return type
    
_Optional_[_UnshardHandle_]
Note
If `async_op=True`, then FSDP will wait on the pending unshard in the module’s pre-forward for the user. The user only needs to call `wait()` explicitly if the wait should happen before pre-forward. 

_class_ torch.distributed.fsdp.UnshardHandle
    
A handle to wait on a `FSDPModule.unshard()` op. 

wait()[source][source]
    
Waits on the unshard op. This ensures that the current stream can use the unsharded parameters, which are now registered to the module. 

torch.distributed.fsdp.register_fsdp_forward_method(_module_ , _method_name_)[source]
    
Registers a method on `module` to be considered a forward method for FSDP.
FSDP all-gathers parameters pre-forward and optionally frees parameters post-forward (depending on `reshard_after_forward`). FSDP only knows to do this for `nn.Module.forward()` by default. This function patches a user-specified method to run the pre/post-forward hooks before/after the method, respectively. If `module` is not an `FSDPModule`, then this is a no-op. 

Parameters
    
  * **module** (_nn.Module_) – Module to register the forward method on.
  * **method_name** (_str_) – Name of the forward method.



_class_ torch.distributed.fsdp.MixedPrecisionPolicy(_param_dtype =None_, _reduce_dtype =None_, _output_dtype =None_, _cast_forward_inputs =True_)
    
This configures FSDP’s mixed precision. Unlike autocast, this applies mixed precision at the module level, not op level, which means low-precision activations are saved for backward and high-to-low-precision casts are incurred only at module boundaries.
FSDP works well with module-level mixed precision since it keeps the high-precision sharded parameters in memory anyway. In other words, FSDP does not require any extra memory to keep a high-precision copy of the parameters for the optimizer step. 

Variables
    
  * **param_dtype** (_Optional_ _[__torch.dtype_ _]_) – This specifies the dtype for the unsharded parameter and hence the dtype for forward/backward computation and the parameter all-gather. If this is `None`, then the unsharded parameter uses the original dtype. The optimizer step uses the sharded parameter in the original dtype. (Default: `None`)
  * **reduce_dtype** (_Optional_ _[__torch.dtype_ _]_) – This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is `None` but `param_dtype` is not `None`, then the reduction uses the compute dtype. This can be used to run gradient reduction in full precision while using low precision for compute. If also gradient reduction is disabled via `set_requires_gradient_sync()`, then FSDP will accumulate gradients using `reduce_dtype`. (Default: `None`)
  * **output_dtype** (_Optional_ _[__torch.dtype_ _]_) – This specifies the dtype for casting floating-point forward outputs. This can be used to help implement cases where different modules have different mixed precision policies. (Default: `None`)
  * **cast_forward_inputs** (_bool_) – This specifies whether FSDP should cast the forward’s floating-point input tensors to `param_dtype` or not.



_class_ torch.distributed.fsdp.OffloadPolicy
    
This base class represents the policy of no offloading and is only used as the default value for the `offload_policy` arg. 

_class_ torch.distributed.fsdp.CPUOffloadPolicy(_pin_memory =True_)
    
This offload policy offloads parameters, gradients, and optimizer states to CPU. Sharded parameters are copied host-to-device before all-gather. The all-gathered parameters are freed according to `reshard_after_forward`. Sharded gradients are copied device-to-host in backward, and the optimizer step runs on CPU with CPU optimizer states. 

Variables
    
**pin_memory** (_bool_) – Whether to pin sharded parameter and gradient memory. Pinning memory allows both more efficient H2D/D2H copies and for the copies to overlap with compute. However, the pinned memory cannot be used by other processes. Set this to `False` if you have insufficient CPU memory. (Default: `True`)
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.distributed.fsdp.fully_shard
    * PyTorch FSDP2 (`fully_shard`)
      * `fully_shard()`
      * `FSDPModule`
        * `FSDPModule.reshard()`
        * `FSDPModule.set_all_reduce_hook()`
        * `FSDPModule.set_is_last_backward()`
        * `FSDPModule.set_modules_to_backward_prefetch()`
        * `FSDPModule.set_modules_to_forward_prefetch()`
        * `FSDPModule.set_post_optim_event()`
        * `FSDPModule.set_reduce_scatter_divide_factor()`
        * `FSDPModule.set_requires_all_reduce()`
        * `FSDPModule.set_requires_gradient_sync()`
        * `FSDPModule.set_reshard_after_backward()`
        * `FSDPModule.set_unshard_in_backward()`
        * `FSDPModule.unshard()`
      * `UnshardHandle`
        * `UnshardHandle.wait()`
      * `register_fsdp_forward_method()`
      * `MixedPrecisionPolicy`
      * `OffloadPolicy`
      * `CPUOffloadPolicy`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.deterministic
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.deterministic 

torch.utils.deterministic.fill_uninitialized_memory
    
A `bool` that, if True, causes uninitialized memory to be filled with a known value when `torch.use_deterministic_algorithms()` is set to `True`. Floating point and complex values are set to NaN, and integer values are set to the maximum value.
Default: `True`
Filling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance and still be deterministic.
The following operations will fill uninitialized memory when this setting is turned on:
>   * `torch.Tensor.resize_()` when called with a tensor that is not quantized
>   * `torch.empty()`
>   * `torch.empty_strided()`
>   * `torch.empty_permuted()`
>   * `torch.empty_like()`
> 

Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.deterministic
    * `fill_uninitialized_memory`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch::deploy has been moved to pytorch/multipy
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch::deploy has been moved to pytorch/multipy
`torch::deploy` has been moved to its new home at https://github.com/pytorch/multipy.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch::deploy has been moved to pytorch/multipy


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Distributed Checkpoint - torch.distributed.checkpoint
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Distributed Checkpoint - torch.distributed.checkpoint
Distributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel. It handles load-time resharding which enables saving in one cluster topology and loading into another.
DCP is different than torch.save and torch.load in a few significant ways:
  * It produces multiple files per checkpoint, with at least one per rank.
  * It operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.


The entrypoints to load and save a checkpoint are the following:
## Additional resources:
  * Getting Started with Distributed Checkpoint (DCP)
  * Asynchronous Saving with Distributed Checkpoint (DCP)
  * TorchTitan Checkpointing Docs
  * TorchTitan DCP Implementation



_class_ torch.distributed.checkpoint.state_dict_saver.AsyncCheckpointerType(_value_)[source][source]
    
Enum for async checkpointer type. 

torch.distributed.checkpoint.state_dict_saver.save(_state_dict_ , _*_ , _checkpoint_id =None_, _storage_writer =None_, _planner =None_, _process_group =None_, _no_dist =False_)[source][source]
    
Save a distributed model in SPMD style.
This function is different from `torch.save()` as it handles `ShardedTensor` , and `DTensor` by having each rank only save their local shards.
For each `Stateful` object (having both a `state_dict` and a `load_state_dict`), save will call `state_dict` before serialization.
Warning
There is no guarantees of Backwards Compatibility across PyTorch versions for saved state_dicts.
Warning
If using the process_group argument, make sure that only its ranks call save_state_dict and that all data in state_dict belong to it.
Note
When saving checkpoint for FSDP’s ShardingStrategy.HYBRID_SHARD, only one of the shard_group should be calling save_state_dict and the corresponding process group needs to be passed in.
Note 

If no process group is available, this function assumes the intention is to save the
    
state_dict in the local process. 

Parameters
    
  * **state_dict** (_Dict_ _[__str_ _,__Any_ _]_) – The state_dict to save.
  * **checkpoint_id** (_Union_ _[__str_ _,__os.PathLike_ _,__None_ _]_) – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: `None`)
  * **storage_writer** (_Optional_ _[__StorageWriter_ _]_) – Instance of StorageWriter used to perform writes. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: `None`)
  * **planner** (_Optional_ _[__SavePlanner_ _]_) – Instance of SavePlanner. If this is not specificed, the default planner will be used. (Default: `None`)
  * **process_group** (_Optional_ _[__ProcessGroup_ _]_) – ProcessGroup to be used for cross-rank synchronization. (Default: `None`)
  * **no_dist** (_bool_) – If `True`, this function will assume the intent is to load a checkpoint without using cross-rank synchronization. (Default: `False`)



Returns
    
Metadata object for the saved checkpoint. 

Return type
    
Metadata
Example
```
>>> my_model = MyModule()

```
Copy to clipboard
```
>>> state_dict = {"model": my_model}

```
Copy to clipboard
```
>>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(
...   "/checkpoint/1"
... )
>>> torch.distributed.checkpoint.save(
>>>   state_dict=state_dict,
>>>   storage_writer=fs_storage_writer,
>>> )

```
Copy to clipboard
Note
save_state_dict uses collectives to coordinate writes across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by `torch.cuda.current_device()` and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via `torch.cuda.set_device()`. 

torch.distributed.checkpoint.state_dict_saver.async_save(_state_dict_ , _*_ , _checkpoint_id =None_, _storage_writer =None_, _planner =None_, _process_group =None_, _async_checkpointer_type =AsyncCheckpointerType.THREAD_)[source][source]
    
Asynchronous version of `save`. This code first de-stages the state_dict on to the staging storage (defaults to CPU memory), and then calls the save in a separate thread.
Warning
This feature is experimental and subject to change. 

Parameters
    
  * **state_dict** (_Dict_ _[__str_ _,__Any_ _]_) – The state_dict to save.
  * **checkpoint_id** (_Union_ _[__str_ _,__os.PathLike_ _,__None_ _]_) – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: `None`)
  * **storage_writer** (_Optional_ _[__StorageWriter_ _]_) – Instance of StorageWriter used to perform ‘stage’ and ‘save’. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: `None`)
  * **planner** (_Optional_ _[__SavePlanner_ _]_) – Instance of SavePlanner. If this is not specificed, the default planner will be used. (Default: `None`)
  * **process_group** (_Optional_ _[__ProcessGroup_ _]_) – ProcessGroup to be used for cross-rank synchronization. (Default: `None`)



Returns
    
A future holding the resultant Metadata object from save. 

Return type
    
Future
Example
```
>>> my_model = MyModule()

```
Copy to clipboard
```
>>> state_dict = {"model": my_model}

```
Copy to clipboard
```
>>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(
...   "/checkpoint/1"
... )
>>> checkpoint_future = torch.distributed.checkpoint.async_save(
>>>   state_dict=state_dict,
>>>   storage_writer=fs_storage_writer,
>>> )
>>>
>>> # ... do some work ...
>>>
>>> checkpoint_future.result()

```
Copy to clipboard 

torch.distributed.checkpoint.state_dict_saver.save_state_dict(_state_dict_ , _storage_writer_ , _process_group =None_, _coordinator_rank =0_, _no_dist =False_, _planner =None_)[source][source]
    
This method is deprecated. Please switch to ‘save’. 

Return type
    
_Metadata_ 

torch.distributed.checkpoint.state_dict_loader.load(_state_dict_ , _*_ , _checkpoint_id =None_, _storage_reader =None_, _planner =None_, _process_group =None_, _no_dist =False_)[source][source]
    
Load a checkpoint into a distributed state dict in SPMD style.
Each rank must have the same keys in their `state_dict` provided to this API. Mismatched keys may result in hangs or errors. If unsure, you can use the `utils._assert_same_keys` API to check (but may incur communication costs).
Each rank will try to read the least amount of data necessary to fulfill the requested state_dict. When loading `ShardedTensor` or `DTensor` instances, each rank only reads data for their local shards.
For each `Stateful` object (having both a `state_dict` and a `load_state_dict`), load will first call `state_dict` before attempting deserialization, followed by `load_state_dict` once the deserialization is complete. For each non-`Stateful` object, load will deserailize the object, and then replace it in the `state_dict` with the deserialized object.
Warning
All tensors in `state_dict` must be allocated on their destination device _prior to_ calling this function.
All non-tensor data is loaded using torch.load() and modified in place on state_dict.
Warning
Users must call load_state_dict on the root module to ensure load pos-processing and non-tensor data properly propagates. 

Parameters
    
  * **state_dict** (_Dict_ _[__str_ _,__Any_ _]_) – The state_dict to load the checkpoint into.
  * **checkpoint_id** (_Union_ _[__str_ _,__os.PathLike_ _,__None_ _]_) – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: `None`)
  * **storage_reader** (_Optional_ _[__StorageReader_ _]_) – Instance of StorageWriter used to perform reads. If this is not specified, DCP will automatically infer the reader based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: `None`)
  * **planner** (_Optional_ _[__LoadPlanner_ _]_) – Instance of LoadPlanner. If this is not specificed, the default planner will be used. (Default: `None`)
  * **process_group** (_Optional_ _[__ProcessGroup_ _]_) – ProcessGroup to be used for cross-rank synchronization. (Default: `None`)
  * **no_dist** (_bool_) – If `True`, this function will assume the intent is to load a checkpoint without using cross-rank synchronization. (Default: `False`)



Returns
    
None. 

Return type
    
None 

Examples
    
```
>>> my_model = MyModule()
>>> optimizer = Adagrad(my_model.parameters())
>>> model_state_dict = my_model.state_dict()
>>> fs_storage_reader = torch.distributed.checkpoint.FileSystemReader(
...   "/checkpoint/1"
... )

```
Copy to clipboard
```
>>> torch.distributed.checkpoint.load_state_dict(
>>>   state_dict=model_state_dict,
>>>   storage_reader=fs_storage_reader,
>>> )

```
Copy to clipboard
```
>>> # module.load_state_dict() function might have customized steps
>>> # to flush the state_dict, must call it to
>>> # ensure correct behavior.
>>> my_model.load_state_dict(model_state_dict)

```
Copy to clipboard
Note
load_state_dict uses collectives to coordinate reads across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by `torch.cuda.current_device()` and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via `torch.cuda.set_device()`. 

torch.distributed.checkpoint.state_dict_loader.load_state_dict(_state_dict_ , _storage_reader_ , _process_group =None_, _coordinator_rank =0_, _no_dist =False_, _planner =None_)[source][source]
    
This method is deprecated. Please switch to ‘load’.
The following module is also useful for additional customization of the staging mechanisms used for asynchronous checkpointing (torch.distributed.checkpoint.async_save): 

_class_ torch.distributed.checkpoint.staging.AsyncStager(_* args_, _** kwargs_)[source][source]
    
This protocol is meant to provide customization and extensibility for dcp.async_save, allowing users to customize how data is staged previous to executing the usual dcp.save path in parallel. The expected order of operations (concretely defined in torch.distributed.state_dict_saver.async_save) is the following:
  1. 

AsyncStager.stage_data(state_dict):
    
This call gives the AsyncStager the opportunity to ‘stage’ the state_dict. The expectation and purpose of staging in this context is to create a “training-safe” representation of the state dict, meaning that any updates to module data after staging is complete should not be reflected in the state dict returned from this method. For example, in the default case a copy of the entire state dict is created on CPU RAM and returned here, allowing users to continue training without risking changes to data which is being serialized.
  2. 

dcp.save is called on the state_dict returned from stage in parallel. This call is responsible
    
for serializing the state_dict and writing it to storage.
  3. 

If AsyncStager.should_synchronize_after_execute is True, this method will be called immediately after
    
the serialization thread starts and before returning from dcp.async_save. If this is set to False, the assumption is the user has defined a custom synchronization point for the the purpose of further optimizing save latency in the training loop (for example, by overlapping staging with the forward/backward pass), and it is the respondsibility of the user to call AsyncStager.synchronize_staging at the appropriate time.



_property_ should_synchronize_after_execute _: bool_
    
Whether to synchronize after executing the stage. 

stage(_state_dict_)[source][source]
    
Returns a “staged” copy of state_dict. The expectation of the staged copy is that it is innoculated from any updates incurred after the stage call is complete. 

Return type
    
dict[str, _Union_[~StatefulT, _Any_]] 

synchronize_staging()[source][source]
    
In the case stage is async in some way, this method should be called to ensure staging is complete and it is safe to begin modifying the original state_dict 

_class_ torch.distributed.checkpoint.staging.BlockingAsyncStager(_cache_staged_state_dict =False_, _type_check =False_)[source][source]
    
An implementation of AsyncStager which stages the state_dict on CPU RAM and blocks until the copy is complete. This implementation also provides an option to optimize stage latency using pinned memory.
N.B. synchronize_staging is a no-op in this case. 

stage(_state_dict_)[source][source]
    
Returns a copy of state_dict on the CPU. 

Return type
    
dict[str, _Union_[~StatefulT, _Any_]] 

synchronize_staging()[source][source]
    
No-op function, since staging is blocking.
In addition to the above entrypoints, Stateful objects, as described below, provide additional customization during saving/loading .. automodule:: torch.distributed.checkpoint.stateful 

_class_ torch.distributed.checkpoint.stateful.Stateful(_* args_, _** kwargs_)[source][source]
    
Stateful protocol for objects that can be checkpointed and restored. 

load_state_dict(_state_dict_)[source][source]
    
Restore the object’s state from the provided state_dict. 

Parameters
    
**state_dict** (_dict_ _[__str_ _,__Any_ _]_) – The state dict to restore from 

state_dict()[source][source]
    
Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in load_state_dict().
Warning
Because of the inplace nature of restoring a checkpoint, this function is also called during torch.distributed.checkpoint.load. 

Returns
    
The objects state dict 

Return type
    
Dict
This example shows how to use Pytorch Distributed Checkpoint to save a FSDP model.
The following types define the IO interface used during checkpoint: 

_class_ torch.distributed.checkpoint.StorageReader[source][source]
    
Interface used by `load_state_dict` to read from storage.
One StorageReader instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.
A subclass should expected the following sequence of calls by `load_state_dict`:
  1. (all ranks) set checkpoint_id if users pass a valid checkpoint_id.
  2. (all ranks) read_metadata()
  3. (all ranks) set_up_storage_reader()
  4. (all ranks) prepare_local_plan()
  5. (coordinator) prepare_global_plan()
  6. (all ranks) read_data()



_abstract_ prepare_global_plan(_plans_)[source][source]
    
Perform centralized planning of storage loading.
This method is only called on the coordinator instance.
While this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data. 

Parameters
    
**plans** (_list_ _[__torch.distributed.checkpoint.planner.LoadPlan_ _]_) – A list of `LoadPlan` instances, one for each rank. 

Returns
    
A list of transformed `LoadPlan` after storage global planning 

Return type
    
list[torch.distributed.checkpoint.planner.LoadPlan] 

_abstract_ prepare_local_plan(_plan_)[source][source]
    
Perform storage-specific local planning.
While this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data. 

Parameters
    
**plan** (_LoadPlan_) – The local plan from the `LoadPlan` in use. 

Returns
    
A transformed `LoadPlan` after storage local planning 

Return type
    
_LoadPlan_ 

_abstract_ read_data(_plan_ , _planner_)[source][source]
    
Read all items from `plan` using `planner` to resolve the data.
A subclass should call `LoadPlanner::load_bytes` to deserialize a BytesIO object into the right place.
A subclass should call `LoadPlanner::resolve_tensor` to get access to the tensors that in should load data into.
It’s the StorageLayer responsibility to properly schedule any cross device copies required. 

Parameters
    
  * **plan** (_LoadPlan_) – The local plan to execute on
  * **planner** (_LoadPlanner_) – The planner object to use to resolve items.



Returns
    
A future that completes once all reads are finished. 

Return type
    
_Future_[None] 

_abstract_ read_metadata()[source][source]
    
Read the checkpoint metadata. 

Returns
    
The metadata object associated with the checkpoint being loaded. 

Return type
    
_Metadata_ 

_abstract_ reset(_checkpoint_id =None_)[source][source]
    
Calls to indicates a brand new checkpoint read is going to happen. A checkpoint_id may be present if users set the checkpoint_id for this checkpoint read. The meaning of the checkpiont_id is storage-dependent. It can be a path to a folder/file or a key for a key-value storage. 

Parameters
    
**checkpoint_id** (_Union_ _[__str_ _,__os.PathLike_ _,__None_ _]_) – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is more like a key-value store. (Default: `None`) 

_abstract_ set_up_storage_reader(_metadata_ , _is_coordinator_)[source][source]
    
Initialize this instance. 

Parameters
    
  * **metadata** (_Metadata_) – The metadata schema to use.
  * **is_coordinator** (_bool_) – Whether this instance is responsible for coordinating the checkpoint.



_abstract classmethod_validate_checkpoint_id(_checkpoint_id_)[source][source]
    
Check if the given checkpoint_id is supported by the stroage. This allow us to enable automatic storage selection. 

Return type
    
bool 

_class_ torch.distributed.checkpoint.StorageWriter[source][source]
    
Interface used by `save_state_dict` to write to storage.
One StorageWriter instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.
A subclass should expect the following sequence of calls.
  1. (all ranks) set checkpoint_id if users pass a valid checkpoint_id.
  2. (all ranks) set_up_storage_writer()
  3. (all ranks) prepare_local_plan()
  4. (coordinator) prepare_global_plan()
  5. (all ranks) write_data()
  6. (coordinator) finish()



_abstract_ finish(_metadata_ , _results_)[source][source]
    
Write the metadata and marks the current checkpoint as successful.
The actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it’s recoverable in to the same object graph. 

Parameters
    
  * **metadata** (_Metadata_) – metadata for the new checkpoint
  * **results** (_list_ _[__list_ _[__torch.distributed.checkpoint.storage.WriteResult_ _]__]_) – A list of WriteResults from all ranks.



Returns
    
None 

Return type
    
None 

_abstract_ prepare_global_plan(_plans_)[source][source]
    
Perform centralized planning of storage.
This method is only called on the coordinator instance.
While this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data. 

Parameters
    
**plans** (_list_ _[__torch.distributed.checkpoint.planner.SavePlan_ _]_) – A list of `SavePlan` instances, one for each rank. 

Returns
    
A list of transformed `SavePlan` after storage global planning 

Return type
    
list[torch.distributed.checkpoint.planner.SavePlan] 

_abstract_ prepare_local_plan(_plan_)[source][source]
    
Perform storage-specific local planning.
While this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data. 

Parameters
    
**plan** (_SavePlan_) – The local plan from the `SavePlanner` in use. 

Returns
    
A transformed `SavePlan` after storage local planning 

Return type
    
_SavePlan_ 

_abstract_ reset(_checkpoint_id =None_)[source][source]
    
Calls to indicates a brand new checkpoint write is going to happen. A checkpoint_id may be present if users set the checkpoint_id for this checkpoint write. The meaning of the checkpiont_id is storage-dependent. It can be a path to a folder/file or a key for a key-value storage. 

Parameters
    
**checkpoint_id** (_Union_ _[__str_ _,__os.PathLike_ _,__None_ _]_) – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: `None`) 

_abstract_ set_up_storage_writer(_is_coordinator_)[source][source]
    
Initialize this instance. 

Parameters
    
**is_coordinator** (_bool_) – Whether this instance is responsible for coordinating the checkpoint. 

storage_meta()[source][source]
    
Return the storage-specific metadata. This is used to store additional information in a checkpoint that can be useful for providing request-level observability. StorageMeta is passed to the `SavePlanner` during save calls. Returns None by default.
TODO: provide an example 

Return type
    
_Optional_[_StorageMeta_] 

_abstract classmethod_validate_checkpoint_id(_checkpoint_id_)[source][source]
    
Check if the given checkpoint_id is supported by the stroage. This allow us to enable automatic storage selection. 

Return type
    
bool 

_abstract_ write_data(_plan_ , _planner_)[source][source]
    
Write all items from `plan` using `planner` to resolve the data.
A subclass should call `SavePlanner::resolve_data` on each item from the plan to get access to the underlying object to write.
Subclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:
  * They might be on any device, including not matching the one on `WriteItem::tensor_data`
  * They might be views or not contiguous. Only the projection needs to be saved.



Parameters
    
  * **plan** (_SavePlan_) – The save plan to execute.
  * **planner** (_SavePlanner_) – Planner object to be used to resolve items to data.



Returns
    
A future that completes to a list of WriteResult 

Return type
    
_Future_[list[torch.distributed.checkpoint.storage.WriteResult]]
The following types define the planner interface used during checkpoint: 

_class_ torch.distributed.checkpoint.LoadPlanner[source][source]
    
Abstract class defining the protocol used by load_state_dict to plan the load process.
LoadPlanner are stateful objects that can be used to customize the whole load process.
LoadPlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.
A planner subclass can expect the following sequence of calls during load_state_dict:
  1. 

set_up_planner - called on all ranks.
    
Signals the start of loading a checkpoint.
  2. 

create_local_plan - called on all ranks.
    
Process the state_dict and produces a LoadPlan that will be sent for global planning.
  3. 

create_global_plan - called on the coordinator rank only.
    
Takes the LoadPlan from all ranks and make any global decision.
  4. 

load_bytes - called multiple times on each rank
    
This is called once per non-tensor value in state_dict.
  5. 

resolve_tensor and commit_tensor - called multiple times on each rank
    
They are called in pair for each Tensor value in state_dict.


Users are recommended to extend DefaultLoadPlanner instead of this interface directly as most changes can be expressed by changes in a single method.
There are two usual patterns of extension:
Rewriting state_dict. This is the simplest way to extend the load process as it doesn’t requite understanding the intrincacies of how LoadPlan works. We need to keep a reference to the original state_dict as load happens in place so we need to be able to perform it in place
```
>>> class RenamePlanner(DefaultLoadPlanner):
>>>   def set_up_planner(
>>>     self,
>>>     state_dict: STATE_DICT_TYPE,
>>>     metadata: Metadata,
>>>     is_coordinator: bool,
>>>   ) -> None:
>>>     self.original_state_dict = state_dict
>>>     state_dict = {"foo_" + k: v for k, v in state_dict.items()}
>>>
>>>     if self.flatten_sharded_tensors:
>>>       state_dict = _flatten_sharded_tensors(state_dict)
>>>
>>>     if self.flatten_state_dict:
>>>       state_dict, self.mappings = flatten_state_dict(state_dict)
>>>
>>>     self.state_dict = state_dict
>>>     self.metadata = metadata
>>>     self.is_coordinator = is_coordinator
>>>
>>>   def load_bytes(self, read_item, value):
>>> # Remove the "foo_" prefix
>>>     self.original_state_dict[read_item.dest_index.fqn[4:]] = torch.load(value, weights_only=False)

```
Copy to clipboard
Modifying resolve_tensor and commit_tensor to handle load time transformation.
```
>>> class MetaModelMaterialize(DefaultSavePlanner):
>>>   def resolve_tensor(self, read_item):
>>>     tensor = super().resolve_tensor(read_item)
>>>     return torch.empty_like(tensor, device="cpu")
>>>
>>>   def commit_tensor(self, read_item, tensor):
>>>     self.state_dict[read_item.dest_index.fqn] = tensor

```
Copy to clipboard 

_abstract_ commit_tensor(_read_item_ , _tensor_)[source][source]
    
Call once the StorageReader finished loading data into `tensor`.
The provided tensor is the same one returned by the call to `resolve_tensor`. This method is only needed if this LoadPlanner needs to post process `tensor` prior to copying it back to the one in the state_dict.
The contents of tensor will follow its device synchronization model. 

_abstract_ create_global_plan(_global_plan_)[source][source]
    
Compute the global load plan and return plans for each rank.
. N.B. This is called on the coordinator rank only 

Return type
    
list[torch.distributed.checkpoint.planner.LoadPlan] 

_abstract_ create_local_plan()[source][source]
    
Create a LoadPlan based on state_dict and metadata provided by set_up_planner.
. N.B. This is called on every rank. 

Return type
    
_LoadPlan_ 

_abstract_ finish_plan(_central_plan_)[source][source]
    
Accept the plan from coordinator and return final LoadPlan. 

Return type
    
_LoadPlan_ 

_abstract_ load_bytes(_read_item_ , _value_)[source][source]
    
Load the item described by `read_item``and ``value`.
This method is expected to modify in-place the underlying state_dict.
The contents of `value` are defined by the SavePlanner used to produce the checkpoint being loaded. 

resolve_bytes(_read_item_)[source][source]
    
Return the BytesIO to be used by the StorageReader to load read_item.
The BytesIO should alias with one on the underlying state_dict as StorageReader will replace its contents. 

Return type
    
_BytesIO_ 

_abstract_ resolve_tensor(_read_item_)[source][source]
    
Return the tensor described by `read_item` to be used by the StorageReader to load read_item.
The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that’s not possible, the planner can use the `commit_tensor` method to copy the data back to the one in state_dict. 

Return type
    
_Tensor_ 

_abstract_ set_up_planner(_state_dict_ , _metadata =None_, _is_coordinator =False_)[source][source]
    
Initialize this instance to load data into `state_dict`.
. N.B. This is called on every rank. 

_class_ torch.distributed.checkpoint.LoadPlan(_items :list[torch.distributed.checkpoint.planner.ReadItem]_, _storage_data :Any=None_, _planner_data :Any=None_)[source][source]


_class_ torch.distributed.checkpoint.ReadItem(_type :torch.distributed.checkpoint.planner.LoadItemType_, _dest_index :torch.distributed.checkpoint.metadata.MetadataIndex_, _dest_offsets :torch.Size_, _storage_index :torch.distributed.checkpoint.metadata.MetadataIndex_, _storage_offsets :torch.Size_, _lengths :torch.Size_)[source][source]


_class_ torch.distributed.checkpoint.SavePlanner[source][source]
    
Abstract class defining the protocol used by save_state_dict to plan the save process.
SavePlanners are stateful objects that can be used to customize the whole save process.
SavePlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.
A planner subclass can expect the following sequence of calls during save_state_dict:
  1. 

set_up_planner - called on all ranks.
    
Signals the start of a checkpoint save.
  2. 

create_local_plan - called on all ranks.
    
Process the state_dict and produces a SavePlan that will be sent for global planning.
  3. 

create_global_plan - called on the coordinator rank only.
    
Takes the SavePlan from all ranks and make any global decision.
  4. 

finish_plan - called on all ranks.
    
This gives each rank a chance to adjust to global planning decisions.
  5. 

resolve_data - called multiple times on each rank
    
Lookups a value on the state_dict for the storage layer to write.


Users are recommended to extend DefaultSavePlanner instead of this interface directly as most changes can be expressed by changes in a single method.
There are 3 usual patterns of extension:
Rewriting state_dict. This is the simplest way to extend the save process as it doesn’t requite understanding the intrincacies of how SavePlan works:
```
>>> class RenamePlanner(DefaultSavePlanner):
>>>   def set_up_planner(
>>>     self,
>>>     state_dict: STATE_DICT_TYPE,
>>>     storage_meta: Optional[StorageMeta],
>>>     is_coordinator: bool,
>>>   ) -> None:
>>> # prefix all keys with `foo_``
>>>     super().set_up_planner({"foo_" + k: v for k, v in state_dict.items()}, storage_meta, is_coordinator)

```
Copy to clipboard
Modifying local plan and lookup in tandem. This is useful when fine control of how data is persisted
```
>>> class FP16Planner(DefaultSavePlanner):
>>>   def create_local_plan(self):
>>>     plan = super().create_local_plan()
>>>     for p in plan:
>>>       if p.tensor_data is not None:
>>>         p.tensor_data.properties.dtype = torch.float16
>>>     return plan
>>>
>>>   def resolve_data(self, write_item):
>>>     item = super().resolve_data(write_item)
>>>     return item if write_item.type == WriteItemType.BYTE_IO else item.to(torch.float16)

```
Copy to clipboard
Using the global planning step to make central decisions that can’t be made individually by each rank
```
>>> from itertools import zip_longest
>>> from dataclasses import replace
>>> class DDPLoadBalancingPlanner(DefaultSavePlanner):
>>> # This uses the default local plan behavior of having all non-sharded writes in rank 0
>>> # This sample doesn't handle ShardedTensors
>>>   def create_global_plan(self, all_plans):
>>>     iters = [iter(all_plans[0].items)] * len(all_plans)
>>>     items_per_rank = [
>>>       [item for item in items if item is not None]
>>>       for items in zip(*zip_longest(*iters), strict=True)
>>>     ]
>>>     all_plans = [
>>>       replace(plan, items=items)
>>>       for plan, items in zip(all_plans, items_per_rank, strict=True)
>>>     ]
>>>     return super().create_global_plan(all_plans)

```
Copy to clipboard
Finally, some planners need to save additional metadata in the checkpoint, this is accomplished by having each rank contribute their data items in the local plan and the global planner aggregate them:
```
>>> class SaveExtraDataPlanner(DefaultSavePlanner):
>>>   def create_local_plan(self) -> SavePlan:
>>>     plan = super().create_local_plan()
>>>     return replace(plan, planner_data="per-rank-data")
>>>
>>>   def create_global_plan(self, all_plans: List[SavePlan]) -> Tuple[List[SavePlan], Metadata]:
>>>     global_plan, metadata = super().create_global_plan(all_plans)
>>>     merged_data = [p.planner_data for p in global_plan]
>>>     metadata = replace(metadata, planner_data=merged_data)
>>>     return global_plan, metadata

```
Copy to clipboard 

_abstract_ create_global_plan(_all_plans_)[source][source]
    
Compute the global checkpoint plan and return the local plan of each rank.
This is called on the coordinator rank only. 

Return type
    
tuple[list[torch.distributed.checkpoint.planner.SavePlan], torch.distributed.checkpoint.metadata.Metadata] 

_abstract_ create_local_plan()[source][source]
    
Compute the save plan for the current rank.
This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data.
This is called on all ranks. 

Return type
    
_SavePlan_ 

_abstract_ finish_plan(_new_plan_)[source][source]
    
Merge the plan created by create_local_plan and the result of create_global_plan.
This is called on all ranks. 

Return type
    
_SavePlan_ 

_abstract_ resolve_data(_write_item_)[source][source]
    
Transform and prepare `write_item` from `state_dict` for storage, ensuring idempotency and thread-safety.
Lookup the object associated with `write_item` in `state_dict` and apply any transformation (such as serialization) prior to the storage layer consuming it.
Called on each rank multiple times, at least once per WriteItem in the final SavePlan.
This method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need.
Any transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing.
When returning tensors, they can be on any device or format, they can be views too. It’s the storage layer responsibility to figure out how to save them. 

Return type
    
_Union_[_Tensor_, _BytesIO_] 

_abstract_ set_up_planner(_state_dict_ , _storage_meta =None_, _is_coordinator =False_)[source][source]
    
Initialize this planner to save `state_dict`.
Implementations should save those values as they won’t be provided lated in the save process.
This is called on all ranks. 

_class_ torch.distributed.checkpoint.SavePlan(_items :list[torch.distributed.checkpoint.planner.WriteItem]_, _storage_data :Any=None_, _planner_data :Any=None_, _usable :bool=True_)[source][source]


_class_ torch.distributed.checkpoint.planner.WriteItem(_index_ , _type_ , _tensor_data =None_)[source][source]
    
Dataclass which holds information about what needs to be written to storage. 

tensor_storage_size()[source][source]
    
Calculates the storage size of the underlying tensor, or None if this is not a tensor write. 

Returns
    
Optional[int] storage size, in bytes of underlying tensor if any. 

Return type
    
_Optional_[int]
We provide a filesystem based storage layer: 

_class_ torch.distributed.checkpoint.FileSystemReader(_path_ , __extension_registry =None_)[source][source]
     

_property_ checkpoint_id _: Union[str,PathLike]_
    
return the checkpoint_id that will be used to load the checkpoint. 

_class_ torch.distributed.checkpoint.FileSystemWriter(_path_ , _single_file_per_rank =True_, _sync_files =True_, _thread_count =1_, _per_thread_copy_ahead =10000000_, _cache_staged_state_dict =False_, _overwrite =True_, __extensions =None_)[source][source]
    
Basic implementation of StorageWriter using file IO.
This implementation makes the following assumptions and simplifications:
  * The checkpoint path is an empty or non-existing directory.
  * File creation is atomic


The checkpoint consist of one file per write request plus a .metadata file with the serialized metadata. 

stage(_state_dict_)[source][source]
    
Override of AsyncStager.stage 

Return type
    
dict[str, _Union_[~StatefulT, _Any_]]
We provide default implementations of LoadPlanner and SavePlanner that can handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor. 

_class_ torch.distributed.checkpoint.DefaultSavePlanner(_flatten_state_dict =True_, _flatten_sharded_tensors =True_, _dedup_replicated_tensors =None_, _dedup_save_to_lowest_rank =False_, _enable_plan_caching =False_)[source][source]
     

lookup_object(_index_)[source][source]
    
Extension from the planner interface to make it easy to extend the default planner. 

Return type
    
_Any_ 

transform_object(_write_item_ , _object_)[source][source]
    
Extension from the planner interface to make it easy to extend the default planner. 

_class_ torch.distributed.checkpoint.DefaultLoadPlanner(_flatten_state_dict =True_, _flatten_sharded_tensors =True_, _allow_partial_load =False_)[source][source]
    
DefaultLoadPlanner that adds multiple features on top of LoadPlanner.
In particular it adds the following:
flatten_state_dict: Handle state_dict with nested dicts flatten_sharded_tensors: For FSDP in 2D parallel mode allow_partial_load: If False, will raise a runtime error if a key is present in state_dict, but not in the checkpoint. 

lookup_tensor(_index_)[source][source]
    
Extension from the planner interface to make it easy to extend the default planner. 

Return type
    
_Tensor_ 

transform_tensor(_read_item_ , _tensor_)[source][source]
    
Extension from the planner interface to make it easy to extend the default planner.
Due to legacy design decisions, the state dictionaries of FSDP and DDP may have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover, FSDP offers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism).
To tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts. get_model_state_dict() returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly, get_optimizer_state_dict() provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency, get_optimizer_state_dict() converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary.
Note that results returned by these APIs can be used directly with the torch.distributed.checkpoint.save() and torch.distributed.checkpoint.load() methods without requiring any additional conversions.
set_model_state_dict() and set_optimizer_state_dict() are provided to load the model and optimizer state_dict generated by by their respective getter APIs.
Note that set_optimizer_state_dict() can only be called before backward() or after step() is called on optimizers.
Note that this feature is experimental, and API signatures might change in the future. 

torch.distributed.checkpoint.state_dict.get_state_dict(_model_ , _optimizers_ , _*_ , _submodules =None_, _options =None_)[source][source]
    
Return the model state_dict and optimizers state_dict.
`get_state_dict` can process any module that is parallelized by PyTorch FSDP/fully_shard, DDP/replicate, tensor_parallel/parallelize_module, and any combination of these parallelisms. The main functions of `get_state_dict` are: 1.) returning a model and optimizer state_dict that can be resharded with a different number of trainers and/or different parallelisms. 2.) hiding the parallelism-specific state_dict APIs. Users don’t have to call these APIs. 3.) sanity checking the result state_dict.
The keys of the result state dictionary are the canonical FQNs (Fully Qualified Names). A canonical FQN refers to the FQN based on a parameter’s position in an nn.Module hierarchy. More specifically, a canonical FQN to a parameter is the FQN returned by `module.named_parameters()` or `module.named_buffers()` when the module is not distributed by any parallelisms. Since the optimizer internally uses parameter IDs to represent a parameter, there will be a conversion from the parameter IDs to the canonical FQNs when calling this API.
`get_state_dict` can also process a module that is not parallelized. In such a case, `get_state_dict` only performs one function – converting the optimizer parameter IDs to the canonical FQNs.
Example
```
>>> import torch
>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
>>> from torch.nn.parallel import DistributedDataParallel as DDP
>>> from torch.distributed.checkpoint.state_dict import get_state_dict

```
Copy to clipboard
```
>>> fsdp_model = FSDP(copy.deepcopy(model))
>>> fsdp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)
>>> ddp_model = DDP(copy.deepcopy(model))
>>> ddp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)

```
Copy to clipboard
```
>>> ddp_state_dict, ddp_optim_state_dict = get_state_dict(ddp_model, ddp_optim)
>>> fsdp_state_dict, fsdp_optim_state_dict = get_state_dict(
...   fsdp_model, fsdp_optim
... )

```
Copy to clipboard
```
>>> # if we simply call ddp_model.state_dict() and fsdp_model.state_dict(),
>>> # the asserts will fail.
>>> assert ddp_state_dict == fsdp_state_dict
>>> assert ddp_optim_state == fsdp_optim_state_dict

```
Copy to clipboard 

Parameters
    
  * **model** (_nn.Module_) – the nn.Module to the model.
  * **optimizers** (_Union_ _[__None_ _,__Optimizer_ _,__Iterable_ _[__Optimizer_ _]__]_) – The optimizers that are used to optimize `model`.
  * **submodules** (_deprecated_) – Optional[set[nn.Module]]: only return the model parameters that belong to the submodules.
  * **options** (_StateDictOptions_) – the options to control how model state_dict and optimizer state_dict should be returned. See StateDictOptions for the details.



Returns
    
`Tuple` that contain model state_dict and optimizer state_dict. 

Return type
    
_Tuple_[_Dict_[str, ValueType], OptimizerStateType] 

torch.distributed.checkpoint.state_dict.get_model_state_dict(_model_ , _*_ , _submodules =None_, _options =None_)[source][source]
    
Return the model state_dict of `model`.
See `get_state_dict` for the detail usage. 

Parameters
    
  * **model** (_nn.Module_) – the nn.Module to the model.
  * **submodules** (_deprecated_) – Optional[set[nn.Module]]: only return the model parameters that belong to the submodules.
  * **options** (_StateDictOptions_) – the options to control how model state_dict and optimizer state_dict should be returned. See StateDictOptions for the details.



Returns
    
The state_dict for `model`. 

Return type
    
_Dict_[str, ValueType] 

torch.distributed.checkpoint.state_dict.get_optimizer_state_dict(_model_ , _optimizers_ , _*_ , _submodules =None_, _options =None_)[source][source]
    
Return the combined state_dict for optimizers.
See `get_state_dict` for the detail usage. 

Parameters
    
  * **model** (_nn.Module_) – the nn.Module to the model.
  * **optimizers** (_Union_ _[__None_ _,__Optimizer_ _,__Iterable_ _[__Optimizer_ _]__]_) – The optimizers that are used to optimize `model`.
  * **submodules** (_deprecated_) – Optional[set[nn.Module]]: only return the model parameters that belong to the submodules.
  * **options** (_StateDictOptions_) – the options to control how model state_dict and optimizer state_dict should be returned. See StateDictOptions for the details.



Returns
    
The state_dict for `optimizers`. 

Return type
    
OptimizerStateType 

torch.distributed.checkpoint.state_dict.set_state_dict(_model_ , _optimizers_ , _*_ , _model_state_dict_ , _optim_state_dict_ , _options =None_)[source][source]
    
Load the model state_dict and optimizers state_dict.
The counterpart of `get_state_dict` to set the state_dict to the model and optimizers. The given `model_state_dict` and `optim_state_dict` do not have to be returned by `get_state_dict` but must meet the following requirements: 1) all FQNs are canonical FQNs as defined in `get_state_dict`, 2) if a tensor is sharded, it must be either a ShardedTensor or DTensor, 3) optimizer state_dict cannot contain the parameter IDs; the keys should be the canonical FQNs. 

WARN: `set_state_dict` can only be called before `backward()` or after `step()`
    
is called on the optimizers. Otherwise, the optimizer states won’t be initialized correctly. 

Parameters
    
  * **model** (_nn.Module_) – the nn.Module to the model.
  * **optimizers** (_Union_ _[__Optimizer_ _,__Iterable_ _[__Optimizer_ _]__]_) – The optimizers that are used to optimize `model`.
  * **model_state_dict** (_Dict_ _[__str_ _,__ValueType_ _]_) – (Union[Dict[nn.Module, Dict[str, ValueType]], Dict[str, ValueType]]): the model state_dict to load. If the key of the `model_state_dict` is nn.Module, the key is a submodule of `model` and the value should be the state_dict of the submodule. When loading the state_dict, the prefix of the submodule will be append to the state_dict.
  * **optim_state_dict** (_OptimizerStateType_) – OptimizerStateType: the optimizer state_dict to load.
  * **options** (_StateDictOptions_) – the options to control how model state_dict and optimizer state_dict should be loaded. See StateDictOptions for the details.



Returns
    
  * **missing_keys** is a list of str containing the missing keys of the model state_dict.
  * **unexpected_keys** is a list of str containing the unexpected keys of the model state_dict.



Return type
    
`NamedTuple` with `missing_keys` and `unexpected_keys` fields 

torch.distributed.checkpoint.state_dict.set_model_state_dict(_model_ , _model_state_dict_ , _*_ , _options =None_)[source][source]
    
Load the model state_dict.
The counterpart of `get_model_state_dict` to set the state_dict to the model. See `set_state_dict` for the detail usage. 

Parameters
    
  * **model** (_nn.Module_) – the nn.Module to the model.
  * **model_state_dict** (_Dict_ _[__str_ _,__ValueType_ _]_) – (Dict[str, ValueType]): the model state_dict to load. If the key of the `model_state_dict` is nn.Module, the key is a submodule of `model` and the value should be the state_dict of the submodule. When loading the state_dict, the prefix of the submodule will be append to the state_dict.
  * **options** (_StateDictOptions_) – the options to control how model state_dict and optimizer state_dict should be loaded. See StateDictOptions for the details.



Returns
    
  * **missing_keys** is a list of str containing the missing keys
  * **unexpected_keys** is a list of str containing the unexpected keys



Return type
    
`NamedTuple` with `missing_keys` and `unexpected_keys` fields 

torch.distributed.checkpoint.state_dict.set_optimizer_state_dict(_model_ , _optimizers_ , _optim_state_dict_ , _*_ , _options =None_)[source][source]
    
Load the optimizers state_dict.
The counterpart of `get_optimizer_state_dict` to set the state_dict to the optimizers. See `set_state_dict` for the detail usage. 

WARN: `set_optimizer_state_dict` can only be called before `backward()` or after
    
`step()` is called on the optimizers. Otherwise, the optimizer states won’t be initialized correctly. 

Parameters
    
  * **model** (_nn.Module_) – the nn.Module to the model.
  * **optimizers** (_Union_ _[__Optimizer_ _,__Iterable_ _[__Optimizer_ _]__]_) – The optimizers that are used to optimize `model`.
  * **optim_state_dict** (_OptimizerStateType_) – OptimizerStateType: the optimizer state_dict to load.
  * **options** (_StateDictOptions_) – the options to control how model state_dict and optimizer state_dict should be loaded. See StateDictOptions for the details.



Returns
    
None 

Return type
    
None 

_class_ torch.distributed.checkpoint.state_dict.StateDictOptions(_full_state_dict =False_, _cpu_offload =False_, _ignore_frozen_params =False_, _keep_submodule_prefixes =True_, _strict =True_, _broadcast_from_rank0 =False_, _flatten_optimizer_state_dict =False_, _dsd_fqn_modifiers ='_fqn_modifiers'_)[source][source]
    
This dataclass specifies how get_state_dict/set_state_dict will work.
  * `full_state_dict`: if this is set to True, all the tensors in the returned state_dict will be gathered. No ShardedTensor and DTensor will be in the returned state_dict.
  * `cpu_offload`: offload all the tensors to cpu. To prevent CPU OOM, if `full_state_dict` is also true, then only the rank0 will get the state_dict and all other ranks will get empty state_dict.
  * `ignore_frozen_params`: if the value is True, the returned state_dict won’t contain any frozen parameters – the `requires_grad` is False. The default value is False.
  * `keep_submodule_prefixes` (deprecated): when `submodules` is not None, this option indicates whether to keep the submodule prefixes from the state_dict keys. or example, if the submodule is `module.pretrain` and the full FQN of the parameter is `pretrain.layer1.weight` of the param. When this option is True, the parameter’s key in the returned state_dict will be `pretrain.layer1.weight`. If the options is False, the key will be `layer1.weight`. Note that if `keep_submodule_prefixes` is False, there may be conflicted FQNs, hence there should be only one submodule in `submodules`.
  * `strict`: the `strict` option when `set_state_dict` calls model.load_state_dict().
  * 

`broadcast_from_rank0`: when the option is True, rank0 should receive a
    
full state_dict and will broadcast the tensors in the state_dict/ optim_state_dict one by one to other ranks. Other ranks will receive the tensors and shard according to the local shards in the model and optimizer. `full_state_dict` must be set to True when using this option. This option currently only supports DTensor, not the legacy ShardedTensor.


For users which are used to using and sharing models in the torch.save format, the following methods are provided which provide offline utilities for converting betweeing formats. 

torch.distributed.checkpoint.format_utils.dcp_to_torch_save(_dcp_checkpoint_dir_ , _torch_save_path_)[source][source]
    
Given a directory containing a DCP checkpoint, this function will convert it into a Torch save file. 

Parameters
    
  * **dcp_checkpoint_dir** (_Union_ _[__str_ _,__PathLike_ _]_) – Directory containing the DCP checkpoint.
  * **torch_save_path** (_Union_ _[__str_ _,__PathLike_ _]_) – Filename to store the converted Torch save file.


Warning
To avoid OOM, it’s recommended to only run this function on a single rank. 

torch.distributed.checkpoint.format_utils.torch_save_to_dcp(_torch_save_path_ , _dcp_checkpoint_dir_)[source][source]
    
Given the location of a torch save file, converts it into a DCP checkpoint. 

Parameters
    
  * **torch_save_path** (_Union_ _[__str_ _,__PathLike_ _]_) – Filename of the Torch save file.
  * **dcp_checkpoint_dir** (_Union_ _[__str_ _,__PathLike_ _]_) – Directory to store the DCP checkpoint.


Warning
To avoid OOM, it’s recommended to only run this function on a single rank.
The following classes can also be utilized for online loading and resharding of models from the torch.save format. 

_class_ torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader(_checkpoint_id =None_, _coordinator_rank =0_)[source][source]
    
StorageReader for reading a Torch Save file. This reader will read the entire checkpoint on the coordinator rank, and then broadcast and shard each tensor to all ranks.
. N.B. Intended to be used with DynamicMetaLoadPlanner
Warning
Current implementation only supports loading Tensors.
```
>>> sd = {"mode": model}
>>> dcp.load(
>>>   sd,
>>>   storage_reader=BroadcastingTorchSaveReader(),
>>>   planner=DynamicMetaLoadPlanner(),
>>>   checkpoint_id="path_to_model.pt"
>>> )

```
Copy to clipboard 

prepare_global_plan(_global_plan_)[source][source]
    
Implementation of the StorageReader method 

Return type
    
list[torch.distributed.checkpoint.planner.LoadPlan] 

prepare_local_plan(_plan_)[source][source]
    
Implementation of the StorageReader method 

Return type
    
_LoadPlan_ 

read_data(_plan_ , _planner_)[source][source]
    
Reads torch save data on the coordinator rank, and broadcast afterwards this incurrs a communication cost, but avoids having to load the entire checkpoint on each rank, hopefully preventing OOM issues 

Return type
    
_Future_[None] 

read_metadata()[source][source]
    
Extends the default StorageReader to support building the metadata file 

Return type
    
_Metadata_ 

reset(_checkpoint_id =None_)[source][source]
    
Implementation of the StorageReader method 

set_up_storage_reader(_metadata_ , _is_coordinator_)[source][source]
    
Implementation of the StorageReader method 

_classmethod_ validate_checkpoint_id(_checkpoint_id_)[source][source]
    
Implementation of the StorageReader method 

Return type
    
bool 

_class_ torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner(_flatten_state_dict =True_, _flatten_sharded_tensors =True_, _allow_partial_load =False_)[source][source]
    
Extension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict, avoiding the need to read metadata from disk. This is useful when reading formats which don’t have a metadata file, like Torch Save files.
. N.B. Intended to be used with BroadcastingTorchSaveReader
Warning
Current implementation only supports loading Tensors.
```
>>> sd = {"mode": model}
>>> dcp.load(
>>>   sd,
>>>   storage_reader=BroadcastingTorchSaveReader(),
>>>   planner=DynamicMetaLoadPlanner(),
>>>   checkpoint_id="path_to_model.pt"
>>> )

```
Copy to clipboard 

set_up_planner(_state_dict_ , _metadata =None_, _is_coordinator =False_)[source][source]
    
Setups of the planner, extnding default behavior by creating the Metadata object from the state dict
The following experimental interfaces are provided for improved observability in production environments:
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Distributed Checkpoint - torch.distributed.checkpoint
    * Additional resources:
      * `AsyncCheckpointerType`
      * `save()`
      * `async_save()`
      * `save_state_dict()`
      * `load()`
      * `load_state_dict()`
      * `AsyncStager`
        * `AsyncStager.should_synchronize_after_execute`
        * `AsyncStager.stage()`
        * `AsyncStager.synchronize_staging()`
      * `BlockingAsyncStager`
        * `BlockingAsyncStager.stage()`
        * `BlockingAsyncStager.synchronize_staging()`
      * `Stateful`
        * `Stateful.load_state_dict()`
        * `Stateful.state_dict()`
      * `StorageReader`
        * `StorageReader.prepare_global_plan()`
        * `StorageReader.prepare_local_plan()`
        * `StorageReader.read_data()`
        * `StorageReader.read_metadata()`
        * `StorageReader.reset()`
        * `StorageReader.set_up_storage_reader()`
        * `StorageReader.validate_checkpoint_id()`
      * `StorageWriter`
        * `StorageWriter.finish()`
        * `StorageWriter.prepare_global_plan()`
        * `StorageWriter.prepare_local_plan()`
        * `StorageWriter.reset()`
        * `StorageWriter.set_up_storage_writer()`
        * `StorageWriter.storage_meta()`
        * `StorageWriter.validate_checkpoint_id()`
        * `StorageWriter.write_data()`
      * `LoadPlanner`
        * `LoadPlanner.commit_tensor()`
        * `LoadPlanner.create_global_plan()`
        * `LoadPlanner.create_local_plan()`
        * `LoadPlanner.finish_plan()`
        * `LoadPlanner.load_bytes()`
        * `LoadPlanner.resolve_bytes()`
        * `LoadPlanner.resolve_tensor()`
        * `LoadPlanner.set_up_planner()`
      * `LoadPlan`
      * `ReadItem`
      * `SavePlanner`
        * `SavePlanner.create_global_plan()`
        * `SavePlanner.create_local_plan()`
        * `SavePlanner.finish_plan()`
        * `SavePlanner.resolve_data()`
        * `SavePlanner.set_up_planner()`
      * `SavePlan`
      * `WriteItem`
        * `WriteItem.tensor_storage_size()`
      * `FileSystemReader`
        * `FileSystemReader.checkpoint_id`
      * `FileSystemWriter`
        * `FileSystemWriter.stage()`
      * `DefaultSavePlanner`
        * `DefaultSavePlanner.lookup_object()`
        * `DefaultSavePlanner.transform_object()`
      * `DefaultLoadPlanner`
        * `DefaultLoadPlanner.lookup_tensor()`
        * `DefaultLoadPlanner.transform_tensor()`
      * `get_state_dict()`
      * `get_model_state_dict()`
      * `get_optimizer_state_dict()`
      * `set_state_dict()`
      * `set_model_state_dict()`
      * `set_optimizer_state_dict()`
      * `StateDictOptions`
      * `dcp_to_torch_save()`
      * `torch_save_to_dcp()`
      * `BroadcastingTorchSaveReader`
        * `BroadcastingTorchSaveReader.prepare_global_plan()`
        * `BroadcastingTorchSaveReader.prepare_local_plan()`
        * `BroadcastingTorchSaveReader.read_data()`
        * `BroadcastingTorchSaveReader.read_metadata()`
        * `BroadcastingTorchSaveReader.reset()`
        * `BroadcastingTorchSaveReader.set_up_storage_reader()`
        * `BroadcastingTorchSaveReader.validate_checkpoint_id()`
      * `DynamicMetaLoadPlanner`
        * `DynamicMetaLoadPlanner.set_up_planner()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Torch Distributed Elastic
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Torch Distributed Elastic
Makes distributed PyTorch fault-tolerant and elastic.
## Get Started
Usage
  * Quickstart
  * Train script
  * Examples


## Documentation
API
  * torchrun (Elastic Launch)
  * Elastic Agent
  * Multiprocessing
  * Error Propagation
  * Rendezvous
  * Expiration Timers
  * Metrics
  * Events
  * Subprocess Handling
  * Control Plane


Advanced
  * Customization


Plugins
  * TorchElastic Kubernetes


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Torch Distributed Elastic
    * Get Started
    * Documentation


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Distributed communication package - torch.distributed
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Distributed communication package - torch.distributed
Note
Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.
## Backends
`torch.distributed` supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.
Backend | `gloo` | `mpi` | `nccl`  
---|---|---|---  
Device | CPU | GPU | CPU | GPU | CPU | GPU  
send | ✓ | ✘ | ✓ | ? | ✘ | ✓  
recv | ✓ | ✘ | ✓ | ? | ✘ | ✓  
broadcast | ✓ | ✓ | ✓ | ? | ✘ | ✓  
all_reduce | ✓ | ✓ | ✓ | ? | ✘ | ✓  
reduce | ✓ | ✘ | ✓ | ? | ✘ | ✓  
all_gather | ✓ | ✘ | ✓ | ? | ✘ | ✓  
gather | ✓ | ✘ | ✓ | ? | ✘ | ✓  
scatter | ✓ | ✘ | ✓ | ? | ✘ | ✓  
reduce_scatter | ✘ | ✘ | ✘ | ✘ | ✘ | ✓  
all_to_all | ✘ | ✘ | ✓ | ? | ✘ | ✓  
barrier | ✓ | ✘ | ✓ | ? | ✘ | ✓  
### Backends that come with PyTorch
PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI installed.)
Note
As of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the init_method argument of `init_process_group()` points to a file it must adhere to the following schema:
  * Local file system, `init_method="file:///d:/tmp/some_file"`
  * Shared file system, `init_method="file://////{machine_name}/{share_folder_name}/some_file"`


Same as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.
### Which backend to use?
In the past, we were often asked: “which backend should I use?”.
  * Rule of thumb
    * Use the NCCL backend for distributed **GPU** training
    * Use the Gloo backend for distributed **CPU** training.
  * GPU hosts with InfiniBand interconnect
    * Use NCCL, since it’s the only backend that currently supports InfiniBand and GPUDirect.
  * GPU hosts with Ethernet interconnect
    * Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)
  * CPU hosts with InfiniBand interconnect
    * If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.
  * CPU hosts with Ethernet interconnect
    * Use Gloo, unless you have specific reasons to use MPI.


### Common environment variables
#### Choosing the network interface to use
By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):
  * **NCCL_SOCKET_IFNAME** , for example `export NCCL_SOCKET_IFNAME=eth0`
  * **GLOO_SOCKET_IFNAME** , for example `export GLOO_SOCKET_IFNAME=eth0`


If you’re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: `export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3`. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.
#### Other NCCL environment variables
**Debugging** - in case of NCCL failure, you can set `NCCL_DEBUG=INFO` to print an explicit warning message as well as basic NCCL initialization information.
You may also use `NCCL_DEBUG_SUBSYS` to get more details about a specific aspect of NCCL. For example, `NCCL_DEBUG_SUBSYS=COLL` would print logs of collective calls, which may be helpful when debugging hangs, especially those caused by collective type or message size mismatch. In case of topology detection failure, it would be helpful to set `NCCL_DEBUG_SUBSYS=GRAPH` to inspect the detailed detection result and save as reference if further help from NCCL team is needed.
**Performance tuning** - NCCL performs automatic tuning based on its topology detection to save users’ tuning effort. On some socket-based systems, users may still try tuning `NCCL_SOCKET_NTHREADS` and `NCCL_NSOCKS_PERTHREAD` to increase socket network bandwidth. These two environment variables have been pre-tuned by NCCL for some cloud providers, such as AWS or GCP.
For a full list of NCCL environment variables, please refer to NVIDIA NCCL’s official documentation
## Basics
The torch.distributed package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class `torch.nn.parallel.DistributedDataParallel()` builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and `torch.nn.DataParallel()` in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.
In the single-machine synchronous case, torch.distributed or the `torch.nn.parallel.DistributedDataParallel()` wrapper may still have advantages over other approaches to data-parallelism, including `torch.nn.DataParallel()`:
  * Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.
  * Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.


## Initialization
The package needs to be initialized using the `torch.distributed.init_process_group()` or `torch.distributed.device_mesh.init_device_mesh()` function before calling any other methods. Both block until all processes have joined.
Warning
Initialization is not thread-safe. Process group creation should be performed from a single thread, to prevent inconsistent ‘UUID’ assignment across ranks, and to prevent races during initialization that can lead to hangs. 

torch.distributed.is_available()[source][source]
    
Return `True` if the distributed package is available.
Otherwise, `torch.distributed` does not expose any other APIs. Currently, `torch.distributed` is available on Linux, MacOS and Windows. Set `USE_DISTRIBUTED=1` to enable it when building PyTorch from source. Currently, the default value is `USE_DISTRIBUTED=1` for Linux and Windows, `USE_DISTRIBUTED=0` for MacOS. 

Return type
    
bool 

torch.distributed.init_process_group(_backend =None_, _init_method =None_, _timeout =None_, _world_size =-1_, _rank =-1_, _store =None_, _group_name =''_, _pg_options =None_, _device_id =None_)[source][source]
    
Initialize the default distributed process group.
This will also initialize the distributed package. 

There are 2 main ways to initialize a process group:
    
  1. Specify `store`, `rank`, and `world_size` explicitly.
  2. Specify `init_method` (a URL string) which indicates where/how to discover peers. Optionally specify `rank` and `world_size`, or encode all required parameters in the URL and omit them.


If neither is specified, `init_method` is assumed to be “env://”. 

Parameters
    
  * **backend** (_str_ _or_ _Backend_ _,__optional_) – The backend to use. Depending on build-time configurations, valid values include `mpi`, `gloo`, `nccl`, `ucc`, or one that is registered by a third-party plugin. Since 2.6, if `backend` is not provided, c10d will use a backend registered for the device type indicated by the device_id kwarg (if provided). The known default registrations today are: `nccl` for `cuda`, `gloo` for `cpu`. If neither `backend` nor `device_id` is provided, c10d will detect the accelerator on the run-time machine and use a backend registered for that detected accelerator (or `cpu`). This field can be given as a lowercase string (e.g., `"gloo"`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`). If using multiple processes per machine with `nccl` backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlock or NCCL invalid usage. `ucc` backend is experimental.
  * **init_method** (_str_ _,__optional_) – URL specifying how to initialize the process group. Default is “env://” if no `init_method` or `store` is specified. Mutually exclusive with `store`.
  * **world_size** (_int_ _,__optional_) – Number of processes participating in the job. Required if `store` is specified.
  * **rank** (_int_ _,__optional_) – Rank of the current process (it should be a number between 0 and `world_size`-1). Required if `store` is specified.
  * **store** (_Store_ _,__optional_) – Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with `init_method`.
  * **timeout** (_timedelta_ _,__optional_) – Timeout for operations executed against the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends. This is the duration after which collectives will be aborted asynchronously and the process will crash. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.
  * **group_name** (_str_ _,__optional_ _,__deprecated_) – Group name. This argument is ignored
  * **pg_options** (_ProcessGroupOptions_ _,__optional_) – process group options specifying what additional options need to be passed in during the construction of specific process groups. As of now, the only options we support is `ProcessGroupNCCL.Options` for the `nccl` backend, `is_high_priority_stream` can be specified so that the nccl backend can pick up high priority cuda streams when there’re compute kernels waiting. For other availble options to config nccl, See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t
  * **device_id** (_torch.device_ _,__optional_) – a single, specific device to “bind” this process to, allowing for backend-specific optimizations. Currently this has two effects, only under NCCL: the communicator is immediately formed (calling `ncclCommInit*` immediately rather than the normal lazy call) and sub-groups will use `ncclCommSplit` when possible to avoid unnecessary overhead of group creation. If you want to know NCCL initialization error early, you can also use this field.


Note
To enable `backend == Backend.MPI`, PyTorch needs to be built from source on a system that supports MPI.
Note
Support for multiple backends is experimental. Currently when no backend is specified, both `gloo` and `nccl` backends will be created. The `gloo` backend will be used for collectives with CPU tensors and the `nccl` backend will be used for collectives with CUDA tensors. A custom backend can be specified by passing in a string with format “<device_type>:<backend_name>,<device_type>:<backend_name>”, e.g. “cpu:gloo,cuda:custom_backend”. 

torch.distributed.device_mesh.init_device_mesh(_device_type_ , _mesh_shape_ , _*_ , _mesh_dim_names =None_)[source][source]
    
Initializes a DeviceMesh based on device_type, mesh_shape, and mesh_dim_names parameters.
This creates a DeviceMesh with an n-dimensional array layout, where n is the length of mesh_shape. If mesh_dim_names is provided, each dimension is labeled as mesh_dim_names[i].
Note
init_device_mesh follows SPMD programming model, meaning the same PyTorch Python program runs on all processes/ranks in the cluster. Ensure mesh_shape (the dimensions of the nD array describing device layout) is identical across all ranks. Inconsistent mesh_shape may lead to hanging.
Note
If no process group is found, init_device_mesh will initialize distributed process group/groups required for distributed communications behind the scene. 

Parameters
    
  * **device_type** (_str_) – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”. Passing in a device type with a GPU index, such as “cuda:0”, is not allowed.
  * **mesh_shape** (_Tuple_ _[__int_ _]_) – A tuple defining the dimensions of the multi-dimensional array describing the layout of devices.
  * **mesh_dim_names** (_Tuple_ _[__str_ _]__,__optional_) – A tuple of mesh dimension names to assign to each dimension of the multi-dimensional array describing the layout of devices. Its length must match the length of mesh_shape. Each string in mesh_dim_names must be unique.



Returns
    
A `DeviceMesh` object representing the device layout. 

Return type
    
DeviceMesh 

Example::
    
```
>>> from torch.distributed.device_mesh import init_device_mesh
>>>
>>> mesh_1d = init_device_mesh("cuda", mesh_shape=(8,))
>>> mesh_2d = init_device_mesh("cuda", mesh_shape=(2, 8), mesh_dim_names=("dp", "tp"))

```
Copy to clipboard 

torch.distributed.is_initialized()[source][source]
    
Check if the default process group has been initialized. 

Return type
    
bool 

torch.distributed.is_mpi_available()[source][source]
    
Check if the MPI backend is available. 

Return type
    
bool 

torch.distributed.is_nccl_available()[source][source]
    
Check if the NCCL backend is available. 

Return type
    
bool 

torch.distributed.is_gloo_available()[source][source]
    
Check if the Gloo backend is available. 

Return type
    
bool 

torch.distributed.distributed_c10d.is_xccl_available()[source][source]
    
Check if the XCCL backend is available. 

Return type
    
bool 

torch.distributed.is_torchelastic_launched()[source][source]
    
Check whether this process was launched with `torch.distributed.elastic` (aka torchelastic).
The existence of `TORCHELASTIC_RUN_ID` environment variable is used as a proxy to determine whether the current process was launched with torchelastic. This is a reasonable proxy since `TORCHELASTIC_RUN_ID` maps to the rendezvous id which is always a non-null value indicating the job id for peer discovery purposes.. 

Return type
    
bool
Currently three initialization methods are supported:
### TCP initialization
There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired `world_size`. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.
Note that multicast address is not supported anymore in the latest distributed package. `group_name` is deprecated as well.
```
import torch.distributed as dist
# Use address of one of the machines
dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',
            rank=args.rank, world_size=4)

```
Copy to clipboard
### Shared file-system initialization
Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired `world_size`. The URL should start with `file://` and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn’t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next `init_process_group()` call on the same file path/name.
Note that automatic rank assignment is not supported anymore in the latest distributed package and `group_name` is deprecated as well.
Warning
This method assumes that the file system supports locking using `fcntl` - most local systems and NFS support it.
Warning
This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call `init_process_group()` multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call `init_process_group()` again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time `init_process_group()` is called.
```
import torch.distributed as dist
# rank should always be specified
dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',
            world_size=4, rank=args.rank)

```
Copy to clipboard
### Environment variable initialization
This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:
  * `MASTER_PORT` - required; has to be a free port on machine with rank 0
  * `MASTER_ADDR` - required (except for rank 0); address of rank 0 node
  * `WORLD_SIZE` - required; can be set either here, or in a call to init function
  * `RANK` - required; can be set either here, or in a call to init function


The machine with rank 0 will be used to set up all connections.
This is the default method, meaning that `init_method` does not have to be specified (or can be `env://`).
## Post-Initialization
Once `torch.distributed.init_process_group()` was run, the following functions can be used. To check whether the process group has already been initialized use `torch.distributed.is_initialized()`. 

_class_ torch.distributed.Backend(_name_)[source][source]
    
An enum-like class for backends.
Available backends: GLOO, NCCL, UCC, MPI, XCCL, and other registered backends.
The values of this class are lowercase strings, e.g., `"gloo"`. They can be accessed as attributes, e.g., `Backend.NCCL`.
This class can be directly called to parse the string, e.g., `Backend(backend_str)` will check if `backend_str` is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., `Backend("GLOO")` returns `"gloo"`.
Note
The entry `Backend.UNDEFINED` is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence. 

_classmethod_ register_backend(_name_ , _func_ , _extended_api =False_, _devices =None_)[source][source]
    
Register a new backend with the given name and instantiating function.
This class method is used by 3rd party `ProcessGroup` extension to register new backends. 

Parameters
    
  * **name** (_str_) – Backend name of the `ProcessGroup` extension. It should match the one in `init_process_group()`.
  * **func** (_function_) – Function handler that instantiates the backend. The function should be implemented in the backend extension and takes four arguments, including `store`, `rank`, `world_size`, and `timeout`.
  * **extended_api** (_bool_ _,__optional_) – Whether the backend supports extended argument structure. Default: `False`. If set to `True`, the backend will get an instance of `c10d::DistributedBackendOptions`, and a process group options object as defined by the backend implementation.
  * **device** (_str_ _or_ _list_ _of_ _str_ _,__optional_) – device type this backend supports, e.g. “cpu”, “cuda”, etc. If None, assuming both “cpu” and “cuda”


Note
This support of 3rd party backend is experimental and subject to change. 

torch.distributed.get_backend(_group =None_)[source][source]
    
Return the backend of the given process group. 

Parameters
    
**group** (_ProcessGroup_ _,__optional_) – The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of `group`. 

Returns
    
The backend of the given process group as a lower case string. 

Return type
    
_Backend_ 

torch.distributed.get_rank(_group =None_)[source][source]
    
Return the rank of the current process in the provided `group`, default otherwise.
Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to `world_size`. 

Parameters
    
**group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used. 

Returns
    
The rank of the process group -1, if not part of the group 

Return type
    
int 

torch.distributed.get_world_size(_group =None_)[source][source]
    
Return the number of processes in the current process group. 

Parameters
    
**group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used. 

Returns
    
The world size of the process group -1, if not part of the group 

Return type
    
int
## Shutdown
It is important to clean up resources on exit by calling `destroy_process_group()`.
The simplest pattern to follow is to destroy every process group and backend by calling `destroy_process_group()` with the default value of None for the group argument, at a point in the training script where communications are no longer needed, usually near the end of main(). The call should be made once per trainer-process, not at the outer process-launcher level.
if `destroy_process_group()` is not called by all ranks in a pg within the timeout duration, especially when there are multiple process-groups in the application e.g. for N-D parallelism, hangs on exit are possible. This is because the destructor for ProcessGroupNCCL calls ncclCommAbort, which must be called collectively, but the order of calling ProcessGroupNCCL’s destructor if called by python’s GC is not deterministic. Calling `destroy_process_group()` helps by ensuring ncclCommAbort is called in a consistent order across ranks, and avoids calling ncclCommAbort during ProcessGroupNCCL’s destructor.
### Reinitialization
destroy_process_group can also be used to destroy individual process groups. One use case could be fault tolerant training, where a process group may be destroyed and then a new one initialized during runtime. In this case, it’s critical to synchronize the trainer processes using some means other than torch.distributed primitives _after_ calling destroy and before subsequently initializing. This behavior is currently unsupported/untested, due to the difficulty of achieving this synchronization, and is considered a known issue. Please file a github issue or RFC if this is a use case that’s blocking you.
## Groups
By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. `new_group()` function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a `group` argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns). 

torch.distributed.new_group(_ranks =None_, _timeout =None_, _backend =None_, _pg_options =None_, _use_local_synchronization =False_, _group_desc =None_, _device_id =None_)[source][source]
    
Create a new distributed group.
This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.
Warning
Safe concurrent usage: When using multiple process groups with the `NCCL` backend, the user must ensure a globally consistent execution order of collectives across ranks.
If multiple threads within a process issue collectives, explicit synchronization is necessary to ensure consistent ordering.
When using async variants of torch.distributed communication APIs, a work object is returned and the communication kernel is enqueued on a separate CUDA stream, allowing overlap of communication and computation. Once one or more async ops have been issued on one process group, they must be synchronized with other cuda streams by calling work.wait() before using another process group.
See Using multiple NCCL communicators concurrently for more details. 

Parameters
    
  * **ranks** (_list_ _[__int_ _]_) – List of ranks of group members. If `None`, will be set to all ranks. Default is `None`.
  * **timeout** (_timedelta_ _,__optional_) – see init_process_group for details and default value.
  * **backend** (_str_ _or_ _Backend_ _,__optional_) – The backend to use. Depending on build-time configurations, valid values are `gloo` and `nccl`. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., `"gloo"`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`). If `None` is passed in, the backend corresponding to the default process group will be used. Default is `None`.
  * **pg_options** (_ProcessGroupOptions_ _,__optional_) – process group options specifying what additional options need to be passed in during the construction of specific process groups. i.e. for the `nccl` backend, `is_high_priority_stream` can be specified so that process group can pick up high priority cuda streams. For other availble options to config nccl, See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t
  * **use_local_synchronization** (_bool_ _,__optional_) – perform a group-local barrier at the end of the process group creation. This is different in that non-member ranks don’t need to call into API and don’t join the barrier.
  * **group_desc** (_str_ _,__optional_) – a string to describe the process group.
  * **device_id** (_torch.device_ _,__optional_) – a single, specific device to “bind” this process to, The new_group call will try to initialize a communication backend immediately for the device if this field is given.



Returns
    
A handle of distributed group that can be given to collective calls or GroupMember.NON_GROUP_MEMBER if the rank is not part of `ranks`.
N.B. use_local_synchronization doesn’t work with MPI.
N.B. While use_local_synchronization=True can be significantly faster with larger clusters and small process groups, care must be taken since it changes cluster behavior as non-member ranks don’t join the group barrier().
N.B. use_local_synchronization=True can lead to deadlocks when each rank creates multiple overlaping process groups. To avoid that, make sure all ranks follow the same global creation order. 

torch.distributed.get_group_rank(_group_ , _global_rank_)[source][source]
    
Translate a global rank into a group rank.
`global_rank` must be part of `group` otherwise this raises RuntimeError. 

Parameters
    
  * **group** (_ProcessGroup_) – ProcessGroup to find the relative rank.
  * **global_rank** (_int_) – Global rank to query.



Returns
    
Group rank of `global_rank` relative to `group` 

Return type
    
int
N.B. calling this function on the default process group returns identity 

torch.distributed.get_global_rank(_group_ , _group_rank_)[source][source]
    
Translate a group rank into a global rank.
`group_rank` must be part of group otherwise this raises RuntimeError. 

Parameters
    
  * **group** (_ProcessGroup_) – ProcessGroup to find the global rank from.
  * **group_rank** (_int_) – Group rank to query.



Returns
    
Global rank of `group_rank` relative to `group` 

Return type
    
int
N.B. calling this function on the default process group returns identity 

torch.distributed.get_process_group_ranks(_group_)[source][source]
    
Get all ranks associated with `group`. 

Parameters
    
**group** (_ProcessGroup_) – ProcessGroup to get all ranks from. 

Returns
    
List of global ranks ordered by group rank. 

Return type
    
list[int]
## DeviceMesh
DeviceMesh is a higher level abstraction that manages process groups (or NCCL communicators). It allows user to easily create inter node and intra node process groups without worrying about how to set up the ranks correctly for different sub process groups, and it helps manage those distributed process group easily. `init_device_mesh()` function can be used to create new DeviceMesh, with a mesh shape describing the device topology. 

_class_ torch.distributed.device_mesh.DeviceMesh(_device_type_ , _mesh_ , _*_ , _mesh_dim_names =None_, __init_backend =True_)[source][source]
    
DeviceMesh represents a mesh of devices, where layout of devices could be represented as a n-d dimension array, and each value of the n-d dimensional array is the global id of the default process group ranks.
DeviceMesh could be used to describe the layout of devices across the cluster, and serves as a proxy for communication among the device lists within the cluster.
DeviceMesh can be used as a context manager.
Note
DeviceMesh follows SPMD programming model, which means the same PyTorch Python program is running on all processes/ranks in the cluster. Therefore, users need to make sure the mesh array (which describes the layout of devices) should be identical across all ranks. Inconsistent mesh will lead to silent hang. 

Parameters
    
  * **device_type** (_str_) – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
  * **mesh** (_ndarray_) – A multi-dimensional array or an integer tensor describing the layout of devices, where the IDs are global IDs of the default process group.



Returns
    
A `DeviceMesh` object representing the device layout. 

Return type
    
DeviceMesh
The following program runs on each process/rank in an SPMD manner. In this example, we have 2 hosts with 4 GPUs each. A reduction over the first dimension of mesh will reduce across columns (0, 4), .. and (3, 7), a reduction over the second dimension of mesh reduces across rows (0, 1, 2, 3) and (4, 5, 6, 7). 

Example::
    
```
>>> from torch.distributed.device_mesh import DeviceMesh
>>>
>>> # Initialize device mesh as (2, 4) to represent the topology
>>> # of cross-host(dim 0), and within-host (dim 1).
>>> mesh = DeviceMesh(device_type="cuda", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]])

```
Copy to clipboard 

_static_ from_group(_group_ , _device_type_ , _mesh =None_, _*_ , _mesh_dim_names =None_)[source][source]
    
Constructs a `DeviceMesh` with `device_type` from an existing `ProcessGroup` or a list of existing `ProcessGroup`.
The constructed device mesh has number of dimensions equal to the number of groups passed. For example, if a single process group is passed in, the resulted DeviceMesh is a 1D mesh. If a list of 2 process groups is passed in, the resulted DeviceMesh is a 2D mesh.
If more than one group is passed, then the `mesh` and `mesh_dim_names` arguments are required. The order of the process groups passed in determines the topology of the mesh. For example, the first process group will be the 0th dimension of the DeviceMesh. The mesh tensor passed in must have the same number of dimensions as the number of process groups passed in, and the order of the dimensions in the mesh tensor must match the order in the process groups passed in. 

Parameters
    
  * **group** (_ProcessGroup_ _or_ _list_ _[__ProcessGroup_ _]_) – the existing ProcessGroup or a list of existing ProcessGroups.
  * **device_type** (_str_) – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”. Passing in a device type with a GPU index, such as “cuda:0”, is not allowed.
  * **mesh** (_torch.Tensor_ _or_ _ArrayLike_ _,__optional_) – A multi-dimensional array or an integer tensor describing the layout of devices, where the IDs are global IDs of the default process group. Default is None.
  * **mesh_dim_names** (_tuple_ _[__str_ _]__,__optional_) – A tuple of mesh dimension names to assign to each dimension of the multi-dimensional array describing the layout of devices. Its length must match the length of mesh_shape. Each string in mesh_dim_names must be unique. Default is None.



Returns
    
A `DeviceMesh` object representing the device layout. 

Return type
    
DeviceMesh 

get_all_groups()[source][source]
    
Returns a list of ProcessGroups for all mesh dimensions. 

Returns
    
A list of `ProcessGroup` object. 

Return type
    
list[torch.distributed.distributed_c10d.ProcessGroup] 

get_coordinate()[source][source]
    
Return the relative indices of this rank relative to all dimensions of the mesh. If this rank is not part of the mesh, return None. 

Return type
    
_Optional_[list[int]] 

get_group(_mesh_dim =None_)[source][source]
    
Returns the single ProcessGroup specified by mesh_dim, or, if mesh_dim is not specified and the DeviceMesh is 1-dimensional, returns the only ProcessGroup in the mesh. 

Parameters
    
  * **mesh_dim** (_str/python:int_ _,__optional_) – it can be the name of the mesh dimension or the index
  * **None.** (_of the mesh dimension. Default is_) – 



Returns
    
A `ProcessGroup` object. 

Return type
    
_ProcessGroup_ 

get_local_rank(_mesh_dim =None_)[source][source]
    
Returns the local rank of the given mesh_dim of the DeviceMesh. 

Parameters
    
  * **mesh_dim** (_str/python:int_ _,__optional_) – it can be the name of the mesh dimension or the index
  * **None.** (_of the mesh dimension. Default is_) – 



Returns
    
An integer denotes the local rank. 

Return type
    
int
The following program runs on each process/rank in an SPMD manner. In this example, we have 2 hosts with 4 GPUs each. Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0. Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3. 

Example::
    
```
>>> from torch.distributed.device_mesh import DeviceMesh
>>>
>>> # Initialize device mesh as (2, 4) to represent the topology
>>> # of cross-host(dim 0), and within-host (dim 1).
>>> mesh = DeviceMesh(device_type="cuda", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]])

```
Copy to clipboard 

get_rank()[source][source]
    
Returns the current global rank. 

Return type
    
int
## Point-to-point communication 

torch.distributed.send(_tensor_ , _dst =None_, _group =None_, _tag =0_, _group_dst =None_)[source][source]
    
Send a tensor synchronously.
Warning
`tag` is not supported with the NCCL backend. 

Parameters
    
  * **tensor** (_Tensor_) – Tensor to send.
  * **dst** (_int_) – Destination rank on global process group (regardless of `group` argument). Destination rank should not be the same as the rank of the current process.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **tag** (_int_ _,__optional_) – Tag to match send with remote recv
  * **group_dst** (_int_ _,__optional_) – Destination rank on `group`. Invalid to specify both `dst` and `group_dst`.



torch.distributed.recv(_tensor_ , _src =None_, _group =None_, _tag =0_, _group_src =None_)[source][source]
    
Receives a tensor synchronously.
Warning
`tag` is not supported with the NCCL backend. 

Parameters
    
  * **tensor** (_Tensor_) – Tensor to fill with received data.
  * **src** (_int_ _,__optional_) – Source rank on global process group (regardless of `group` argument). Will receive from any process if unspecified.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **tag** (_int_ _,__optional_) – Tag to match recv with remote send
  * **group_src** (_int_ _,__optional_) – Destination rank on `group`. Invalid to specify both `src` and `group_src`.



Returns
    
Sender rank -1, if not part of the group 

Return type
    
int
`isend()` and `irecv()` return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:
  * `is_completed()` - returns True if the operation has finished
  * `wait()` - will block the process until the operation is finished. `is_completed()` is guaranteed to return True once it returns.



torch.distributed.isend(_tensor_ , _dst =None_, _group =None_, _tag =0_, _group_dst =None_)[source][source]
    
Send a tensor asynchronously.
Warning
Modifying `tensor` before the request completes causes undefined behavior.
Warning
`tag` is not supported with the NCCL backend.
Unlike send, which is blocking, isend allows src == dst rank, i.e. send to self. 

Parameters
    
  * **tensor** (_Tensor_) – Tensor to send.
  * **dst** (_int_) – Destination rank on global process group (regardless of `group` argument)
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **tag** (_int_ _,__optional_) – Tag to match send with remote recv
  * **group_dst** (_int_ _,__optional_) – Destination rank on `group`. Invalid to specify both `dst` and `group_dst`



Returns
    
A distributed request object. None, if not part of the group 

Return type
    
_Optional_[_Work_] 

torch.distributed.irecv(_tensor_ , _src =None_, _group =None_, _tag =0_, _group_src =None_)[source][source]
    
Receives a tensor asynchronously.
Warning
`tag` is not supported with the NCCL backend.
Unlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self. 

Parameters
    
  * **tensor** (_Tensor_) – Tensor to fill with received data.
  * **src** (_int_ _,__optional_) – Source rank on global process group (regardless of `group` argument). Will receive from any process if unspecified.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **tag** (_int_ _,__optional_) – Tag to match recv with remote send
  * **group_src** (_int_ _,__optional_) – Destination rank on `group`. Invalid to specify both `src` and `group_src`.



Returns
    
A distributed request object. None, if not part of the group 

Return type
    
_Optional_[_Work_] 

torch.distributed.send_object_list(_object_list_ , _dst =None_, _group =None_, _device =None_, _group_dst =None_)[source][source]
    
Sends picklable objects in `object_list` synchronously.
Similar to `send()`, but Python objects can be passed in. Note that all objects in `object_list` must be picklable in order to be sent. 

Parameters
    
  * **object_list** (_List_ _[__Any_ _]_) – List of input objects to sent. Each object must be picklable. Receiver must provide lists of equal sizes.
  * **dst** (_int_) – Destination rank to send `object_list` to. Destination rank is based on global process group (regardless of `group` argument)
  * **group** (_Optional_ _[__ProcessGroup_ _]_) – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is `None`.
  * **device** (`torch.device`, optional) – If not None, the objects are serialized and converted to tensors which are moved to the `device` before sending. Default is `None`.
  * **group_dst** (_int_ _,__optional_) – Destination rank on `group`. Must specify one of `dst` and `group_dst` but not both



Returns
    
`None`.
Note
For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by `torch.cuda.current_device()` and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via `torch.cuda.set_device()`.
Warning
`send_object_list()` uses `pickle` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.
Warning
Calling `send_object_list()` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using `send()` instead. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> # Assumes backend is not NCCL
>>> device = torch.device("cpu")
>>> if dist.get_rank() == 0:
>>>   # Assumes world_size of 2.
>>>   objects = ["foo", 12, {1: 2}] # any picklable object
>>>   dist.send_object_list(objects, dst=1, device=device)
>>> else:
>>>   objects = [None, None, None]
>>>   dist.recv_object_list(objects, src=0, device=device)
>>> objects
['foo', 12, {1: 2}]

```
Copy to clipboard 

torch.distributed.recv_object_list(_object_list_ , _src =None_, _group =None_, _device =None_, _group_src =None_)[source][source]
    
Receives picklable objects in `object_list` synchronously.
Similar to `recv()`, but can receive Python objects. 

Parameters
    
  * **object_list** (_List_ _[__Any_ _]_) – List of objects to receive into. Must provide a list of sizes equal to the size of the list being sent.
  * **src** (_int_ _,__optional_) – Source rank from which to recv `object_list`. Source rank is based on global process group (regardless of `group` argument) Will receive from any rank if set to None. Default is `None`.
  * **group** (_Optional_ _[__ProcessGroup_ _]_) – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is `None`.
  * **device** (`torch.device`, optional) – If not None, receives on this device. Default is `None`.
  * **group_src** (_int_ _,__optional_) – Destination rank on `group`. Invalid to specify both `src` and `group_src`.



Returns
    
Sender rank. -1 if rank is not part of the group. If rank is part of the group, `object_list` will contain the sent objects from `src` rank.
Note
For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by `torch.cuda.current_device()` and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via `torch.cuda.set_device()`.
Warning
`recv_object_list()` uses `pickle` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.
Warning
Calling `recv_object_list()` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using `recv()` instead. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> # Assumes backend is not NCCL
>>> device = torch.device("cpu")
>>> if dist.get_rank() == 0:
>>>   # Assumes world_size of 2.
>>>   objects = ["foo", 12, {1: 2}] # any picklable object
>>>   dist.send_object_list(objects, dst=1, device=device)
>>> else:
>>>   objects = [None, None, None]
>>>   dist.recv_object_list(objects, src=0, device=device)
>>> objects
['foo', 12, {1: 2}]

```
Copy to clipboard 

torch.distributed.batch_isend_irecv(_p2p_op_list_)[source][source]
    
Send or Receive a batch of tensors asynchronously and return a list of requests.
Process each of the operations in `p2p_op_list` and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported. 

Parameters
    
**p2p_op_list** (_list_ _[__torch.distributed.distributed_c10d.P2POp_ _]_) – A list of point-to-point operations(type of each operator is `torch.distributed.P2POp`). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end. 

Returns
    
A list of distributed request objects returned by calling the corresponding op in the op_list. 

Return type
    
list[torch.distributed.distributed_c10d.Work]
Examples
```
>>> send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rank
>>> recv_tensor = torch.randn(2, dtype=torch.float32)
>>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1) % world_size)
>>> recv_op = dist.P2POp(
...   dist.irecv, recv_tensor, (rank - 1 + world_size) % world_size
... )
>>> reqs = batch_isend_irecv([send_op, recv_op])
>>> for req in reqs:
>>>   req.wait()
>>> recv_tensor
tensor([2, 3])   # Rank 0
tensor([0, 1])   # Rank 1

```
Copy to clipboard
Note
Note that when this API is used with the NCCL PG backend, users must set the current GPU device with torch.cuda.set_device, otherwise it will lead to unexpected hang issues.
In addition, if this API is the first collective call in the `group` passed to `dist.P2POp`, all ranks of the `group` must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the `group`, batched P2P operations involving only a subset of ranks of the `group` are allowed. 

_class_ torch.distributed.P2POp(_op_ , _tensor_ , _peer =None_, _group =None_, _tag =0_, _group_peer =None_)[source][source]
    
A class to build point-to-point operations for `batch_isend_irecv`.
This class builds the type of P2P operation, communication buffer, peer rank, Process Group, and tag. Instances of this class will be passed to `batch_isend_irecv` for point-to-point communications. 

Parameters
    
  * **op** (_Callable_) – A function to send data to or receive data from a peer process. The type of `op` is either `torch.distributed.isend` or `torch.distributed.irecv`.
  * **tensor** (_Tensor_) – Tensor to send or receive.
  * **peer** (_int_ _,__optional_) – Destination or source rank.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **tag** (_int_ _,__optional_) – Tag to match send with recv.
  * **group_peer** (_int_ _,__optional_) – Destination or source rank.


## Synchronous and asynchronous collective operations
Every collective operation function supports the following two kinds of operations, depending on the setting of the `async_op` flag passed into the collective:
**Synchronous operation** - the default mode, when `async_op` is set to `False`. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see CUDA Semantics. See the below script to see examples of differences in these semantics for CPU and CUDA operations.
**Asynchronous operation** - when `async_op` is set to True. The collective operation function returns a distributed request object. In general, you don’t need to create it manually and it is guaranteed to support two methods:
  * `is_completed()` - in the case of CPU collectives, returns `True` if completed. In the case of CUDA operations, returns `True` if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.
  * `wait()` - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU).
  * `get_future()` - returns `torch._C.Future` object. Supported for NCCL, also supported for most operations on GLOO and MPI, except for peer to peer operations. Note: as we continue adopting Futures and merging APIs, `get_future()` call might become redundant.


**Example**
The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams:
```
# Code runs on each rank.
dist.init_process_group("nccl", rank=rank, world_size=2)
output = torch.tensor([rank]).cuda(rank)
s = torch.cuda.Stream()
handle = dist.all_reduce(output, async_op=True)
# Wait ensures the operation is enqueued, but not necessarily complete.
handle.wait()
# Using result on non-default stream.
with torch.cuda.stream(s):
  s.wait_stream(torch.cuda.default_stream())
  output.add_(100)
if rank == 0:
  # if the explicit call to wait_stream was omitted, the output below will be
  # non-deterministically 1 or 101, depending on whether the allreduce overwrote
  # the value after the add completed.
  print(output)

```
Copy to clipboard
## Collective functions 

torch.distributed.broadcast(_tensor_ , _src =None_, _group =None_, _async_op =False_, _group_src =None_)[source][source]
    
Broadcasts the tensor to the whole group.
`tensor` must have the same number of elements in all processes participating in the collective. 

Parameters
    
  * **tensor** (_Tensor_) – Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.
  * **src** (_int_) – Source rank on global process group (regardless of `group` argument).
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op
  * **group_src** (_int_) – Source rank on `group`. Must specify one of `group_src` and `src` but not both.



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group 

torch.distributed.broadcast_object_list(_object_list_ , _src =None_, _group =None_, _device =None_, _group_src =None_)[source][source]
    
Broadcasts picklable objects in `object_list` to the whole group.
Similar to `broadcast()`, but Python objects can be passed in. Note that all objects in `object_list` must be picklable in order to be broadcasted. 

Parameters
    
  * **object_list** (_List_ _[__Any_ _]_) – List of input objects to broadcast. Each object must be picklable. Only objects on the `src` rank will be broadcast, but each rank must provide lists of equal sizes.
  * **src** (_int_) – Source rank from which to broadcast `object_list`. Source rank is based on global process group (regardless of `group` argument)
  * **group** (_Optional_ _[__ProcessGroup_ _]_) – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is `None`.
  * **device** (`torch.device`, optional) – If not None, the objects are serialized and converted to tensors which are moved to the `device` before broadcasting. Default is `None`.
  * **group_src** (_int_) – Source rank on `group`. Must not specify one of `group_src` and `src` but not both.



Returns
    
`None`. If rank is part of the group, `object_list` will contain the broadcasted objects from `src` rank.
Note
For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by `torch.cuda.current_device()` and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via `torch.cuda.set_device()`.
Note
Note that this API differs slightly from the `broadcast()` collective since it does not provide an `async_op` handle and thus will be a blocking call.
Warning
`broadcast_object_list()` uses `pickle` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.
Warning
Calling `broadcast_object_list()` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using `broadcast()` instead. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> if dist.get_rank() == 0:
>>>   # Assumes world_size of 3.
>>>   objects = ["foo", 12, {1: 2}] # any picklable object
>>> else:
>>>   objects = [None, None, None]
>>> # Assumes backend is not NCCL
>>> device = torch.device("cpu")
>>> dist.broadcast_object_list(objects, src=0, device=device)
>>> objects
['foo', 12, {1: 2}]

```
Copy to clipboard 

torch.distributed.all_reduce(_tensor_ , _op= <RedOpType.SUM: 0>_, _group=None_ , _async_op=False_)[source][source]
    
Reduces the tensor data across all machines in a way that all get the final result.
After the call `tensor` is going to be bitwise identical in all processes.
Complex tensors are supported. 

Parameters
    
  * **tensor** (_Tensor_) – Input and output of the collective. The function operates in-place.
  * **op** (_optional_) – One of the values from `torch.distributed.ReduceOp` enum. Specifies an operation used for element-wise reductions.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group
Examples
```
>>> # All tensors below are of torch.int64 type.
>>> # We have 2 process groups, 2 ranks.
>>> device = torch.device(f"cuda:{rank}")
>>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank
>>> tensor
tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
>>> dist.all_reduce(tensor, op=ReduceOp.SUM)
>>> tensor
tensor([4, 6], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1

```
Copy to clipboard
```
>>> # All tensors below are of torch.cfloat type.
>>> # We have 2 process groups, 2 ranks.
>>> tensor = torch.tensor(
...   [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device
... ) + 2 * rank * (1 + 1j)
>>> tensor
tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0
tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1
>>> dist.all_reduce(tensor, op=ReduceOp.SUM)
>>> tensor
tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0
tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1

```
Copy to clipboard 

torch.distributed.reduce(_tensor_ , _dst=None_ , _op= <RedOpType.SUM: 0>_, _group=None_ , _async_op=False_ , _group_dst=None_)[source][source]
    
Reduces the tensor data across all machines.
Only the process with rank `dst` is going to receive the final result. 

Parameters
    
  * **tensor** (_Tensor_) – Input and output of the collective. The function operates in-place.
  * **dst** (_int_) – Destination rank on global process group (regardless of `group` argument)
  * **op** (_optional_) – One of the values from `torch.distributed.ReduceOp` enum. Specifies an operation used for element-wise reductions.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op
  * **group_dst** (_int_) – Destination rank on `group`. Must specify one of `group_dst` and `dst` but not both.



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group 

torch.distributed.all_gather(_tensor_list_ , _tensor_ , _group =None_, _async_op =False_)[source][source]
    
Gathers tensors from the whole group in a list.
Complex and uneven sized tensors are supported. 

Parameters
    
  * **tensor_list** (_list_ _[__Tensor_ _]_) – Output list. It should contain correctly-sized tensors to be used for output of the collective. Uneven sized tensors are supported.
  * **tensor** (_Tensor_) – Tensor to be broadcast from current process.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group
Examples
```
>>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
>>> device = torch.device(f"cuda:{rank}")
>>> tensor_list = [
...   torch.zeros(2, dtype=torch.int64, device=device) for _ in range(2)
... ]
>>> tensor_list
[tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0
[tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1
>>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank
>>> tensor
tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
>>> dist.all_gather(tensor_list, tensor)
>>> tensor_list
[tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0
[tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1

```
Copy to clipboard
```
>>> # All tensors below are of torch.cfloat dtype.
>>> # We have 2 process groups, 2 ranks.
>>> tensor_list = [
...   torch.zeros(2, dtype=torch.cfloat, device=device) for _ in range(2)
... ]
>>> tensor_list
[tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0
[tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1
>>> tensor = torch.tensor(
...   [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device
... ) + 2 * rank * (1 + 1j)
>>> tensor
tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0
tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1
>>> dist.all_gather(tensor_list, tensor)
>>> tensor_list
[tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0
[tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1

```
Copy to clipboard 

torch.distributed.all_gather_into_tensor(_output_tensor_ , _input_tensor_ , _group =None_, _async_op =False_)[source][source]
    
Gather tensors from all ranks and put them in a single output tensor.
This function requires all tensors to be the same size on each process. 

Parameters
    
  * **output_tensor** (_Tensor_) – Output tensor to accommodate tensor elements from all ranks. It must be correctly sized to have one of the following forms: (i) a concatenation of all the input tensors along the primary dimension; for definition of “concatenation”, see `torch.cat()`; (ii) a stack of all the input tensors along the primary dimension; for definition of “stack”, see `torch.stack()`. Examples below may better explain the supported output forms.
  * **input_tensor** (_Tensor_) – Tensor to be gathered from current rank. Different from the `all_gather` API, the input tensors in this API must have the same size across all ranks.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group
Examples
```
>>> # All tensors below are of torch.int64 dtype and on CUDA devices.
>>> # We have two ranks.
>>> device = torch.device(f"cuda:{rank}")
>>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank
>>> tensor_in
tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
>>> # Output in concatenation form
>>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)
>>> dist.all_gather_into_tensor(tensor_out, tensor_in)
>>> tensor_out
tensor([1, 2, 3, 4], device='cuda:0') # Rank 0
tensor([1, 2, 3, 4], device='cuda:1') # Rank 1
>>> # Output in stack form
>>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)
>>> dist.all_gather_into_tensor(tensor_out2, tensor_in)
>>> tensor_out2
tensor([[1, 2],
    [3, 4]], device='cuda:0') # Rank 0
tensor([[1, 2],
    [3, 4]], device='cuda:1') # Rank 1

```
Copy to clipboard
Warning
The Gloo backend does not support this API. 

torch.distributed.all_gather_object(_object_list_ , _obj_ , _group =None_)[source][source]
    
Gathers picklable objects from the whole group into a list.
Similar to `all_gather()`, but Python objects can be passed in. Note that the object must be picklable in order to be gathered. 

Parameters
    
  * **object_list** (_list_ _[__Any_ _]_) – Output list. It should be correctly sized as the size of the group for this collective and will contain the output.
  * **obj** (_Any_) – Pickable Python object to be broadcast from current process.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used. Default is `None`.



Returns
    
None. If the calling rank is part of this group, the output of the collective will be populated into the input `object_list`. If the calling rank is not part of the group, the passed in `object_list` will be unmodified.
Note
Note that this API differs slightly from the `all_gather()` collective since it does not provide an `async_op` handle and thus will be a blocking call.
Note
For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by `torch.cuda.current_device()` and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via `torch.cuda.set_device()`.
Warning
`all_gather_object()` uses `pickle` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.
Warning
Calling `all_gather_object()` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using `all_gather()` instead. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> # Assumes world_size of 3.
>>> gather_objects = ["foo", 12, {1: 2}] # any picklable object
>>> output = [None for _ in gather_objects]
>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])
>>> output
['foo', 12, {1: 2}]

```
Copy to clipboard 

torch.distributed.gather(_tensor_ , _gather_list =None_, _dst =None_, _group =None_, _async_op =False_, _group_dst =None_)[source][source]
    
Gathers a list of tensors in a single process.
This function requires all tensors to be the same size on each process. 

Parameters
    
  * **tensor** (_Tensor_) – Input tensor.
  * **gather_list** (_list_ _[__Tensor_ _]__,__optional_) – List of appropriately, same-sized tensors to use for gathered data (default is None, must be specified on the destination rank)
  * **dst** (_int_ _,__optional_) – Destination rank on global process group (regardless of `group` argument). (If both `dst` and `group_dst` are None, default is global rank 0)
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op
  * **group_dst** (_int_ _,__optional_) – Destination rank on `group`. Invalid to specify both `dst` and `group_dst`



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group
Note
Note that all Tensors in gather_list must have the same size. 

Example::
    
```
>>> # We have 2 process groups, 2 ranks.
>>> tensor_size = 2
>>> device = torch.device(f'cuda:{rank}')
>>> tensor = torch.ones(tensor_size, device=device) + rank
>>> if dist.get_rank() == 0:
>>>   gather_list = [torch.zeros_like(tensor, device=device) for i in range(2)]
>>> else:
>>>   gather_list = None
>>> dist.gather(tensor, gather_list, dst=0)
>>> # Rank 0 gets gathered data.
>>> gather_list
[tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0
None                                  # Rank 1

```
Copy to clipboard 

torch.distributed.gather_object(_obj_ , _object_gather_list =None_, _dst =None_, _group =None_, _group_dst =None_)[source][source]
    
Gathers picklable objects from the whole group in a single process.
Similar to `gather()`, but Python objects can be passed in. Note that the object must be picklable in order to be gathered. 

Parameters
    
  * **obj** (_Any_) – Input object. Must be picklable.
  * **object_gather_list** (_list_ _[__Any_ _]_) – Output list. On the `dst` rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be `None` on non-dst ranks. (default is `None`)
  * **dst** (_int_ _,__optional_) – Destination rank on global process group (regardless of `group` argument). (If both `dst` and `group_dst` are None, default is global rank 0)
  * **group** (_Optional_ _[__ProcessGroup_ _]_) – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is `None`.
  * **group_dst** (_int_ _,__optional_) – Destination rank on `group`. Invalid to specify both `dst` and `group_dst`



Returns
    
None. On the `dst` rank, `object_gather_list` will contain the output of the collective.
Note
Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.
Note
For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by `torch.cuda.current_device()` and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via `torch.cuda.set_device()`.
Warning
`gather_object()` uses `pickle` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.
Warning
Calling `gather_object()` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using `gather()` instead. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> # Assumes world_size of 3.
>>> gather_objects = ["foo", 12, {1: 2}] # any picklable object
>>> output = [None for _ in gather_objects]
>>> dist.gather_object(
...   gather_objects[dist.get_rank()],
...   output if dist.get_rank() == 0 else None,
...   dst=0
... )
>>> # On rank 0
>>> output
['foo', 12, {1: 2}]

```
Copy to clipboard 

torch.distributed.scatter(_tensor_ , _scatter_list =None_, _src =None_, _group =None_, _async_op =False_, _group_src =None_)[source][source]
    
Scatters a list of tensors to all processes in a group.
Each process will receive exactly one tensor and store its data in the `tensor` argument.
Complex tensors are supported. 

Parameters
    
  * **tensor** (_Tensor_) – Output tensor.
  * **scatter_list** (_list_ _[__Tensor_ _]_) – List of tensors to scatter (default is None, must be specified on the source rank)
  * **src** (_int_) – Source rank on global process group (regardless of `group` argument). (If both `src` and `group_src` are None, default is global rank 0)
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op
  * **group_src** (_int_ _,__optional_) – Source rank on `group`. Invalid to specify both `src` and `group_src`



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group
Note
Note that all Tensors in scatter_list must have the same size. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> tensor_size = 2
>>> device = torch.device(f'cuda:{rank}')
>>> output_tensor = torch.zeros(tensor_size, device=device)
>>> if dist.get_rank() == 0:
>>>   # Assumes world_size of 2.
>>>   # Only tensors, all of which must be the same size.
>>>   t_ones = torch.ones(tensor_size, device=device)
>>>   t_fives = torch.ones(tensor_size, device=device) * 5
>>>   scatter_list = [t_ones, t_fives]
>>> else:
>>>   scatter_list = None
>>> dist.scatter(output_tensor, scatter_list, src=0)
>>> # Rank i gets scatter_list[i].
>>> output_tensor
tensor([1., 1.], device='cuda:0') # Rank 0
tensor([5., 5.], device='cuda:1') # Rank 1

```
Copy to clipboard 

torch.distributed.scatter_object_list(_scatter_object_output_list_ , _scatter_object_input_list =None_, _src =None_, _group =None_, _group_src =None_)[source][source]
    
Scatters picklable objects in `scatter_object_input_list` to the whole group.
Similar to `scatter()`, but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of `scatter_object_output_list`. Note that all objects in `scatter_object_input_list` must be picklable in order to be scattered. 

Parameters
    
  * **scatter_object_output_list** (_List_ _[__Any_ _]_) – Non-empty list whose first element will store the object scattered to this rank.
  * **scatter_object_input_list** (_List_ _[__Any_ _]__,__optional_) – List of input objects to scatter. Each object must be picklable. Only objects on the `src` rank will be scattered, and the argument can be `None` for non-src ranks.
  * **src** (_int_) – Source rank from which to scatter `scatter_object_input_list`. Source rank is based on global process group (regardless of `group` argument). (If both `src` and `group_src` are None, default is global rank 0)
  * **group** (_Optional_ _[__ProcessGroup_ _]_) – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is `None`.
  * **group_src** (_int_ _,__optional_) – Source rank on `group`. Invalid to specify both `src` and `group_src`



Returns
    
`None`. If rank is part of the group, `scatter_object_output_list` will have its first element set to the scattered object for this rank.
Note
Note that this API differs slightly from the scatter collective since it does not provide an `async_op` handle and thus will be a blocking call.
Warning
`scatter_object_list()` uses `pickle` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.
Warning
Calling `scatter_object_list()` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using `scatter()` instead. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> if dist.get_rank() == 0:
>>>   # Assumes world_size of 3.
>>>   objects = ["foo", 12, {1: 2}] # any picklable object
>>> else:
>>>   # Can be any list on non-src ranks, elements are not used.
>>>   objects = [None, None, None]
>>> output_list = [None]
>>> dist.scatter_object_list(output_list, objects, src=0)
>>> # Rank i gets objects[i]. For example, on rank 2:
>>> output_list
[{1: 2}]

```
Copy to clipboard 

torch.distributed.reduce_scatter(_output_ , _input_list_ , _op= <RedOpType.SUM: 0>_, _group=None_ , _async_op=False_)[source][source]
    
Reduces, then scatters a list of tensors to all processes in a group. 

Parameters
    
  * **output** (_Tensor_) – Output tensor.
  * **input_list** (_list_ _[__Tensor_ _]_) – List of tensors to reduce and scatter.
  * **op** (_optional_) – One of the values from `torch.distributed.ReduceOp` enum. Specifies an operation used for element-wise reductions.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op.



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group. 

torch.distributed.reduce_scatter_tensor(_output_ , _input_ , _op= <RedOpType.SUM: 0>_, _group=None_ , _async_op=False_)[source][source]
    
Reduces, then scatters a tensor to all ranks in a group. 

Parameters
    
  * **output** (_Tensor_) – Output tensor. It should have the same size across all ranks.
  * **input** (_Tensor_) – Input tensor to be reduced and scattered. Its size should be output tensor size times the world size. The input tensor can have one of the following shapes: (i) a concatenation of the output tensors along the primary dimension, or (ii) a stack of the output tensors along the primary dimension. For definition of “concatenation”, see `torch.cat()`. For definition of “stack”, see `torch.stack()`.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op.



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.
Examples
```
>>> # All tensors below are of torch.int64 dtype and on CUDA devices.
>>> # We have two ranks.
>>> device = torch.device(f"cuda:{rank}")
>>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device)
>>> # Input in concatenation form
>>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)
>>> tensor_in
tensor([0, 1, 2, 3], device='cuda:0') # Rank 0
tensor([0, 1, 2, 3], device='cuda:1') # Rank 1
>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)
>>> tensor_out
tensor([0, 2], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1
>>> # Input in stack form
>>> tensor_in = torch.reshape(tensor_in, (world_size, 2))
>>> tensor_in
tensor([[0, 1],
    [2, 3]], device='cuda:0') # Rank 0
tensor([[0, 1],
    [2, 3]], device='cuda:1') # Rank 1
>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)
>>> tensor_out
tensor([0, 2], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1

```
Copy to clipboard
Warning
The Gloo backend does not support this API. 

torch.distributed.all_to_all_single(_output_ , _input_ , _output_split_sizes =None_, _input_split_sizes =None_, _group =None_, _async_op =False_)[source][source]
    
Split input tensor and then scatter the split list to all processes in a group.
Later the received tensors are concatenated from all the processes in the group and returned as a single output tensor.
Complex tensors are supported. 

Parameters
    
  * **output** (_Tensor_) – Gathered concatenated output tensor.
  * **input** (_Tensor_) – Input tensor to scatter.
  * **output_split_sizes** – (list[Int], optional): Output split sizes for dim 0 if specified None or empty, dim 0 of `output` tensor must divide equally by `world_size`.
  * **input_split_sizes** – (list[Int], optional): Input split sizes for dim 0 if specified None or empty, dim 0 of `input` tensor must divide equally by `world_size`.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op.



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.
Warning
all_to_all_single is experimental and subject to change.
Examples
```
>>> input = torch.arange(4) + rank * 4
>>> input
tensor([0, 1, 2, 3])   # Rank 0
tensor([4, 5, 6, 7])   # Rank 1
tensor([8, 9, 10, 11])  # Rank 2
tensor([12, 13, 14, 15]) # Rank 3
>>> output = torch.empty([4], dtype=torch.int64)
>>> dist.all_to_all_single(output, input)
>>> output
tensor([0, 4, 8, 12])  # Rank 0
tensor([1, 5, 9, 13])  # Rank 1
tensor([2, 6, 10, 14])  # Rank 2
tensor([3, 7, 11, 15])  # Rank 3

```
Copy to clipboard
```
>>> # Essentially, it is similar to following operation:
>>> scatter_list = list(input.chunk(world_size))
>>> gather_list = list(output.chunk(world_size))
>>> for i in range(world_size):
>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)

```
Copy to clipboard
```
>>> # Another example with uneven split
>>> input
tensor([0, 1, 2, 3, 4, 5])                    # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])           # Rank 1
tensor([20, 21, 22, 23, 24])                   # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])               # Rank 3
>>> input_splits
[2, 2, 1, 1]                           # Rank 0
[3, 2, 2, 2]                           # Rank 1
[2, 1, 1, 1]                           # Rank 2
[2, 2, 2, 1]                           # Rank 3
>>> output_splits
[2, 3, 2, 2]                           # Rank 0
[2, 2, 1, 2]                           # Rank 1
[1, 2, 1, 2]                           # Rank 2
[1, 2, 1, 1]                           # Rank 3
>>> output = ...
>>> dist.all_to_all_single(output, input, output_splits, input_splits)
>>> output
tensor([ 0, 1, 10, 11, 12, 20, 21, 30, 31])           # Rank 0
tensor([ 2, 3, 13, 14, 22, 32, 33])               # Rank 1
tensor([ 4, 15, 16, 23, 34, 35])                 # Rank 2
tensor([ 5, 17, 18, 24, 36])                   # Rank 3

```
Copy to clipboard
```
>>> # Another example with tensors of torch.cfloat type.
>>> input = torch.tensor(
...   [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat
... ) + 4 * rank * (1 + 1j)
>>> input
tensor([1+1j, 2+2j, 3+3j, 4+4j])                # Rank 0
tensor([5+5j, 6+6j, 7+7j, 8+8j])                # Rank 1
tensor([9+9j, 10+10j, 11+11j, 12+12j])             # Rank 2
tensor([13+13j, 14+14j, 15+15j, 16+16j])            # Rank 3
>>> output = torch.empty([4], dtype=torch.int64)
>>> dist.all_to_all_single(output, input)
>>> output
tensor([1+1j, 5+5j, 9+9j, 13+13j])               # Rank 0
tensor([2+2j, 6+6j, 10+10j, 14+14j])              # Rank 1
tensor([3+3j, 7+7j, 11+11j, 15+15j])              # Rank 2
tensor([4+4j, 8+8j, 12+12j, 16+16j])              # Rank 3

```
Copy to clipboard 

torch.distributed.all_to_all(_output_tensor_list_ , _input_tensor_list_ , _group =None_, _async_op =False_)[source][source]
    
Scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.
Complex tensors are supported. 

Parameters
    
  * **output_tensor_list** (_list_ _[__Tensor_ _]_) – List of tensors to be gathered one per rank.
  * **input_tensor_list** (_list_ _[__Tensor_ _]_) – List of tensors to scatter one per rank.
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op.



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.
Warning
all_to_all is experimental and subject to change.
Examples
```
>>> input = torch.arange(4) + rank * 4
>>> input = list(input.chunk(4))
>>> input
[tensor([0]), tensor([1]), tensor([2]), tensor([3])]   # Rank 0
[tensor([4]), tensor([5]), tensor([6]), tensor([7])]   # Rank 1
[tensor([8]), tensor([9]), tensor([10]), tensor([11])]  # Rank 2
[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3
>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))
>>> dist.all_to_all(output, input)
>>> output
[tensor([0]), tensor([4]), tensor([8]), tensor([12])]  # Rank 0
[tensor([1]), tensor([5]), tensor([9]), tensor([13])]  # Rank 1
[tensor([2]), tensor([6]), tensor([10]), tensor([14])]  # Rank 2
[tensor([3]), tensor([7]), tensor([11]), tensor([15])]  # Rank 3

```
Copy to clipboard
```
>>> # Essentially, it is similar to following operation:
>>> scatter_list = input
>>> gather_list = output
>>> for i in range(world_size):
>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)

```
Copy to clipboard
```
>>> input
tensor([0, 1, 2, 3, 4, 5])                    # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])           # Rank 1
tensor([20, 21, 22, 23, 24])                   # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])               # Rank 3
>>> input_splits
[2, 2, 1, 1]                           # Rank 0
[3, 2, 2, 2]                           # Rank 1
[2, 1, 1, 1]                           # Rank 2
[2, 2, 2, 1]                           # Rank 3
>>> output_splits
[2, 3, 2, 2]                           # Rank 0
[2, 2, 1, 2]                           # Rank 1
[1, 2, 1, 2]                           # Rank 2
[1, 2, 1, 1]                           # Rank 3
>>> input = list(input.split(input_splits))
>>> input
[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]          # Rank 0
[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1
[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]         # Rank 2
[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]     # Rank 3
>>> output = ...
>>> dist.all_to_all(output, input)
>>> output
[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]  # Rank 0
[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]      # Rank 1
[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]       # Rank 2
[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]         # Rank 3

```
Copy to clipboard
```
>>> # Another example with tensors of torch.cfloat type.
>>> input = torch.tensor(
...   [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat
... ) + 4 * rank * (1 + 1j)
>>> input = list(input.chunk(4))
>>> input
[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]      # Rank 0
[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]      # Rank 1
[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]   # Rank 2
[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]  # Rank 3
>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))
>>> dist.all_to_all(output, input)
>>> output
[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]     # Rank 0
[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]    # Rank 1
[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]    # Rank 2
[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]    # Rank 3

```
Copy to clipboard 

torch.distributed.barrier(_group =None_, _async_op =False_, _device_ids =None_)[source][source]
    
Synchronize all processes.
This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait(). 

Parameters
    
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If None, the default process group will be used.
  * **async_op** (_bool_ _,__optional_) – Whether this op should be an async op
  * **device_ids** (_[__int_ _]__,__optional_) – List of device/GPU ids.



Returns
    
Async work handle, if async_op is set to True. None, if not async_op or if not part of the group
Note
ProcessGroupNCCL now blocks the cpu thread till the completion of the barrier collective. 

torch.distributed.monitored_barrier(_group =None_, _timeout =None_, _wait_all_ranks =False_)[source][source]
    
Synchronize processes similar to `torch.distributed.barrier`, but consider a configurable timeout.
It is able to report ranks that did not pass this barrier within the provided timeout. Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0. Rank 0 will block until all send /recv from other ranks are processed, and will report failures for ranks that failed to respond in time. Note that if one rank does not reach the monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.
This collective will block all processes/ranks in the group, until the whole group exits the function successfully, making it useful for debugging and synchronizing. However, it can have a performance impact and should only be used for debugging or scenarios that require full synchronization points on the host-side. For debugging purposes, this barrier can be inserted before the application’s collective calls to check if any ranks are desynchronized.
Note
Note that this collective is only supported with the GLOO backend. 

Parameters
    
  * **group** (_ProcessGroup_ _,__optional_) – The process group to work on. If `None`, the default process group will be used.
  * **timeout** (_datetime.timedelta_ _,__optional_) – Timeout for monitored_barrier. If `None`, the default process group timeout will be used.
  * **wait_all_ranks** (_bool_ _,__optional_) – Whether to collect all failed ranks or not. By default, this is `False` and `monitored_barrier` on rank 0 will throw on the first failed rank it encounters in order to fail fast. By setting `wait_all_ranks=True` `monitored_barrier` will collect all failed ranks and throw an error containing information about all failed ranks.



Returns
    
`None`. 

Example::
    
```
>>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> if dist.get_rank() != 1:
>>>   dist.monitored_barrier() # Raises exception indicating that
>>> # rank 1 did not call into monitored_barrier.
>>> # Example with wait_all_ranks=True
>>> if dist.get_rank() == 0:
>>>   dist.monitored_barrier(wait_all_ranks=True) # Raises exception
>>> # indicating that ranks 1, 2, ... world_size - 1 did not call into
>>> # monitored_barrier.

```
Copy to clipboard 

_class_ torch.distributed.Work
    
A Work object represents the handle to a pending asynchronous operation in PyTorch’s distributed package. It is returned by non-blocking collective operations, such as dist.all_reduce(tensor, async_op=True). 

boxed(_self :torch._C._distributed_c10d.Work_) → object


exception(_self :torch._C._distributed_c10d.Work_) → std::__exception_ptr::exception_ptr


get_future(_self :torch._C._distributed_c10d.Work_) → torch.Future
     

Returns
    
A `torch.futures.Future` object which is associated with the completion of the `Work`. As an example, a future object can be retrieved by `fut = process_group.allreduce(tensors).get_future()`. 

Example::
    
Below is an example of a simple allreduce DDP communication hook that uses `get_future` API to retrieve a Future associated with the completion of ``allreduce`.
```
>>> def allreduce(process_group: dist.ProcessGroup, bucket: dist.GradBucket): -> torch.futures.Future
>>>   group_to_use = process_group if process_group is not None else torch.distributed.group.WORLD
>>>   tensor = bucket.buffer().div_(group_to_use.size())
>>>   return torch.distributed.all_reduce(tensor, group=group_to_use, async_op=True).get_future()
>>> ddp_model.register_comm_hook(state=None, hook=allreduce)

```
Copy to clipboard
Warning
`get_future` API supports NCCL, and partially GLOO and MPI backends (no support for peer-to-peer operations like send/recv) and will return a `torch.futures.Future`.
In the example above, `allreduce` work will be done on GPU using NCCL backend, `fut.wait()` will return after synchronizing the appropriate NCCL streams with PyTorch’s current device streams to ensure we can have asynchronous CUDA execution and it does not wait for the entire operation to complete on GPU. Note that `CUDAFuture` does not support `TORCH_NCCL_BLOCKING_WAIT` flag or NCCL’s `barrier()`. In addition, if a callback function was added by `fut.then()`, it will wait until `WorkNCCL`’s NCCL streams synchronize with `ProcessGroupNCCL`’s dedicated callback stream and invoke the callback inline after running the callback on the callback stream. `fut.then()` will return another `CUDAFuture` that holds the return value of the callback and a `CUDAEvent` that recorded the callback stream.
>   1. For CPU work, `fut.done()` returns true when work has been completed and value() tensors are ready.
>   2. For GPU work, `fut.done()` returns true only whether the operation has been enqueued.
>   3. For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), `fut.done()` returns true when tensors have arrived on respective nodes, but not yet necessarily synched on respective GPUs (similarly to GPU work).
> 


get_future_result(_self :torch._C._distributed_c10d.Work_) → torch.Future
     

Returns
    
A `torch.futures.Future` object of int type which maps to the enum type of WorkResult As an example, a future object can be retrieved by `fut = process_group.allreduce(tensor).get_future_result()`. 

Example::
    
users can use `fut.wait()` to blocking wait for the completion of the work and get the WorkResult by `fut.value()`. Also, users can use `fut.then(call_back_func)` to register a callback function to be called when the work is completed, without blocking the current thread.
Warning
`get_future_result` API supports NCCL 

is_completed(_self :torch._C._distributed_c10d.Work_) → bool


is_success(_self :torch._C._distributed_c10d.Work_) → bool


result(_self :torch._C._distributed_c10d.Work_) → list[torch.Tensor]


source_rank(_self :torch._C._distributed_c10d.Work_) → int


synchronize(_self :torch._C._distributed_c10d.Work_) → None


_static_ unbox(_arg0 :object_) → torch._C._distributed_c10d.Work


wait(_self :torch._C._distributed_c10d.Work_, _timeout :datetime.timedelta=datetime.timedelta(0)_) → bool
     

Returns
    
true/false. 

Example::
     

try:
    
work.wait(timeout) 

except:
    
# some handling
Warning
In normal cases, users do not need to set the timeout. calling wait() is the same as calling synchronize(): Letting the current stream block on the completion of the NCCL work. However, if timeout is set, it will block the CPU thread until the NCCL work is completed or timed out. If timeout, exception will be thrown. 

_class_ torch.distributed.ReduceOp
    
An enum-like class for available reduction operations: `SUM`, `PRODUCT`, `MIN`, `MAX`, `BAND`, `BOR`, `BXOR`, and `PREMUL_SUM`.
`BAND`, `BOR`, and `BXOR` reductions are not available when using the `NCCL` backend.
`AVG` divides values by the world size before summing across ranks. `AVG` is only available with the `NCCL` backend, and only for NCCL versions 2.10 or later.
`PREMUL_SUM` multiplies inputs by a given scalar locally before reduction. `PREMUL_SUM` is only available with the `NCCL` backend, and only available for NCCL versions 2.11 or later. Users are supposed to use `torch.distributed._make_nccl_premul_sum`.
Additionally, `MAX`, `MIN` and `PRODUCT` are not supported for complex tensors.
The values of this class can be accessed as attributes, e.g., `ReduceOp.SUM`. They are used in specifying strategies for reduction collectives, e.g., `reduce()`.
This class does not support `__members__` property. 

_class_ torch.distributed.reduce_op
    
Deprecated enum-like class for reduction operations: `SUM`, `PRODUCT`, `MIN`, and `MAX`.
`ReduceOp` is recommended to use instead.
## Distributed Key-Value Store
The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed package in `torch.distributed.init_process_group()` (by explicitly creating the store as an alternative to specifying `init_method`.) There are 3 choices for Key-Value Stores: `TCPStore`, `FileStore`, and `HashStore`. 

_class_ torch.distributed.Store
    
Base class for all store implementations, such as the 3 provided by PyTorch distributed: (`TCPStore`, `FileStore`, and `HashStore`). 

__init__(_self :torch._C._distributed_c10d.Store_) → None


add(_self :torch._C._distributed_c10d.Store_, _arg0 :str_, _arg1 :int_) → int
    
The first call to add for a given `key` creates a counter associated with `key` in the store, initialized to `amount`. Subsequent calls to add with the same `key` increment the counter by the specified `amount`. Calling `add()` with a key that has already been set in the store by `set()` will result in an exception. 

Parameters
    
  * **key** (_str_) – The key in the store whose counter will be incremented.
  * **amount** (_int_) – The quantity by which the counter will be incremented.



Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.add("first_key", 1)
>>> store.add("first_key", 6)
>>> # Should return 7
>>> store.get("first_key")

```
Copy to clipboard 

append(_self :torch._C._distributed_c10d.Store_, _arg0 :str_, _arg1 :str_) → None
    
Append the key-value pair into the store based on the supplied `key` and `value`. If `key` does not exists in the store, it will be created. 

Parameters
    
  * **key** (_str_) – The key to be appended to the store.
  * **value** (_str_) – The value associated with `key` to be added to the store.



Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.append("first_key", "po")
>>> store.append("first_key", "tato")
>>> # Should return "potato"
>>> store.get("first_key")

```
Copy to clipboard 

check(_self :torch._C._distributed_c10d.Store_, _arg0 :list[str]_) → bool
    
The call to check whether a given list of `keys` have value stored in the store. This call immediately returns in normal cases but still suffers from some edge deadlock cases, e.g, calling check after TCPStore has been destroyed. Calling `check()` with a list of keys that one wants to check whether stored in the store or not. 

Parameters
    
**keys** (_lisr_ _[__str_ _]_) – The keys to query whether stored in the store. 

Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.add("first_key", 1)
>>> # Should return 7
>>> store.check(["first_key"])

```
Copy to clipboard 

compare_set(_self :torch._C._distributed_c10d.Store_, _arg0 :str_, _arg1 :str_, _arg2 :str_) → bytes
    
Inserts the key-value pair into the store based on the supplied `key` and performs comparison between `expected_value` and `desired_value` before inserting. `desired_value` will only be set if `expected_value` for the `key` already exists in the store or if `expected_value` is an empty string. 

Parameters
    
  * **key** (_str_) – The key to be checked in the store.
  * **expected_value** (_str_) – The value associated with `key` to be checked before insertion.
  * **desired_value** (_str_) – The value associated with `key` to be added to the store.



Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.set("key", "first_value")
>>> store.compare_set("key", "first_value", "second_value")
>>> # Should return "second_value"
>>> store.get("key")

```
Copy to clipboard 

delete_key(_self :torch._C._distributed_c10d.Store_, _arg0 :str_) → bool
    
Deletes the key-value pair associated with `key` from the store. Returns true if the key was successfully deleted, and false if it was not.
Warning
The `delete_key` API is only supported by the `TCPStore` and `HashStore`. Using this API with the `FileStore` will result in an exception. 

Parameters
    
**key** (_str_) – The key to be deleted from the store 

Returns
    
True if `key` was deleted, otherwise False. 

Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, HashStore can also be used
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.set("first_key")
>>> # This should return true
>>> store.delete_key("first_key")
>>> # This should return false
>>> store.delete_key("bad_key")

```
Copy to clipboard 

get(_self :torch._C._distributed_c10d.Store_, _arg0 :str_) → bytes
    
Retrieves the value associated with the given `key` in the store. If `key` is not present in the store, the function will wait for `timeout`, which is defined when initializing the store, before throwing an exception. 

Parameters
    
**key** (_str_) – The function will return the value associated with this key. 

Returns
    
Value associated with `key` if `key` is in the store. 

Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.set("first_key", "first_value")
>>> # Should return "first_value"
>>> store.get("first_key")

```
Copy to clipboard 

has_extended_api(_self :torch._C._distributed_c10d.Store_) → bool
    
Returns true if the store supports extended operations. 

multi_get(_self :torch._C._distributed_c10d.Store_, _arg0 :list[str]_) → list[bytes]
    
Retrieve all values in `keys`. If any key in `keys` is not present in the store, the function will wait for `timeout` 

Parameters
    
**keys** (_List_ _[__str_ _]_) – The keys to be retrieved from the store. 

Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.set("first_key", "po")
>>> store.set("second_key", "tato")
>>> # Should return [b"po", b"tato"]
>>> store.multi_get(["first_key", "second_key"])

```
Copy to clipboard 

multi_set(_self :torch._C._distributed_c10d.Store_, _arg0 :list[str]_, _arg1 :list[str]_) → None
    
Inserts a list key-value pair into the store based on the supplied `keys` and `values` 

Parameters
    
  * **keys** (_List_ _[__str_ _]_) – The keys to insert.
  * **values** (_List_ _[__str_ _]_) – The values to insert.



Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.multi_set(["first_key", "second_key"], ["po", "tato"])
>>> # Should return b"po"
>>> store.get("first_key")

```
Copy to clipboard 

num_keys(_self :torch._C._distributed_c10d.Store_) → int
    
Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by `set()` and `add()` since one key is used to coordinate all the workers using the store.
Warning
When used with the `TCPStore`, `num_keys` returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained. 

Returns
    
The number of keys present in the store. 

Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.set("first_key", "first_value")
>>> # This should return 2
>>> store.num_keys()

```
Copy to clipboard 

set(_self :torch._C._distributed_c10d.Store_, _arg0 :str_, _arg1 :str_) → None
    
Inserts the key-value pair into the store based on the supplied `key` and `value`. If `key` already exists in the store, it will overwrite the old value with the new supplied `value`. 

Parameters
    
  * **key** (_str_) – The key to be added to the store.
  * **value** (_str_) – The value associated with `key` to be added to the store.



Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.set("first_key", "first_value")
>>> # Should return "first_value"
>>> store.get("first_key")

```
Copy to clipboard 

set_timeout(_self :torch._C._distributed_c10d.Store_, _arg0 :datetime.timedelta_) → None
    
Sets the store’s default timeout. This timeout is used during initialization and in `wait()` and `get()`. 

Parameters
    
**timeout** (_timedelta_) – timeout to be set in the store. 

Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> store.set_timeout(timedelta(seconds=10))
>>> # This will throw an exception after 10 seconds
>>> store.wait(["bad_key"])

```
Copy to clipboard 

_property_ timeout
    
Gets the timeout of the store. 

wait(_* args_, _** kwargs_)
    
Overloaded function.
  1. wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None


Waits for each key in `keys` to be added to the store. If not all keys are set before the `timeout` (set during store initialization), then `wait` will throw an exception. 

Parameters
    
**keys** (_list_) – List of keys on which to wait until they are set in the store. 

Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> # This will throw an exception after 30 seconds
>>> store.wait(["bad_key"])

```
Copy to clipboard
  1. wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None


Waits for each key in `keys` to be added to the store, and throws an exception if the keys have not been set by the supplied `timeout`. 

Parameters
    
  * **keys** (_list_) – List of keys on which to wait until they are set in the store.
  * **timeout** (_timedelta_) – Time to wait for the keys to be added before throwing an exception.



Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore("127.0.0.1", 0, 1, True, timedelta(seconds=30))
>>> # This will throw an exception after 10 seconds
>>> store.wait(["bad_key"], timedelta(seconds=10))

```
Copy to clipboard 

_class_ torch.distributed.TCPStore
    
A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as `set()` to insert a key-value pair, `get()` to retrieve a key-value pair, etc. There should always be one server store initialized because the client store(s) will wait for the server to establish a connection. 

Parameters
    
  * **host_name** (_str_) – The hostname or IP Address the server store should run on.
  * **port** (_int_) – The port on which the server store should listen for incoming requests.
  * **world_size** (_int_ _,__optional_) – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).
  * **is_master** (_bool_ _,__optional_) – True when initializing the server store and False for client stores. Default is False.
  * **timeout** (_timedelta_ _,__optional_) – Timeout used by the store during initialization and for methods such as `get()` and `wait()`. Default is timedelta(seconds=300)
  * **wait_for_workers** (_bool_ _,__optional_) – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.
  * **multi_tenant** (_bool_ _,__optional_) – If True, all `TCPStore` instances in the current process with the same host/port will use the same underlying `TCPServer`. Default is False.
  * **master_listen_fd** (_int_ _,__optional_) – If specified, the underlying `TCPServer` will listen on this file descriptor, which must be a socket already bound to `port`. Useful to avoid port assignment races in some scenarios. Default is None (meaning the server creates a new socket and attempts to bind it to `port`).
  * **use_libuv** (_bool_ _,__optional_) – If True, use libuv for `TCPServer` backend. Default is True.



Example::
    
```
>>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Run on process 1 (server)
>>> server_store = dist.TCPStore("127.0.0.1", 1234, 2, True, timedelta(seconds=30))
>>> # Run on process 2 (client)
>>> client_store = dist.TCPStore("127.0.0.1", 1234, 2, False)
>>> # Use any of the store methods from either the client or server after initialization
>>> server_store.set("first_key", "first_value")
>>> client_store.get("first_key")

```
Copy to clipboard 

__init__(_self :torch._C._distributed_c10d.TCPStore_, _host_name :str_, _port :int_, _world_size :Optional[int]=None_, _is_master :bool=False_, _timeout :datetime.timedelta=datetime.timedelta(seconds=300)_, _wait_for_workers :bool=True_, _multi_tenant :bool=False_, _master_listen_fd :Optional[int]=None_, _use_libuv :bool=True_) → None
    
Creates a new TCPStore. 

_property_ host
    
Gets the hostname on which the store listens for requests. 

_property_ libuvBackend
    
Returns True if it’s using the libuv backend. 

_property_ port
    
Gets the port number on which the store listens for requests. 

_class_ torch.distributed.HashStore
    
A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes. 

Example::
    
```
>>> import torch.distributed as dist
>>> store = dist.HashStore()
>>> # store can be used from other threads
>>> # Use any of the store methods after initialization
>>> store.set("first_key", "first_value")

```
Copy to clipboard 

__init__(_self :torch._C._distributed_c10d.HashStore_) → None
    
Creates a new HashStore. 

_class_ torch.distributed.FileStore
    
A store implementation that uses a file to store the underlying key-value pairs. 

Parameters
    
  * **file_name** (_str_) – path of the file in which to store the key-value pairs
  * **world_size** (_int_ _,__optional_) – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).



Example::
    
```
>>> import torch.distributed as dist
>>> store1 = dist.FileStore("/tmp/filestore", 2)
>>> store2 = dist.FileStore("/tmp/filestore", 2)
>>> # Use any of the store methods from either the client or server after initialization
>>> store1.set("first_key", "first_value")
>>> store2.get("first_key")

```
Copy to clipboard 

__init__(_self :torch._C._distributed_c10d.FileStore_, _file_name :str_, _world_size :int=-1_) → None
    
Creates a new FileStore. 

_property_ path
    
Gets the path of the file used by FileStore to store key-value pairs. 

_class_ torch.distributed.PrefixStore
    
A wrapper around any of the 3 key-value stores (`TCPStore`, `FileStore`, and `HashStore`) that adds a prefix to each key inserted to the store. 

Parameters
    
  * **prefix** (_str_) – The prefix string that is prepended to each key before being inserted into the store.
  * **store** (_torch.distributed.store_) – A store object that forms the underlying key-value store.



__init__(_self :torch._C._distributed_c10d.PrefixStore_, _prefix :str_, _store :torch._C._distributed_c10d.Store_) → None
    
Creates a new PrefixStore. 

_property_ underlying_store
    
Gets the underlying store object that PrefixStore wraps around.
## Profiling Collective Communication
Note that you can use `torch.profiler` (recommended, only available after 1.8.1) or `torch.autograd.profiler` to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (`gloo`, `nccl`, `mpi`) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:
```
import torch
import torch.distributed as dist
with torch.profiler():
  tensor = torch.randn(20, 10)
  dist.all_reduce(tensor)

```
Copy to clipboard
Please refer to the profiler documentation for a full overview of profiler features.
## Multi-GPU collective functions
Warning
The multi-GPU functions (which stand for multiple GPUs per CPU thread) are deprecated. As of today, PyTorch Distributed’s preferred programming model is one device per thread, as exemplified by the APIs in this document. If you are a backend developer and want to support multiple devices per thread, please contact PyTorch Distributed’s maintainers.
## Third-party backends
Besides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to Tutorials - Custom C++ and CUDA Extensions and `test/cpp_extensions/cpp_c10d_extension.cpp`. The capability of third-party backends are decided by their own implementations.
The new backend derives from `c10d::ProcessGroup` and registers the backend name and the instantiating interface through `torch.distributed.Backend.register_backend()` when imported.
When manually importing this backend and invoking `torch.distributed.init_process_group()` with the corresponding backend name, the `torch.distributed` package runs on the new backend.
Warning
The support of third-party backend is experimental and subject to change.
## Launch utility
The torch.distributed package also provides a launch utility in torch.distributed.launch. This helper utility can be used to launch multiple processes per node for distributed training.
Module `torch.distributed.launch`.
`torch.distributed.launch` is a module that spawns up multiple distributed training processes on each of the training nodes.
Warning
This module is going to be deprecated in favor of torchrun.
The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be beneficial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.
In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (`--nproc-per-node`). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (`nproc_per_node`), and each process will be operating on a single GPU from _GPU 0 to GPU (nproc_per_node - 1)_.
**How to use this module:**
  1. Single-Node multi-process distributed training


```
python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE
      YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other
      arguments of your training script)

```
Copy to clipboard
  1. Multi-Node multi-process distributed training: (e.g. two nodes)


Node 1: _(IP: 192.168.1.1, and has a free port: 1234)_
```
python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE
      --nnodes=2 --node-rank=0 --master-addr="192.168.1.1"
      --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
      and all other arguments of your training script)

```
Copy to clipboard
Node 2:
```
python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE
      --nnodes=2 --node-rank=1 --master-addr="192.168.1.1"
      --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
      and all other arguments of your training script)

```
Copy to clipboard
  1. To look up what optional arguments this module offers:


```
python -m torch.distributed.launch --help

```
Copy to clipboard
**Important Notices:**
1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training.
2. In your training program, you must parse the command-line argument: `--local-rank=LOCAL_PROCESS_RANK`, which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:
Parsing the local_rank argument
```
>>> import argparse
>>> parser = argparse.ArgumentParser()
>>> parser.add_argument("--local-rank", "--local_rank", type=int)
>>> args = parser.parse_args()

```
Copy to clipboard
Set your device to local rank using either
```
>>> torch.cuda.set_device(args.local_rank) # before your code runs

```
Copy to clipboard
or
```
>>> with torch.cuda.device(args.local_rank):
>>>   # your code to run
>>>   ...

```
Copy to clipboard
Changed in version 2.0.0: The launcher will passes the `--local-rank=<rank>` argument to your script. From PyTorch 2.0.0 onwards, the dashed `--local-rank` is preferred over the previously used underscored `--local_rank`.
For backward compatibility, it may be necessary for users to handle both cases in their argument parsing code. This means including both `"--local-rank"` and `"--local_rank"` in the argument parser. If only `"--local_rank"` is provided, the launcher will trigger an error: “error: unrecognized arguments: –local-rank=<rank>”. For training code that only supports PyTorch 2.0.0+, including `"--local-rank"` should be sufficient.
3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that `init_method=env://`. Other init methods (e.g. `tcp://`) may work, but `env://` is the one that is officially supported by this module.
```
>>> torch.distributed.init_process_group(backend='YOUR BACKEND',
>>>                    init_method='env://')

```
Copy to clipboard
4. In your training program, you can either use regular distributed functions or use `torch.nn.parallel.DistributedDataParallel()` module. If your training program uses GPUs for training and you would like to use `torch.nn.parallel.DistributedDataParallel()` module, here is how to configure it.
```
>>> model = torch.nn.parallel.DistributedDataParallel(model,
>>>                          device_ids=[args.local_rank],
>>>                          output_device=args.local_rank)

```
Copy to clipboard
Please ensure that `device_ids` argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the `device_ids` needs to be `[args.local_rank]`, and `output_device` needs to be `args.local_rank` in order to use this utility
5. Another way to pass `local_rank` to the subprocesses via environment variable `LOCAL_RANK`. This behavior is enabled when you launch the script with `--use-env=True`. You must adjust the subprocess example above to replace `args.local_rank` with `os.environ['LOCAL_RANK']`; the launcher will not pass `--local-rank` when you specify this flag.
Warning
`local_rank` is NOT globally unique: it is only unique per process on a machine. Thus, don’t use it to decide if you should, e.g., write to a networked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for an example of how things can go wrong if you don’t do this correctly.
## Spawn utility
The Multiprocessing package - torch.multiprocessing package also provides a `spawn` function in `torch.multiprocessing.spawn()`. This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.
For references on how to use it, please refer to PyTorch example - ImageNet implementation
Note that this function requires Python 3.4 or higher.
## Debugging `torch.distributed` applications
Debugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. `torch.distributed` provides a suite of tools to help debug training applications in a self-serve fashion:
### Python Breakpoint
It is extremely convenient to use python’s debugger in a distributed environment, but because it does not work out of the box many people do not use it at all. PyTorch offers a customized wrapper around pdb that streamlines the process.
torch.distributed.breakpoint makes this process easy. Internally, it customizes pdb’s breakpoint behavior in two ways but otherwise behaves as normal pdb. 1. Attaches the debugger only on one rank (specified by the user). 2. Ensures all other ranks stop, by using a torch.distributed.barrier() that will release once the debugged rank issues a continue 3. Reroutes stdin from the child process such that it connects to your terminal.
To use it, simply issue torch.distributed.breakpoint(rank) on all ranks, using the same value for rank in each case.
### Monitored Barrier
As of v1.10, `torch.distributed.monitored_barrier()` exists as an alternative to `torch.distributed.barrier()` which fails with helpful information about which rank may be faulty when crashing, i.e. not all ranks calling into `torch.distributed.monitored_barrier()` within the provided timeout. `torch.distributed.monitored_barrier()` implements a host-side barrier using `send`/`recv` communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge the barrier in time. As an example, consider the following function where rank 1 fails to call into `torch.distributed.monitored_barrier()` (in practice this could be due to an application bug or hang in a previous collective):
```
import os
from datetime import timedelta
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def worker(rank):
  dist.init_process_group("nccl", rank=rank, world_size=2)
  # monitored barrier requires gloo process group to perform host-side sync.
  group_gloo = dist.new_group(backend="gloo")
  if rank not in [1]:
    dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))

if __name__ == "__main__":
  os.environ["MASTER_ADDR"] = "localhost"
  os.environ["MASTER_PORT"] = "29501"
  mp.spawn(worker, nprocs=2, args=())

```
Copy to clipboard
The following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:
```
RuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms
 Original exception:
[gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594

```
Copy to clipboard
### `TORCH_DISTRIBUTED_DEBUG`
With `TORCH_CPP_LOG_LEVEL=INFO`, the environment variable `TORCH_DISTRIBUTED_DEBUG` can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks are synchronized appropriately. `TORCH_DISTRIBUTED_DEBUG` can be set to either `OFF` (default), `INFO`, or `DETAIL` depending on the debugging level required. Please note that the most verbose option, `DETAIL` may impact the application performance and thus should only be used when debugging issues.
Setting `TORCH_DISTRIBUTED_DEBUG=INFO` will result in additional debug logging when models trained with `torch.nn.parallel.DistributedDataParallel()` are initialized, and `TORCH_DISTRIBUTED_DEBUG=DETAIL` will additionally log runtime performance statistics a select number of iterations. These runtime statistics include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:
```
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

class TwoLinLayerNet(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.a = torch.nn.Linear(10, 10, bias=False)
    self.b = torch.nn.Linear(10, 1, bias=False)
  def forward(self, x):
    a = self.a(x)
    b = self.b(x)
    return (a, b)

def worker(rank):
  dist.init_process_group("nccl", rank=rank, world_size=2)
  torch.cuda.set_device(rank)
  print("init model")
  model = TwoLinLayerNet().cuda()
  print("init ddp")
  ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])
  inp = torch.randn(10, 10).cuda()
  print("train")
  for _ in range(20):
    output = ddp_model(inp)
    loss = output[0] + output[1]
    loss.sum().backward()

if __name__ == "__main__":
  os.environ["MASTER_ADDR"] = "localhost"
  os.environ["MASTER_PORT"] = "29501"
  os.environ["TORCH_CPP_LOG_LEVEL"]="INFO"
  os.environ[
    "TORCH_DISTRIBUTED_DEBUG"
  ] = "DETAIL" # set to DETAIL for runtime logging.
  mp.spawn(worker, nprocs=2, args=())

```
Copy to clipboard
The following logs are rendered at initialization time:
```
I0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:
broadcast_buffers: 1
bucket_cap_bytes: 26214400
find_unused_parameters: 0
gradient_as_bucket_view: 0
is_multi_device_module: 0
iteration: 0
num_parameter_tensors: 2
output_device: 0
rank: 0
total_parameter_size_bytes: 440
world_size: 2
backend_name: nccl
bucket_sizes: 440
cuda_visible_devices: N/A
device_ids: 0
dtypes: float
master_addr: localhost
master_port: 29501
module_name: TwoLinLayerNet
nccl_async_error_handling: N/A
nccl_blocking_wait: N/A
nccl_debug: WARN
nccl_ib_timeout: N/A
nccl_nthreads: N/A
nccl_socket_ifname: N/A
torch_distributed_debug: INFO

```
Copy to clipboard
The following logs are rendered during runtime (when `TORCH_DISTRIBUTED_DEBUG=DETAIL` is set):
```
I0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0
 Avg forward compute time: 40838608
 Avg backward compute time: 5983335
Avg backward comm. time: 4326421
 Avg backward comm/comp overlap time: 4207652
I0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0
 Avg forward compute time: 42850427
 Avg backward compute time: 3885553
Avg backward comm. time: 2357981
 Avg backward comm/comp overlap time: 2234674

```
Copy to clipboard
In addition, `TORCH_DISTRIBUTED_DEBUG=INFO` enhances crash logging in `torch.nn.parallel.DistributedDataParallel()` due to unused parameters in the model. Currently, `find_unused_parameters=True` must be passed into `torch.nn.parallel.DistributedDataParallel()` initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required to be used in loss computation as `torch.nn.parallel.DistributedDataParallel()` does not support unused parameters in the backwards pass. These constraints are challenging especially for larger models, thus when crashing with an error, `torch.nn.parallel.DistributedDataParallel()` will log the fully qualified name of all parameters that went unused. For example, in the above application, if we modify `loss` to be instead computed as `loss = output[1]`, then `TwoLinLayerNet.a` does not receive a gradient in the backwards pass, and thus results in `DDP` failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:
```
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing
 the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va
lue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: a.weight
Parameter indices which did not receive grad for rank 0: 0

```
Copy to clipboard
Setting `TORCH_DISTRIBUTED_DEBUG=DETAIL` will trigger additional consistency and synchronization checks on every collective call issued by the user either directly or indirectly (such as DDP `allreduce`). This is done by creating a wrapper process group that wraps all process groups returned by `torch.distributed.init_process_group()` and `torch.distributed.new_group()` APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a `torch.distributed.monitored_barrier()`, which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into `torch.distributed.all_reduce()`:
```
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def worker(rank):
  dist.init_process_group("nccl", rank=rank, world_size=2)
  torch.cuda.set_device(rank)
  tensor = torch.randn(10 if rank == 0 else 20).cuda()
  dist.all_reduce(tensor)
  torch.cuda.synchronize(device=rank)

if __name__ == "__main__":
  os.environ["MASTER_ADDR"] = "localhost"
  os.environ["MASTER_PORT"] = "29501"
  os.environ["TORCH_CPP_LOG_LEVEL"]="INFO"
  os.environ["TORCH_DISTRIBUTED_DEBUG"] = "DETAIL"
  mp.spawn(worker, nprocs=2, args=())

```
Copy to clipboard
With the `NCCL` backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables `TORCH_DISTRIBUTED_DEBUG=DETAIL` and reruns the application, the following error message reveals the root cause:
```
work = default_pg.allreduce([tensor], opts)
RuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes: 10
[ torch.LongTensor{1} ]

```
Copy to clipboard
Note
For fine-grained control of the debug level during runtime the functions `torch.distributed.set_debug_level()`, `torch.distributed.set_debug_level_from_env()`, and `torch.distributed.get_debug_level()` can also be used.
In addition, TORCH_DISTRIBUTED_DEBUG=DETAIL can be used in conjunction with TORCH_SHOW_CPP_STACKTRACES=1 to log the entire callstack when a collective desynchronization is detected. These collective desynchronization checks will work for all applications that use `c10d` collective calls backed by process groups created with the `torch.distributed.init_process_group()` and `torch.distributed.new_group()` APIs.
## Logging
In addition to explicit debugging support via `torch.distributed.monitored_barrier()` and `TORCH_DISTRIBUTED_DEBUG`, the underlying C++ library of `torch.distributed` also outputs log messages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The following matrix shows how the log level can be adjusted via the combination of `TORCH_CPP_LOG_LEVEL` and `TORCH_DISTRIBUTED_DEBUG` environment variables.
`TORCH_CPP_LOG_LEVEL` | `TORCH_DISTRIBUTED_DEBUG` | Effective Log Level  
---|---|---  
`ERROR` | ignored | Error  
`WARNING` | ignored | Warning  
`INFO` | ignored | Info  
`INFO` | `INFO` | Debug  
`INFO` | `DETAIL` | Trace (a.k.a. All)  
Distributed components raise custom Exception types derived from RuntimeError:
  * torch.distributed.DistError: This is the base type of all distributed exceptions.
  * torch.distributed.DistBackendError: This exception is thrown when a backend-specific error occurs. For example, if the NCCL backend is used and the user attempts to use a GPU that is not available to the NCCL library.
  * torch.distributed.DistNetworkError: This exception is thrown when networking libraries encounter errors (ex: Connection reset by peer)
  * torch.distributed.DistStoreError: This exception is thrown when the Store encounters an error (ex: TCPStore timeout)



_class_ torch.distributed.DistError
    
Exception raised when an error occurs in the distributed library 

_class_ torch.distributed.DistBackendError
    
Exception raised when a backend error occurs in distributed 

_class_ torch.distributed.DistNetworkError
    
Exception raised when a network error occurs in distributed 

_class_ torch.distributed.DistStoreError
    
Exception raised when an error occurs in the distributed store
If you are running single node training, it may be convenient to interactively breakpoint your script. We offer a way to conveniently breakpoint a single rank: 

torch.distributed.breakpoint(_rank =0_, _skip =0_)[source][source]
    
Set a breakpoint, but only on a single rank. All other ranks will wait for you to be done with the breakpoint before continuing. 

Parameters
    
  * **rank** (_int_) – Which rank to break on. Default: `0`
  * **skip** (_int_) – Skip the first `skip` calls to this breakpoint. Default: `0`.


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Distributed communication package - torch.distributed
    * Backends
      * Backends that come with PyTorch
      * Which backend to use?
      * Common environment variables
        * Choosing the network interface to use
        * Other NCCL environment variables
    * Basics
    * Initialization
      * `is_available()`
      * `init_process_group()`
      * `init_device_mesh()`
      * `is_initialized()`
      * `is_mpi_available()`
      * `is_nccl_available()`
      * `is_gloo_available()`
      * `is_xccl_available()`
      * `is_torchelastic_launched()`
      * TCP initialization
      * Shared file-system initialization
      * Environment variable initialization
    * Post-Initialization
      * `Backend`
        * `Backend.register_backend()`
      * `get_backend()`
      * `get_rank()`
      * `get_world_size()`
    * Shutdown
      * Reinitialization
    * Groups
      * `new_group()`
      * `get_group_rank()`
      * `get_global_rank()`
      * `get_process_group_ranks()`
    * DeviceMesh
      * `DeviceMesh`
        * `DeviceMesh.from_group()`
        * `DeviceMesh.get_all_groups()`
        * `DeviceMesh.get_coordinate()`
        * `DeviceMesh.get_group()`
        * `DeviceMesh.get_local_rank()`
        * `DeviceMesh.get_rank()`
    * Point-to-point communication
      * `send()`
      * `recv()`
      * `isend()`
      * `irecv()`
      * `send_object_list()`
      * `recv_object_list()`
      * `batch_isend_irecv()`
      * `P2POp`
    * Synchronous and asynchronous collective operations
    * Collective functions
      * `broadcast()`
      * `broadcast_object_list()`
      * `all_reduce()`
      * `reduce()`
      * `all_gather()`
      * `all_gather_into_tensor()`
      * `all_gather_object()`
      * `gather()`
      * `gather_object()`
      * `scatter()`
      * `scatter_object_list()`
      * `reduce_scatter()`
      * `reduce_scatter_tensor()`
      * `all_to_all_single()`
      * `all_to_all()`
      * `barrier()`
      * `monitored_barrier()`
      * `Work`
        * `Work.boxed()`
        * `Work.exception()`
        * `Work.get_future()`
        * `Work.get_future_result()`
        * `Work.is_completed()`
        * `Work.is_success()`
        * `Work.result()`
        * `Work.source_rank()`
        * `Work.synchronize()`
        * `Work.unbox()`
        * `Work.wait()`
      * `ReduceOp`
      * `reduce_op`
    * Distributed Key-Value Store
      * `Store`
        * `Store.__init__()`
        * `Store.add()`
        * `Store.append()`
        * `Store.check()`
        * `Store.compare_set()`
        * `Store.delete_key()`
        * `Store.get()`
        * `Store.has_extended_api()`
        * `Store.multi_get()`
        * `Store.multi_set()`
        * `Store.num_keys()`
        * `Store.set()`
        * `Store.set_timeout()`
        * `Store.timeout`
        * `Store.wait()`
      * `TCPStore`
        * `TCPStore.__init__()`
        * `TCPStore.host`
        * `TCPStore.libuvBackend`
        * `TCPStore.port`
      * `HashStore`
        * `HashStore.__init__()`
      * `FileStore`
        * `FileStore.__init__()`
        * `FileStore.path`
      * `PrefixStore`
        * `PrefixStore.__init__()`
        * `PrefixStore.underlying_store`
    * Profiling Collective Communication
    * Multi-GPU collective functions
    * Third-party backends
    * Launch utility
    * Spawn utility
    * Debugging `torch.distributed` applications
      * Python Breakpoint
      * Monitored Barrier
      * `TORCH_DISTRIBUTED_DEBUG`
    * Logging
      * `DistError`
      * `DistBackendError`
      * `DistNetworkError`
      * `DistStoreError`
      * `breakpoint()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Distributed Optimizers
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Distributed Optimizers
Warning
Distributed optimizer is not currently supported when using CUDA tensors
`torch.distributed.optim` exposes DistributedOptimizer, which takes a list of remote parameters (`RRef`) and runs the optimizer locally on the workers where the parameters live. The distributed optimizer can use any of the local optimizer Base class to apply the gradients on each worker. 

_class_ torch.distributed.optim.DistributedOptimizer(_optimizer_class_ , _params_rref_ , _* args_, _** kwargs_)[source][source]
    
DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.
This class uses `get_gradients()` in order to retrieve the gradients for specific parameters.
Concurrent calls to `step()`, either from the same or different clients, will be serialized on each worker – as each worker’s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.
DistributedOptimizer creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed Model Parallel). This feature is currently enabled for most optimizers. You can also follow the recipe in PyTorch tutorials to enable TorchScript support for your own custom optimizers. 

Parameters
    
  * **optimizer_class** (_optim.Optimizer_) – the class of optimizer to instantiate on each worker.
  * **params_rref** (_list_ _[__RRef_ _]_) – list of RRefs to local or remote parameters to optimize.
  * **args** – arguments to pass to the optimizer constructor on each worker.
  * **kwargs** – arguments to pass to the optimizer constructor on each worker.



Example::
    
```
>>> import torch.distributed.autograd as dist_autograd
>>> import torch.distributed.rpc as rpc
>>> from torch import optim
>>> from torch.distributed.optim import DistributedOptimizer
>>>
>>> with dist_autograd.context() as context_id:
>>>  # Forward pass.
>>>  rref1 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 3))
>>>  rref2 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 1))
>>>  loss = rref1.to_here() + rref2.to_here()
>>>
>>>  # Backward pass.
>>>  dist_autograd.backward(context_id, [loss.sum()])
>>>
>>>  # Optimizer.
>>>  dist_optim = DistributedOptimizer(
>>>    optim.SGD,
>>>    [rref1, rref2],
>>>    lr=0.05,
>>>  )
>>>  dist_optim.step(context_id)

```
Copy to clipboard 

step(_context_id_)[source][source]
    
Performs a single optimization step.
This will call `torch.optim.Optimizer.step()` on each worker containing parameters to be optimized, and will block until all workers return. The provided `context_id` will be used to retrieve the corresponding `context` that contains the gradients that should be applied to the parameters. 

Parameters
    
**context_id** – the autograd context id for which we should run the optimizer step. 

_class_ torch.distributed.optim.PostLocalSGDOptimizer(_optim_ , _averager_)[source][source]
    
Wraps an arbitrary `torch.optim.Optimizer` and runs post-local SGD, This optimizer runs local optimizer at every step. After the warm-up stage, it averages parameters periodically afer the local optimizer is applied. 

Parameters
    
  * **optim** (_Optimizer_) – The local optimizer.
  * **averager** (_ModelAverager_) – A model averager instance to run post-localSGD algorithm.


Example:
```
>>> import torch
>>> import torch.distributed as dist
>>> import torch.distributed.algorithms.model_averaging.averagers as averagers
>>> import torch.nn as nn
>>> from torch.distributed.optim import PostLocalSGDOptimizer
>>> from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (
>>>  PostLocalSGDState,
>>>  post_localSGD_hook,
>>> )
>>>
>>> model = nn.parallel.DistributedDataParallel(
>>>   module, device_ids=[rank], output_device=rank
>>> )
>>>
>>> # Register a post-localSGD communication hook.
>>> state = PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=100)
>>> model.register_comm_hook(state, post_localSGD_hook)
>>>
>>> # Create a post-localSGD optimizer that wraps a local optimizer.
>>> # Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as
>>> # ``start_localSGD_iter`` used in ``PostLocalSGDState``.
>>> local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)
>>> opt = PostLocalSGDOptimizer(
>>>   optim=local_optim,
>>>   averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)
>>> )
>>>
>>> # In the first 100 steps, DDP runs global gradient averaging at every step.
>>> # After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),
>>> # and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.
>>> for step in range(0, 200):
>>>   opt.zero_grad()
>>>   loss = loss_fn(output, labels)
>>>   loss.backward()
>>>   opt.step()

```
Copy to clipboard 

load_state_dict(_state_dict_)[source][source]
    
This is the same as `torch.optim.Optimizer` `load_state_dict()`, but also restores model averager’s step value to the one saved in the provided `state_dict`.
If there is no `"step"` entry in `state_dict`, it will raise a warning and initialize the model averager’s step to 0. 

state_dict()[source][source]
    
This is the same as `torch.optim.Optimizer` `state_dict()`, but adds an extra entry to record model averager’s step to the checkpoint to ensure reload does not cause unnecessary warm up again. 

step()[source][source]
    
Performs a single optimization step (parameter update). 

_class_ torch.distributed.optim.ZeroRedundancyOptimizer(_params_ , _optimizer_class_ , _process_group =None_, _parameters_as_bucket_view =False_, _overlap_with_ddp =False_, _** defaults_)[source][source]
    
Wrap an arbitrary `optim.Optimizer` and shards its states across ranks in the group.
The sharing is done as described by ZeRO.
The local optimizer instance in each rank is only responsible for updating approximately `1 / world_size` parameters and hence only needs to keep `1 / world_size` optimizer states. After parameters are updated locally, each rank will broadcast its parameters to all other peers to keep all model replicas in the same state. `ZeroRedundancyOptimizer` can be used in conjunction with `torch.nn.parallel.DistributedDataParallel` to reduce per-rank peak memory consumption.
`ZeroRedundancyOptimizer` uses a sorted-greedy algorithm to pack a number of parameters at each rank. Each parameter belongs to a single rank and is not divided among ranks. The partition is arbitrary and might not match the the parameter registration or usage order. 

Parameters
    
**params** (`Iterable`) – an `Iterable` of `torch.Tensor` s or `dict` s giving all parameters, which will be sharded across ranks. 

Keyword Arguments
    
  * **optimizer_class** (`torch.nn.Optimizer`) – the class of the local optimizer.
  * **process_group** (`ProcessGroup`, optional) – `torch.distributed` `ProcessGroup` (default: `dist.group.WORLD` initialized by `torch.distributed.init_process_group()`).
  * **parameters_as_bucket_view** (_bool_ _,__optional_) – if `True`, parameters are packed into buckets to speed up communication, and `param.data` fields point to bucket views at different offsets; if `False`, each individual parameter is communicated separately, and each `params.data` stays intact (default: `False`).
  * **overlap_with_ddp** (_bool_ _,__optional_) – if `True`, `step()` is overlapped with `DistributedDataParallel` ‘s gradient synchronization; this requires (1) either a functional optimizer for the `optimizer_class` argument or one with a functional equivalent and (2) registering a DDP communication hook constructed from one of the functions in `ddp_zero_hook.py`; parameters are packed into buckets matching those in `DistributedDataParallel`, meaning that the `parameters_as_bucket_view` argument is ignored. If `False`, `step()` runs disjointly after the backward pass (per normal). (default: `False`)
  * ****defaults** – any trailing arguments, which are forwarded to the local optimizer.


Example:
```
>>> import torch.nn as nn
>>> from torch.distributed.optim import ZeroRedundancyOptimizer
>>> from torch.nn.parallel import DistributedDataParallel as DDP
>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])
>>> ddp = DDP(model, device_ids=[rank])
>>> opt = ZeroRedundancyOptimizer(
>>>   ddp.parameters(),
>>>   optimizer_class=torch.optim.Adam,
>>>   lr=0.01
>>> )
>>> ddp(inputs).sum().backward()
>>> opt.step()

```
Copy to clipboard
Warning
Currently, `ZeroRedundancyOptimizer` requires that all of the passed-in parameters are the same dense type.
Warning
If you pass `overlap_with_ddp=True`, be wary of the following: Given the way that overlapping `DistributedDataParallel` with `ZeroRedundancyOptimizer` is currently implemented, the first two or three training iterations do not perform parameter updates in the optimizer step, depending on if `static_graph=False` or `static_graph=True`, respectively. This is because it needs information about the gradient bucketing strategy used by `DistributedDataParallel`, which is not finalized until the second forward pass if `static_graph=False` or until the third forward pass if `static_graph=True`. To adjust for this, one option is to prepend dummy inputs.
Warning
ZeroRedundancyOptimizer is experimental and subject to change. 

add_param_group(_param_group_)[source][source]
    
Add a parameter group to the `Optimizer` ‘s `param_groups`.
This can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the `Optimizer` as training progresses. 

Parameters
    
**param_group** (_dict_) – specifies the parameters to be optimized and group-specific optimization options.
Warning
This method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters. 

consolidate_state_dict(_to =0_)[source][source]
    
Consolidate a list of `state_dict` s (one per rank) on the target rank. 

Parameters
    
**to** (_int_) – the rank that receives the optimizer states (default: 0). 

Raises
    
**RuntimeError** – if `overlap_with_ddp=True` and this method is called before this `ZeroRedundancyOptimizer` instance has been fully initialized, which happens once `DistributedDataParallel` gradient buckets have been rebuilt.
Warning
This needs to be called on all ranks. 

_property_ join_device _: device_
    
Return default device. 

join_hook(_** kwargs_)[source][source]
    
Return the ZeRO join hook.
It enables training on uneven inputs by shadowing the collective communications in the optimizer step.
Gradients must be properly set before this hook is called. 

Parameters
    
**kwargs** (_dict_) – a `dict` containing any keyword arguments to modify the behavior of the join hook at run time; all `Joinable` instances sharing the same join context manager are forwarded the same value for `kwargs`.
This hook does not support any keyword arguments; i.e. `kwargs` is unused. 

_property_ join_process_group _: Any_
    
Return process group. 

load_state_dict(_state_dict_)[source][source]
    
Load the state pertaining to the given rank from the input `state_dict`, updating the local optimizer as needed. 

Parameters
    
**state_dict** (_dict_) – optimizer state; should be an object returned from a call to `state_dict()`. 

Raises
    
**RuntimeError** – if `overlap_with_ddp=True` and this method is called before this `ZeroRedundancyOptimizer` instance has been fully initialized, which happens once `DistributedDataParallel` gradient buckets have been rebuilt. 

state_dict()[source][source]
    
Return the last global optimizer state known to this rank. 

Raises
    
**RuntimeError** – if `overlap_with_ddp=True` and this method is called before this `ZeroRedundancyOptimizer` instance has been fully initialized, which happens once `DistributedDataParallel` gradient buckets have been rebuilt; or if this method is called without a preceding call to `consolidate_state_dict()`. 

Return type
    
dict[str, _Any_] 

step(_closure =None_, _** kwargs_)[source][source]
    
Perform a single optimizer step and syncs parameters across all ranks. 

Parameters
    
**closure** (_Callable_) – a closure that re-evaluates the model and returns the loss; optional for most optimizers. 

Returns
    
Optional loss depending on the underlying local optimizer. 

Return type
    
_Optional_[float]
Note
Any extra parameters are passed to the base optimizer as-is.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Distributed Optimizers
    * `DistributedOptimizer`
      * `DistributedOptimizer.step()`
    * `PostLocalSGDOptimizer`
      * `PostLocalSGDOptimizer.load_state_dict()`
      * `PostLocalSGDOptimizer.state_dict()`
      * `PostLocalSGDOptimizer.step()`
    * `ZeroRedundancyOptimizer`
      * `ZeroRedundancyOptimizer.add_param_group()`
      * `ZeroRedundancyOptimizer.consolidate_state_dict()`
      * `ZeroRedundancyOptimizer.join_device`
      * `ZeroRedundancyOptimizer.join_hook()`
      * `ZeroRedundancyOptimizer.join_process_group`
      * `ZeroRedundancyOptimizer.load_state_dict()`
      * `ZeroRedundancyOptimizer.state_dict()`
      * `ZeroRedundancyOptimizer.step()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.distributed.tensor
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.distributed.tensor
Note
`torch.distributed.tensor` is currently in alpha state and under development, we are committing backward compatibility for the most APIs listed in the doc, but there might be API changes if necessary.
## PyTorch DTensor (Distributed Tensor)
PyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed logic, including sharded storage, operator computation and collective communications across devices/hosts. `DTensor` could be used to build different paralleism solutions and support sharded state_dict representation when working with multi-dimensional sharding.
Please see examples from the PyTorch native parallelism solutions that are built on top of `DTensor`:
  * Tensor Parallel
  * FSDP2


`DTensor` follows the SPMD (single program, multiple data) programming model to empower users to write distributed program as if it’s a **single-device program with the same convergence property**. It provides a uniform tensor sharding layout (DTensor Layout) through specifying the `DeviceMesh` and `Placement`:
  * `DeviceMesh` represents the device topology and the communicators of the cluster using an n-dimensional array.
  * `Placement` describes the sharding layout of the logical tensor on the `DeviceMesh`. DTensor supports three types of placements: `Shard`, `Replicate` and `Partial`.


### DTensor Class APIs
`DTensor` is a `torch.Tensor` subclass. This means once a `DTensor` is created, it could be used in very similar way to `torch.Tensor`, including running different types of PyTorch operators as if running them in a single device, allowing proper distributed computation for PyTorch operators.
In addition to existing `torch.Tensor` methods, it also offers a set of additional methods to interact with `torch.Tensor`, `redistribute` the DTensor Layout to a new DTensor, get the full tensor content on all devices, etc. 

_class_ torch.distributed.tensor.DTensor(_local_tensor_ , _spec_ , _*_ , _requires_grad_)
    
`DTensor` (Distributed Tensor) is a subclass of `torch.Tensor` that provides single-device like abstraction to program with multi-device `torch.Tensor`. It describes the distributed tensor sharding layout (DTensor Layout) through the `DeviceMesh` and following types of `Placement`:
  * `Shard`: Tensor sharded on the tensor dimension `dim` on the devices of the `DeviceMesh` dimension
  * `Replicate`: Tensor replicated on the devices of the `DeviceMesh` dimension
  * `Partial`: Tensor is pending reduction on the devices of the `DeviceMesh` dimension


When calling PyTorch operators, `DTensor` overrides the PyTorch operators to perform sharded computation and issue communications whenever necessary. Along with the operator computation, `DTensor` will transform or propagate the placements (DTensor Layout) properly (based on the operator semantic itself) and generate new `DTensor` outputs.
To ensure numerical correctness of the `DTensor` sharded computation when calling PyTorch operators, `DTensor` requires every Tensor argument of the operator be DTensor.
Note
Directly using the Tensor subclass constructor here is not the recommended way to create a `DTensor` (i.e. it does not handle autograd correctly hence is not the public API). Please refer to the create_dtensor section to see how to create a `DTensor`. 

Return type
    
DTensor 

__create_chunk_list__()[source][source]
    
Return a list of ChunkStorageMetadata, which is a dataclass that describes the size/offset of the local shard/replica on current rank. For DTensor, each rank will have a single local shard/replica, so the returned list usually only has one element.
This dunder method is primariy used for distributed checkpoint purpose. 

Returns
    
A List[`ChunkStorageMetadata`] object that represents the shard size/offset on the current rank. 

_static_ from_local(_local_tensor_ , _device_mesh =None_, _placements =None_, _*_ , _run_check =False_, _shape =None_, _stride =None_)[source][source]
    
Create a `DTensor` from a local torch.Tensor on each rank according to the `device_mesh` and `placements` specified. 

Parameters
    
  * **local_tensor** (_torch.Tensor_) – local torch.Tensor on each rank.
  * **device_mesh** (`DeviceMesh`, optional) – DeviceMesh to place the tensor, if not specified, must be called under a DeviceMesh context manager, default: None
  * **placements** (List[`Placement`], optional) – the placements that describes how to place the local torch.Tensor on DeviceMesh, must have the same number of elements as `device_mesh.ndim`.



Keyword Arguments
    
  * **run_check** (_bool_ _,__optional_) – at a cost of extra communications, perform sanity check across ranks to check each local tensor’s meta information to ensure correctness. If have `Replicate` in `placements`, the data on first rank of the device mesh dimension will be broadcasted to other ranks. default: False
  * **shape** (_torch.Size_ _,__optional_) – A List of int which specifies the size of DTensor which build on top of local_tensor. Note this needs to be provided if the shape of `local_tensor` are different across the ranks. If not provided, `shape` will be computed assuming the given distributed tensor is evenly sharded across ranks. default: None
  * **stride** (_tuple_ _,__optional_) – A List of int which specifies the stride of DTensor. If not provided, `stride` will be computed assuming the given distributed tensor is evenly sharded across ranks. default: None



Returns
    
A `DTensor` object 

Return type
    
_DTensor_
Note
When `run_check=False`, it is the user’s responsibility to ensure the local tensor passed in is correct across ranks (i.e. the tensor is sharded for the `Shard(dim)` placement or replicated for the `Replicate()` placement). If not, the behavior of the created DTensor is undefined.
Note
`from_local` is differentiable, the requires_grad of the created DTensor object will depend on if local_tensor requires_grad or not. 

full_tensor(_*_ , _grad_placements =None_)[source][source]
    
Return the full tensor of this DTensor. It will perform necessary collectives to gather the local tensors from other ranks in its DeviceMesh and concatenate them together. It’s a syntatic sugar of the following code:
`dtensor.redistribute(placements=[Replicate()] * mesh.ndim).to_local()` 

Keyword Arguments
    
**grad_placements** (List[`Placement`], optional) – the placements describes the future layout of any gradient layout of the full Tensor returned from this function. full_tensor converts DTensor to a full torch.Tensor and the returned torch.tensor might not be used as the original replicated DTensor layout later in the code. This argument is the hint that user can give to autograd in case the gradient layout of the returned tensor does not match the original replicated DTensor layout. If not specified, we will assume the gradient layout of the full tensor be replicated. 

Returns
    
A `torch.Tensor` object that represents the full tensor of this DTensor. 

Return type
    
_Tensor_
Note
`full_tensor` is differentiable. 

redistribute(_device_mesh =None_, _placements =None_, _*_ , _async_op =False_)[source][source]
    
`redistribute` performs necessary collective operations that redistribute the current DTensor from its current placements to a new placements, or from is current DeviceMesh to a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by specifying a Replicate placement for each dimension of the DeviceMesh.
When redistributing from current to the new placements on one device mesh dimension, we will perform the following operations including communication collective or local operation:
  1. `Shard(dim)` -> `Replicate()`: `all_gather`
  2. `Shard(src_dim)` -> `Shard(dst_dim)`: `all_to_all`
  3. `Replicate()` -> `Shard(dim)`: local chunking (i.e. `torch.chunk`)
  4. `Partial()` -> `Replicate()`: `all_reduce`
  5. `Partial()` -> `Shard(dim)`: `reduce_scatter`


`redistribute` would correctly figure out the necessary redistribute steps for DTensors that are created either on 1-D or N-D DeviceMesh. 

Parameters
    
  * **device_mesh** (`DeviceMesh`, optional) – DeviceMesh to place the DTensor. If not specified, it would use the current DTensor’s DeviceMesh. default: None
  * **placements** (List[`Placement`], optional) – the new placements that describes how to place the DTensor into the DeviceMesh, must have the same number of elements as `device_mesh.ndim`. default: replicate on all mesh dimensions



Keyword Arguments
    
**async_op** (_bool_ _,__optional_) – whether to perform the DTensor redistribute operation asynchronously or not. Default: False 

Returns
    
A `DTensor` object 

Return type
    
_DTensor_
Note
`redistribute` is differentiable, which means user do not need to worry about the backward formula of the redistribute operation.
Note
`redistribute` currently only supports redistributing DTensor on the same DeviceMesh, Please file an issue if you need to redistribute DTensor to different DeviceMesh. 

to_local(_*_ , _grad_placements =None_)[source][source]
    
Get the local tensor of this DTensor on its current rank. For sharding it returns a local shard of the logical tensor view, for replication it returns the replica on its current rank. 

Keyword Arguments
    
**grad_placements** (List[`Placement`], optional) – the placements describes the future layout of any gradient layout of the Tensor returned from this function. to_local converts DTensor to local tensor and the returned local tensor might not be used as the original DTensor layout later in the code. This argument is the hint that user can give to autograd in case the gradient layout of the returned tensor does not match the original DTensor layout. If not specified, we will assume the gradient layout remains the same as the original DTensor and use that for gradient computation. 

Returns
    
A `torch.Tensor` or `AsyncCollectiveTensor` object. it represents the local tensor on its current rank. When an `AsyncCollectiveTensor` object is returned, it means the local tensor is not ready yet (i.e. communication is not finished). In this case, user needs to call `wait` to wait the local tensor to be ready. 

Return type
    
_Tensor_
Note
`to_local` is differentiable, the `requires_grad` of the local tensor returned will depend on if the DTensor requires_grad or not. 

_property_ device_mesh _: DeviceMesh_
    
The `DeviceMesh` attribute that associates with this DTensor object.
Note
`device_mesh` is a read-only property, it can not be set. 

_property_ placements _: tuple[torch.distributed.tensor.placement_types.Placement,...]_
    
The placements attribute of this DTensor that describes the layout of this DTensor on the its DeviceMesh.
Note
`placements` is a read-only property, it can not be set.
### DeviceMesh as the distributed communicator
`DeviceMesh` was built from DTensor as the abstraction to describe cluster’s device topology and represent multi-dimensional communicators (on top of `ProcessGroup`). To see the details of how to create/use a DeviceMesh, please refer to the DeviceMesh recipe.
### DTensor Placement Types
DTensor supports the following types of `Placement` on each `DeviceMesh` dimension: 

_class_ torch.distributed.tensor.placement_types.Shard(_dim_)[source][source]
    
The `Shard(dim)` placement describes the DTensor sharding on tensor dimension `dim` over a corresponding `DeviceMesh` dimension, where each rank on the DeviceMesh dimension only holds a shard/piece of the global Tensor. The `Shard(dim)` placement follows the `torch.chunk(dim)` semantic, where the last few shards on the DeviceMesh dimension might be empty when the tensor dimension is not evenly divisible on the DeviceMesh dimension. The `Shard` placement can be used by all DTensor APIs (i.e. distribute_tensor, from_local, etc.) 

Parameters
    
**dim** (_int_) – The tensor dimension that describes the DTensor is sharded over its corresponding DeviceMesh dimension.
Warning
sharding on a tensor dimension where the tensor dimension size is not evenly divisible on a DeviceMesh dimension is currently experimental and subject to change. 

dim _: int_


_class_ torch.distributed.tensor.placement_types.Replicate[source][source]
    
The `Replicate()` placement describes the DTensor replicating on a corresponding `DeviceMesh` dimension, where each rank on the DeviceMesh dimension holds a replica of the global Tensor. The `Replicate` placement can be used by all DTensor APIs (i.e. `distribute_tensor`, `DTensor.from_local`, etc.) 

_class_ torch.distributed.tensor.placement_types.Partial(_reduce_op ='sum'_)[source][source]
    
The `Partial(reduce_op)` placement describes the DTensor that is pending reduction on a specified `DeviceMesh` dimension, where each rank on the DeviceMesh dimension holds the partial value of the global Tensor. User can redistribute the `Partial` DTensor to a `Replicate` or `Shard(dim)` placement on the specified `DeviceMesh` dimension using `redistribute`, which would trigger necessary communication operations under the hood (i.e. `allreduce`, `reduce_scatter`). 

Parameters
    
**reduce_op** (_str_ _,__optional_) – The reduction op to be used for the partial DTensor to produce Replicated/Sharded DTensor. Only element-wise reduction operations are supported, including: “sum”, “avg”, “product”, “max”, “min”, default: “sum”.
Note
The `Partial` placement can be generated as a result of the DTensor operators, and can only be used by the `DTensor.from_local` API. 

reduce_op _: str_ _= 'sum'_


_class_ torch.distributed.tensor.placement_types.Placement[source][source]
    
The base class for the Placement type, where it describes how a DTensor is placed onto the `DeviceMesh`. `Placement` and `DeviceMesh` together could describe the DTensor Layout. It is the base class of the three main DTensor Placement types: `Shard`, `Replicate`, and `Partial`.
This class is not meant to be used directly, mainly served as a typing stub. 

is_partial()[source][source]
     

Return type
    
bool 

is_replicate()[source][source]
     

Return type
    
bool 

is_shard(_dim =None_)[source][source]
     

Return type
    
bool
## Different ways to create a DTensor 

There’re three ways to construct a `DTensor`:
    
  * `distribute_tensor()` creates a `DTensor` from a logical or “global” `torch.Tensor` on each rank. This could be used to shard the leaf `torch.Tensor` s (i.e. model parameters/buffers and inputs).
  * `DTensor.from_local()` creates a `DTensor` from a local `torch.Tensor` on each rank, which can be used to create `DTensor` from a non-leaf `torch.Tensor` s (i.e. intermediate activation tensors during forward/backward).
  * DTensor provides dedicated tensor factory functions (e.g. `empty()`, `ones()`, `randn()`, etc.) to allow different `DTensor` creations by directly specifying the `DeviceMesh` and `Placement`. Compare to `distribute_tensor()`, this could directly materializing the sharded memory on device, instead of performing sharding after initializing the logical Tensor memory.


### Create DTensor from a logical torch.Tensor
The SPMD (single program, multiple data) programming model in `torch.distributed` launches multiple processes (i.e. via `torchrun`) to execute the same program, this means that the model inside the program would be initialized on different processes first (i.e. the model might be initialized on CPU, or meta device, or directly on GPU if enough memory).
`DTensor` offers a `distribute_tensor()` API that could shard the model weights or Tensors to `DTensor` s, where it would create a DTensor from the “logical” Tensor on each process. This would empower the created `DTensor` s to comply with the single device semantic, which is critical for **numerical correctness**. 

torch.distributed.tensor.distribute_tensor(_tensor_ , _device_mesh =None_, _placements =None_, _*_ , _src_data_rank =0_)[source]
    
Distribute a leaf `torch.Tensor` (i.e. nn.Parameter/buffers) to the `device_mesh` according to the `placements` specified. The rank of `device_mesh` and `placements` must be the same. The `tensor` to distribute is the logical or “global” tensor, and the API would use the `tensor` from first rank of the DeviceMesh dimension as the source of truth to preserve the single-device semantic. If you want to construct a DTensor in the middle of the Autograd computation, please use `DTensor.from_local()` instead. 

Parameters
    
  * **tensor** (_torch.Tensor_) – torch.Tensor to be distributed. Note that if you want to shard a tensor on a dimension that is not evenly divisible by the number of devices in that mesh dimension, we use `torch.chunk` semantic to shard the tensor and scatter the shards. The uneven sharding behavior is experimental and subject to change.
  * **device_mesh** (`DeviceMesh`, optional) – DeviceMesh to distribute the tensor, if not specified, must be called under a DeviceMesh context manager, default: None
  * **placements** (List[`Placement`], optional) – the placements that describes how to place the tensor on DeviceMesh, must have the same number of elements as `device_mesh.ndim`. If not specified, we will by default replicate the tensor across the `device_mesh` from the first rank of each dimension of the device_mesh.



Keyword Arguments
    
**src_data_rank** (_int_ _,__optional_) – the rank of the source data for the logical/global tensor, it is used by `distribute_tensor()` to scatter/broadcast the shards/replicas to other ranks. By default, we use `group_rank=0` on each DeviceMesh dimension as the source data to preserve the single-device semantic. If passing `None` explicitly, `distribute_tensor()` simply uses its local data instead of trying to preserve the single-device semantic via scatter/broadcast. Default: 0 

Returns
    
A `DTensor` or `XLAShardedTensor` object. 

Return type
    
_DTensor_
Note
When initialize the DeviceMesh with the `xla` device_type, `distribute_tensor` return XLAShardedTensor instead. see this issue for more details. The XLA integration is experimental and subject to change.
Along with `distribute_tensor()`, DTensor also offers a `distribute_module()` API to allow easier sharding on the `nn.Module` level 

torch.distributed.tensor.distribute_module(_module_ , _device_mesh =None_, _partition_fn =None_, _input_fn =None_, _output_fn =None_)[source]
    
This function expose three functions to control the parameters/inputs/outputs of the module:
1. To perform sharding on the module before runtime execution by specifying the `partition_fn` (i.e. allow user to convert Module parameters to `DTensor` parameters according to the partition_fn specified). 2. To control the inputs or outputs of the module during runtime execution by specifying the `input_fn` and `output_fn`. (i.e. convert the input to `DTensor`, convert the output back to `torch.Tensor`) 

Parameters
    
  * **module** (`nn.Module`) – user module to be partitioned.
  * **device_mesh** (`DeviceMesh`) – the device mesh to place the module.
  * **partition_fn** (_Callable_) – the function to partition parameters (i.e. shard certain parameters across the `device_mesh`). If `partition_fn` is not specified, by default we replicate all module parameters of `module` across the mesh.
  * **input_fn** (_Callable_) – specify the input distribution, i.e. could control how the input of the module is sharded. `input_fn` will be installed as a module `forward_pre_hook` (pre forward hook).
  * **output_fn** (_Callable_) – specify the output distribution, i.e. could control how the output is sharded, or convert it back to torch.Tensor. `output_fn` will be installed as a module `forward_hook` (post forward hook).



Returns
    
A module that contains parameters/buffers that are all `DTensor` s. 

Return type
    
_Module_
Note
When initialize the DeviceMesh with the `xla` device_type, `distribute_module` return nn.Module with PyTorch/XLA SPMD annotated parameters. See this issue for more details. The XLA integration is experimental and subject to change.
### DTensor Factory Functions
DTensor also provides dedicated tensor factory functions to allow creating `DTensor` directly using torch.Tensor like factory function APIs (i.e. torch.ones, torch.empty, etc), by additionally specifying the `DeviceMesh` and `Placement` for the `DTensor` created: 

torch.distributed.tensor.zeros(_* size_, _requires_grad =False_, _dtype =None_, _layout =torch.strided_, _device_mesh =None_, _placements =None_)[source]
    
Returns a `DTensor` filled with the scalar value 0. 

Parameters
    
**size** (_int_ _..._) – a sequence of integers defining the shape of the output `DTensor`. Can be a variable number of arguments or a collection like a list or tuple. E.g.: zeros(1,2,3..) or zeros([1,2,3..]) or zeros((1,2,3..)) 

Keyword Arguments
    
  * **requires_grad** (_bool_ _,__optional_) – If autograd should record operations on the returned `DTensor`. Default: `False`.
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned `DTensor`. Default: if `None`, uses a global default (see `torch.set_default_dtype()`).
  * **layout** (`torch.layout`, optional) – the desired layout of returned `DTensor`. Default: `torch.strided`.
  * **device_mesh** – `DeviceMesh` type, contains the mesh info of ranks
  * **placements** – a sequence of `Placement` type: `Shard`, `Replicate`



Returns
    
A `DTensor` object on each rank 

Return type
    
_DTensor_ 

torch.distributed.tensor.ones(_* size_, _dtype =None_, _layout =torch.strided_, _requires_grad =False_, _device_mesh =None_, _placements =None_)[source]
    
Returns a `DTensor` filled with the scalar value 1, with the shape defined by the variable argument `size`. 

Parameters
    
**size** (_int_ _..._) – a sequence of integers defining the shape of the output `DTensor`. Can be a variable number of arguments or a collection like a list or tuple. E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..)) 

Keyword Arguments
    
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned `DTensor`. Default: if `None`, uses a global default (see `torch.set_default_dtype()`).
  * **layout** (`torch.layout`, optional) – the desired layout of returned DTensor. Default: `torch.strided`.
  * **requires_grad** (_bool_ _,__optional_) – If autograd should record operations on the returned `DTensor`. Default: `False`.
  * **device_mesh** – `DeviceMesh` type, contains the mesh info of ranks
  * **placements** – a sequence of `Placement` type: `Shard`, `Replicate`



Returns
    
A `DTensor` object on each rank 

Return type
    
_DTensor_ 

torch.distributed.tensor.empty(_* size_, _dtype =None_, _layout =torch.strided_, _requires_grad =False_, _device_mesh =None_, _placements =None_)[source]
    
Returns a `DTensor` filled with uninitialized data. The shape of the `DTensor` is defined by the variable argument `size`. 

Parameters
    
**size** (_int_ _..._) – a sequence of integers defining the shape of the output `DTensor`. Can be a variable number of arguments or a collection like a list or tuple. E.g.: empty(1,2,3..) or empty([1,2,3..]) or empty((1,2,3..)) 

Keyword Arguments
    
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned `DTensor`. Default: if `None`, uses a global default (see `torch.set_default_dtype()`). layout (`torch.layout`, optional): the desired layout of returned `DTensor`. Default: `torch.strided`.
  * **requires_grad** (_bool_ _,__optional_) – If autograd should record operations on the returned `DTensor`. Default: `False`.
  * **device_mesh** – `DeviceMesh` type, contains the mesh info of ranks
  * **placements** – a sequence of `Placement` type: `Shard`, `Replicate`



Returns
    
A `DTensor` object on each rank 

Return type
    
_DTensor_ 

torch.distributed.tensor.full(_size_ , _fill_value_ , _*_ , _dtype =None_, _layout =torch.strided_, _requires_grad =False_, _device_mesh =None_, _placements =None_)[source]
    
Returns a `DTensor` filled with `fill_value` according to `device_mesh` and `placements`, with the shape defined by the argument `size`. 

Parameters
    
  * **size** (_int_ _..._) – a sequence of integers defining the shape of the output `DTensor`. Can be a variable number of arguments or a collection like a list or tuple. E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))
  * **fill_value** (_Scalar_) – the value to fill the output tensor with.



Keyword Arguments
    
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned `DTensor`. Default: if `None`, uses a global default (see `torch.set_default_dtype()`).
  * **layout** (`torch.layout`, optional) – the desired layout of returned DTensor. Default: `torch.strided`.
  * **requires_grad** (_bool_ _,__optional_) – If autograd should record operations on the returned `DTensor`. Default: `False`.
  * **device_mesh** – `DeviceMesh` type, contains the mesh info of ranks.
  * **placements** – a sequence of `Placement` type: `Shard`, `Replicate`



Returns
    
A `DTensor` object on each rank 

Return type
    
_DTensor_ 

torch.distributed.tensor.rand(_* size_, _requires_grad =False_, _dtype =None_, _layout =torch.strided_, _device_mesh =None_, _placements =None_)[source]
    
Returns a `DTensor` filled with random numbers from a uniform distribution on the interval `[0, 1)`. The shape of the tensor is defined by the variable argument `size`. 

Parameters
    
**size** (_int_ _..._) – a sequence of integers defining the shape of the output `DTensor`. Can be a variable number of arguments or a collection like a list or tuple. E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..)) 

Keyword Arguments
    
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned `DTensor`. Default: if `None`, uses a global default (see `torch.set_default_dtype()`).
  * **layout** (`torch.layout`, optional) – the desired layout of returned DTensor. Default: `torch.strided`.
  * **requires_grad** (_bool_ _,__optional_) – If autograd should record operations on the returned `DTensor`. Default: `False`.
  * **device_mesh** – `DeviceMesh` type, contains the mesh info of ranks.
  * **placements** – a sequence of `Placement` type: `Shard`, `Replicate`



Returns
    
A `DTensor` object on each rank 

Return type
    
_DTensor_ 

torch.distributed.tensor.randn(_* size_, _requires_grad =False_, _dtype =None_, _layout =torch.strided_, _device_mesh =None_, _placements =None_)[source]
    
Returns a `DTensor` filled with random numbers from a normal distribution with mean 0 and variance 1. The shape of the tensor is defined by the variable argument `size`. 

Parameters
    
**size** (_int_ _..._) – a sequence of integers defining the shape of the output `DTensor`. Can be a variable number of arguments or a collection like a list or tuple. E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..)) 

Keyword Arguments
    
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned `DTensor`. Default: if `None`, uses a global default (see `torch.set_default_dtype()`).
  * **layout** (`torch.layout`, optional) – the desired layout of returned DTensor. Default: `torch.strided`.
  * **requires_grad** (_bool_ _,__optional_) – If autograd should record operations on the returned `DTensor`. Default: `False`.
  * **device_mesh** – `DeviceMesh` type, contains the mesh info of ranks.
  * **placements** – a sequence of `Placement` type: `Shard`, `Replicate`



Returns
    
A `DTensor` object on each rank 

Return type
    
_DTensor_
## Debugging
### Logging
When launching the program, you can turn on additional logging using the TORCH_LOGS environment variable from torch._logging :
  * TORCH_LOGS=+dtensor will display logging.DEBUG messages and all levels above it.
  * TORCH_LOGS=dtensor will display logging.INFO messages and above.
  * TORCH_LOGS=-dtensor will display logging.WARNING messages and above.


### Debugging Tools
To debug the program that applied DTensor, and understand more details about what collectives happened under the hood, DTensor provides a `CommDebugMode`: 

_class_ torch.distributed.tensor.debug.CommDebugMode
    
`CommDebugMode` is a context manager that counts the number of functional collectives within its context. It does this using a `TorchDispatchMode`.
Note
Not all collectives are supported yet.
Example usage
```
mod = ...
comm_mode = CommDebugMode()
with comm_mode:
  mod.sum().backward()
print(comm_mode.get_comm_counts())

```
Copy to clipboard 

generate_comm_debug_tracing_table(_noise_level =3_)[source][source]
    
Generates detailed table displaying operations and collective tracing information on a module level. Amount of information is dependent on noise_level
  1. prints module-level collective counts
  2. prints dTensor operations not included in trivial operations, module information
  3. prints operations not included in trivial operations
  4. prints all operations



generate_json_dump(_file_name ='comm_mode_log.json'_, _noise_level =3_)[source][source]
    
Creates json file used to build browser visual 0. prints module-level collective counts 1. prints dTensor operations not included in trivial operations 2. prints operations not included in trivial operations 3. prints all operations 

get_comm_counts()[source][source]
    
Returns the communication counts as a dictionary. 

Returns
    
The communication counts as a dictionary. 

Return type
    
Dict[Any, int] 

get_parameter_info()[source][source]
     

Return type
    
dict[str, dict[str, _Any_]] 

get_sharding_info()[source][source]
     

Return type
    
dict[str, dict[str, _Any_]] 

get_total_counts()[source][source]
     

Return type
    
int 

log_comm_debug_tracing_table_to_file(_file_name ='comm_mode_log.txt'_, _noise_level =3_)[source][source]
    
Alternative to console CommDebugMode output, writes to file specified by the user
To visualize the sharding of a DTensor that have less than 3 dimensions, DTensor provides `visualize_sharding()`: 

torch.distributed.tensor.debug.visualize_sharding(_dtensor_ , _header =''_)[source]
    
Visualizes sharding in the terminal for `DTensor` that are 1D or 2D.
Note
This requires the `tabulate` package. No sharding info will be printed for empty tensors
## Experimental Features
`DTensor` also provides a set of experimental features. These features are either in prototyping stage, or the basic functionality is done and but looking for user feedbacks. Please submit a issue to PyTorch if you have feedbacks to these features. 

torch.distributed.tensor.experimental.context_parallel(_mesh_ , _*_ , _buffers =None_, _buffer_seq_dims =None_, _no_restore_buffers =None_)[source]
    
`context_parallel` is an experimental API to enable context parallelism (CP). This API performs two actions: 1) patch the SDPA (`torch.nn.functional.scaled_dot_product_attention`) with the CP-enabled one, 2) shard `buffers` along the sequence dimension and each rank will preserve the corresponding shard according `mesh`. 

Parameters
    
  * **mesh** (`DeviceMesh`) – the device mesh for the context parallelism.
  * **buffers** (_Optional_ _[__List_ _[__torch.Tensor_ _]__]_) – buffers that the usage depend on the sequence dimension. Examples are input batch, labels and positional embedding buffers. These buffers must be sharded along the sequence dimension to ensure the accuracy. The sharding will happen in-place, the buffer’s shape will change within the context. The buffers will be restored after the context finishes. `no_restore_buffers` can be used to specify which buffers don’t need to be restored. Note that `buffers` should not contain any nn.Parameter.
  * **buffer_seq_dims** (_Optional_ _[__List_ _[__int_ _]__]_) – the sequence dimensions of `buffers`.
  * **no_restore_buffers** (_Optional_ _[__Set_ _[__torch.Tensor_ _]__]_) – buffers in these set won’t be restored after the context exits. This set must be a subset of `buffers`. If the buffers won’t be used after the context exits, these buffers can be put in this list to avoid extra restore time.



Return type
    
_Generator_[None, None, None]
Warning
torch.distributed._tensor.experimental.attention.context_parallel is a prototype feature in PyTorch. The API is subject to change. 

torch.distributed.tensor.experimental.local_map(_func_ , _out_placements_ , _in_placements =None_, _device_mesh =None_, _*_ , _redistribute_inputs =False_)[source]
    
`local_map()` is an experimental API that allows users to pass `DTensor` s to a function that is written to be applied on `torch.Tensor` s. It is done by extracting the local components of `DTensor`, call the function, and wrap the outputs to `DTensor` according to the `out_placements`. 

Parameters
    
  * **func** (_Callable_) – the function to be applied on each local shard of `DTensor` s.
  * **out_placements** (Union[PlacementType, Tuple[PlacementType, …]]) – the desired placements of the `DTensor` s in `func`’s flattened output. If the flattened `output` is a single value, the `out_placements` should be of type PlacementType. Otherwise if the flattened `output` has multiple values, the `out_placements` should be a tuple of PlacementType values 1:1 mapping to the flattened `output`. Besides, for `Tensor` output, we use PlacementType as its placements (a Tuple[Placement] value). For non-Tensor output, the PlacementType should be None. Note that the only exception is when no `DTensor` argument is passed in. In this case, even if out_placements is not None, the result function should ignore the desired placements because the function is not running with `DTensor` s.
  * **in_placements** (Tuple[PlacementType, …], optional) – the required placements of the `DTensor` s in the flattened inputs of `func`. If `in_placements` is specified, `local_map()` would examine whether the placements of each `DTensor` argument is the same as the required placements or not. If the placements are not the same and `redistribute_inputs` is `False`, an exception will be raised. Otherwise if `redistribute_inputs` is `True`, the argument will be first redistributed to the required sharding placements before passing its local tensor to `func`. The only exception is when required placements are not `None` and the argument is a `torch.Tensor`. In this case, the placements examination will be skipped and the argument will be directly passed to `func`. If `in_placements` is `None`, no placements examination will be performed. Default: None
  * **device_mesh** (`DeviceMesh`, optional) – the device mesh that all the `DTensor` s are placed on. If not specified, this will be inferred from the input `DTensor` s’ device mesh. local_map requires every `DTensor` s to be placed on the same device mesh. Default: None.
  * **redistribute_inputs** (_bool_ _,__optional_) – the bool value indicating whether to reshard the input `DTensor` s when their placements are different from the required input placements. If this value is `False` and some `DTensor` input has a different placement, an exception will be raised. Default: False.



Returns
    
A `Callable` that applies `func` to each local shard of the input `DTensor` and returns a `DTensor` constructed from the return value of `func`. 

Raises
    
  * **AssertionError** – If the input `DTensor` is not placed on the same device mesh, or if they are placed on a different device mesh than the `device_mesh` argument passed in.
  * **AssertionError** – For any non-DTensor output, we require its corresponding output placement in `out_placements` be None. An AssertionError will be raised if this is not the case.
  * **ValueError** – If `redistribute_inputs=False` but the input `DTensor` needs a redistribution according to `in_placements`.


Example
```
>>> def mm_allreduce_forward(device_mesh, W, X):
>>>   partial_sum_tensor = torch.mm(W, X)
>>>   reduced_tensor = funcol.all_reduce(partial_sum_tensor, "sum", device_mesh)
>>>   return reduced_tensor
>>>
>>> W = torch.randn(12, 8, requires_grad=False)
>>> X = torch.randn(8, 16, requires_grad=False)
>>> Y = torch.mm(W, X)
>>> row_wise = [Shard(0)] # row-wise sharding placements on 1-d mesh
>>> col_wise = [Shard(1)] # col-wise sharding placements on 1-d mesh
>>>
>>> # local_mm_allreduce_forward is the function wrapped with DTensor/Tensor convertion
>>> local_mm_allreduce_forward = local_map(
>>>   mm_allreduce_forward,
>>>   out_placements=[Replicate()],
>>>   in_placements=[col_wise, row_wise],
>>>   device_mesh=device_mesh,
>>> )
>>>
>>> W_dt = distribute_tensor(
...   W, device_mesh, (col_wise)
... ) # col-wisely sharded W tensor
>>> X_dt = distribute_tensor(
...   X, device_mesh, (row_wise)
... ) # row-wisely sharded X tensor
>>> Y_dt = local_mm_allreduce_forward(
...   device_mesh, W_dt, X_dt
... ) # apply local_mm_allreduce_forward to DTensors

```
Copy to clipboard
Note
This API is currently experimental and subject to change 

torch.distributed.tensor.experimental.register_sharding(_op_)[source]
    
`register_sharding()` is an experimental API that allows users to register sharding strategies for an operator when the tensor inputs and outputs are DTensor. It can be useful when: (1) there doesn’t exist a default sharding strategy for `op`, e.g. when `op` is a custom operator that is not supported by `DTensor`; (2) when users would like to overwrite default sharding strategies of existing operators. 

Parameters
    
**op** (_Union_ _[__OpOverload_ _,__List_ _[__OpOverload_ _]__]_) – An op or a list of ops to register the customized sharding function. 

Returns
    
A function decorator which can be used to wrap a function that defines the sharding strategy for the operator specified in `op`. The defined sharding strategy will be registered to DTensor and will override the default sharding strategy if DTensor has already implemented the operator. The customized sharding function takes the same inputs as the original op (except that if an arg is a `torch.Tensor`, it will be replaced by a tensor-like object that DTensor uses internally). The function should return a sequence of 2-tuples, each specifying acceptable output placements and its corresponding intput placements.
Example
```
>>> @register_sharding(aten._softmax.default)
>>> def custom_softmax_sharding(x, dim, half_to_float):
>>>   softmax_dim = dim if dim >= 0 else dim + x.ndim
>>>   acceptable_shardings = []
>>>
>>>   all_replicate = ([Replicate()], [Replicate(), None, None])
>>>   acceptable_shardings.append(all_replicate)
>>>
>>>   for sharding_dim in range(x.ndim):
>>>     if sharding_dim != softmax_dim:
>>>       all_sharded = (
>>>         [Shard(sharding_dim)],
>>>         [Shard(sharding_dim), None, None],
>>>       )
>>>       acceptable_shardings.append(all_sharded)
>>>
>>>   return acceptable_shardings

```
Copy to clipboard
Note
This API is currently experimental and subject to change
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.distributed.tensor
    * PyTorch DTensor (Distributed Tensor)
      * DTensor Class APIs
        * `DTensor`
          * `DTensor.__create_chunk_list__()`
          * `DTensor.from_local()`
          * `DTensor.full_tensor()`
          * `DTensor.redistribute()`
          * `DTensor.to_local()`
          * `DTensor.device_mesh`
          * `DTensor.placements`
      * DeviceMesh as the distributed communicator
      * DTensor Placement Types
        * `Shard`
          * `Shard.dim`
        * `Replicate`
        * `Partial`
          * `Partial.reduce_op`
        * `Placement`
          * `Placement.is_partial()`
          * `Placement.is_replicate()`
          * `Placement.is_shard()`
    * Different ways to create a DTensor
      * Create DTensor from a logical torch.Tensor
        * `distribute_tensor()`
        * `distribute_module()`
      * DTensor Factory Functions
        * `zeros()`
        * `ones()`
        * `empty()`
        * `full()`
        * `rand()`
        * `randn()`
    * Debugging
      * Logging
      * Debugging Tools
        * `CommDebugMode`
          * `CommDebugMode.generate_comm_debug_tracing_table()`
          * `CommDebugMode.generate_json_dump()`
          * `CommDebugMode.get_comm_counts()`
          * `CommDebugMode.get_parameter_info()`
          * `CommDebugMode.get_sharding_info()`
          * `CommDebugMode.get_total_counts()`
          * `CommDebugMode.log_comm_debug_tracing_table_to_file()`
        * `visualize_sharding()`
    * Experimental Features
      * `context_parallel()`
      * `local_map()`
      * `register_sharding()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.fft
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.fft
Discrete Fourier transforms and related functions.
## Fast Fourier Transforms
`fft` | Computes the one dimensional discrete Fourier transform of `input`.  
---|---  
`ifft` | Computes the one dimensional inverse discrete Fourier transform of `input`.  
`fft2` | Computes the 2 dimensional discrete Fourier transform of `input`.  
`ifft2` | Computes the 2 dimensional inverse discrete Fourier transform of `input`.  
`fftn` | Computes the N dimensional discrete Fourier transform of `input`.  
`ifftn` | Computes the N dimensional inverse discrete Fourier transform of `input`.  
`rfft` | Computes the one dimensional Fourier transform of real-valued `input`.  
`irfft` | Computes the inverse of `rfft()`.  
`rfft2` | Computes the 2-dimensional discrete Fourier transform of real `input`.  
`irfft2` | Computes the inverse of `rfft2()`.  
`rfftn` | Computes the N-dimensional discrete Fourier transform of real `input`.  
`irfftn` | Computes the inverse of `rfftn()`.  
`hfft` | Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal.  
`ihfft` | Computes the inverse of `hfft()`.  
`hfft2` | Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric `input` signal.  
`ihfft2` | Computes the 2-dimensional inverse discrete Fourier transform of real `input`.  
`hfftn` | Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric `input` signal.  
`ihfftn` | Computes the N-dimensional inverse discrete Fourier transform of real `input`.  
## Helper Functions
`fftfreq` | Computes the discrete Fourier Transform sample frequencies for a signal of size `n`.  
---|---  
`rfftfreq` | Computes the sample frequencies for `rfft()` with a signal of size `n`.  
`fftshift` | Reorders n-dimensional FFT data, as provided by `fftn()`, to have negative frequency terms first.  
`ifftshift` | Inverse of `fftshift()`.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.fft
    * Fast Fourier Transforms
    * Helper Functions


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Tensor Parallelism - torch.distributed.tensor.parallel
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Tensor Parallelism - torch.distributed.tensor.parallel
Tensor Parallelism(TP) is built on top of the PyTorch DistributedTensor (DTensor) and provides different parallelism styles: Colwise, Rowwise, and Sequence Parallelism.
Warning
Tensor Parallelism APIs are experimental and subject to change.
The entrypoint to parallelize your `nn.Module` using Tensor Parallelism is: 

torch.distributed.tensor.parallel.parallelize_module(_module_ , _device_mesh =None_, _parallelize_plan =None_, _*_ , _src_data_rank =0_)[source][source]
    
Apply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules based on a user-specified plan.
We parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan contains `ParallelStyle`, which indicates how user wants the module or sub_module to be parallelized.
User can also specify different parallel style per module fully qualified name (FQN).
Note that `parallelize_module` only accepts a 1-D `DeviceMesh`, if you have a 2-D or N-D `DeviceMesh`, slice the DeviceMesh to a 1-D sub DeviceMesh first then pass to this API(i.e. `device_mesh["tp"]`) 

Parameters
    
  * **module** (`nn.Module`) – Module to be parallelized.
  * **device_mesh** (`DeviceMesh`, optional) – Object which describes the mesh topology of devices for the DTensor. If not specified, the call must be under a DeviceMesh context.
  * **parallelize_plan** (Union[`ParallelStyle`, Dict[str, `ParallelStyle`]], optional) – The plan used to parallelize the module. It can be either a `ParallelStyle` object which contains how we prepare input/output for Tensor Parallelism or it can be a dict of module FQN and its corresponding `ParallelStyle` object. If not specified, the call will do nothing at the moment.



Keyword Arguments
    
**src_data_rank** (_int_ _,__optional_) – the rank of the source data for the logical/global tensor, it is used by `distribute_tensor()` to scatter/broadcast the shards/replicas to other ranks. By default, we use `group_rank=0` on each DeviceMesh dimension as the source data to preserve the single-device semantic. If passing `None` explicitly, `parallelize_module()` simply uses its local data instead of trying to preserve the single-device semantic via scatter/broadcast. Default: 0 

Returns
    
A `nn.Module` object parallelized. 

Return type
    
_Module_ 

Example::
    
```
>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel
>>> from torch.distributed.device_mesh import init_device_mesh
>>>
>>> # Define the module.
>>> m = Model(...)
>>> tp_mesh = init_device_mesh("cuda", (8,))
>>> m = parallelize_module(m, tp_mesh, {"w1": ColwiseParallel(), "w2": RowwiseParallel()})
>>>

```
Copy to clipboard
Note
For complex module architecture like Attention, MLP layers, we recommend composing different ParallelStyles together (i.e. `ColwiseParallel` and `RowwiseParallel`) and pass as a parallelize_plan, to achieves the desired sharding computation.
Tensor Parallelism supports the following parallel styles: 

_class_ torch.distributed.tensor.parallel.ColwiseParallel(_*_ , _input_layouts =None_, _output_layouts =None_, _use_local_output =True_)[source][source]
    
Partition a compatible nn.Module in a column-wise fashion. Currently supports nn.Linear and nn.Embedding. Users can compose it together with RowwiseParallel to achieve the sharding of more complicated modules. (i.e. MLP, Attention) 

Keyword Arguments
    
  * **input_layouts** (_Placement_ _,__optional_) – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to become a DTensor. If not specified, we assume the input tensor to be replicated.
  * **output_layouts** (_Placement_ _,__optional_) – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module with the user desired layout. If not specified, the output tensor is sharded on the last dimension.
  * **use_local_output** (_bool_ _,__optional_) – Whether to use local `torch.Tensor` instead of `DTensor` for the module output, default: True.



Returns
    
A `ParallelStyle` object that represents Colwise sharding of the nn.Module. 

Example::
    
```
>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel
>>> from torch.distributed.device_mesh import init_device_mesh
>>> ...
>>> m = Model(...) # m is a nn.Module that contains a "w1" nn.Linear submodule
>>> tp_mesh = init_device_mesh("cuda", (8,))
>>>
>>> # By default, the input of the "w1" Linear will be converted to Replicated DTensor
>>> # and the output of "w1" will return :class:`torch.Tensor` that shards on the last dim.
>>>
>>> sharded_mod = parallelize_module(m, tp_mesh, {"w1": ColwiseParallel()})
>>> ...

```
Copy to clipboard
Note
By default `ColwiseParallel` output is sharded on the last dimension if the `output_layouts` not specified, if there’re operators that require specific tensor shape (i.e. before the paired `RowwiseParallel`), keep in mind that if the output is sharded the operator might need to be adjusted to the sharded size. 

_class_ torch.distributed.tensor.parallel.RowwiseParallel(_*_ , _input_layouts =None_, _output_layouts =None_, _use_local_output =True_)[source][source]
    
Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding. Users can compose it with ColwiseParallel to achieve the sharding of more complicated modules. (i.e. MLP, Attention) 

Keyword Arguments
    
  * **input_layouts** (_Placement_ _,__optional_) – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to become a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.
  * **output_layouts** (_Placement_ _,__optional_) – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module with the user desired layout. If not specified, the output tensor is replicated.
  * **use_local_output** (_bool_ _,__optional_) – Whether to use local `torch.Tensor` instead of `DTensor` for the module output, default: True.



Returns
    
A `ParallelStyle` object that represents Rowwise sharding of the nn.Module. 

Example::
    
```
>>> from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel
>>> from torch.distributed.device_mesh import init_device_mesh
>>> ...
>>> m = Model(...) # m is a nn.Module that contains a "w2" nn.Linear submodule
>>> tp_mesh = init_device_mesh("cuda", (8,))
>>>
>>> # By default, the input of the "w2" Linear will be converted to DTensor that shards on the last dim
>>> # and the output of "w2" will return a replicated :class:`torch.Tensor`.
>>>
>>> sharded_mod = parallelize_module(m, tp_mesh, {"w2": RowwiseParallel()}),
>>> ...

```
Copy to clipboard 

_class_ torch.distributed.tensor.parallel.SequenceParallel(_*_ , _sequence_dim =1_, _use_local_output =False_)[source][source]
    
SequenceParallel replicates a compatible `nn.Module` parameters and runs the sharded computation with input sharded on the sequence dimension. This currently supports `nn.LayerNorm`, `nn.Dropout`, and the RMSNorm python implementation
This style implements the operation that is described in the paper Reducing Activation Recomputation in Large Transformer Models
If the input passed in to this `nn.Module` is a `torch.Tensor`, it assumes that the input is already sharded on the sequence dimension and converts the input to a `DTensor` sharded on the sequence dimension. If the input passed in to this `nn.Module` is already a `DTensor` but is not sharded on the sequence dimension, it would redistribute the input to be sharded on the sequence dimension.
The output of the `nn.Module` will be sharded on the sequence dimension. 

Keyword Arguments
    
  * **sequence_dim** (_int_ _,__optional_) – The sequence dimension of the input tensor for the `nn.Module`, this is used to annotate the input tensor to become a DTensor that is sharded on the sequence dimension, default: 1.
  * **use_local_output** (_bool_ _,__optional_) – Whether to use local `torch.Tensor` instead of `DTensor` for the module output, default: False.



Returns
    
A `ParallelStyle` object that represents Sequence Parallel of the `nn.Module`. 

Example::
    
```
>>> from torch.distributed.tensor.parallel import parallelize_module, SequenceParallel
>>> from torch.distributed.device_mesh import init_device_mesh
>>> ...
>>> m = Model(...) # m is a nn.Module that contains a "norm" nn.LayerNorm submodule
>>> tp_mesh = init_device_mesh("cuda", (8,))
>>>
>>> # By default, the input of the "norm" will be converted to DTensor that shards on the sequence dim
>>> # and the output of "norm" will return a sharded on sequence dimension :class:`DTensor`.
>>>
>>> sharded_mod = parallelize_module(m, tp_mesh, {"norm": SequenceParallel()}),
>>> ...

```
Copy to clipboard
Note
SequenceParallel style assumes ones initialization if there are weights in the nn.Module (i.e. `nn.LayerNorm` or `RMSNorm`, and they by default have ones initialization). If you have custom inits for the weights on those modules, you need to broadcast the weights before/after parallelizing to ensure that they are replicated.
To simply configure the nn.Module’s inputs and outputs with DTensor layouts and perform necessary layout redistributions, without distribute the module parameters to DTensors, the following `ParallelStyle` s can be used in the `parallelize_plan` when calling `parallelize_module`: 

_class_ torch.distributed.tensor.parallel.PrepareModuleInput(_*_ , _input_layouts =None_, _desired_input_layouts =None_, _input_kwarg_layouts =None_, _desired_input_kwarg_layouts =None_, _use_local_output =False_)[source][source]
    
Configure the nn.Module’s inputs to convert the input tensors of the nn.Module to DTensors at runtime according to `input_layouts`, and perform layout redistribution according to the `desired_input_layouts`. 

Keyword Arguments
    
  * **input_layouts** (_Union_ _[__Placement_ _,__Tuple_ _[__Optional_ _[__Placement_ _]__]__]_) – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors, `None` need to be specified as a placeholder. default: None.
  * **desired_input_layouts** (_Union_ _[__Placement_ _,__Tuple_ _[__Optional_ _[__Placement_ _]__]__]_) – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module have the desired DTensor layouts. This argument needs to have the same length with `input_layouts`. default: None.
  * **input_kwarg_layouts** (_Dict_ _[__str_ _,__Placement_ _]_) – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors. default: None
  * **desired_input_kwarg_layouts** – (Dict[str, Placement]): The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module have the desired DTensor layouts. default: None.
  * **use_local_output** (_bool_ _,__optional_) – Whether to use local `torch.Tensor` instead of `DTensor` for the module inputs, default: False.



Returns
    
A `ParallelStyle` object that prepares the sharding layouts of the nn.Module’s inputs. 

Example::
    
```
>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput
>>> from torch.distributed.device_mesh import init_device_mesh
>>> ...
>>> block = TransformerBlock(...) # block is a nn.Module that contains an "attn" Attention submodule
>>> tp_mesh = init_device_mesh("cuda", (8,))
>>>
>>> # According to the style specified below, the first input of attn will be annotated to Sharded DTensor
>>> # and then redistributed to Replicated DTensor.
>>> parallelize_module(
>>>   block, # this can be a submodule or module
>>>   tp_mesh,
>>>   parallelize_plan={
>>>     "attn": PrepareModuleInput(
>>>       input_layouts=(Shard(0), None, None, ...),
>>>       desired_input_layouts=(Replicate(), None, None, ...)
>>>     ),
>>>   }
>>> )

```
Copy to clipboard 

_class_ torch.distributed.tensor.parallel.PrepareModuleOutput(_*_ , _output_layouts_ , _desired_output_layouts_ , _use_local_output =True_)[source][source]
    
Configure the nn.Module’s outputs to convert the output tensors of the nn.Module to DTensors at runtime according to `output_layouts`, and perform layout redistribution according to the `desired_output_layouts`. 

Keyword Arguments
    
  * **output_layouts** (_Union_ _[__Placement_ _,__Tuple_ _[__Placement_ _]__]_) – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to DTensors if they are `torch.Tensor`. If some outputs are not torch.Tensor or no need to convert to DTensors, `None` need to be specified as a placeholder.
  * **desired_output_layouts** (_Union_ _[__Placement_ _,__Tuple_ _[__Placement_ _]__]_) – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module have the desired DTensor layouts.
  * **use_local_output** (_bool_ _,__optional_) – Whether to use local `torch.Tensor` instead of `DTensor` for the module outputs, default: True.



Returns
    
A ParallelStyle object that prepares the sharding layouts of the nn.Module’s outputs. 

Example::
    
```
>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleOutput
>>> from torch.distributed.device_mesh import init_device_mesh
>>> ...
>>> block = TransformerBlock(...) # block is a nn.Module that contains an "attn" Attention submodule
>>> tp_mesh = init_device_mesh("cuda", (8,))
>>>
>>> # According to the style specified below, the output of the TransformerBlock will be converted to Replicated DTensor
>>> # and then redistributed to Sharded DTensor.
>>> parallelize_module(
>>>   block, # this can be a submodule or module
>>>   tp_mesh,
>>>   parallelize_plan = PrepareModuleOutput(
>>>     output_layouts=Replicate(),
>>>     desired_output_layouts=Shard(0)
>>>   )
>>> )

```
Copy to clipboard
Note
when using the `Shard(dim)` as the input/output layouts for the above `ParallelStyle` s, we assume the input/output activation tensors are evenly sharded on the tensor dimension `dim` on the `DeviceMesh` that TP operates on. For instance, since `RowwiseParallel` accepts input that is sharded on the last dimension, it assumes the input tensor has already been evenly sharded on the last dimension. For the case of uneven sharded activation tensors, one could pass in DTensor directly to the partitioned modules, and use `use_local_output=False` to return DTensor after each `ParallelStyle`, where DTensor could track the uneven sharding information.
For models like Transformer, we recommend users to use `ColwiseParallel` and `RowwiseParallel` together in the parallelize_plan for achieve the desired sharding for the entire model (i.e. Attention and MLP).
Parallelized cross-entropy loss computation (loss parallelism), is supported via the following context manager: 

torch.distributed.tensor.parallel.loss_parallel()[source][source]
    
A context manager that enables loss parallelism, where efficient parallelized loss computation can be performed when the input is sharded on the class dimension. Currently only the cross-entropy loss is supported.
Within this context manager, one can use `cross_entropy()` or `CrossEntropyLoss` as usual, with the following assumptions on the input parameters. The corresponding `backward()` call, if any, also needs to happen under this context manager. 

Parameters
    
  * **input** (`DTensor`) – Input logits. Assumed to be sharded on the class dimension.
  * **target** (Union[`torch.Tensor`, `DTensor`]) – Must be ground truth class indices (class probabilities currently not supported). Assumed to be replicated across the `DeviceMesh`.
  * **weight** (Union[`torch.Tensor`, `DTensor`], optional) – If given, assumed to be replicated across the `DeviceMesh`.
  * **label_smoothing** – Currently not supported.



Returns
    
A replicated `DTensor`.
Example
A sharded DTensor is manually created here to showcase the usage. In practice, it is usually the output of a TP module.
```
>>> from torch.distributed.tensor.parallel import loss_parallel
>>> from torch.distributed.device_mesh import init_device_mesh
>>> ...
>>> device_mesh = init_device_mesh("cuda", (8,))
>>> input = torch.randn(4, 16, device="cuda", requires_grad=True)
>>> dist_input = distribute_tensor(input, device_mesh, placements=[Shard(1)])
>>> target = torch.randint(16, (4,), device="cuda")
>>> with loss_parallel():
>>>   loss = F.cross_entropy(dist_input, target, reduction="mean")
>>>   loss.backward()
>>> ...

```
Copy to clipboard
Warning
The loss_parallel API is experimental and subject to change.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Tensor Parallelism - torch.distributed.tensor.parallel
    * `parallelize_module()`
    * `ColwiseParallel`
    * `RowwiseParallel`
    * `SequenceParallel`
    * `PrepareModuleInput`
    * `PrepareModuleOutput`
    * `loss_parallel()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.func
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.func
torch.func, previously known as “functorch”, is JAX-like composable function transforms for PyTorch.
Note
This library is currently in beta. What this means is that the features generally work (unless otherwise documented) and we (the PyTorch team) are committed to bringing this library forward. However, the APIs may change under user feedback and we don’t have full coverage over PyTorch operations.
If you have suggestions on the API or use-cases you’d like to be covered, please open an GitHub issue or reach out. We’d love to hear about how you’re using the library.
## What are composable function transforms?
  * A “function transform” is a higher-order function that accepts a numerical function and returns a new function that computes a different quantity.
  * `torch.func` has auto-differentiation transforms (`grad(f)` returns a function that computes the gradient of `f`), a vectorization/batching transform (`vmap(f)` returns a function that computes `f` over batches of inputs), and others.
  * These function transforms can compose with each other arbitrarily. For example, composing `vmap(grad(f))` computes a quantity called per-sample-gradients that stock PyTorch cannot efficiently compute today.


## Why composable function transforms?
There are a number of use cases that are tricky to do in PyTorch today:
  * computing per-sample-gradients (or other per-sample quantities)
  * running ensembles of models on a single machine
  * efficiently batching together tasks in the inner-loop of MAML
  * efficiently computing Jacobians and Hessians
  * efficiently computing batched Jacobians and Hessians


Composing `vmap()`, `grad()`, and `vjp()` transforms allows us to express the above without designing a separate subsystem for each. This idea of composable function transforms comes from the JAX framework.
## Read More
  * torch.func Whirlwind Tour
    * What is torch.func?
    * Why composable function transforms?
    * What are the transforms?
  * torch.func API Reference
    * Function Transforms
    * Utilities for working with torch.nn.Modules
    * Debug utilities
  * UX Limitations
    * General limitations
    * torch.autograd APIs
    * vmap limitations
    * Randomness
  * Migrating from functorch to torch.func
    * function transforms
    * NN module utilities
    * functorch.compile


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.func
    * What are composable function transforms?
    * Why composable function transforms?
    * Read More


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Pipeline Parallelism
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Pipeline Parallelism
Note
`torch.distributed.pipelining` is currently in alpha state and under development. API changes may be possible. It was migrated from the PiPPy project.
## Why Pipeline Parallel?
Pipeline Parallelism is one of the **primitive** parallelism for deep learning. It allows the **execution** of a model to be partitioned such that multiple **micro-batches** can execute different parts of the model code concurrently. Pipeline parallelism can be an effective technique for:
  * large-scale training
  * bandwidth-limited clusters
  * large model inference


The above scenarios share a commonality that the computation per device cannot hide the communication of conventional parallelism, for example, the weight all-gather of FSDP.
## What is `torch.distributed.pipelining`?
While promising for scaling, pipelining is often difficult to implement because it needs to **partition the execution** of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from **scheduling micro-batches in a distributed environment** , with **data flow dependency** considered.
The `pipelining` package provides a toolkit that does said things **automatically** which allows easy implementation of pipeline parallelism on **general** models.
It consists of two parts: a **splitting frontend** and a **distributed runtime**. The splitting frontend takes your model code as-is, splits it up into “model partitions”, and captures the data-flow relationship. The distributed runtime executes the pipeline stages on different devices in parallel, handling things like micro-batch splitting, scheduling, communication, and gradient propagation, etc.
Overall, the `pipelining` package provides the following features:
  * Splitting of model code based on simple specification.
  * Rich support for pipeline schedules, including GPipe, 1F1B, Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing customized schedules.
  * First-class support for cross-host pipeline parallelism, as this is where PP is typically used (over slower interconnects).
  * Composability with other PyTorch parallel techniques such as data parallel (DDP, FSDP) or tensor parallel. The TorchTitan project demonstrates a “3D parallel” application on the Llama model.


## Step 1: build `PipelineStage`
Before we can use a `PipelineSchedule`, we need to create `PipelineStage` objects that wrap the part of the model running in that stage. The `PipelineStage` is responsible for allocating communication buffers and creating send/recv ops to communicate with its peers. It manages intermediate buffers e.g. for the outputs of forward that have not been consumed yet, and it provides a utility for running the backwards for the stage model.
A `PipelineStage` needs to know the input and output shapes for the stage model, so that it can correctly allocate communication buffers. The shapes must be static, e.g. at runtime the shapes can not change from step to step. A class `PipeliningShapeError` will be raised if runtime shapes do not match the expected shapes. When composing with other paralleisms or applying mixed precision, these techniques must be taken into account so the `PipelineStage` knows the correct shape (and dtype) for the output of the stage module at runtime.
Users may construct a `PipelineStage` instance directly, by passing in an `nn.Module` representing the portion of the model that should run on the stage. This may require changes to the original model code. See the example in Option 1: splitting a model manually.
Alternatively, the splitting frontend can use graph partitioning to split your model into a series of `nn.Module` automatically. This technique requires the model is traceable with `torch.Export`. Composability of the resulting `nn.Module` with other parallelism techniques is experimental, and may require some workarounds. Usage of this frontend may be more appealing if the user cannot easily change the model code. See Option 2: splitting a model automatically for more information.
## Step 2: use `PipelineSchedule` for execution
We can now attach the `PipelineStage` to a pipeline schedule, and run the schedule with input data. Here is a GPipe example:
```
from torch.distributed.pipelining import ScheduleGPipe
# Create a schedule
schedule = ScheduleGPipe(stage, n_microbatches)
# Input data (whole batch)
x = torch.randn(batch_size, in_dim, device=device)
# Run the pipeline with input `x`
# `x` will be divided into microbatches automatically
if rank == 0:
  schedule.step(x)
else:
  output = schedule.step()

```
Copy to clipboard
Note that the above code needs to be launched for each worker, thus we use a launcher service to launch multiple processes:
```
torchrun--nproc_per_node=2example.py

```
Copy to clipboard
## Options for Splitting a Model
### Option 1: splitting a model manually
To directly construct a `PipelineStage`, the user is responsible for providing a single `nn.Module` instance that owns the relevant `nn.Parameters` and `nn.Buffers`, and defines a `forward()` method that executes the operations relevant for that stage. For example, a condensed version of the Transformer class defined in Torchtitan shows a pattern of building an easily partitionable model.
```
class Transformer(nn.Module):
  def __init__(self, model_args: ModelArgs):
    super().__init__()
    self.tok_embeddings = nn.Embedding(...)
    # Using a ModuleDict lets us delete layers without affecting names,
    # ensuring checkpoints will correctly save and load.
    self.layers = torch.nn.ModuleDict()
    for layer_id in range(model_args.n_layers):
      self.layers[str(layer_id)] = TransformerBlock(...)
    self.output = nn.Linear(...)
  def forward(self, tokens: torch.Tensor):
    # Handling layers being 'None' at runtime enables easy pipeline splitting
    h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens
    for layer in self.layers.values():
      h = layer(h, self.freqs_cis)
    h = self.norm(h) if self.norm else h
    output = self.output(h).float() if self.output else h
    return output

```
Copy to clipboard
A model defined in this manner can be easily configured per stage by first initializing the whole model (using meta-device to avoid OOM errors), deleting undesired layers for that stage, and then creating a PipelineStage that wraps the model. For example:
```
with torch.device("meta"):
  assert num_stages == 2, "This is a simple 2-stage example"
  # we construct the entire model, then delete the parts we do not need for this stage
  # in practice, this can be done using a helper function that automatically divides up layers across stages.
  model = Transformer()
  if stage_index == 0:
    # prepare the first stage model
    del model.layers["1"]
    model.norm = None
    model.output = None
  elif stage_index == 1:
    # prepare the second stage model
    model.tok_embeddings = None
    del model.layers["0"]
  from torch.distributed.pipelining import PipelineStage
  stage = PipelineStage(
    model,
    stage_index,
    num_stages,
    device,
  )

```
Copy to clipboard
When composing with other Data or Model parallelism techniques, `output_args` may also be required, if the output shape/dtype of the model chunk will be affected.
### Option 2: splitting a model automatically
If you have a full model and do not want to spend time on modifying it into a sequence of “model partitions”, the `pipeline` API is here to help. Here is a brief example:
```
class Model(torch.nn.Module):
  def __init__(self) -> None:
    super().__init__()
    self.emb = torch.nn.Embedding(10, 3)
    self.layers = torch.nn.ModuleList(
      Layer() for _ in range(2)
    )
    self.lm = LMHead()
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    x = self.emb(x)
    for layer in self.layers:
      x = layer(x)
    x = self.lm(x)
    return x

```
Copy to clipboard
If we print the model, we can see multiple hierarchies, which makes it hard to split by hand:
```
Model(
 (emb): Embedding(10, 3)
 (layers): ModuleList(
  (0-1): 2 x Layer(
   (lin): Linear(in_features=3, out_features=3, bias=True)
  )
 )
 (lm): LMHead(
  (proj): Linear(in_features=3, out_features=3, bias=True)
 )
)

```
Copy to clipboard
Let us see how the `pipeline` API works:
```
from torch.distributed.pipelining import pipeline, SplitPoint
# An example micro-batch input
x = torch.LongTensor([1, 2, 4, 5])
pipe = pipeline(
  module=mod,
  mb_args=(x,),
  split_spec={
    "layers.1": SplitPoint.BEGINNING,
  }
)

```
Copy to clipboard
The `pipeline` API splits your model given a `split_spec`, where `SplitPoint.BEGINNING` stands for adding a split point _before_ execution of certain submodule in the `forward` function, and similarly, `SplitPoint.END` for split point _after_ such.
If we `print(pipe)`, we can see:
```
GraphModule(
 (submod_0): GraphModule(
  (emb): InterpreterModule()
  (layers): Module(
   (0): InterpreterModule(
    (lin): InterpreterModule()
   )
  )
 )
 (submod_1): GraphModule(
  (layers): Module(
   (1): InterpreterModule(
    (lin): InterpreterModule()
   )
  )
  (lm): InterpreterModule(
   (proj): InterpreterModule()
  )
 )
)
def forward(self, x):
  submod_0 = self.submod_0(x); x = None
  submod_1 = self.submod_1(submod_0); submod_0 = None
  return (submod_1,)

```
Copy to clipboard
The “model partitions” are represented by submodules (`submod_0`, `submod_1`), each of which is reconstructed with original model operations, weights and hierarchies. In addition, a “root-level” `forward` function is reconstructed to capture the data flow between those partitions. Such data flow will be replayed by the pipeline runtime later, in a distributed fashion.
The `Pipe` object provides a method for retrieving the “model partitions”:
```
stage_mod : nn.Module = pipe.get_stage_module(stage_idx)

```
Copy to clipboard
The returned `stage_mod` is a `nn.Module`, with which you can create an optimizer, save or load checkpoints, or apply other parallelisms.
`Pipe` also allows you to create a distributed stage runtime on a device given a `ProcessGroup`:
```
stage = pipe.build_stage(stage_idx, device, group)

```
Copy to clipboard
Alternatively, if you would like to build the stage runtime later after some modification to the `stage_mod`, you can use a functional version of the `build_stage` API. For example:
```
from torch.distributed.pipelining import build_stage
from torch.nn.parallel import DistributedDataParallel
dp_mod = DistributedDataParallel(stage_mod)
info = pipe.info()
stage = build_stage(dp_mod, stage_idx, info, device, group)

```
Copy to clipboard
Note
The `pipeline` frontend uses a tracer (`torch.export`) to capture your model into a single graph. If your model is not full-graph’able, you can use our manual frontend below.
## Hugging Face Examples
In the PiPPy repo where this package was original created, we kept examples based on unmodified Hugging Face models. See the examples/huggingface directory.
Examples include:
  * GPT2
  * Llama


## Technical Deep Dive
### How does the `pipeline` API split a model?
First, the `pipeline` API turns our model into a directed acyclic graph (DAG) by tracing the model. It traces the model using `torch.export` – a PyTorch 2 full-graph capturing tool.
Then, it groups together the **operations and parameters** needed by a stage into a reconstructed submodule: `submod_0`, `submod_1`, …
Different from conventional submodule access methods like `Module.children()`, the `pipeline` API does not only cut the module structure of your model, but also the **forward** function of your model.
This is necessary because model structure like `Module.children()` merely captures information during `Module.__init__()`, and does not capture any information about `Module.forward()`. Said differently, `Module.children()` lacks information about the following aspects key to pipelininig:
  * Execution order of child modules in `forward`
  * Activation flows between child modules
  * Whether there are any functional operators between child modules (for example, `relu` or `add` operations will not be captured by `Module.children()`).


The `pipeline` API, on the contrary, makes sure that the `forward` behavior is truly preserved. It also captures the activation flow between the partitions, helping the distributed runtime to make correct send/receive calls without human intervention.
Another flexibility of the `pipeline` API is that split points can be at arbitrary levels within your model hierarchy. In the split partitions, the original model hierarchy related to that partition will be reconstructed at no cost to you. At a result, fully-qualified names (FQNs) pointing to a submodule or parameter would be still valid, and services that relies on FQNs (such as FSDP, TP or checkpointing) can still run with your partitioned modules with almost zero code change.
## Implementing Your Own Schedule
You can implement your own pipeline schedule by extending one of the following two class:
  * `PipelineScheduleSingle`
  * `PipelineScheduleMulti`


`PipelineScheduleSingle` is for schedules that assigns _only one_ stage per rank. `PipelineScheduleMulti` is for schedules that assigns multiple stages per rank.
For example, `ScheduleGPipe` and `Schedule1F1B` are subclasses of `PipelineScheduleSingle`. Whereas, `ScheduleInterleaved1F1B`, `ScheduleLoopedBFS`, `ScheduleInterleavedZeroBubble`, and `ScheduleZBVZeroBubble` are subclasses of `PipelineScheduleMulti`.
## Logging
You can turn on additional logging using the TORCH_LOGS environment variable from torch._logging:
  * TORCH_LOGS=+pp will display logging.DEBUG messages and all levels above it.
  * TORCH_LOGS=pp will display logging.INFO messages and above.
  * TORCH_LOGS=-pp will display logging.WARNING messages and above.


## API Reference
### Model Split APIs
The following set of APIs transform your model into a pipeline representation. 

_class_ torch.distributed.pipelining.SplitPoint(_value_)[source][source]
    
Enum representing the points at which a split can occur in the execution of a submodule. :ivar BEGINNING: Represents adding a split point _before_ the execution of a certain submodule in the forward function. :ivar END: Represents adding a split point _after_ the execution of a certain submodule in the forward function. 

torch.distributed.pipelining.pipeline(_module_ , _mb_args_ , _mb_kwargs =None_, _split_spec =None_, _split_policy =None_)[source][source]
    
Split a module based on a specification.
See Pipe for more details. 

Parameters
    
  * **module** (_Module_) – The module to be splitted.
  * **mb_args** (_tuple_ _[__Any_ _,__...__]_) – Example positional inputs, in micro-batch form.
  * **mb_kwargs** (_Optional_ _[__dict_ _[__str_ _,__Any_ _]__]_) – Example keyword inputs, in micro-batch form. (default: None)
  * **split_spec** (_Optional_ _[__dict_ _[__str_ _,__torch.distributed.pipelining._IR.SplitPoint_ _]__]_) – A dictionary using submodule names as split marker. (default: None)
  * **split_policy** (_Optional_ _[__Callable_ _[__[__GraphModule_ _]__,__GraphModule_ _]__]_) – The policy to use for splitting the module. (default: None)



Return type
    
A pipeline representation of class Pipe. 

_class_ torch.distributed.pipelining.Pipe(_split_gm_ , _num_stages_ , _has_loss_and_backward_ , _loss_spec_)[source][source]


torch.distributed.pipelining.pipe_split()[source][source]
    
pipe_split is a special operator that is used to mark the boundary between stages in a module. It is used to split the module into stages. It is a no-op if your annotated module is run eagerly.
Example
```
>>> def forward(self, x):
>>>   x = torch.mm(x, self.mm_param)
>>>   x = torch.relu(x)
>>>   pipe_split()
>>>   x = self.lin(x)
>>>   return x

```
Copy to clipboard
The above example will be split into two stages.
### Microbatch Utilities 

_class_ torch.distributed.pipelining.microbatch.TensorChunkSpec(_split_dim_)[source][source]
    
Class used to specify chunking of inputs 

torch.distributed.pipelining.microbatch.split_args_kwargs_into_chunks(_args_ , _kwargs_ , _chunks_ , _args_chunk_spec =None_, _kwargs_chunk_spec =None_)[source][source]
    
Given a sequence of args and kwargs, split them into a number of chunks according to their respective chunking specs. 

Parameters
    
  * **args** (_tuple_ _[__Any_ _,__...__]_) – Tuple of args
  * **kwargs** (_Optional_ _[__dict_ _[__str_ _,__Any_ _]__]_) – Dict of kwargs
  * **chunks** (_int_) – Number of chunks to split the args and kwargs into
  * **args_chunk_spec** (_Optional_ _[__tuple_ _[__torch.distributed.pipelining.microbatch.TensorChunkSpec_ _,__...__]__]_) – chunking specs for args, in same shape as args
  * **kwargs_chunk_spec** (_Optional_ _[__dict_ _[__str_ _,__torch.distributed.pipelining.microbatch.TensorChunkSpec_ _]__]_) – chunking specs for kwargs, in same shape as kwargs



Returns
    
List of sharded args kwargs_split: List of sharded kwargs 

Return type
    
args_split 

torch.distributed.pipelining.microbatch.merge_chunks(_chunks_ , _chunk_spec_)[source][source]
    
Given a list of chunks, merge them into a single value according to the chunk spec. 

Parameters
    
  * **chunks** (_list_ _[__Any_ _]_) – list of chunks
  * **chunk_spec** – Chunking spec for the chunks



Returns
    
Merged value 

Return type
    
value
### Pipeline Stages 

_class_ torch.distributed.pipelining.stage.PipelineStage(_submodule_ , _stage_index_ , _num_stages_ , _device_ , _input_args =None_, _output_args =None_, _group =None_, _dw_builder =None_)[source][source]
    
A class representing a pipeline stage in a pipeline parallelism setup.
PipelineStage assumes sequential partitioning of the model, i.e. the model is split into chunks where outputs from one chunk feed into inputs of the next chunk, with no skip connections.
PipelineStage performs runtime shape/dtype inference automatically by propagating the outputs from stage0 to stage1 and so forth, in linear order. To bypass shape inference, pass the input_args and output_args to each PipelineStage instance. 

Parameters
    
  * **submodule** (_nn.Module_) – The PyTorch module wrapped by this stage.
  * **stage_index** (_int_) – The ID of this stage.
  * **num_stages** (_int_) – The total number of stages.
  * **device** (_torch.device_) – The device where this stage is located.
  * **input_args** (_Union_ _[__torch.Tensor_ _,__Tuple_ _[__torch.tensor_ _]__]__,__optional_) – The input arguments for the submodule.
  * **output_args** (_Union_ _[__torch.Tensor_ _,__Tuple_ _[__torch.tensor_ _]__]__,__optional_) – The output arguments for the submodule.
  * **group** (_dist.ProcessGroup_ _,__optional_) – The process group for distributed training. If None, default group.
  * **dw_builder** (_Optional_ _[__Callable_ _[__[__]__,__Callable_ _[__...__,__None_ _]__]_) – If provided, dw_builder will build a new dw_runner function that will the W action (input weights) for F, I, W (Fwd, Input, Weight) zero bubble schedules.



torch.distributed.pipelining.stage.build_stage(_stage_module_ , _stage_index_ , _pipe_info_ , _device_ , _group =None_)[source][source]
    
Create a pipeline stage given a stage_module to be wrapped by this stage and pipeline information. 

Parameters
    
  * **stage_module** (_torch.nn.Module_) – the module to be wrapped by this stage
  * **stage_index** (_int_) – the index of this stage in the pipeline
  * **pipe_info** (_PipeInfo_) – information about the pipeline, can be retrieved by pipe.info()
  * **device** (_torch.device_) – the device to be used by this stage
  * **group** (_Optional_ _[__dist.ProcessGroup_ _]_) – the process group to be used by this stage



Returns
    
a pipeline stage that can run with PipelineSchedules. 

Return type
    
_PipelineStage
### Pipeline Schedules 

_class_ torch.distributed.pipelining.schedules.ScheduleGPipe(_stage_ , _n_microbatches_ , _loss_fn =None_, _args_chunk_spec =None_, _kwargs_chunk_spec =None_, _output_merge_spec =None_, _scale_grads =True_)[source][source]
    
The GPipe schedule. Will go through all the microbatches in a fill-drain manner. 

_class_ torch.distributed.pipelining.schedules.Schedule1F1B(_stage_ , _n_microbatches_ , _loss_fn =None_, _args_chunk_spec =None_, _kwargs_chunk_spec =None_, _output_merge_spec =None_, _scale_grads =True_)[source][source]
    
The 1F1B schedule. Will perform one forward and one backward on the microbatches in steady state. 

_class_ torch.distributed.pipelining.schedules.ScheduleInterleaved1F1B(_stages_ , _n_microbatches_ , _loss_fn =None_, _args_chunk_spec =None_, _kwargs_chunk_spec =None_, _output_merge_spec =None_, _scale_grads =True_)[source][source]
    
The Interleaved 1F1B schedule. See https://arxiv.org/pdf/2104.04473 for details. Will perform one forward and one backward on the microbatches in steady state and supports multiple stages per rank. When microbatches are ready for multiple local stages, Interleaved 1F1B prioritizes the earlier microbatch (also called “depth first”).
This schedule is mostly similar to the original paper. It differs by being relaxing the requirement of num_microbatch % pp_size == 0. Using the flex_pp schedule, we will have num_rounds = max(1, n_microbatches // pp_group_size) and it works as long as n_microbatches % num_rounds is 0. As a few examples, support
  1. pp_group_size = 4, n_microbatches = 10. We will have num_rounds = 2 and n_microbatches % 2 is 0.
  2. pp_group_size = 4, n_microbatches = 3. We will have num_rounds = 1 and n_microbatches % 1 is 0.



_class_ torch.distributed.pipelining.schedules.ScheduleLoopedBFS(_stages_ , _n_microbatches_ , _loss_fn =None_, _output_merge_spec =None_, _scale_grads =True_)[source][source]
    
Breadth-First Pipeline Parallelism. See https://arxiv.org/abs/2211.05953 for details. Simliar to Interleaved 1F1B, Looped BFS supports multiple stages per rank. What is different is that when microbatches are ready for multiple local stages, Loops BFS will prioritizes the earlier stage, running all available microbatches at once. 

_class_ torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble(_stages_ , _n_microbatches_ , _loss_fn =None_, _args_chunk_spec =None_, _kwargs_chunk_spec =None_, _output_merge_spec =None_, _scale_grads =True_)[source][source]
    
The Interleaved Zero Bubble schedule. See https://arxiv.org/pdf/2401.10241 for details. Will perform one forward and one backward on inputs for the microbatches in steady state and supports multiple stages per rank. Uses the backward for weights to fill in the pipeline bubble.
In particular this is implementing the ZB1P schedule in the paper. 

_class_ torch.distributed.pipelining.schedules.ScheduleZBVZeroBubble(_stages_ , _n_microbatches_ , _loss_fn =None_, _args_chunk_spec =None_, _kwargs_chunk_spec =None_, _output_merge_spec =None_, _scale_grads =True_)[source][source]
    
The Zero Bubble schedule (ZBV variant). See https://arxiv.org/pdf/2401.10241 Section 6 for details.
This schedules requires exactly two stages per rank.
This schedule will perform one forward and one backward on inputs for the microbatches in steady state and supports multiple stages per rank. Uses backward with respect to weights to fill in the pipeline bubble.
This ZB-V schedule would have the “zero bubble” property only if time forward == time backward input == time backward weights. In practice, this is not likely true for real models so alternatively a greedy scheduler could be implemented for unequal/unbalanced time. 

_class_ torch.distributed.pipelining.schedules.PipelineScheduleSingle(_stage_ , _n_microbatches_ , _loss_fn =None_, _args_chunk_spec =None_, _kwargs_chunk_spec =None_, _output_merge_spec =None_, _scale_grads =True_)[source][source]
    
Base class for single-stage schedules. Implements the step method. Derived classes should implement _step_microbatches.
Gradients are scaled by num_microbatches depending on the scale_grads argument, defaulting to True. This setting should match the configuration of your loss_fn, which may either average losses (scale_grads=True) or sum losses (scale_grads=False). 

step(_* args_, _target =None_, _losses =None_, _** kwargs_)[source][source]
    
Run one iteration of the pipeline schedule with _whole-batch_ input. Will chunk the input into microbatches automatically, and go through the microbatches according to the schedule implementation.
args: positional arguments to the model (as in non-pipeline case). kwargs: keyword arguments to the model (as in non-pipeline case). target: target for the loss function. losses: a list to store the losses for each microbatch. 

_class_ torch.distributed.pipelining.schedules.PipelineScheduleMulti(_stages_ , _n_microbatches_ , _loss_fn =None_, _args_chunk_spec =None_, _kwargs_chunk_spec =None_, _output_merge_spec =None_, _use_full_backward =None_, _scale_grads =True_)[source][source]
    
Base class for multi-stage schedules. Implements the step method.
Gradients are scaled by num_microbatches depending on the scale_grads argument, defaulting to True. This setting should match the configuration of your loss_fn, which may either average losses (scale_grads=True) or sum losses (scale_grads=False). 

step(_* args_, _target =None_, _losses =None_, _** kwargs_)[source][source]
    
Run one iteration of the pipeline schedule with _whole-batch_ input. Will chunk the input into microbatches automatically, and go through the microbatches according to the schedule implementation.
args: positional arguments to the model (as in non-pipeline case). kwargs: keyword arguments to the model (as in non-pipeline case). target: target for the loss function. losses: a list to store the losses for each microbatch.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Pipeline Parallelism
    * Why Pipeline Parallel?
    * What is `torch.distributed.pipelining`?
    * Step 1: build `PipelineStage`
    * Step 2: use `PipelineSchedule` for execution
    * Options for Splitting a Model
      * Option 1: splitting a model manually
      * Option 2: splitting a model automatically
    * Hugging Face Examples
    * Technical Deep Dive
      * How does the `pipeline` API split a model?
    * Implementing Your Own Schedule
    * Logging
    * API Reference
      * Model Split APIs
        * `SplitPoint`
        * `pipeline()`
        * `Pipe`
        * `pipe_split()`
      * Microbatch Utilities
        * `TensorChunkSpec`
        * `split_args_kwargs_into_chunks()`
        * `merge_chunks()`
      * Pipeline Stages
        * `PipelineStage`
        * `build_stage()`
      * Pipeline Schedules
        * `ScheduleGPipe`
        * `Schedule1F1B`
        * `ScheduleInterleaved1F1B`
        * `ScheduleLoopedBFS`
        * `ScheduleInterleavedZeroBubble`
        * `ScheduleZBVZeroBubble`
        * `PipelineScheduleSingle`
          * `PipelineScheduleSingle.step()`
        * `PipelineScheduleMulti`
          * `PipelineScheduleMulti.step()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.dlpack
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.dlpack 

torch.utils.dlpack.from_dlpack(_ext_tensor_) → Tensor[source][source]
    
Converts a tensor from an external library into a `torch.Tensor`.
The returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine. 

Parameters
    
**ext_tensor** (object with `__dlpack__` attribute, or a DLPack capsule) – 
The tensor or DLPack capsule to convert.
If `ext_tensor` is a tensor (or ndarray) object, it must support the `__dlpack__` protocol (i.e., have a `ext_tensor.__dlpack__` method). Otherwise `ext_tensor` may be a DLPack capsule, which is an opaque `PyCapsule` instance, typically produced by a `to_dlpack` function or method. 

Return type
    
_Tensor_
Examples:
```
>>> import torch.utils.dlpack
>>> t = torch.arange(4)
# Convert a tensor directly (supported in PyTorch >= 1.10)
>>> t2 = torch.from_dlpack(t)
>>> t2[:2] = -1 # show that memory is shared
>>> t2
tensor([-1, -1, 2, 3])
>>> t
tensor([-1, -1, 2, 3])
# The old-style DLPack usage, with an intermediate capsule object
>>> capsule = torch.utils.dlpack.to_dlpack(t)
>>> capsule
<capsule object "dltensor" at ...>
>>> t3 = torch.from_dlpack(capsule)
>>> t3
tensor([-1, -1, 2, 3])
>>> t3[0] = -9 # now we're sharing memory between 3 tensors
>>> t3
tensor([-9, -1, 2, 3])
>>> t2
tensor([-9, -1, 2, 3])
>>> t
tensor([-9, -1, 2, 3])

```
Copy to clipboard 

torch.utils.dlpack.to_dlpack(_tensor_) → PyCapsule
    
Returns an opaque object (a “DLPack capsule”) representing the tensor.
Note
`to_dlpack` is a legacy DLPack interface. The capsule it returns cannot be used for anything in Python other than use it as input to `from_dlpack`. The more idiomatic use of DLPack is to call `from_dlpack` directly on the tensor object - this works when that object has a `__dlpack__` method, which PyTorch and most other libraries indeed have now.
Warning
Only call `from_dlpack` once per capsule produced with `to_dlpack`. Behavior when a capsule is consumed multiple times is undefined. 

Parameters
    
**tensor** – a tensor to be exported
The DLPack capsule shares the tensor’s memory.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.dlpack
    * `from_dlpack()`
    * `to_dlpack()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.export
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.export
Warning
This feature is a prototype under active development and there WILL BE BREAKING CHANGES in the future.
## Overview
`torch.export.export()` takes a `torch.nn.Module` and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized.
```
import torch
from torch.export import export
class Mod(torch.nn.Module):
  def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    a = torch.sin(x)
    b = torch.cos(y)
    return a + b
example_args = (torch.randn(10, 10), torch.randn(10, 10))
exported_program: torch.export.ExportedProgram = export(
  Mod(), args=example_args
)
print(exported_program)

```
Copy to clipboard
```
ExportedProgram:
  class GraphModule(torch.nn.Module):
    def forward(self, x: "f32[10, 10]", y: "f32[10, 10]"):
      # code: a = torch.sin(x)
      sin: "f32[10, 10]" = torch.ops.aten.sin.default(x)
      # code: b = torch.cos(y)
      cos: "f32[10, 10]" = torch.ops.aten.cos.default(y)
      # code: return a + b
      add: f32[10, 10] = torch.ops.aten.add.Tensor(sin, cos)
      return (add,)
  Graph signature:
    ExportGraphSignature(
      input_specs=[
        InputSpec(
          kind=<InputKind.USER_INPUT: 1>,
          arg=TensorArgument(name='x'),
          target=None,
          persistent=None
        ),
        InputSpec(
          kind=<InputKind.USER_INPUT: 1>,
          arg=TensorArgument(name='y'),
          target=None,
          persistent=None
        )
      ],
      output_specs=[
        OutputSpec(
          kind=<OutputKind.USER_OUTPUT: 1>,
          arg=TensorArgument(name='add'),
          target=None
        )
      ]
    )
  Range constraints: {}

```
Copy to clipboard
`torch.export` produces a clean intermediate representation (IR) with the following invariants. More specifications about the IR can be found here.
  * **Soundness** : It is guaranteed to be a sound representation of the original program, and maintains the same calling conventions of the original program.
  * **Normalized** : There are no Python semantics within the graph. Submodules from the original programs are inlined to form one fully flattened computational graph.
  * **Graph properties** : The graph is purely functional, meaning it does not contain operations with side effects such as mutations or aliasing. It does not mutate any intermediate values, parameters, or buffers.
  * **Metadata** : The graph contains metadata captured during tracing, such as a stacktrace from user’s code.


Under the hood, `torch.export` leverages the following latest technologies:
  * **TorchDynamo (torch._dynamo)** is an internal API that uses a CPython feature called the Frame Evaluation API to safely trace PyTorch graphs. This provides a massively improved graph capturing experience, with much fewer rewrites needed in order to fully trace the PyTorch code.
  * **AOT Autograd** provides a functionalized PyTorch graph and ensures the graph is decomposed/lowered to the ATen operator set.
  * **Torch FX (torch.fx)** is the underlying representation of the graph, allowing flexible Python-based transformations.


### Existing frameworks
`torch.compile()` also utilizes the same PT2 stack as `torch.export`, but is slightly different:
  * **JIT vs. AOT** : `torch.compile()` is a JIT compiler whereas which is not intended to be used to produce compiled artifacts outside of deployment.
  * **Partial vs. Full Graph Capture** : When `torch.compile()` runs into an untraceable part of a model, it will “graph break” and fall back to running the program in the eager Python runtime. In comparison, `torch.export` aims to get a full graph representation of a PyTorch model, so it will error out when something untraceable is reached. Since `torch.export` produces a full graph disjoint from any Python features or runtime, this graph can then be saved, loaded, and run in different environments and languages.
  * **Usability tradeoff** : Since `torch.compile()` is able to fallback to the Python runtime whenever it reaches something untraceable, it is a lot more flexible. `torch.export` will instead require users to provide more information or rewrite their code to make it traceable.


Compared to `torch.fx.symbolic_trace()`, `torch.export` traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, `torch.export` keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, `torch.export` is expected to work on more user programs, and produce lower-level graphs (at the `torch.ops.aten` operator level). Note that users can still use `torch.fx.symbolic_trace()` as a preprocessing step before `torch.export`.
Compared to `torch.jit.script()`, `torch.export` does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators).
Compared to `torch.jit.trace()`, `torch.export` is sound: it is able to trace code that performs integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.
## Exporting a PyTorch Model
### An Example
The main entrypoint is through `torch.export.export()`, which takes a callable (`torch.nn.Module`, function, or method) and sample inputs, and captures the computation graph into an `torch.export.ExportedProgram`. An example:
```
import torch
from torch.export import export
# Simple module for demonstration
class M(torch.nn.Module):
  def __init__(self) -> None:
    super().__init__()
    self.conv = torch.nn.Conv2d(
      in_channels=3, out_channels=16, kernel_size=3, padding=1
    )
    self.relu = torch.nn.ReLU()
    self.maxpool = torch.nn.MaxPool2d(kernel_size=3)
  def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:
    a = self.conv(x)
    a.add_(constant)
    return self.maxpool(self.relu(a))
example_args = (torch.randn(1, 3, 256, 256),)
example_kwargs = {"constant": torch.ones(1, 16, 256, 256)}
exported_program: torch.export.ExportedProgram = export(
  M(), args=example_args, kwargs=example_kwargs
)
print(exported_program)

```
Copy to clipboard
```
ExportedProgram:
  class GraphModule(torch.nn.Module):
  def forward(self, p_conv_weight: "f32[16, 3, 3, 3]", p_conv_bias: "f32[16]", x: "f32[1, 3, 256, 256]", constant: "f32[1, 16, 256, 256]"):
      # code: a = self.conv(x)
      conv2d: "f32[1, 16, 256, 256]" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias, [1, 1], [1, 1])
      # code: a.add_(constant)
      add_: "f32[1, 16, 256, 256]" = torch.ops.aten.add_.Tensor(conv2d, constant)
      # code: return self.maxpool(self.relu(a))
      relu: "f32[1, 16, 256, 256]" = torch.ops.aten.relu.default(add_)
      max_pool2d: "f32[1, 16, 85, 85]" = torch.ops.aten.max_pool2d.default(relu, [3, 3], [3, 3])
      return (max_pool2d,)
Graph signature:
  ExportGraphSignature(
    input_specs=[
      InputSpec(
        kind=<InputKind.PARAMETER: 2>,
        arg=TensorArgument(name='p_conv_weight'),
        target='conv.weight',
        persistent=None
      ),
      InputSpec(
        kind=<InputKind.PARAMETER: 2>,
        arg=TensorArgument(name='p_conv_bias'),
        target='conv.bias',
        persistent=None
      ),
      InputSpec(
        kind=<InputKind.USER_INPUT: 1>,
        arg=TensorArgument(name='x'),
        target=None,
        persistent=None
      ),
      InputSpec(
        kind=<InputKind.USER_INPUT: 1>,
        arg=TensorArgument(name='constant'),
        target=None,
        persistent=None
      )
    ],
    output_specs=[
      OutputSpec(
        kind=<OutputKind.USER_OUTPUT: 1>,
        arg=TensorArgument(name='max_pool2d'),
        target=None
      )
    ]
  )
Range constraints: {}

```
Copy to clipboard
Inspecting the `ExportedProgram`, we can note the following:
  * The `torch.fx.Graph` contains the computation graph of the original program, along with records of the original code for easy debugging.
  * The graph contains only `torch.ops.aten` operators found here and custom operators, and is fully functional, without any inplace operators such as `torch.add_`.
  * The parameters (weight and bias to conv) are lifted as inputs to the graph, resulting in no `get_attr` nodes in the graph, which previously existed in the result of `torch.fx.symbolic_trace()`.
  * The `torch.export.ExportGraphSignature` models the input and output signature, along with specifying which inputs are parameters.
  * The resulting shape and dtype of tensors produced by each node in the graph is noted. For example, the `convolution` node will result in a tensor of dtype `torch.float32` and shape (1, 16, 256, 256).


### Non-Strict Export
In PyTorch 2.3, we introduced a new mode of tracing called **non-strict mode**. It’s still going through hardening, so if you run into any issues, please file them to Github with the “oncall: export” tag.
In _non-strict mode_ , we trace through the program using the Python interpreter. Your code will execute exactly as it would in eager mode; the only difference is that all Tensor objects will be replaced by ProxyTensors, which will record all their operations into a graph.
In _strict_ mode, which is currently the default, we first trace through the program using TorchDynamo, a bytecode analysis engine. TorchDynamo does not actually execute your Python code. Instead, it symbolically analyzes it and builds a graph based on the results. This analysis allows torch.export to provide stronger guarantees about safety, but not all Python code is supported.
An example of a case where one might want to use non-strict mode is if you run into a unsupported TorchDynamo feature that might not be easily solved, and you know the python code is not exactly needed for computation. For example:
```
import contextlib
import torch
class ContextManager():
  def __init__(self):
    self.count = 0
  def __enter__(self):
    self.count += 1
  def __exit__(self, exc_type, exc_value, traceback):
    self.count -= 1
class M(torch.nn.Module):
  def forward(self, x):
    with ContextManager():
      return x.sin() + x.cos()
export(M(), (torch.ones(3, 3),), strict=False) # Non-strict traces successfully
export(M(), (torch.ones(3, 3),)) # Strict mode fails with torch._dynamo.exc.Unsupported: ContextManager

```
Copy to clipboard
In this example, the first call using non-strict mode (through the `strict=False` flag) traces successfully whereas the second call using strict mode (default) results with a failure, where TorchDynamo is unable to support context managers. One option is to rewrite the code (see Limitations of torch.export), but seeing as the context manager does not affect the tensor computations in the model, we can go with the non-strict mode’s result.
### Export for Training and Inference
In PyTorch 2.5, we introduced a new API called `export_for_training()`. It’s still going through hardening, so if you run into any issues, please file them to Github with the “oncall: export” tag.
In this API, we produce the most generic IR that contains all ATen operators (including both functional and non-functional) which can be used to train in eager PyTorch Autograd. This API is intended for eager training use cases such as PT2 Quantization and will soon be the default IR of torch.export.export. To read further about the motivation behind this change, please refer to https://dev-discuss.pytorch.org/t/why-pytorch-does-not-need-a-new-standardized-operator-set/2206
When this API is combined with `run_decompositions()`, you should be able to get inference IR with any desired decomposition behavior.
To show some examples:
```
class ConvBatchnorm(torch.nn.Module):
  def __init__(self) -> None:
    super().__init__()
    self.conv = torch.nn.Conv2d(1, 3, 1, 1)
    self.bn = torch.nn.BatchNorm2d(3)
  def forward(self, x):
    x = self.conv(x)
    x = self.bn(x)
    return (x,)
mod = ConvBatchnorm()
inp = torch.randn(1, 1, 3, 3)
ep_for_training = torch.export.export_for_training(mod, (inp,))
print(ep_for_training)

```
Copy to clipboard
```
ExportedProgram:
  class GraphModule(torch.nn.Module):
    def forward(self, p_conv_weight: "f32[3, 1, 1, 1]", p_conv_bias: "f32[3]", p_bn_weight: "f32[3]", p_bn_bias: "f32[3]", b_bn_running_mean: "f32[3]", b_bn_running_var: "f32[3]", b_bn_num_batches_tracked: "i64[]", x: "f32[1, 1, 3, 3]"):
      conv2d: "f32[1, 3, 3, 3]" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias)
      add_: "i64[]" = torch.ops.aten.add_.Tensor(b_bn_num_batches_tracked, 1)
      batch_norm: "f32[1, 3, 3, 3]" = torch.ops.aten.batch_norm.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05, True)
      return (batch_norm,)

```
Copy to clipboard
From the above output, you can see that `export_for_training()` produces pretty much the same ExportedProgram as `export()` except for the operators in the graph. You can see that we captured batch_norm in the most general form. This op is non-functional and will be lowered to different ops when running inference.
You can also go from this IR to an inference IR via `run_decompositions()` with arbitrary customizations.
```
# Lower to core aten inference IR, but keep conv2d
decomp_table = torch.export.default_decompositions()
del decomp_table[torch.ops.aten.conv2d.default]
ep_for_inference = ep_for_training.run_decompositions(decomp_table)
print(ep_for_inference)

```
Copy to clipboard
```
ExportedProgram:
  class GraphModule(torch.nn.Module):
    def forward(self, p_conv_weight: "f32[3, 1, 1, 1]", p_conv_bias: "f32[3]", p_bn_weight: "f32[3]", p_bn_bias: "f32[3]", b_bn_running_mean: "f32[3]", b_bn_running_var: "f32[3]", b_bn_num_batches_tracked: "i64[]", x: "f32[1, 1, 3, 3]"):
      conv2d: "f32[1, 3, 3, 3]" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias)
      add: "i64[]" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1)
      _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05)
      getitem: "f32[1, 3, 3, 3]" = _native_batch_norm_legit_functional[0]
      getitem_3: "f32[3]" = _native_batch_norm_legit_functional[3]
      getitem_4: "f32[3]" = _native_batch_norm_legit_functional[4]
      return (getitem_3, getitem_4, add, getitem)

```
Copy to clipboard
Here you can see that we kept `conv2d` op in the IR while decomposing the rest. Now the IR is a functional IR containing core aten operators except for `conv2d`.
You can do even more customization by directly registering your chosen decomposition behaviors.
You can do even more customizations by directly registering custom decomp behaviour
```
# Lower to core aten inference IR, but customize conv2d
decomp_table = torch.export.default_decompositions()
def my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1):
  return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups)
decomp_table[torch.ops.aten.conv2d.default] = my_awesome_conv2d_function
ep_for_inference = ep_for_training.run_decompositions(decomp_table)
print(ep_for_inference)

```
Copy to clipboard
```
ExportedProgram:
  class GraphModule(torch.nn.Module):
    def forward(self, p_conv_weight: "f32[3, 1, 1, 1]", p_conv_bias: "f32[3]", p_bn_weight: "f32[3]", p_bn_bias: "f32[3]", b_bn_running_mean: "f32[3]", b_bn_running_var: "f32[3]", b_bn_num_batches_tracked: "i64[]", x: "f32[1, 1, 3, 3]"):
      convolution: "f32[1, 3, 3, 3]" = torch.ops.aten.convolution.default(x, p_conv_weight, p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)
      mul: "f32[1, 3, 3, 3]" = torch.ops.aten.mul.Tensor(convolution, 2)
      add: "i64[]" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1)
      _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(mul, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05)
      getitem: "f32[1, 3, 3, 3]" = _native_batch_norm_legit_functional[0]
      getitem_3: "f32[3]" = _native_batch_norm_legit_functional[3]
      getitem_4: "f32[3]" = _native_batch_norm_legit_functional[4];
      return (getitem_3, getitem_4, add, getitem)

```
Copy to clipboard
### Expressing Dynamism
By default `torch.export` will trace the program assuming all input shapes are **static** , and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be specified by using the `torch.export.Dim()` API to create them and by passing them into `torch.export.export()` through the `dynamic_shapes` argument. An example:
```
import torch
from torch.export import Dim, export
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.branch1 = torch.nn.Sequential(
      torch.nn.Linear(64, 32), torch.nn.ReLU()
    )
    self.branch2 = torch.nn.Sequential(
      torch.nn.Linear(128, 64), torch.nn.ReLU()
    )
    self.buffer = torch.ones(32)
  def forward(self, x1, x2):
    out1 = self.branch1(x1)
    out2 = self.branch2(x2)
    return (out1 + self.buffer, out2)
example_args = (torch.randn(32, 64), torch.randn(32, 128))
# Create a dynamic batch size
batch = Dim("batch")
# Specify that the first dimension of each input is that batch size
dynamic_shapes = {"x1": {0: batch}, "x2": {0: batch}}
exported_program: torch.export.ExportedProgram = export(
  M(), args=example_args, dynamic_shapes=dynamic_shapes
)
print(exported_program)

```
Copy to clipboard
```
ExportedProgram:
class GraphModule(torch.nn.Module):
  def forward(self, p_branch1_0_weight: "f32[32, 64]", p_branch1_0_bias: "f32[32]", p_branch2_0_weight: "f32[64, 128]", p_branch2_0_bias: "f32[64]", c_buffer: "f32[32]", x1: "f32[s0, 64]", x2: "f32[s0, 128]"):
     # code: out1 = self.branch1(x1)
    linear: "f32[s0, 32]" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias)
    relu: "f32[s0, 32]" = torch.ops.aten.relu.default(linear)
     # code: out2 = self.branch2(x2)
    linear_1: "f32[s0, 64]" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias)
    relu_1: "f32[s0, 64]" = torch.ops.aten.relu.default(linear_1)
     # code: return (out1 + self.buffer, out2)
    add: "f32[s0, 32]" = torch.ops.aten.add.Tensor(relu, c_buffer)
    return (add, relu_1)
Range constraints: {s0: VR[0, int_oo]}

```
Copy to clipboard
Some additional things to note:
  * Through the `torch.export.Dim()` API and the `dynamic_shapes` argument, we specified the first dimension of each input to be dynamic. Looking at the inputs `x1` and `x2`, they have a symbolic shape of (s0, 64) and (s0, 128), instead of the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs. `s0` is a symbol representing that this dimension can be a range of values.
  * `exported_program.range_constraints` describes the ranges of each symbol appearing in the graph. In this case, we see that `s0` has the range [0, int_oo]. For technical reasons that are difficult to explain here, they are assumed to be not 0 or 1. This is not a bug, and does not necessarily mean that the exported program will not work for dimensions 0 or 1. See The 0/1 Specialization Problem for an in-depth discussion of this topic.


We can also specify more expressive relationships between input shapes, such as where a pair of shapes might differ by one, a shape might be double of another, or a shape is even. An example:
```
class M(torch.nn.Module):
  def forward(self, x, y):
    return x + y[1:]
x, y = torch.randn(5), torch.randn(6)
dimx = torch.export.Dim("dimx", min=3, max=6)
dimy = dimx + 1
exported_program = torch.export.export(
  M(), (x, y), dynamic_shapes=({0: dimx}, {0: dimy}),
)
print(exported_program)

```
Copy to clipboard
```
ExportedProgram:
class GraphModule(torch.nn.Module):
  def forward(self, x: "f32[s0]", y: "f32[s0 + 1]"):
    # code: return x + y[1:]
    slice_1: "f32[s0]" = torch.ops.aten.slice.Tensor(y, 0, 1, 9223372036854775807)
    add: "f32[s0]" = torch.ops.aten.add.Tensor(x, slice_1)
    return (add,)
Range constraints: {s0: VR[3, 6], s0 + 1: VR[4, 7]}

```
Copy to clipboard
Some things to note:
  * By specifying `{0: dimx}` for the first input, we see that the resulting shape of the first input is now dynamic, being `[s0]`. And now by specifying `{0: dimy}` for the second input, we see that the resulting shape of the second input is also dynamic. However, because we expressed `dimy = dimx + 1`, instead of `y`’s shape containing a new symbol, we see that it is now being represented with the same symbol used in `x`, `s0`. We can see that relationship of `dimy = dimx + 1` is being shown through `s0 + 1`.
  * Looking at the range constraints, we see that `s0` has the range [3, 6], which is specified initially, and we can see that `s0 + 1` has the solved range of [4, 7].


### Serialization
To save the `ExportedProgram`, users can use the `torch.export.save()` and `torch.export.load()` APIs. A convention is to save the `ExportedProgram` using a `.pt2` file extension.
An example:
```
import torch
import io
class MyModule(torch.nn.Module):
  def forward(self, x):
    return x + 10
exported_program = torch.export.export(MyModule(), torch.randn(5))
torch.export.save(exported_program, 'exported_program.pt2')
saved_exported_program = torch.export.load('exported_program.pt2')

```
Copy to clipboard
### Specializations
A key concept in understanding the behavior of `torch.export` is the difference between _static_ and _dynamic_ values.
A _dynamic_ value is one that can change from run to run. These behave like normal arguments to a Python function—you can pass different values for an argument and expect your function to do the right thing. Tensor _data_ is treated as dynamic.
A _static_ value is a value that is fixed at export time and cannot change between executions of the exported program. When the value is encountered during tracing, the exporter will treat it as a constant and hard-code it into the graph.
When an operation is performed (e.g. `x + y`) and all inputs are static, then the output of the operation will be directly hard-coded into the graph, and the operation won’t show up (i.e. it will get constant-folded).
When a value has been hard-coded into the graph, we say that the graph has been _specialized_ to that value.
The following values are static:
#### Input Tensor Shapes
By default, `torch.export` will trace the program specializing on the input tensors’ shapes, unless a dimension is specified as dynamic via the `dynamic_shapes` argument to `torch.export`. This means that if there exists shape-dependent control flow, `torch.export` will specialize on the branch that is being taken with the given sample inputs. For example:
```
import torch
from torch.export import export
class Mod(torch.nn.Module):
  def forward(self, x):
    if x.shape[0] > 5:
      return x + 1
    else:
      return x - 1
example_inputs = (torch.rand(10, 2),)
exported_program = export(Mod(), example_inputs)
print(exported_program)

```
Copy to clipboard
```
ExportedProgram:
class GraphModule(torch.nn.Module):
  def forward(self, x: "f32[10, 2]"):
    # code: return x + 1
    add: "f32[10, 2]" = torch.ops.aten.add.Tensor(x, 1)
    return (add,)

```
Copy to clipboard
The conditional of (`x.shape[0] > 5`) does not appear in the `ExportedProgram` because the example inputs have the static shape of (10, 2). Since `torch.export` specializes on the inputs’ static shapes, the else branch (`x - 1`) will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, `torch.export.Dim()` will need to be used to specify the dimension of the input tensor (`x.shape[0]`) to be dynamic, and the source code will need to be rewritten.
Note that tensors that are part of the module state (e.g. parameters and buffers) always have static shapes.
#### Python Primitives
`torch.export` also specializes on Python primtivies, such as `int`, `float`, `bool`, and `str`. However they do have dynamic variants such as `SymInt`, `SymFloat`, and `SymBool`.
For example:
```
import torch
from torch.export import export
class Mod(torch.nn.Module):
  def forward(self, x: torch.Tensor, const: int, times: int):
    for i in range(times):
      x = x + const
    return x
example_inputs = (torch.rand(2, 2), 1, 3)
exported_program = export(Mod(), example_inputs)
print(exported_program)

```
Copy to clipboard
```
ExportedProgram:
  class GraphModule(torch.nn.Module):
    def forward(self, x: "f32[2, 2]", const, times):
      # code: x = x + const
      add: "f32[2, 2]" = torch.ops.aten.add.Tensor(x, 1)
      add_1: "f32[2, 2]" = torch.ops.aten.add.Tensor(add, 1)
      add_2: "f32[2, 2]" = torch.ops.aten.add.Tensor(add_1, 1)
      return (add_2,)

```
Copy to clipboard
Because integers are specialized, the `torch.ops.aten.add.Tensor` operations are all computed with the hard-coded constant `1`, rather than `const`. If a user passes a different value for `const` at runtime, like 2, than the one used during export time, 1, this will result in an error. Additionally, the `times` iterator used in the `for` loop is also “inlined” in the graph through the 3 repeated `torch.ops.aten.add.Tensor` calls, and the input `times` is never used.
#### Python Containers
Python containers (`List`, `Dict`, `NamedTuple`, etc.) are considered to have static structure.
## Limitations of torch.export
### Graph Breaks
As `torch.export` is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of `torch.compile`, an unsupported operation will cause a “graph break” and the unsupported operation will be run with default Python evaluation. In contrast, `torch.export` will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks.
When a graph break is encountered, ExportDB is a great resource for learning about the kinds of programs that are supported and unsupported, along with ways to rewrite programs to make them traceable.
An option to get past dealing with this graph breaks is by using non-strict export
### Data/Shape-Dependent Control Flow
Graph breaks can also be encountered on data-dependent control flow (`if x.shape[0] > 2`) when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators. Currently, we support torch.cond to express if-else like control flow (more coming soon!).
### Missing Fake/Meta/Abstract Kernels for Operators
When tracing, a FakeTensor kernel (aka meta kernel, abstract impl) is required for all operators. This is used to reason about the input/output shapes for this operator.
Please see `torch.library.register_fake()` for more details.
In the unfortunate case where your model uses an ATen operator that is does not have a FakeTensor kernel implementation yet, please file an issue.
## Read More
Additional Links for Export Users
  * torch.export Programming Model
  * torch.export IR Specification
  * Writing Graph Transformations on ATen IR
  * IRs
  * ExportDB
  * Control Flow - Cond


Deep Dive for PyTorch Developers
  * Dynamo Overview
  * Dynamo Deep-Dive
  * Dynamic shapes
  * Fake tensor


## API Reference 

torch.export.export(_mod_ , _args_ , _kwargs =None_, _*_ , _dynamic_shapes =None_, _strict =True_, _preserve_module_call_signature =()_)[source][source]
    
`export()` takes any nn.Module along with example inputs, and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different inputs or serialized. The traced graph (1) produces normalized operators in the functional ATen operator set (as well as any user-specified custom operators), (2) has eliminated all Python control flow and data structures (with certain exceptions), and (3) records the set of shape constraints needed to show that this normalization and control-flow elimination is sound for future inputs.
**Soundness Guarantee**
While tracing, `export()` takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output `ExportedProgram` is considered valid only when these assumptions hold true.
Tracing makes assumptions on the shapes (not values) of input tensors. Such assumptions must be validated at graph capture time for `export()` to succeed. Specifically:
  * Assumptions on static shapes of input tensors are automatically validated without additional effort.
  * Assumptions on dynamic shape of input tensors require explicit specification by using the `Dim()` API to construct dynamic dimensions and by associating them with example inputs through the `dynamic_shapes` argument.


If any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested fixes to the specification that are needed to validate the assumptions. For example `export()` might suggest the following fix to the definition of a dynamic dimension `dim0_x`, say appearing in the shape associated with input `x`, that was previously defined as `Dim("dim0_x")`:
```
dim = Dim("dim0_x", max=5)

```
Copy to clipboard
This example means the generated code requires dimension 0 of input `x` to be less than or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension definitions and then copy them verbatim into your code without needing to change the `dynamic_shapes` argument to your `export()` call. 

Parameters
    
  * **mod** (_Module_) – We will trace the forward method of this module.
  * **args** (_tuple_ _[__Any_ _,__...__]_) – Example positional inputs.
  * **kwargs** (_Optional_ _[__dict_ _[__str_ _,__Any_ _]__]_) – Optional example keyword inputs.
  * **dynamic_shapes** (_Optional_ _[__Union_ _[__dict_ _[__str_ _,__Any_ _]__,__tuple_ _[__Any_ _]__,__list_ _[__Any_ _]__]__]_) – 
An optional argument where the type should either be: 1) a dict from argument names of `f` to their dynamic shape specifications, 2) a tuple that specifies dynamic shape specifications for each input in original order. If you are specifying dynamism on keyword args, you will need to pass them in the order that is defined in the original function signature.
The dynamic shape of a tensor argument can be specified as either (1) a dict from dynamic dimension indices to `Dim()` types, where it is not required to include static dimension indices in this dict, but when they are, they should be mapped to None; or (2) a tuple / list of `Dim()` types or None, where the `Dim()` types correspond to dynamic dimensions, and static dimensions are denoted by None. Arguments that are dicts or tuples / lists of tensors are recursively specified by using mappings or sequences of contained specifications.
  * **strict** (_bool_) – When enabled (default), the export function will trace the program through TorchDynamo which will ensure the soundness of the resulting graph. Otherwise, the exported program will not validate the implicit assumptions baked into the graph and may cause behavior divergence between the original model and the exported one. This is useful when users need to workaround bugs in the tracer, or simply want incrementally enable safety in their models. Note that this does not affect the resulting IR spec to be different and the model will be serialized in the same way regardless of what value is passed here. WARNING: This option is experimental and use this at your own risk.



Returns
    
An `ExportedProgram` containing the traced callable. 

Return type
    
_ExportedProgram_
**Acceptable input/output types**
Acceptable types of inputs (for `args` and `kwargs`) and outputs include:
  * Primitive types, i.e. `torch.Tensor`, `int`, `float`, `bool` and `str`.
  * Dataclasses, but they must be registered by calling `register_dataclass()` first.
  * (Nested) Data structures comprising of `dict`, `list`, `tuple`, `namedtuple` and `OrderedDict` containing all above types.



torch.export.save(_ep_ , _f_ , _*_ , _extra_files =None_, _opset_version =None_, _pickle_protocol =2_)[source][source]
    
Warning
Under active development, saved files may not be usable in newer versions of PyTorch.
Saves an `ExportedProgram` to a file-like object. It can then be loaded using the Python API `torch.export.load`. 

Parameters
    
  * **ep** (_ExportedProgram_) – The exported program to save.
  * **f** (_str_ _|__os.PathLike_ _[__str_ _]__|__IO_ _[__bytes_ _]_) – implement write and flush) or a string containing a file name.
  * **extra_files** (_Optional_ _[__Dict_ _[__str_ _,__Any_ _]__]_) – Map from filename to contents which will be stored as part of f.
  * **opset_version** (_Optional_ _[__Dict_ _[__str_ _,__int_ _]__]_) – A map of opset names to the version of this opset
  * **pickle_protocol** (_int_) – can be specified to override the default protocol


Example:
```
import torch
import io
class MyModule(torch.nn.Module):
  def forward(self, x):
    return x + 10
ep = torch.export.export(MyModule(), (torch.randn(5),))
# Save to file
torch.export.save(ep, 'exported_program.pt2')
# Save to io.BytesIO buffer
buffer = io.BytesIO()
torch.export.save(ep, buffer)
# Save with extra files
extra_files = {'foo.txt': b'bar'.decode('utf-8')}
torch.export.save(ep, 'exported_program.pt2', extra_files=extra_files)

```
Copy to clipboard 

torch.export.load(_f_ , _*_ , _extra_files =None_, _expected_opset_version =None_)[source][source]
    
Warning
Under active development, saved files may not be usable in newer versions of PyTorch.
Loads an `ExportedProgram` previously saved with `torch.export.save`. 

Parameters
    
  * **f** (_str_ _|__os.PathLike_ _[__str_ _]__|__IO_ _[__bytes_ _]_) – A file-like object (has to implement write and flush) or a string containing a file name.
  * **extra_files** (_Optional_ _[__Dict_ _[__str_ _,__Any_ _]__]_) – The extra filenames given in this map would be loaded and their content would be stored in the provided map.
  * **expected_opset_version** (_Optional_ _[__Dict_ _[__str_ _,__int_ _]__]_) – A map of opset names to expected opset versions



Returns
    
An `ExportedProgram` object 

Return type
    
_ExportedProgram_
Example:
```
import torch
import io
# Load ExportedProgram from file
ep = torch.export.load('exported_program.pt2')
# Load ExportedProgram from io.BytesIO object
with open('exported_program.pt2', 'rb') as f:
  buffer = io.BytesIO(f.read())
buffer.seek(0)
ep = torch.export.load(buffer)
# Load with extra files.
extra_files = {'foo.txt': ''} # values will be replaced with data
ep = torch.export.load('exported_program.pt2', extra_files=extra_files)
print(extra_files['foo.txt'])
print(ep(torch.randn(5)))

```
Copy to clipboard 

torch.export.register_dataclass(_cls_ , _*_ , _serialized_type_name =None_)[source][source]
    
Registers a dataclass as a valid input/output type for `torch.export.export()`. 

Parameters
    
  * **cls** (_type_ _[__Any_ _]_) – the dataclass type to register
  * **serialized_type_name** (_Optional_ _[__str_ _]_) – The serialized name for the dataclass. This is
  * **this** (_required if you want to serialize the pytree TreeSpec containing_) – 
  * **dataclass.** – 


Example:
```
import torch
from dataclasses import dataclass
@dataclass
class InputDataClass:
  feature: torch.Tensor
  bias: int
@dataclass
class OutputDataClass:
  res: torch.Tensor
torch.export.register_dataclass(InputDataClass)
torch.export.register_dataclass(OutputDataClass)
class Mod(torch.nn.Module):
  def forward(self, x: InputDataClass) -> OutputDataClass:
    res = x.feature + x.bias
    return OutputDataClass(res=res)
ep = torch.export.export(Mod(), (InputDataClass(torch.ones(2, 2), 1), ))
print(ep)

```
Copy to clipboard 

torch.export.dynamic_shapes.Dim(_name_ , _*_ , _min =None_, _max =None_)[source][source]
    
`Dim()` constructs a type analogous to a named symbolic integer with a range. It can be used to describe multiple possible values of a dynamic tensor dimension. Note that different dynamic dimensions of the same tensor, or of different tensors, can be described by the same type. 

Parameters
    
  * **name** (_str_) – Human-readable name for debugging.
  * **min** (_Optional_ _[__int_ _]_) – Minimum possible value of given symbol (inclusive)
  * **max** (_Optional_ _[__int_ _]_) – Maximum possible value of given symbol (inclusive)



Returns
    
A type that can be used in dynamic shape specifications for tensors. 

torch.export.exported_program.default_decompositions()[source][source]
    
This is the default decomposition table which contains decomposition of all ATEN operators to core aten opset. Use this API together with `run_decompositions()` 

Return type
    
_CustomDecompTable_ 

torch.export.dims(_* names_, _min =None_, _max =None_)[source][source]
    
Util to create multiple `Dim()` types. 

Returns
    
A tuple of `Dim()` types. 

Return type
    
tuple[torch.export.dynamic_shapes._Dim, …] 

_class_ torch.export.dynamic_shapes.ShapesCollection[source][source]
    
Builder for dynamic_shapes. Used to assign dynamic shape specifications to tensors that appear in inputs.
This is useful particularly when `args()` is a nested input structure, and it’s easier to index the input tensors, than to replicate the structure of `args()` in the `dynamic_shapes()` specification.
Example:
```
args = ({"x": tensor_x, "others": [tensor_y, tensor_z]})
dim = torch.export.Dim(...)
dynamic_shapes = torch.export.ShapesCollection()
dynamic_shapes[tensor_x] = (dim, dim + 1, 8)
dynamic_shapes[tensor_y] = {0: dim * 2}
# This is equivalent to the following (now auto-generated):
# dynamic_shapes = {"x": (dim, dim + 1, 8), "others": [{0: dim * 2}, None]}
torch.export(..., args, dynamic_shapes=dynamic_shapes)

```
Copy to clipboard 

dynamic_shapes(_m_ , _args_ , _kwargs =None_)[source][source]
    
Generates the `dynamic_shapes()` pytree structure according to `args()` and `kwargs()`. 

torch.export.dynamic_shapes.refine_dynamic_shapes_from_suggested_fixes(_msg_ , _dynamic_shapes_)[source][source]
    
When exporting with `dynamic_shapes()`, export may fail with a ConstraintViolation error if the specification doesn’t match the constraints inferred from tracing the model. The error message may provide suggested fixes - changes that can be made to `dynamic_shapes()` to export successfully.
Example ConstraintViolation error message:
```
Suggested fixes:
  dim = Dim('dim', min=3, max=6) # this just refines the dim's range
  dim = 4 # this specializes to a constant
  dy = dx + 1 # dy was specified as an independent dim, but is actually tied to dx with this relation

```
Copy to clipboard
This is a helper function that takes the ConstraintViolation error message and the original `dynamic_shapes()` spec, and returns a new `dynamic_shapes()` spec that incorporates the suggested fixes.
Example usage:
```
try:
  ep = export(mod, args, dynamic_shapes=dynamic_shapes)
except torch._dynamo.exc.UserError as exc:
  new_shapes = refine_dynamic_shapes_from_suggested_fixes(
    exc.msg, dynamic_shapes
  )
  ep = export(mod, args, dynamic_shapes=new_shapes)

```
Copy to clipboard 

Return type
    
_Union_[dict[str, _Any_], tuple[_Any_], list[_Any_]] 

torch.export.Constraint
    
alias of `Union`[`_Constraint`, `_DerivedConstraint`, `_RelaxedConstraint`] 

_class_ torch.export.ExportedProgram(_root_ , _graph_ , _graph_signature_ , _state_dict_ , _range_constraints_ , _module_call_graph_ , _example_inputs =None_, _constants =None_, _*_ , _verifiers =None_)[source][source]
    
Package of a program from `export()`. It contains an `torch.fx.Graph` that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.
You can call an ExportedProgram like the original callable traced by `export()` with the same calling convention.
To perform transformations on the graph, use `.module` property to access an `torch.fx.GraphModule`. You can then use FX transformation to rewrite the graph. Afterwards, you can simply use `export()` again to construct a correct ExportedProgram. 

module()[source][source]
    
Returns a self contained GraphModule with all the parameters/buffers inlined. 

Return type
    
_Module_ 

buffers()[source][source]
    
Returns an iterator over original module buffers.
Warning
This API is experimental and is _NOT_ backward-compatible. 

Return type
    
_Iterator_[_Tensor_] 

named_buffers()[source][source]
    
Returns an iterator over original module buffers, yielding both the name of the buffer as well as the buffer itself.
Warning
This API is experimental and is _NOT_ backward-compatible. 

Return type
    
_Iterator_[tuple[str, torch.Tensor]] 

parameters()[source][source]
    
Returns an iterator over original module’s parameters.
Warning
This API is experimental and is _NOT_ backward-compatible. 

Return type
    
_Iterator_[_Parameter_] 

named_parameters()[source][source]
    
Returns an iterator over original module parameters, yielding both the name of the parameter as well as the parameter itself.
Warning
This API is experimental and is _NOT_ backward-compatible. 

Return type
    
_Iterator_[tuple[str, torch.nn.parameter.Parameter]] 

run_decompositions(_decomp_table =None_, _decompose_custom_triton_ops =False_)[source][source]
    
Run a set of decompositions on the exported program and returns a new exported program. By default we will run the Core ATen decompositions to get operators in the Core ATen Operator Set.
For now, we do not decompose joint graphs. 

Parameters
    
**decomp_table** (_Optional_ _[__dict_ _[__torch._ops.OperatorBase_ _,__Callable_ _]__]_) – An optional argument that specifies decomp behaviour for Aten ops (1) If None, we decompose to core aten decompositions (2) If empty, we don’t decompose any operator 

Return type
    
_ExportedProgram_
Some examples:
If you don’t want to decompose anything
```
ep = torch.export.export(model, ...)
ep = ep.run_decompositions(decomp_table={})

```
Copy to clipboard
If you want to get a core aten operator set except for certain operator, you can do following:
```
ep = torch.export.export(model, ...)
decomp_table = torch.export.default_decompositions()
decomp_table[your_op] = your_custom_decomp
ep = ep.run_decompositions(decomp_table=decomp_table)

```
Copy to clipboard 

_class_ torch.export.ExportBackwardSignature(_gradients_to_parameters :dict[str,str]_, _gradients_to_user_inputs :dict[str,str]_, _loss_output :str_)[source][source]


_class_ torch.export.ExportGraphSignature(_input_specs_ , _output_specs_)[source][source]
    
`ExportGraphSignature` models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.
Export Graph is functional and does not access “states” like parameters or buffers within the graph via `getattr` nodes. Instead, `export()` gurantees that parameters, buffers, and constant tensors are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.
The ordering of all inputs and outputs are:
```
Inputs = [*parameters_buffers_constant_tensors, *flattened_user_inputs]
Outputs = [*mutated_inputs, *flattened_user_outputs]

```
Copy to clipboard
e.g. If following module is exported:
```
class CustomModule(nn.Module):
  def __init__(self) -> None:
    super(CustomModule, self).__init__()
    # Define a parameter
    self.my_parameter = nn.Parameter(torch.tensor(2.0))
    # Define two buffers
    self.register_buffer('my_buffer1', torch.tensor(3.0))
    self.register_buffer('my_buffer2', torch.tensor(4.0))
  def forward(self, x1, x2):
    # Use the parameter, buffers, and both inputs in the forward method
    output = (x1 + self.my_parameter) * self.my_buffer1 + x2 * self.my_buffer2
    # Mutate one of the buffers (e.g., increment it by 1)
    self.my_buffer2.add_(1.0) # In-place addition
    return output

```
Copy to clipboard
Resulting Graph would be:
```
graph():
  %arg0_1 := placeholder[target=arg0_1]
  %arg1_1 := placeholder[target=arg1_1]
  %arg2_1 := placeholder[target=arg2_1]
  %arg3_1 := placeholder[target=arg3_1]
  %arg4_1 := placeholder[target=arg4_1]
  %add_tensor := call_function[target=torch.ops.aten.add.Tensor](args = (%arg3_1, %arg0_1), kwargs = {})
  %mul_tensor := call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, %arg1_1), kwargs = {})
  %mul_tensor_1 := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg4_1, %arg2_1), kwargs = {})
  %add_tensor_1 := call_function[target=torch.ops.aten.add.Tensor](args = (%mul_tensor, %mul_tensor_1), kwargs = {})
  %add_tensor_2 := call_function[target=torch.ops.aten.add.Tensor](args = (%arg2_1, 1.0), kwargs = {})
  return (add_tensor_2, add_tensor_1)

```
Copy to clipboard
Resulting ExportGraphSignature would be:
```
ExportGraphSignature(
  input_specs=[
    InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg0_1'), target='my_parameter'),
    InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg1_1'), target='my_buffer1'),
    InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg2_1'), target='my_buffer2'),
    InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg3_1'), target=None),
    InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg4_1'), target=None)
  ],
  output_specs=[
    OutputSpec(kind=<OutputKind.BUFFER_MUTATION: 3>, arg=TensorArgument(name='add_2'), target='my_buffer2'),
    OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_1'), target=None)
  ]
)

```
Copy to clipboard 

_class_ torch.export.ModuleCallSignature(_inputs :list[Union[torch.export.graph_signature.TensorArgument,torch.export.graph_signature.SymIntArgument,torch.export.graph_signature.SymFloatArgument,torch.export.graph_signature.SymBoolArgument,torch.export.graph_signature.ConstantArgument,torch.export.graph_signature.CustomObjArgument,torch.export.graph_signature.TokenArgument]]_, _outputs :list[Union[torch.export.graph_signature.TensorArgument,torch.export.graph_signature.SymIntArgument,torch.export.graph_signature.SymFloatArgument,torch.export.graph_signature.SymBoolArgument,torch.export.graph_signature.ConstantArgument,torch.export.graph_signature.CustomObjArgument,torch.export.graph_signature.TokenArgument]]_, _in_spec :torch.utils._pytree.TreeSpec_, _out_spec :torch.utils._pytree.TreeSpec_, _forward_arg_names :Optional[list[str]]=None_)[source][source]


_class_ torch.export.ModuleCallEntry(_fqn :str_, _signature :Optional[torch.export.exported_program.ModuleCallSignature]=None_)[source][source]


_class_ torch.export.decomp_utils.CustomDecompTable[source][source]
    
This is a custom dictionary that is specifically used for handling decomp_table in export. The reason we need this is because in the new world, you can only _delete_ an op from decomp table to preserve it. This is problematic for custom ops because we don’t know when the custom op will actually be loaded to the dispatcher. As a result, we need to record the custom ops operations until we really need to materialize it (which is when we run decomposition pass.) 

Invariants we hold are:
    
  1. All aten decomp is loaded at the init time
  2. We materialize ALL ops when user ever reads from the table to make it more likely that dispatcher picks up the custom op.
  3. If it is write operation, we don’t necessarily materialize
  4. We load the final time during export, right before calling run_decompositions()



copy()[source][source]
     

Return type
    
_CustomDecompTable_ 

items()[source][source]


keys()[source][source]


materialize()[source][source]
     

Return type
    
dict[torch._ops.OperatorBase, _Callable_] 

pop(_* args_)[source][source]


update(_other_dict_)[source][source]


_class_ torch.export.graph_signature.InputKind(_value_)[source][source]
    
An enumeration. 

_class_ torch.export.graph_signature.InputSpec(_kind :torch.export.graph_signature.InputKind_, _arg :Union[torch.export.graph_signature.TensorArgument,torch.export.graph_signature.SymIntArgument,torch.export.graph_signature.SymFloatArgument,torch.export.graph_signature.SymBoolArgument,torch.export.graph_signature.ConstantArgument,torch.export.graph_signature.CustomObjArgument,torch.export.graph_signature.TokenArgument]_, _target :Optional[str]_, _persistent :Optional[bool]=None_)[source][source]


_class_ torch.export.graph_signature.OutputKind(_value_)[source][source]
    
An enumeration. 

_class_ torch.export.graph_signature.OutputSpec(_kind :torch.export.graph_signature.OutputKind_, _arg :Union[torch.export.graph_signature.TensorArgument,torch.export.graph_signature.SymIntArgument,torch.export.graph_signature.SymFloatArgument,torch.export.graph_signature.SymBoolArgument,torch.export.graph_signature.ConstantArgument,torch.export.graph_signature.CustomObjArgument,torch.export.graph_signature.TokenArgument]_, _target :Optional[str]_)[source][source]


_class_ torch.export.graph_signature.SymIntArgument(_name :str_)[source][source]


_class_ torch.export.graph_signature.SymBoolArgument(_name :str_)[source][source]


_class_ torch.export.graph_signature.SymFloatArgument(_name :str_)[source][source]


_class_ torch.export.graph_signature.ExportGraphSignature(_input_specs_ , _output_specs_)[source][source]
    
`ExportGraphSignature` models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.
Export Graph is functional and does not access “states” like parameters or buffers within the graph via `getattr` nodes. Instead, `export()` gurantees that parameters, buffers, and constant tensors are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.
The ordering of all inputs and outputs are:
```
Inputs = [*parameters_buffers_constant_tensors, *flattened_user_inputs]
Outputs = [*mutated_inputs, *flattened_user_outputs]

```
Copy to clipboard
e.g. If following module is exported:
```
class CustomModule(nn.Module):
  def __init__(self) -> None:
    super(CustomModule, self).__init__()
    # Define a parameter
    self.my_parameter = nn.Parameter(torch.tensor(2.0))
    # Define two buffers
    self.register_buffer('my_buffer1', torch.tensor(3.0))
    self.register_buffer('my_buffer2', torch.tensor(4.0))
  def forward(self, x1, x2):
    # Use the parameter, buffers, and both inputs in the forward method
    output = (x1 + self.my_parameter) * self.my_buffer1 + x2 * self.my_buffer2
    # Mutate one of the buffers (e.g., increment it by 1)
    self.my_buffer2.add_(1.0) # In-place addition
    return output

```
Copy to clipboard
Resulting Graph would be:
```
graph():
  %arg0_1 := placeholder[target=arg0_1]
  %arg1_1 := placeholder[target=arg1_1]
  %arg2_1 := placeholder[target=arg2_1]
  %arg3_1 := placeholder[target=arg3_1]
  %arg4_1 := placeholder[target=arg4_1]
  %add_tensor := call_function[target=torch.ops.aten.add.Tensor](args = (%arg3_1, %arg0_1), kwargs = {})
  %mul_tensor := call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, %arg1_1), kwargs = {})
  %mul_tensor_1 := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg4_1, %arg2_1), kwargs = {})
  %add_tensor_1 := call_function[target=torch.ops.aten.add.Tensor](args = (%mul_tensor, %mul_tensor_1), kwargs = {})
  %add_tensor_2 := call_function[target=torch.ops.aten.add.Tensor](args = (%arg2_1, 1.0), kwargs = {})
  return (add_tensor_2, add_tensor_1)

```
Copy to clipboard
Resulting ExportGraphSignature would be:
```
ExportGraphSignature(
  input_specs=[
    InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg0_1'), target='my_parameter'),
    InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg1_1'), target='my_buffer1'),
    InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg2_1'), target='my_buffer2'),
    InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg3_1'), target=None),
    InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg4_1'), target=None)
  ],
  output_specs=[
    OutputSpec(kind=<OutputKind.BUFFER_MUTATION: 3>, arg=TensorArgument(name='add_2'), target='my_buffer2'),
    OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_1'), target=None)
  ]
)

```
Copy to clipboard 

replace_all_uses(_old_ , _new_)[source][source]
    
Replace all uses of the old name with new name in the signature. 

get_replace_hook(_replace_inputs =False_)[source][source]


_class_ torch.export.graph_signature.CustomObjArgument(_name :str_, _class_fqn :str_, _fake_val :Optional[torch._library.fake_class_registry.FakeScriptObject]=None_)[source][source]


_class_ torch.export.unflatten.FlatArgsAdapter[source][source]
    
Adapts input arguments with `input_spec` to align `target_spec`. 

_abstract_ adapt(_target_spec_ , _input_spec_ , _input_args_ , _metadata =None_)[source][source]
    
NOTE: This adapter may mutate given `input_args_with_path`. 

Return type
    
list[_Any_] 

_class_ torch.export.unflatten.InterpreterModule(_graph_ , _ty =None_)[source][source]
    
A module that uses torch.fx.Interpreter to execute instead of the usual codegen that GraphModule uses. This provides better stack trace information and makes it easier to debug execution. 

_class_ torch.export.unflatten.InterpreterModuleDispatcher(_attrs_ , _call_modules_)[source][source]
    
A module that carries a sequence of InterpreterModules corresponding to a sequence of calls of that module. Each call to the module dispatches to the next InterpreterModule, and wraps back around after the last. 

torch.export.unflatten.unflatten(_module_ , _flat_args_adapter =None_)[source][source]
    
Unflatten an ExportedProgram, producing a module with the same module hierarchy as the original eager module. This can be useful if you are trying to use `torch.export` with another system that expects a module hierachy instead of the flat graph that `torch.export` usually produces.
Note
The args/kwargs of unflattened modules will not necessarily match the eager module, so doing a module swap (e.g. `self.submod = new_mod`) will not necessarily work. If you need to swap a module out, you need to set the `preserve_module_call_signature` parameter of `torch.export.export()`. 

Parameters
    
  * **module** (_ExportedProgram_) – The ExportedProgram to unflatten.
  * **flat_args_adapter** (_Optional_ _[__FlatArgsAdapter_ _]_) – Adapt flat args if input TreeSpec does not match with exported module’s.



Returns
    
An instance of `UnflattenedModule`, which has the same module hierarchy as the original eager module pre-export. 

Return type
    
_UnflattenedModule_ 

torch.export.passes.move_to_device_pass(_ep_ , _location_)[source][source]
    
Move the exported program to the given device. 

Parameters
    
  * **ep** (_ExportedProgram_) – The exported program to move.
  * **location** (_Union_ _[__torch.device_ _,__str_ _,__Dict_ _[__str_ _,__str_ _]__]_) – The device to move the exported program to. If a string, it is interpreted as a device name. If a dict, it is interpreted as a mapping from the existing device to the intended one



Returns
    
The moved exported program. 

Return type
    
ExportedProgram
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.export
    * Overview
      * Existing frameworks
    * Exporting a PyTorch Model
      * An Example
      * Non-Strict Export
      * Export for Training and Inference
      * Expressing Dynamism
      * Serialization
      * Specializations
        * Input Tensor Shapes
        * Python Primitives
        * Python Containers
    * Limitations of torch.export
      * Graph Breaks
      * Data/Shape-Dependent Control Flow
      * Missing Fake/Meta/Abstract Kernels for Operators
    * Read More
    * API Reference
      * `export()`
      * `save()`
      * `load()`
      * `register_dataclass()`
      * `Dim()`
      * `default_decompositions()`
      * `dims()`
      * `ShapesCollection`
        * `ShapesCollection.dynamic_shapes()`
      * `refine_dynamic_shapes_from_suggested_fixes()`
      * `Constraint`
      * `ExportedProgram`
        * `ExportedProgram.module()`
        * `ExportedProgram.buffers()`
        * `ExportedProgram.named_buffers()`
        * `ExportedProgram.parameters()`
        * `ExportedProgram.named_parameters()`
        * `ExportedProgram.run_decompositions()`
      * `ExportBackwardSignature`
      * `ExportGraphSignature`
      * `ModuleCallSignature`
      * `ModuleCallEntry`
      * `CustomDecompTable`
        * `CustomDecompTable.copy()`
        * `CustomDecompTable.items()`
        * `CustomDecompTable.keys()`
        * `CustomDecompTable.materialize()`
        * `CustomDecompTable.pop()`
        * `CustomDecompTable.update()`
      * `InputKind`
      * `InputSpec`
      * `OutputKind`
      * `OutputSpec`
      * `SymIntArgument`
      * `SymBoolArgument`
      * `SymFloatArgument`
      * `ExportGraphSignature`
        * `ExportGraphSignature.replace_all_uses()`
        * `ExportGraphSignature.get_replace_hook()`
      * `CustomObjArgument`
      * `FlatArgsAdapter`
        * `FlatArgsAdapter.adapt()`
      * `InterpreterModule`
      * `InterpreterModuleDispatcher`
      * `unflatten()`
      * `move_to_device_pass()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * FullyShardedDataParallel
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# FullyShardedDataParallel 

_class_ torch.distributed.fsdp.FullyShardedDataParallel(_module_ , _process_group =None_, _sharding_strategy =None_, _cpu_offload =None_, _auto_wrap_policy =None_, _backward_prefetch =BackwardPrefetch.BACKWARD_PRE_, _mixed_precision =None_, _ignored_modules =None_, _param_init_fn =None_, _device_id =None_, _sync_module_states =False_, _forward_prefetch =False_, _limit_all_gathers =True_, _use_orig_params =False_, _ignored_states =None_, _device_mesh =None_)[source][source]
    
A wrapper for sharding module parameters across data parallel workers.
This is inspired by Xu et al. as well as the ZeRO Stage 3 from DeepSpeed. FullyShardedDataParallel is commonly shortened to FSDP.
To understand FSDP internals, refer to the FSDP Notes.
Example:
```
>>> import torch
>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
>>> torch.cuda.set_device(device_id)
>>> sharded_module = FSDP(my_module)
>>> optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)
>>> x = sharded_module(x, y=3, z=torch.Tensor([1]))
>>> loss = x.sum()
>>> loss.backward()
>>> optim.step()

```
Copy to clipboard
Using FSDP involves wrapping your module and then initializing your optimizer after. This is required since FSDP changes the parameter variables.
When setting up FSDP, you need to consider the destination CUDA device. If the device has an ID (`dev_id`), you have three options:
  * Place the module on that device
  * Set the device using `torch.cuda.set_device(dev_id)`
  * Pass `dev_id` into the `device_id` constructor argument.


This ensures that the FSDP instance’s compute device is the destination device. For option 1 and 3, the FSDP initialization always occurs on GPU. For option 2, the FSDP initialization happens on module’s current device, which may be a CPU.
If you’re using the `sync_module_states=True` flag, you need to ensure that the module is on a GPU or use the `device_id` argument to specify a CUDA device that FSDP will move the module to in the FSDP constructor. This is necessary because `sync_module_states=True` requires GPU communication.
FSDP also takes care of moving input tensors to the forward method to the GPU compute device, so you don’t need to manually move them from CPU.
For `use_orig_params=True`, `ShardingStrategy.SHARD_GRAD_OP` exposes the unsharded parameters, not the sharded parameters after forward, unlike `ShardingStrategy.FULL_SHARD`. If you want to inspect the gradients, you can use the `summon_full_params` method with `with_grads=True`.
With `limit_all_gathers=True`, you may see a gap in the FSDP pre-forward where the CPU thread is not issuing any kernels. This is intentional and shows the rate limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating memory for subsequent all-gathers, and it should not actually delay GPU kernel execution.
FSDP replaces managed modules’ parameters with `torch.Tensor` views during forward and backward computation for autograd-related reasons. If your module’s forward relies on saved references to the parameters instead of reacquiring the references each iteration, then it will not see FSDP’s newly created views, and autograd will not work correctly.
Finally, when using `sharding_strategy=ShardingStrategy.HYBRID_SHARD` with the sharding process group being intra-node and the replication process group being inter-node, setting `NCCL_CROSS_NIC=1` can help improve the all-reduce times over the replication process group for some cluster setups.
**Limitations**
There are several limitations to be aware of when using FSDP:
  * FSDP currently does not support gradient accumulation outside `no_sync()` when using CPU offloading. This is because FSDP uses the newly-reduced gradient instead of accumulating with any existing gradient, which can lead to incorrect results.
  * FSDP does not support running the forward pass of a submodule that is contained in an FSDP instance. This is because the submodule’s parameters will be sharded, but the submodule itself is not an FSDP instance, so its forward pass will not all-gather the full parameters appropriately.
  * FSDP does not work with double backwards due to the way it registers backward hooks.
  * FSDP has some constraints when freezing parameters. For `use_orig_params=False`, each FSDP instance must manage parameters that are all frozen or all non-frozen. For `use_orig_params=True`, FSDP supports mixing frozen and non-frozen parameters, but it’s recommended to avoid doing so to prevent higher than expected gradient memory usage.
  * As of PyTorch 1.12, FSDP offers limited support for shared parameters. If enhanced shared parameter support is needed for your use case, please post in this issue.
  * You should avoid modifying the parameters between forward and backward without using the `summon_full_params` context, as the modifications may not persist.



Parameters
    
  * **module** (_nn.Module_) – This is the module to be wrapped with FSDP.
  * **process_group** (_Optional_ _[__Union_ _[__ProcessGroup_ _,__Tuple_ _[__ProcessGroup_ _,__ProcessGroup_ _]__]__]_) – This is the process group over which the model is sharded and thus the one used for FSDP’s all-gather and reduce-scatter collective communications. If `None`, then FSDP uses the default process group. For hybrid sharding strategies such as `ShardingStrategy.HYBRID_SHARD`, users can pass in a tuple of process groups, representing the groups over which to shard and replicate, respectively. If `None`, then FSDP constructs process groups for the user to shard intra-node and replicate inter-node. (Default: `None`)
  * **sharding_strategy** (_Optional_ _[__ShardingStrategy_ _]_) – This configures the sharding strategy, which may trade off memory saving and communication overhead. See `ShardingStrategy` for details. (Default: `FULL_SHARD`)
  * **cpu_offload** (_Optional_ _[__CPUOffload_ _]_) – This configures CPU offloading. If this is set to `None`, then no CPU offloading happens. See `CPUOffload` for details. (Default: `None`)
  * **auto_wrap_policy** (_Optional_ _[__Union_ _[__Callable_ _[__[__nn.Module_ _,__bool_ _,__int_ _]__,__bool_ _]__,__ModuleWrapPolicy_ _,__CustomPolicy_ _]__]_) – 
This specifies a policy to apply FSDP to submodules of `module`, which is needed for communication and computation overlap and thus affects performance. If `None`, then FSDP only applies to `module`, and users should manually apply FSDP to parent modules themselves (proceeding bottom-up). For convenience, this accepts `ModuleWrapPolicy` directly, which allows users to specify the module classes to wrap (e.g. the transformer block). Otherwise, this should be a callable that takes in three arguments `module: nn.Module`, `recurse: bool`, and `nonwrapped_numel: int` and should return a `bool` specifying whether the passed-in `module` should have FSDP applied if `recurse=False` or if the traversal should continue into the module’s subtree if `recurse=True`. Users may add additional arguments to the callable. The `size_based_auto_wrap_policy` in `torch.distributed.fsdp.wrap.py` gives an example callable that applies FSDP to a module if the parameters in its subtree exceed 100M numel. We recommend printing the model after applying FSDP and adjusting as needed.
Example:
```
>>> def custom_auto_wrap_policy(
>>>   module: nn.Module,
>>>   recurse: bool,
>>>   nonwrapped_numel: int,
>>>   # Additional custom arguments
>>>   min_num_params: int = int(1e8),
>>> ) -> bool:
>>>   return nonwrapped_numel >= min_num_params
>>> # Configure a custom `min_num_params`
>>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))

```
Copy to clipboard
  * **backward_prefetch** (_Optional_ _[__BackwardPrefetch_ _]_) – This configures explicit backward prefetching of all-gathers. If `None`, then FSDP does not backward prefetch, and there is no communication and computation overlap in the backward pass. See `BackwardPrefetch` for details. (Default: `BACKWARD_PRE`)
  * **mixed_precision** (_Optional_ _[__MixedPrecision_ _]_) – This configures native mixed precision for FSDP. If this is set to `None`, then no mixed precision is used. Otherwise, parameter, buffer, and gradient reduction dtypes can be set. See `MixedPrecision` for details. (Default: `None`)
  * **ignored_modules** (_Optional_ _[__Iterable_ _[__torch.nn.Module_ _]__]_) – Modules whose own parameters and child modules’ parameters and buffers are ignored by this instance. None of the modules directly in `ignored_modules` should be `FullyShardedDataParallel` instances, and any child modules that are already-constructed `FullyShardedDataParallel` instances will not be ignored if they are nested under this instance. This argument may be used to avoid sharding specific parameters at module granularity when using an `auto_wrap_policy` or if parameters’ sharding is not managed by FSDP. (Default: `None`)
  * **param_init_fn** (_Optional_ _[__Callable_ _[__[__nn.Module_ _]__,__None_ _]__]_) – 
A `Callable[torch.nn.Module] -> None` that specifies how modules that are currently on the meta device should be initialized onto an actual device. As of v1.12, FSDP detects modules with parameters or buffers on meta device via `is_meta` and either applies `param_init_fn` if specified or calls `nn.Module.reset_parameters()` otherwise. For both cases, the implementation should _only_ initialize the parameters/buffers of the module, not those of its submodules. This is to avoid re-initialization. In addition, FSDP also supports deferred initialization via torchdistX’s (https://github.com/pytorch/torchdistX) `deferred_init()` API, where the deferred modules are initialized by calling `param_init_fn` if specified or torchdistX’s default `materialize_module()` otherwise. If `param_init_fn` is specified, then it is applied to all meta-device modules, meaning that it should probably case on the module type. FSDP calls the initialization function before parameter flattening and sharding.
Example:
```
>>> module = MyModule(device="meta")
>>> def my_init_fn(module: nn.Module):
>>>   # E.g. initialize depending on the module type
>>>   ...
>>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)
>>> print(next(fsdp_model.parameters()).device) # current CUDA device
>>> # With torchdistX
>>> module = deferred_init.deferred_init(MyModule, device="cuda")
>>> # Will initialize via deferred_init.materialize_module().
>>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)

```
Copy to clipboard
  * **device_id** (_Optional_ _[__Union_ _[__int_ _,__torch.device_ _]__]_) – An `int` or `torch.device` giving the CUDA device on which FSDP initialization takes place, including the module initialization if needed and the parameter sharding. This should be specified to improve initialization speed if `module` is on CPU. If the default CUDA device was set (e.g. via `torch.cuda.set_device`), then the user may pass `torch.cuda.current_device` to this. (Default: `None`)
  * **sync_module_states** (_bool_) – If `True`, then each FSDP module will broadcast module parameters and buffers from rank 0 to ensure that they are replicated across ranks (adding communication overhead to this constructor). This can help load `state_dict` checkpoints via `load_state_dict` in a memory efficient way. See `FullStateDictConfig` for an example of this. (Default: `False`)
  * **forward_prefetch** (_bool_) – If `True`, then FSDP _explicitly_ prefetches the next forward-pass all-gather before the current forward computation. This is only useful for CPU-bound workloads, in which case issuing the next all-gather earlier may improve overlap. This should only be used for static-graph models since the prefetching follows the first iteration’s execution order. (Default: `False`)
  * **limit_all_gathers** (_bool_) – If `True`, then FSDP explicitly synchronizes the CPU thread to ensure GPU memory usage from only _two_ consecutive FSDP instances (the current instance running computation and the next instance whose all-gather is prefetched). If `False`, then FSDP allows the CPU thread to issue all-gathers without any extra synchronization. (Default: `True`) We often refer to this feature as the “rate limiter”. This flag should only be set to `False` for specific CPU-bound workloads with low memory pressure in which case the CPU thread can aggressively issue all kernels without concern for the GPU memory usage.
  * **use_orig_params** (_bool_) – Setting this to `True` has FSDP use `module` ‘s original parameters. FSDP exposes those original parameters to the user via `nn.Module.named_parameters()` instead of FSDP’s internal `FlatParameter` s. This means that the optimizer step runs on the original parameters, enabling per-original-parameter hyperparameters. FSDP preserves the original parameter variables and manipulates their data between unsharded and sharded forms, where they are always views into the underlying unsharded or sharded `FlatParameter`, respectively. With the current algorithm, the sharded form is always 1D, losing the original tensor structure. An original parameter may have all, some, or none of its data present for a given rank. In the none case, its data will be like a size-0 empty tensor. Users should not author programs relying on what data is present for a given original parameter in its sharded form. `True` is required to use `torch.compile()`. Setting this to `False` exposes FSDP’s internal `FlatParameter` s to the user via `nn.Module.named_parameters()`. (Default: `False`)
  * **ignored_states** (_Optional_ _[__Iterable_ _[__torch.nn.Parameter_ _]__]__,__Optional_ _[__Iterable_ _[__torch.nn.Module_ _]__]_) – Ignored parameters or modules that will not be managed by this FSDP instance, meaning that the parameters are not sharded and their gradients are not reduced across ranks. This argument unifies with the existing `ignored_modules` argument, and we may deprecate `ignored_modules` soon. For backward compatibility, we keep both `ignored_states` and ignored_modules`, but FSDP only allows one of them to be specified as not `None`.
  * **device_mesh** (_Optional_ _[__DeviceMesh_ _]_) – DeviceMesh can be used as an altenative to process_group. When device_mesh is passed, FSDP will use the underlying process groups for all-gather and reduce-scatter collective communications. Therefore, these two args need to be mutually exclusive. For hybrid sharding strategies such as `ShardingStrategy.HYBRID_SHARD`, users can pass in a 2D DeviceMesh instead of a tuple of process groups. For 2D FSDP + TP, users are required to pass in device_mesh instead of process_group. For more DeviceMesh info, please visit: https://pytorch.org/tutorials/recipes/distributed_device_mesh.html



apply(_fn_)[source][source]
    
Apply `fn` recursively to every submodule (as returned by `.children()`) as well as self.
Typical use includes initializing the parameters of a model (see also torch.nn.init).
Compared to `torch.nn.Module.apply`, this version additionally gathers the full parameters before applying `fn`. It should not be called from within another `summon_full_params` context. 

Parameters
    
**fn** (`Module` -> None) – function to be applied to each submodule 

Returns
    
self 

Return type
    
Module 

check_is_root()[source][source]
    
Check if this instance is a root FSDP module. 

Return type
    
bool 

clip_grad_norm_(_max_norm_ , _norm_type =2.0_)[source][source]
    
Clip the gradient norm of all parameters.
The norm is computed over all parameters’ gradients as viewed as a single vector, and the gradients are modified in-place. 

Parameters
    
  * **max_norm** (_float_ _or_ _int_) – max norm of the gradients
  * **norm_type** (_float_ _or_ _int_) – type of the used p-norm. Can be `'inf'` for infinity norm.



Returns
    
Total norm of the parameters (viewed as a single vector). 

Return type
    
_Tensor_
If every FSDP instance uses `NO_SHARD`, meaning that no gradients are sharded across ranks, then you may directly use `torch.nn.utils.clip_grad_norm_()`.
If at least some FSDP instance uses a sharded strategy (i.e. one other than `NO_SHARD`), then you should use this method instead of `torch.nn.utils.clip_grad_norm_()` since this method handles the fact that gradients are sharded across ranks.
The total norm returned will have the “largest” dtype across all parameters/gradients as defined by PyTorch’s type promotion semantics. For example, if _all_ parameters/gradients use a low precision dtype, then the returned norm’s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm’s dtype will be FP32.
Warning
This needs to be called on all ranks since it uses collective communications. 

_static_ flatten_sharded_optim_state_dict(_sharded_optim_state_dict_ , _model_ , _optim_)[source][source]
    
Flatten a sharded optimizer state-dict.
The API is similar to `shard_full_optim_state_dict()`. The only difference is that the input `sharded_optim_state_dict` should be returned from `sharded_optim_state_dict()`. Therefore, there will be all-gather calls on each rank to gather `ShardedTensor` s. 

Parameters
    
  * **sharded_optim_state_dict** (_Dict_ _[__str_ _,__Any_ _]_) – Optimizer state dict corresponding to the unflattened parameters and holding the sharded optimizer state.
  * **model** (_torch.nn.Module_) – Refer to `shard_full_optim_state_dict()`.
  * **optim** (_torch.optim.Optimizer_) – Optimizer for `model` ‘s parameters.



Returns
    
Refer to `shard_full_optim_state_dict()`. 

Return type
    
dict[str, _Any_] 

forward(_* args_, _** kwargs_)[source][source]
    
Run the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic. 

Return type
    
_Any_ 

_static_ fsdp_modules(_module_ , _root_only =False_)[source][source]
    
Return all nested FSDP instances.
This possibly includes `module` itself and only includes FSDP root modules if `root_only=True`. 

Parameters
    
  * **module** (_torch.nn.Module_) – Root module, which may or may not be an `FSDP` module.
  * **root_only** (_bool_) – Whether to return only FSDP root modules. (Default: `False`)



Returns
    
FSDP modules that are nested in the input `module`. 

Return type
    
List[FullyShardedDataParallel] 

_static_ full_optim_state_dict(_model_ , _optim_ , _optim_input =None_, _rank0_only =True_, _group =None_)[source][source]
    
Return the full optimizer state-dict.
Consolidates the full optimizer state on rank 0 and returns it as a `dict` following the convention of `torch.optim.Optimizer.state_dict()`, i.e. with keys `"state"` and `"param_groups"`. The flattened parameters in `FSDP` modules contained in `model` are mapped back to their unflattened parameters.
This needs to be called on all ranks since it uses collective communications. However, if `rank0_only=True`, then the state dict is only populated on rank 0, and all other ranks return an empty `dict`.
Unlike `torch.optim.Optimizer.state_dict()`, this method uses full parameter names as keys instead of parameter IDs.
Like in `torch.optim.Optimizer.state_dict()`, the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using `torch.save()`. 

Parameters
    
  * **model** (_torch.nn.Module_) – Root module (which may or may not be a `FullyShardedDataParallel` instance) whose parameters were passed into the optimizer `optim`.
  * **optim** (_torch.optim.Optimizer_) – Optimizer for `model` ‘s parameters.
  * **optim_input** (_Optional_ _[__Union_ _[__List_ _[__Dict_ _[__str_ _,__Any_ _]__]__,__Iterable_ _[__torch.nn.Parameter_ _]__]__]_) – Input passed into the optimizer `optim` representing either a `list` of parameter groups or an iterable of parameters; if `None`, then this method assumes the input was `model.parameters()`. This argument is deprecated, and there is no need to pass it in anymore. (Default: `None`)
  * **rank0_only** (_bool_) – If `True`, saves the populated `dict` only on rank 0; if `False`, saves it on all ranks. (Default: `True`)
  * **group** (_dist.ProcessGroup_) – Model’s process group or `None` if using the default process group. (Default: `None`)



Returns
    
A `dict` containing the optimizer state for `model` ‘s original unflattened parameters and including keys “state” and “param_groups” following the convention of `torch.optim.Optimizer.state_dict()`. If `rank0_only=True`, then nonzero ranks return an empty `dict`. 

Return type
    
Dict[str, Any] 

_static_ get_state_dict_type(_module_)[source][source]
    
Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at `module`.
The target module does not have to be an FSDP module. 

Returns
    
A `StateDictSettings` containing the state_dict_type and state_dict / optim_state_dict configs that are currently set. 

Raises
    
  * **AssertionError` if the StateDictSettings for differen** – 
  * **FSDP submodules differ.** – 



Return type
    
_StateDictSettings_ 

_property_ module _: Module_
    
Return the wrapped module. 

named_buffers(_* args_, _** kwargs_)[source][source]
    
Return an iterator over module buffers, yielding both the name of the buffer and the buffer itself.
Intercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix when inside the `summon_full_params()` context manager. 

Return type
    
_Iterator_[tuple[str, torch.Tensor]] 

named_parameters(_* args_, _** kwargs_)[source][source]
    
Return an iterator over module parameters, yielding both the name of the parameter and the parameter itself.
Intercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix when inside the `summon_full_params()` context manager. 

Return type
    
_Iterator_[tuple[str, torch.nn.parameter.Parameter]] 

no_sync()[source][source]
    
Disable gradient synchronizations across FSDP instances.
Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.
Note
This likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.
Note
When used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync. 

Return type
    
_Generator_ 

_static_ optim_state_dict(_model_ , _optim_ , _optim_state_dict =None_, _group =None_)[source][source]
    
Transform the state-dict of an optimizer corresponding to a sharded model.
The given state-dict can be transformed to one of three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.
For full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via `state_dict_type()` to avoid OOM.
For sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via `state_dict_type()` to further save memory.
For local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet).
Example:
```
>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
>>> from torch.distributed.fsdp import StateDictType
>>> from torch.distributed.fsdp import FullStateDictConfig
>>> from torch.distributed.fsdp import FullOptimStateDictConfig
>>> # Save a checkpoint
>>> model, optim = ...
>>> FSDP.set_state_dict_type(
>>>   model,
>>>   StateDictType.FULL_STATE_DICT,
>>>   FullStateDictConfig(rank0_only=False),
>>>   FullOptimStateDictConfig(rank0_only=False),
>>> )
>>> state_dict = model.state_dict()
>>> optim_state_dict = FSDP.optim_state_dict(model, optim)
>>> save_a_checkpoint(state_dict, optim_state_dict)
>>> # Load a checkpoint
>>> model, optim = ...
>>> state_dict, optim_state_dict = load_a_checkpoint()
>>> FSDP.set_state_dict_type(
>>>   model,
>>>   StateDictType.FULL_STATE_DICT,
>>>   FullStateDictConfig(rank0_only=False),
>>>   FullOptimStateDictConfig(rank0_only=False),
>>> )
>>> model.load_state_dict(state_dict)
>>> optim_state_dict = FSDP.optim_state_dict_to_load(
>>>   model, optim, optim_state_dict
>>> )
>>> optim.load_state_dict(optim_state_dict)

```
Copy to clipboard 

Parameters
    
  * **model** (_torch.nn.Module_) – Root module (which may or may not be a `FullyShardedDataParallel` instance) whose parameters were passed into the optimizer `optim`.
  * **optim** (_torch.optim.Optimizer_) – Optimizer for `model` ‘s parameters.
  * **optim_state_dict** (_Dict_ _[__str_ _,__Any_ _]_) – the target optimizer state_dict to transform. If the value is None, optim.state_dict() will be used. ( Default: `None`)
  * **group** (_dist.ProcessGroup_) – Model’s process group across which parameters are sharded or `None` if using the default process group. ( Default: `None`)



Returns
    
A `dict` containing the optimizer state for `model`. The sharding of the optimizer state is based on `state_dict_type`. 

Return type
    
Dict[str, Any] 

_static_ optim_state_dict_to_load(_model_ , _optim_ , _optim_state_dict_ , _is_named_optimizer =False_, _load_directly =False_, _group =None_)[source][source]
    
Convert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.
Given a `optim_state_dict` that is transformed through `optim_state_dict()`, it gets converted to the flattened optimizer state_dict that can be loaded to `optim` which is the optimizer for `model`. `model` must be sharded by FullyShardedDataParallel.
```
>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
>>> from torch.distributed.fsdp import StateDictType
>>> from torch.distributed.fsdp import FullStateDictConfig
>>> from torch.distributed.fsdp import FullOptimStateDictConfig
>>> # Save a checkpoint
>>> model, optim = ...
>>> FSDP.set_state_dict_type(
>>>   model,
>>>   StateDictType.FULL_STATE_DICT,
>>>   FullStateDictConfig(rank0_only=False),
>>>   FullOptimStateDictConfig(rank0_only=False),
>>> )
>>> state_dict = model.state_dict()
>>> original_osd = optim.state_dict()
>>> optim_state_dict = FSDP.optim_state_dict(
>>>   model,
>>>   optim,
>>>   optim_state_dict=original_osd
>>> )
>>> save_a_checkpoint(state_dict, optim_state_dict)
>>> # Load a checkpoint
>>> model, optim = ...
>>> state_dict, optim_state_dict = load_a_checkpoint()
>>> FSDP.set_state_dict_type(
>>>   model,
>>>   StateDictType.FULL_STATE_DICT,
>>>   FullStateDictConfig(rank0_only=False),
>>>   FullOptimStateDictConfig(rank0_only=False),
>>> )
>>> model.load_state_dict(state_dict)
>>> optim_state_dict = FSDP.optim_state_dict_to_load(
>>>   model, optim, optim_state_dict
>>> )
>>> optim.load_state_dict(optim_state_dict)

```
Copy to clipboard 

Parameters
    
  * **model** (_torch.nn.Module_) – Root module (which may or may not be a `FullyShardedDataParallel` instance) whose parameters were passed into the optimizer `optim`.
  * **optim** (_torch.optim.Optimizer_) – Optimizer for `model` ‘s parameters.
  * **optim_state_dict** (_Dict_ _[__str_ _,__Any_ _]_) – The optimizer states to be loaded.
  * **is_named_optimizer** (_bool_) – Is this optimizer a NamedOptimizer or KeyedOptimizer. Only set to True if `optim` is TorchRec’s KeyedOptimizer or torch.distributed’s NamedOptimizer.
  * **load_directly** (_bool_) – If this is set to True, this API will also call optim.load_state_dict(result) before returning the result. Otherwise, users are responsible to call `optim.load_state_dict()` (Default: `False`)
  * **group** (_dist.ProcessGroup_) – Model’s process group across which parameters are sharded or `None` if using the default process group. ( Default: `None`)



Return type
    
dict[str, _Any_] 

register_comm_hook(_state_ , _hook_)[source][source]
    
Register a communication hook.
This is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with `FullyShardedDataParallel`.
Warning
FSDP communication hook should be registered before running an initial forward pass and only once. 

Parameters
    
  * **state** (_object_) – 
Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.
  * **hook** (_Callable_) – Callable, which has one of the following signatures: 1) `hook: Callable[torch.Tensor] -> None`: This function takes in a Python tensor, which represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). It then performs all necessary processing and returns `None`; 2) `hook: Callable[torch.Tensor, torch.Tensor] -> None`: This function takes in two Python tensors, the first one represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized tensor to store a chunk of a sharded gradient after reduction. In both cases, callable performs all necessary processing and returns `None`. Callables with signature 1 are expected to handle gradient communication for a NO_SHARD case. Callables with signature 2 are expected to handle gradient communication for sharded cases.



_static_ rekey_optim_state_dict(_optim_state_dict_ , _optim_state_key_type_ , _model_ , _optim_input =None_, _optim =None_)[source][source]
    
Re-keys the optimizer state dict `optim_state_dict` to use the key type `optim_state_key_type`.
This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without.
To re-key an FSDP full optimizer state dict (i.e. from `full_optim_state_dict()`) to use parameter IDs and be loadable to a non-wrapped model:
```
>>> wrapped_model, wrapped_optim = ...
>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)
>>> nonwrapped_model, nonwrapped_optim = ...
>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)
>>> nonwrapped_optim.load_state_dict(rekeyed_osd)

```
Copy to clipboard
To re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model:
```
>>> nonwrapped_model, nonwrapped_optim = ...
>>> osd = nonwrapped_optim.state_dict()
>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)
>>> wrapped_model, wrapped_optim = ...
>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)
>>> wrapped_optim.load_state_dict(sharded_osd)

```
Copy to clipboard 

Returns
    
The optimizer state dict re-keyed using the parameter keys specified by `optim_state_key_type`. 

Return type
    
Dict[str, Any] 

_static_ scatter_full_optim_state_dict(_full_optim_state_dict_ , _model_ , _optim_input =None_, _optim =None_, _group =None_)[source][source]
    
Scatter the full optimizer state dict from rank 0 to all other ranks.
Returns the sharded optimizer state dict on each rank. The return value is the same as `shard_full_optim_state_dict()`, and on rank 0, the first argument should be the return value of `full_optim_state_dict()`.
Example:
```
>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
>>> model, optim = ...
>>> full_osd = FSDP.full_optim_state_dict(model, optim) # only non-empty on rank 0
>>> # Define new model with possibly different world size
>>> new_model, new_optim, new_group = ...
>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)
>>> new_optim.load_state_dict(sharded_osd)

```
Copy to clipboard
Note
Both `shard_full_optim_state_dict()` and `scatter_full_optim_state_dict()` may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost. 

Parameters
    
  * **full_optim_state_dict** (_Optional_ _[__Dict_ _[__str_ _,__Any_ _]__]_) – Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state if on rank 0; the argument is ignored on nonzero ranks.
  * **model** (_torch.nn.Module_) – Root module (which may or may not be a `FullyShardedDataParallel` instance) whose parameters correspond to the optimizer state in `full_optim_state_dict`.
  * **optim_input** (_Optional_ _[__Union_ _[__List_ _[__Dict_ _[__str_ _,__Any_ _]__]__,__Iterable_ _[__torch.nn.Parameter_ _]__]__]_) – Input passed into the optimizer representing either a `list` of parameter groups or an iterable of parameters; if `None`, then this method assumes the input was `model.parameters()`. This argument is deprecated, and there is no need to pass it in anymore. (Default: `None`)
  * **optim** (_Optional_ _[__torch.optim.Optimizer_ _]_) – Optimizer that will load the state dict returned by this method. This is the preferred argument to use over `optim_input`. (Default: `None`)
  * **group** (_dist.ProcessGroup_) – Model’s process group or `None` if using the default process group. (Default: `None`)



Returns
    
The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank’s part of the optimizer state. 

Return type
    
Dict[str, Any] 

_static_ set_state_dict_type(_module_ , _state_dict_type_ , _state_dict_config =None_, _optim_state_dict_config =None_)[source][source]
    
Set the `state_dict_type` of all the descendant FSDP modules of the target module.
Also takes (optional) configuration for the model’s and optimizer’s state dict. The target module does not have to be a FSDP module. If the target module is a FSDP module, its `state_dict_type` will also be changed.
Note
This API should be called for only the top-level (root) module.
Note
This API enables users to transparently use the conventional `state_dict` API to take model checkpoints in cases where the root FSDP module is wrapped by another `nn.Module`. For example, the following will ensure `state_dict` is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:
Example:
```
>>> model = DDP(FSDP(...))
>>> FSDP.set_state_dict_type(
>>>   model,
>>>   StateDictType.SHARDED_STATE_DICT,
>>>   state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),
>>>   optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),
>>> )
>>> param_state_dict = model.state_dict()
>>> optim_state_dict = FSDP.optim_state_dict(model, optim)

```
Copy to clipboard 

Parameters
    
  * **module** (_torch.nn.Module_) – Root module.
  * **state_dict_type** (_StateDictType_) – the desired `state_dict_type` to set.
  * **state_dict_config** (_Optional_ _[__StateDictConfig_ _]_) – the configuration for the target `state_dict_type`.
  * **optim_state_dict_config** (_Optional_ _[__OptimStateDictConfig_ _]_) – the configuration for the optimizer state dict.



Returns
    
A StateDictSettings that include the previous state_dict type and configuration for the module. 

Return type
    
_StateDictSettings_ 

_static_ shard_full_optim_state_dict(_full_optim_state_dict_ , _model_ , _optim_input =None_, _optim =None_)[source][source]
    
Shard a full optimizer state-dict.
Remaps the state in `full_optim_state_dict` to flattened parameters instead of unflattened parameters and restricts to only this rank’s part of the optimizer state. The first argument should be the return value of `full_optim_state_dict()`.
Example:
```
>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
>>> model, optim = ...
>>> full_osd = FSDP.full_optim_state_dict(model, optim)
>>> torch.save(full_osd, PATH)
>>> # Define new model with possibly different world size
>>> new_model, new_optim = ...
>>> full_osd = torch.load(PATH)
>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)
>>> new_optim.load_state_dict(sharded_osd)

```
Copy to clipboard
Note
Both `shard_full_optim_state_dict()` and `scatter_full_optim_state_dict()` may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost. 

Parameters
    
  * **full_optim_state_dict** (_Dict_ _[__str_ _,__Any_ _]_) – Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state.
  * **model** (_torch.nn.Module_) – Root module (which may or may not be a `FullyShardedDataParallel` instance) whose parameters correspond to the optimizer state in `full_optim_state_dict`.
  * **optim_input** (_Optional_ _[__Union_ _[__List_ _[__Dict_ _[__str_ _,__Any_ _]__]__,__Iterable_ _[__torch.nn.Parameter_ _]__]__]_) – Input passed into the optimizer representing either a `list` of parameter groups or an iterable of parameters; if `None`, then this method assumes the input was `model.parameters()`. This argument is deprecated, and there is no need to pass it in anymore. (Default: `None`)
  * **optim** (_Optional_ _[__torch.optim.Optimizer_ _]_) – Optimizer that will load the state dict returned by this method. This is the preferred argument to use over `optim_input`. (Default: `None`)



Returns
    
The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank’s part of the optimizer state. 

Return type
    
Dict[str, Any] 

_static_ sharded_optim_state_dict(_model_ , _optim_ , _group =None_)[source][source]
    
Return the optimizer state-dict in its sharded form.
The API is similar to `full_optim_state_dict()` but this API chunks all non-zero-dimension states to `ShardedTensor` to save memory. This API should only be used when the model `state_dict` is derived with the context manager `with state_dict_type(SHARDED_STATE_DICT):`.
For the detailed usage, refer to `full_optim_state_dict()`.
Warning
The returned state dict contains `ShardedTensor` and cannot be directly used by the regular `optim.load_state_dict`. 

Return type
    
dict[str, _Any_] 

_static_ state_dict_type(_module_ , _state_dict_type_ , _state_dict_config =None_, _optim_state_dict_config =None_)[source][source]
    
Set the `state_dict_type` of all the descendant FSDP modules of the target module.
This context manager has the same functions as `set_state_dict_type()`. Read the document of `set_state_dict_type()` for the detail.
Example:
```
>>> model = DDP(FSDP(...))
>>> with FSDP.state_dict_type(
>>>   model,
>>>   StateDictType.SHARDED_STATE_DICT,
>>> ):
>>>   checkpoint = model.state_dict()

```
Copy to clipboard 

Parameters
    
  * **module** (_torch.nn.Module_) – Root module.
  * **state_dict_type** (_StateDictType_) – the desired `state_dict_type` to set.
  * **state_dict_config** (_Optional_ _[__StateDictConfig_ _]_) – the model `state_dict` configuration for the target `state_dict_type`.
  * **optim_state_dict_config** (_Optional_ _[__OptimStateDictConfig_ _]_) – the optimizer `state_dict` configuration for the target `state_dict_type`.



Return type
    
_Generator_ 

_static_ summon_full_params(_module_ , _recurse =True_, _writeback =True_, _rank0_only =False_, _offload_to_cpu =False_, _with_grads =False_)[source][source]
    
Expose full params for FSDP instances with this context manager.
Can be useful _after_ forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the `recurse` argument.
Note
This can be used on inner FSDPs.
Note
This can _not_ be used within a forward or backward pass. Nor can forward and backward be started from within this context.
Note
Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.
Note
The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless `writeback=False`, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when `world_size == 1`, or `NO_SHARD` config, the modification is persisted regardless of `writeback`.
Note
This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.
Warning
Note that `rank0_only=True` in conjunction with `writeback=True` is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.
Warning
Note that `offload_to_cpu` and `rank0_only=False` will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use `offload_to_cpu` with `rank0_only=True`. 

Parameters
    
  * **recurse** (_bool_ _,__Optional_) – recursively summon all params for nested FSDP instances (default: True).
  * **writeback** (_bool_ _,__Optional_) – if `False`, modifications to params are discarded after the context manager exits; disabling this can be slightly more efficient (default: True)
  * **rank0_only** (_bool_ _,__Optional_) – if `True`, full parameters are materialized on only global rank 0. This means that within the context, only rank 0 will have full parameters and the other ranks will have sharded parameters. Note that setting `rank0_only=True` with `writeback=True` is not supported, as model parameter shapes will be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.
  * **offload_to_cpu** (_bool_ _,__Optional_) – If `True`, full parameters are offloaded to CPU. Note that this offloading currently only occurs if the parameter is sharded (which is only not the case for world_size = 1 or `NO_SHARD` config). It is recommended to use `offload_to_cpu` with `rank0_only=True` to avoid redundant copies of model parameters being offloaded to the same CPU memory.
  * **with_grads** (_bool_ _,__Optional_) – If `True`, gradients are also unsharded with the parameters. Currently, this is only supported when passing `use_orig_params=True` to the FSDP constructor and `offload_to_cpu=False` to this method. (Default: `False`)



Return type
    
_Generator_ 

_class_ torch.distributed.fsdp.BackwardPrefetch(_value_)[source][source]
    
This configures explicit backward prefetching, which improves throughput by enabling communication and computation overlap in the backward pass at the cost of slightly increased memory usage.
  * `BACKWARD_PRE`: This enables the most overlap but increases memory usage the most. This prefetches the next set of parameters _before_ the current set of parameters’ gradient computation. This overlaps the _next all-gather_ and the _current gradient computation_ , and at the peak, it holds the current set of parameters, next set of parameters, and current set of gradients in memory.
  * `BACKWARD_POST`: This enables less overlap but requires less memory usage. This prefetches the next set of parameters _after_ the current set of parameters’ gradient computation. This overlaps the _current reduce-scatter_ and the _next gradient computation_ , and it frees the current set of parameters before allocating memory for the next set of parameters, only holding the next set of parameters and current set of gradients in memory at the peak.
  * FSDP’s `backward_prefetch` argument accepts `None`, which disables the backward prefetching altogether. This has no overlap and does not increase memory usage. In general, we do not recommend this setting since it may degrade throughput significantly.


For more technical context: For a single process group using NCCL backend, any collectives, even if issued from different streams, contend for the same per-device NCCL stream, which implies that the relative order in which the collectives are issued matters for overlapping. The two backward prefetching values correspond to different issue orders. 

_class_ torch.distributed.fsdp.ShardingStrategy(_value_)[source][source]
    
This specifies the sharding strategy to be used for distributed training by `FullyShardedDataParallel`.
  * `FULL_SHARD`: Parameters, gradients, and optimizer states are sharded. For the parameters, this strategy unshards (via all-gather) before the forward, reshards after the forward, unshards before the backward computation, and reshards after the backward computation. For gradients, it synchronizes and shards them (via reduce-scatter) after the backward computation. The sharded optimizer states are updated locally per rank.
  * `SHARD_GRAD_OP`: Gradients and optimizer states are sharded during computation, and additionally, parameters are sharded outside computation. For the parameters, this strategy unshards before the forward, does not reshard them after the forward, and only reshards them after the backward computation. The sharded optimizer states are updated locally per rank. Inside `no_sync()`, the parameters are not resharded after the backward computation.
  * `NO_SHARD`: Parameters, gradients, and optimizer states are not sharded but instead replicated across ranks similar to PyTorch’s `DistributedDataParallel` API. For gradients, this strategy synchronizes them (via all-reduce) after the backward computation. The unsharded optimizer states are updated locally per rank.
  * `HYBRID_SHARD`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes. This results in reduced communication volume as expensive all-gathers and reduce-scatters are only done within a node, which can be more performant for medium -sized models.
  * `_HYBRID_SHARD_ZERO2`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes. This is like `HYBRID_SHARD`, except this may provide even higher throughput since the unsharded parameters are not freed after the forward pass, saving the all-gathers in the pre-backward.



_class_ torch.distributed.fsdp.MixedPrecision(_param_dtype=None_ , _reduce_dtype=None_ , _buffer_dtype=None_ , _keep_low_precision_grads=False_ , _cast_forward_inputs=False_ , _cast_root_forward_inputs=True_ , __module_classes_to_ignore=( <class 'torch.nn.modules.batchnorm._BatchNorm'>_, _)_)[source][source]
    
This configures FSDP-native mixed precision training. 

Variables
    
  * **param_dtype** (_Optional_ _[__torch.dtype_ _]_) – This specifies the dtype for model parameters during forward and backward and thus the dtype for forward and backward computation. Outside forward and backward, the _sharded_ parameters are kept in full precision (e.g. for the optimizer step), and for model checkpointing, the parameters are always saved in full precision. (Default: `None`)
  * **reduce_dtype** (_Optional_ _[__torch.dtype_ _]_) – This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is `None` but `param_dtype` is not `None`, then this takes on the `param_dtype` value, still running gradient reduction in low precision. This is permitted to differ from `param_dtype`, e.g. to force gradient reduction to run in full precision. (Default: `None`)
  * **buffer_dtype** (_Optional_ _[__torch.dtype_ _]_) – This specifies the dtype for buffers. FSDP does not shard buffers. Rather, FSDP casts them to `buffer_dtype` in the first forward pass and keeps them in that dtype thereafter. For model checkpointing, the buffers are saved in full precision except for `LOCAL_STATE_DICT`. (Default: `None`)
  * **keep_low_precision_grads** (_bool_) – If `False`, then FSDP upcasts gradients to full precision after the backward pass in preparation for the optimizer step. If `True`, then FSDP keeps the gradients in the dtype used for gradient reduction, which can save memory if using a custom optimizer that supports running in low precision. (Default: `False`)
  * **cast_forward_inputs** (_bool_) – If `True`, then this FSDP module casts its forward args and kwargs to `param_dtype`. This is to ensure that parameter and input dtypes match for forward computation, as required by many ops. This may need to be set to `True` when only applying mixed precision to some but not all FSDP modules, in which case a mixed-precision FSDP submodule needs to recast its inputs. (Default: `False`)
  * **cast_root_forward_inputs** (_bool_) – If `True`, then the root FSDP module casts its forward args and kwargs to `param_dtype`, overriding the value of `cast_forward_inputs`. For non-root FSDP modules, this does not do anything. (Default: `True`)
  * **_module_classes_to_ignore** (_collections.abc.Sequence_ _[__type_ _[__torch.nn.modules.module.Module_ _]__]_) – (Sequence[Type[nn.Module]]): This specifies module classes to ignore for mixed precision when using an `auto_wrap_policy`: Modules of these classes will have FSDP applied to them separately with mixed precision disabled (meaning that the final FSDP construction would deviate from the specified policy). If `auto_wrap_policy` is not specified, then this does not do anything. This API is experimental and subject to change. (Default: `(_BatchNorm,)`)


Note
This API is experimental and subject to change.
Note
Only floating point tensors are cast to their specified dtypes.
Note
In `summon_full_params`, parameters are forced to full precision, but buffers are not.
Note
Layer norm and batch norm accumulate in `float32` even when their inputs are in a low precision like `float16` or `bfloat16`. Disabling FSDP’s mixed precision for those norm modules only means that the affine parameters are kept in `float32`. However, this incurs separate all-gathers and reduce-scatters for those norm modules, which may be inefficient, so if the workload permits, the user should prefer to still apply mixed precision to those modules.
Note
By default, if the user passes a model with any `_BatchNorm` modules and specifies an `auto_wrap_policy`, then the batch norm modules will have FSDP applied to them separately with mixed precision disabled. See the `_module_classes_to_ignore` argument.
Note
`MixedPrecision` has `cast_root_forward_inputs=True` and `cast_forward_inputs=False` by default. For the root FSDP instance, its `cast_root_forward_inputs` takes precedence over its `cast_forward_inputs`. For non-root FSDP instances, their `cast_root_forward_inputs` values are ignored. The default setting is sufficient for the typical case where each FSDP instance has the same `MixedPrecision` configuration and only needs to cast inputs to the `param_dtype` at the beginning of the model’s forward pass.
Note
For nested FSDP instances with different `MixedPrecision` configurations, we recommend setting individual `cast_forward_inputs` values to configure casting inputs or not before each instance’s forward. In such a case, since the casts happen before each FSDP instance’s forward, a parent FSDP instance should have its non-FSDP submodules run before its FSDP submodules to avoid the activation dtype being changed due to a different `MixedPrecision` configuration.
Example:
```
>>> model = nn.Sequential(nn.Linear(3, 3), nn.Linear(3, 3))
>>> model[1] = FSDP(
>>>   model[1],
>>>   mixed_precision=MixedPrecision(param_dtype=torch.float16, cast_forward_inputs=True),
>>> )
>>> model = FSDP(
>>>   model,
>>>   mixed_precision=MixedPrecision(param_dtype=torch.bfloat16, cast_forward_inputs=True),
>>> )

```
Copy to clipboard
The above shows a working example. On the other hand, if `model[1]` were replaced with `model[0]`, meaning that the submodule using different `MixedPrecision` ran its forward first, then `model[1]` would incorrectly see `float16` activations instead of `bfloat16` ones. 

_class_ torch.distributed.fsdp.CPUOffload(_offload_params =False_)[source][source]
    
This configures CPU offloading. 

Variables
    
**offload_params** (_bool_) – This specifies whether to offload parameters to CPU when not involved in computation. If `True`, then this offloads gradients to CPU as well, meaning that the optimizer step runs on CPU. 

_class_ torch.distributed.fsdp.StateDictConfig(_offload_to_cpu =False_)[source][source]
    
`StateDictConfig` is the base class for all `state_dict` configuration classes. Users should instantiate a child class (e.g. `FullStateDictConfig`) in order to configure settings for the corresponding `state_dict` type supported by FSDP. 

Variables
    
**offload_to_cpu** (_bool_) – If `True`, then FSDP offloads the state dict values to CPU, and if `False`, then FSDP keeps them on GPU. (Default: `False`) 

_class_ torch.distributed.fsdp.FullStateDictConfig(_offload_to_cpu =False_, _rank0_only =False_)[source][source]
    
`FullStateDictConfig` is a config class meant to be used with `StateDictType.FULL_STATE_DICT`. We recommend enabling both `offload_to_cpu=True` and `rank0_only=True` when saving full state dicts to save GPU memory and CPU memory, respectively. This config class is meant to be used via the `state_dict_type()` context manager as follows:
```
>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
>>> fsdp = FSDP(model, auto_wrap_policy=...)
>>> cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
>>> with FSDP.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):
>>>   state = fsdp.state_dict()
>>> # `state` will be empty on non rank 0 and contain CPU tensors on rank 0.
>>> # To reload checkpoint for inference, finetuning, transfer learning, etc:
>>> model = model_fn() # Initialize model in preparation for wrapping with FSDP
>>> if dist.get_rank() == 0:
>>> # Load checkpoint only on rank 0 to avoid memory redundancy
>>>   state_dict = torch.load("my_checkpoint.pt")
>>>   model.load_state_dict(state_dict)
>>> # All ranks initialize FSDP module as usual. `sync_module_states` argument
>>> # communicates loaded checkpoint states from rank 0 to rest of the world.
>>> fsdp = FSDP(
...   model,
...   device_id=torch.cuda.current_device(),
...   auto_wrap_policy=...,
...   sync_module_states=True,
... )
>>> # After this point, all ranks have FSDP model with loaded checkpoint.

```
Copy to clipboard 

Variables
    
**rank0_only** (_bool_) – If `True`, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If `False`, then all ranks save the full state dict. (Default: `False`) 

_class_ torch.distributed.fsdp.ShardedStateDictConfig(_offload_to_cpu =False_, __use_dtensor =False_)[source][source]
    
`ShardedStateDictConfig` is a config class meant to be used with `StateDictType.SHARDED_STATE_DICT`. 

Variables
    
**_use_dtensor** (_bool_) – If `True`, then FSDP saves the state dict values as `DTensor`, and if `False`, then FSDP saves them as `ShardedTensor`. (Default: `False`)
Warning
`_use_dtensor` is a private field of `ShardedStateDictConfig` and it is used by FSDP to determine the type of state dict values. Users should not manually modify `_use_dtensor`. 

_class_ torch.distributed.fsdp.LocalStateDictConfig(_offload_to_cpu :bool=False_)[source][source]


_class_ torch.distributed.fsdp.OptimStateDictConfig(_offload_to_cpu =True_)[source][source]
    
`OptimStateDictConfig` is the base class for all `optim_state_dict` configuration classes. Users should instantiate a child class (e.g. `FullOptimStateDictConfig`) in order to configure settings for the corresponding `optim_state_dict` type supported by FSDP. 

Variables
    
**offload_to_cpu** (_bool_) – If `True`, then FSDP offloads the state dict’s tensor values to CPU, and if `False`, then FSDP keeps them on the original device (which is GPU unless parameter CPU offloading is enabled). (Default: `True`) 

_class_ torch.distributed.fsdp.FullOptimStateDictConfig(_offload_to_cpu =True_, _rank0_only =False_)[source][source]
     

Variables
    
**rank0_only** (_bool_) – If `True`, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If `False`, then all ranks save the full state dict. (Default: `False`) 

_class_ torch.distributed.fsdp.ShardedOptimStateDictConfig(_offload_to_cpu =True_, __use_dtensor =False_)[source][source]
    
`ShardedOptimStateDictConfig` is a config class meant to be used with `StateDictType.SHARDED_STATE_DICT`. 

Variables
    
**_use_dtensor** (_bool_) – If `True`, then FSDP saves the state dict values as `DTensor`, and if `False`, then FSDP saves them as `ShardedTensor`. (Default: `False`)
Warning
`_use_dtensor` is a private field of `ShardedOptimStateDictConfig` and it is used by FSDP to determine the type of state dict values. Users should not manually modify `_use_dtensor`. 

_class_ torch.distributed.fsdp.LocalOptimStateDictConfig(_offload_to_cpu :bool=False_)[source][source]


_class_ torch.distributed.fsdp.StateDictSettings(_state_dict_type :torch.distributed.fsdp.api.StateDictType_, _state_dict_config :torch.distributed.fsdp.api.StateDictConfig_, _optim_state_dict_config :torch.distributed.fsdp.api.OptimStateDictConfig_)[source][source]

Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * FullyShardedDataParallel
    * `FullyShardedDataParallel`
      * `FullyShardedDataParallel.apply()`
      * `FullyShardedDataParallel.check_is_root()`
      * `FullyShardedDataParallel.clip_grad_norm_()`
      * `FullyShardedDataParallel.flatten_sharded_optim_state_dict()`
      * `FullyShardedDataParallel.forward()`
      * `FullyShardedDataParallel.fsdp_modules()`
      * `FullyShardedDataParallel.full_optim_state_dict()`
      * `FullyShardedDataParallel.get_state_dict_type()`
      * `FullyShardedDataParallel.module`
      * `FullyShardedDataParallel.named_buffers()`
      * `FullyShardedDataParallel.named_parameters()`
      * `FullyShardedDataParallel.no_sync()`
      * `FullyShardedDataParallel.optim_state_dict()`
      * `FullyShardedDataParallel.optim_state_dict_to_load()`
      * `FullyShardedDataParallel.register_comm_hook()`
      * `FullyShardedDataParallel.rekey_optim_state_dict()`
      * `FullyShardedDataParallel.scatter_full_optim_state_dict()`
      * `FullyShardedDataParallel.set_state_dict_type()`
      * `FullyShardedDataParallel.shard_full_optim_state_dict()`
      * `FullyShardedDataParallel.sharded_optim_state_dict()`
      * `FullyShardedDataParallel.state_dict_type()`
      * `FullyShardedDataParallel.summon_full_params()`
    * `BackwardPrefetch`
    * `ShardingStrategy`
    * `MixedPrecision`
    * `CPUOffload`
    * `StateDictConfig`
    * `FullStateDictConfig`
    * `ShardedStateDictConfig`
    * `LocalStateDictConfig`
    * `OptimStateDictConfig`
    * `FullOptimStateDictConfig`
    * `ShardedOptimStateDictConfig`
    * `LocalOptimStateDictConfig`
    * `StateDictSettings`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Probability distributions - torch.distributions
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Probability distributions - torch.distributions
The `distributions` package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package.
It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples f(x)f(x)f(x), the pathwise derivative requires the derivative f′(x)f'(x)f′(x). The next sections discuss these two in a reinforcement learning example. For more details see Gradient Estimation Using Stochastic Computation Graphs .
## Score function
When the probability density function is differentiable with respect to its parameters, we only need `sample()` and `log_prob()` to implement REINFORCE:
Δθ=αr∂log⁡p(a∣πθ(s))∂θ\Delta\theta = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}Δθ=αr∂θ∂logp(a∣πθ(s))​
where θ\thetaθ are the parameters, α\alphaα is the learning rate, rrr is the reward and p(a∣πθ(s))p(a|\pi^\theta(s))p(a∣πθ(s)) is the probability of taking action aaa in state sss given policy πθ\pi^\thetaπθ.
In practice we would sample an action from the output of a network, apply this action in an environment, and then use `log_prob` to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:
```
probs = policy_network(state)
# Note that this is equivalent to what used to be called multinomial
m = Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()

```
Copy to clipboard
## Pathwise derivative
The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the `rsample()` method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:
```
params = policy_network(state)
m = Normal(*params)
# Any distribution with .has_rsample == True could work based on the application
action = m.rsample()
next_state, reward = env.step(action) # Assuming that reward is differentiable
loss = -reward
loss.backward()

```
Copy to clipboard
## Distribution 

_class_ torch.distributions.distribution.Distribution(_batch_shape =torch.Size([])_, _event_shape =torch.Size([])_, _validate_args =None_)[source][source]
    
Bases: `object`
Distribution is the abstract base class for probability distributions. 

_property_ arg_constraints _: dict[str,torch.distributions.constraints.Constraint]_
    
Returns a dictionary from argument names to `Constraint` objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. 

_property_ batch_shape _: Size_
    
Returns the shape over which parameters are batched. 

cdf(_value_)[source][source]
    
Returns the cumulative density/mass function evaluated at value. 

Parameters
    
**value** (_Tensor_) –  

Return type
    
_Tensor_ 

entropy()[source][source]
    
Returns entropy of distribution, batched over batch_shape. 

Returns
    
Tensor of shape batch_shape. 

Return type
    
_Tensor_ 

enumerate_support(_expand =True_)[source][source]
    
Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).
Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], …]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...
To iterate over the full Cartesian product use itertools.product(m.enumerate_support()). 

Parameters
    
**expand** (_bool_) – whether to expand the support over the batch dims to match the distribution’s batch_shape. 

Returns
    
Tensor iterating over dimension 0. 

Return type
    
_Tensor_ 

_property_ event_shape _: Size_
    
Returns the shape of a single sample (without batching). 

expand(_batch_shape_ , __instance =None_)[source][source]
    
Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls `expand` on the distribution’s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created. 

Parameters
    
  * **batch_shape** (_torch.Size_) – the desired expanded size.
  * **_instance** – new instance provided by subclasses that need to override .expand.



Returns
    
New distribution instance with batch dimensions expanded to batch_size. 

icdf(_value_)[source][source]
    
Returns the inverse cumulative density/mass function evaluated at value. 

Parameters
    
**value** (_Tensor_) –  

Return type
    
_Tensor_ 

log_prob(_value_)[source][source]
    
Returns the log of the probability density/mass function evaluated at value. 

Parameters
    
**value** (_Tensor_) –  

Return type
    
_Tensor_ 

_property_ mean _: Tensor_
    
Returns the mean of the distribution. 

_property_ mode _: Tensor_
    
Returns the mode of the distribution. 

perplexity()[source][source]
    
Returns perplexity of distribution, batched over batch_shape. 

Returns
    
Tensor of shape batch_shape. 

Return type
    
_Tensor_ 

rsample(_sample_shape =torch.Size([])_)[source][source]
    
Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. 

Return type
    
_Tensor_ 

sample(_sample_shape =torch.Size([])_)[source][source]
    
Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. 

Return type
    
_Tensor_ 

sample_n(_n_)[source][source]
    
Generates n samples or n batches of samples if the distribution parameters are batched. 

Return type
    
_Tensor_ 

_static_ set_default_validate_args(_value_)[source][source]
    
Sets whether validation is enabled or disabled.
The default behavior mimics Python’s `assert` statement: validation is on by default, but is disabled if Python is run in optimized mode (via `python -O`). Validation may be expensive, so you may want to disable it once a model is working. 

Parameters
    
**value** (_bool_) – Whether to enable validation. 

_property_ stddev _: Tensor_
    
Returns the standard deviation of the distribution. 

_property_ support _: Optional[Constraint]_
    
Returns a `Constraint` object representing this distribution’s support. 

_property_ variance _: Tensor_
    
Returns the variance of the distribution.
## ExponentialFamily 

_class_ torch.distributions.exp_family.ExponentialFamily(_batch_shape =torch.Size([])_, _event_shape =torch.Size([])_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below
pF(x;θ)=exp⁡(⟨t(x),θ⟩−F(θ)+k(x))p_{F}(x; \theta) = \exp(\langle t(x), \theta\rangle - F(\theta) + k(x))pF​(x;θ)=exp(⟨t(x),θ⟩−F(θ)+k(x))
where θ\thetaθ denotes the natural parameters, t(x)t(x)t(x) denotes the sufficient statistic, F(θ)F(\theta)F(θ) is the log normalizer function for a given family and k(x)k(x)k(x) is the carrier measure.
Note
This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families). 

entropy()[source][source]
    
Method to compute the entropy using Bregman divergence of the log normalizer.
## Bernoulli 

_class_ torch.distributions.bernoulli.Bernoulli(_probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a Bernoulli distribution parameterized by `probs` or `logits` (but not both).
Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p.
Example:
```
>>> m = Bernoulli(torch.tensor([0.3]))
>>> m.sample() # 30% chance 1; 70% chance 0
tensor([ 0.])

```
Copy to clipboard 

Parameters
    
  * **probs** (_Number_ _,__Tensor_) – the probability of sampling 1
  * **logits** (_Number_ _,__Tensor_) – the log-odds of sampling 1



arg_constraints _={'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}_


entropy()[source][source]


enumerate_support(_expand =True_)[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_enumerate_support _= True_


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


support _= Boolean()_


_property_ variance _: Tensor_

## Beta 

_class_ torch.distributions.beta.Beta(_concentration1_ , _concentration0_ , _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Beta distribution parameterized by `concentration1` and `concentration0`.
Example:
```
>>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))
>>> m.sample() # Beta distributed with concentration concentration1 and concentration0
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
  * **concentration1** (_float_ _or_ _Tensor_) – 1st concentration parameter of the distribution (often referred to as alpha)
  * **concentration0** (_float_ _or_ _Tensor_) – 2nd concentration parameter of the distribution (often referred to as beta)



arg_constraints _={'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}_


_property_ concentration0 _: Tensor_


_property_ concentration1 _: Tensor_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =()_)[source][source]
     

Return type
    
_Tensor_ 

support _= Interval(lower_bound=0.0, upper_bound=1.0)_


_property_ variance _: Tensor_

## Binomial 

_class_ torch.distributions.binomial.Binomial(_total_count =1_, _probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a Binomial distribution parameterized by `total_count` and either `probs` or `logits` (but not both). `total_count` must be broadcastable with `probs`/`logits`.
Example:
```
>>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))
>>> x = m.sample()
tensor([  0.,  22.,  71., 100.])
>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))
>>> x = m.sample()
tensor([[ 4., 5.],
    [ 7., 6.]])

```
Copy to clipboard 

Parameters
    
  * **total_count** (_int_ _or_ _Tensor_) – number of Bernoulli trials
  * **probs** (_Tensor_) – Event probabilities
  * **logits** (_Tensor_) – Event log-odds



arg_constraints _={'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)}_


entropy()[source][source]


enumerate_support(_expand =True_)[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_enumerate_support _= True_


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


_property_ support
     

Return type
    
_DependentProperty 

_property_ variance _: Tensor_

## Categorical 

_class_ torch.distributions.categorical.Categorical(_probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a categorical distribution parameterized by either `probs` or `logits` (but not both).
Note
It is equivalent to the distribution that `torch.multinomial()` samples from.
Samples are integers from {0,…,K−1}\\{0, \ldots, K-1\\}{0,…,K−1} where K is `probs.size(-1)`.
If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index.
If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.
Note
The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. `probs` will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. `logits` will return this normalized value.
See also: `torch.multinomial()`
Example:
```
>>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
>>> m.sample() # equal probability of 0, 1, 2, 3
tensor(3)

```
Copy to clipboard 

Parameters
    
  * **probs** (_Tensor_) – event probabilities
  * **logits** (_Tensor_) – event log probabilities (unnormalized)



arg_constraints _={'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}_


entropy()[source][source]


enumerate_support(_expand =True_)[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_enumerate_support _= True_


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


_property_ support
     

Return type
    
_DependentProperty 

_property_ variance _: Tensor_

## Cauchy 

_class_ torch.distributions.cauchy.Cauchy(_loc_ , _scale_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution.
Example:
```
>>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample() # sample from a Cauchy distribution with loc=0 and scale=1
tensor([ 2.3214])

```
Copy to clipboard 

Parameters
    
  * **loc** (_float_ _or_ _Tensor_) – mode or median of the distribution.
  * **scale** (_float_ _or_ _Tensor_) – half width at half maximum.



arg_constraints _={'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_value_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

support _= Real()_


_property_ variance _: Tensor_

## Chi2 

_class_ torch.distributions.chi2.Chi2(_df_ , _validate_args =None_)[source][source]
    
Bases: `Gamma`
Creates a Chi-squared distribution parameterized by shape parameter `df`. This is exactly equivalent to `Gamma(alpha=0.5*df, beta=0.5)`
Example:
```
>>> m = Chi2(torch.tensor([1.0]))
>>> m.sample() # Chi2 distributed with shape df=1
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
**df** (_float_ _or_ _Tensor_) – shape parameter of the distribution 

arg_constraints _={'df': GreaterThan(lower_bound=0.0)}_


_property_ df _: Tensor_


expand(_batch_shape_ , __instance =None_)[source][source]

## ContinuousBernoulli 

_class_ torch.distributions.continuous_bernoulli.ContinuousBernoulli(_probs =None_, _logits =None_, _lims =(0.499, 0.501)_, _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a continuous Bernoulli distribution parameterized by `probs` or `logits` (but not both).
The distribution is supported in [0, 1] and parameterized by ‘probs’ (in (0,1)) or ‘logits’ (real-valued). Note that, unlike the Bernoulli, ‘probs’ does not correspond to a probability and ‘logits’ does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.
Example:
```
>>> m = ContinuousBernoulli(torch.tensor([0.3]))
>>> m.sample()
tensor([ 0.2538])

```
Copy to clipboard 

Parameters
    
  * **probs** (_Number_ _,__Tensor_) – (0,1) valued parameters
  * **logits** (_Number_ _,__Tensor_) – real valued parameters whose sigmoid matches ‘probs’


[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845 

arg_constraints _={'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_value_)[source][source]


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

sample(_sample_shape =torch.Size([])_)[source][source]


_property_ stddev _: Tensor_


support _= Interval(lower_bound=0.0, upper_bound=1.0)_


_property_ variance _: Tensor_

## Dirichlet 

_class_ torch.distributions.dirichlet.Dirichlet(_concentration_ , _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a Dirichlet distribution parameterized by concentration `concentration`.
Example:
```
>>> m = Dirichlet(torch.tensor([0.5, 0.5]))
>>> m.sample() # Dirichlet distributed with concentration [0.5, 0.5]
tensor([ 0.1046, 0.8954])

```
Copy to clipboard 

Parameters
    
**concentration** (_Tensor_) – concentration parameter of the distribution (often referred to as alpha) 

arg_constraints _={'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =()_)[source][source]
     

Return type
    
_Tensor_ 

support _= Simplex()_


_property_ variance _: Tensor_

## Exponential 

_class_ torch.distributions.exponential.Exponential(_rate_ , _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a Exponential distribution parameterized by `rate`.
Example:
```
>>> m = Exponential(torch.tensor([1.0]))
>>> m.sample() # Exponential distributed with rate=1
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
**rate** (_float_ _or_ _Tensor_) – rate = 1 / scale of the distribution 

arg_constraints _={'rate': GreaterThan(lower_bound=0.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_value_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

_property_ stddev _: Tensor_


support _= GreaterThanEq(lower_bound=0.0)_


_property_ variance _: Tensor_

## FisherSnedecor 

_class_ torch.distributions.fishersnedecor.FisherSnedecor(_df1_ , _df2_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a Fisher-Snedecor distribution parameterized by `df1` and `df2`.
Example:
```
>>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))
>>> m.sample() # Fisher-Snedecor-distributed with df1=1 and df2=2
tensor([ 0.2453])

```
Copy to clipboard 

Parameters
    
  * **df1** (_float_ _or_ _Tensor_) – degrees of freedom parameter 1
  * **df2** (_float_ _or_ _Tensor_) – degrees of freedom parameter 2



arg_constraints _={'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)}_


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

support _= GreaterThan(lower_bound=0.0)_


_property_ variance _: Tensor_

## Gamma 

_class_ torch.distributions.gamma.Gamma(_concentration_ , _rate_ , _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a Gamma distribution parameterized by shape `concentration` and `rate`.
Example:
```
>>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample() # Gamma distributed with concentration=1 and rate=1
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
  * **concentration** (_float_ _or_ _Tensor_) – shape parameter of the distribution (often referred to as alpha)
  * **rate** (_float_ _or_ _Tensor_) – rate parameter of the distribution (often referred to as beta), rate = 1 / scale



arg_constraints _={'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

support _= GreaterThanEq(lower_bound=0.0)_


_property_ variance _: Tensor_

## Geometric 

_class_ torch.distributions.geometric.Geometric(_probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a Geometric distribution parameterized by `probs`, where `probs` is the probability of success of Bernoulli trials.
P(X=k)=(1−p)kp,k=0,1,...P(X=k) = (1-p)^{k} p, k = 0, 1, ...P(X=k)=(1−p)kp,k=0,1,...
Note
`torch.distributions.geometric.Geometric()` (k+1)(k+1)(k+1)-th trial is the first success hence draws samples in {0,1,…}\\{0, 1, \ldots\\}{0,1,…}, whereas `torch.Tensor.geometric_()` k-th trial is the first success hence draws samples in {1,2,…}\\{1, 2, \ldots\\}{1,2,…}.
Example:
```
>>> m = Geometric(torch.tensor([0.3]))
>>> m.sample() # underlying Bernoulli has 30% chance 1; 70% chance 0
tensor([ 2.])

```
Copy to clipboard 

Parameters
    
  * **probs** (_Number_ _,__Tensor_) – the probability of sampling 1. Must be in range (0, 1]
  * **logits** (_Number_ _,__Tensor_) – the log-odds of sampling 1.



arg_constraints _={'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ probs _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


support _= IntegerGreaterThan(lower_bound=0)_


_property_ variance _: Tensor_

## Gumbel 

_class_ torch.distributions.gumbel.Gumbel(_loc_ , _scale_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Samples from a Gumbel Distribution.
Examples:
```
>>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))
>>> m.sample() # sample from Gumbel distribution with loc=1, scale=2
tensor([ 1.0124])

```
Copy to clipboard 

Parameters
    
  * **loc** (_float_ _or_ _Tensor_) – Location parameter of the distribution
  * **scale** (_float_ _or_ _Tensor_) – Scale parameter of the distribution



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ stddev _: Tensor_


support _= Real()_


_property_ variance _: Tensor_

## HalfCauchy 

_class_ torch.distributions.half_cauchy.HalfCauchy(_scale_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Creates a half-Cauchy distribution parameterized by scale where:
```
X ~ Cauchy(0, scale)
Y = |X| ~ HalfCauchy(scale)

```
Copy to clipboard
Example:
```
>>> m = HalfCauchy(torch.tensor([1.0]))
>>> m.sample() # half-cauchy distributed with scale=1
tensor([ 2.3214])

```
Copy to clipboard 

Parameters
    
**scale** (_float_ _or_ _Tensor_) – scale of the full Cauchy distribution 

arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'scale': GreaterThan(lower_bound=0.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_prob_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ scale _: Tensor_


support _= GreaterThanEq(lower_bound=0.0)_


_property_ variance _: Tensor_

## HalfNormal 

_class_ torch.distributions.half_normal.HalfNormal(_scale_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Creates a half-normal distribution parameterized by scale where:
```
X ~ Normal(0, scale)
Y = |X| ~ HalfNormal(scale)

```
Copy to clipboard
Example:
```
>>> m = HalfNormal(torch.tensor([1.0]))
>>> m.sample() # half-normal distributed with scale=1
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
**scale** (_float_ _or_ _Tensor_) – scale of the full Normal distribution 

arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'scale': GreaterThan(lower_bound=0.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_prob_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ scale _: Tensor_


support _= GreaterThanEq(lower_bound=0.0)_


_property_ variance _: Tensor_

## Independent 

_class_ torch.distributions.independent.Independent(_base_distribution_ , _reinterpreted_batch_ndims_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
Reinterprets some of the batch dims of a distribution as event dims.
This is mainly useful for changing the shape of the result of `log_prob()`. For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:
```
>>> from torch.distributions.multivariate_normal import MultivariateNormal
>>> from torch.distributions.normal import Normal
>>> loc = torch.zeros(3)
>>> scale = torch.ones(3)
>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))
>>> [mvn.batch_shape, mvn.event_shape]
[torch.Size([]), torch.Size([3])]
>>> normal = Normal(loc, scale)
>>> [normal.batch_shape, normal.event_shape]
[torch.Size([3]), torch.Size([])]
>>> diagn = Independent(normal, 1)
>>> [diagn.batch_shape, diagn.event_shape]
[torch.Size([]), torch.Size([3])]

```
Copy to clipboard 

Parameters
    
  * **base_distribution** (_torch.distributions.distribution.Distribution_) – a base distribution
  * **reinterpreted_batch_ndims** (_int_) – the number of batch dims to reinterpret as event dims



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={}_


entropy()[source][source]


enumerate_support(_expand =True_)[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


_property_ has_enumerate_support _: bool_


_property_ has_rsample _: bool_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

sample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

_property_ support
     

Return type
    
_DependentProperty 

_property_ variance _: Tensor_

## InverseGamma 

_class_ torch.distributions.inverse_gamma.InverseGamma(_concentration_ , _rate_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Creates an inverse gamma distribution parameterized by `concentration` and `rate` where:
```
X ~ Gamma(concentration, rate)
Y = 1 / X ~ InverseGamma(concentration, rate)

```
Copy to clipboard
Example:
```
>>> m = InverseGamma(torch.tensor([2.0]), torch.tensor([3.0]))
>>> m.sample()
tensor([ 1.2953])

```
Copy to clipboard 

Parameters
    
  * **concentration** (_float_ _or_ _Tensor_) – shape parameter of the distribution (often referred to as alpha)
  * **rate** (_float_ _or_ _Tensor_) – rate = 1 / scale of the distribution (often referred to as beta)



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}_


_property_ concentration _: Tensor_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ rate _: Tensor_


support _= GreaterThan(lower_bound=0.0)_


_property_ variance _: Tensor_

## Kumaraswamy 

_class_ torch.distributions.kumaraswamy.Kumaraswamy(_concentration1_ , _concentration0_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Samples from a Kumaraswamy distribution.
Example:
```
>>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample() # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1
tensor([ 0.1729])

```
Copy to clipboard 

Parameters
    
  * **concentration1** (_float_ _or_ _Tensor_) – 1st concentration parameter of the distribution (often referred to as alpha)
  * **concentration0** (_float_ _or_ _Tensor_) – 2nd concentration parameter of the distribution (often referred to as beta)



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


support _= Interval(lower_bound=0.0, upper_bound=1.0)_


_property_ variance _: Tensor_

## LKJCholesky 

_class_ torch.distributions.lkj_cholesky.LKJCholesky(_dim_ , _concentration =1.0_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by `concentration` parameter η\etaη to make the probability of the correlation matrix MMM generated from a Cholesky factor proportional to det⁡(M)η−1\det(M)^{\eta - 1}det(M)η−1. Because of that, when `concentration == 1`, we have a uniform distribution over Cholesky factors of correlation matrices:
```
L ~ LKJCholesky(dim, concentration)
X = L @ L' ~ LKJCorr(dim, concentration)

```
Copy to clipboard
Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3.
Example:
```
>>> l = LKJCholesky(3, 0.5)
>>> l.sample() # l @ l.T is a sample of a correlation 3x3 matrix
tensor([[ 1.0000, 0.0000, 0.0000],
    [ 0.3516, 0.9361, 0.0000],
    [-0.1899, 0.4748, 0.8593]])

```
Copy to clipboard 

Parameters
    
  * **dimension** (_dim_) – dimension of the matrices
  * **concentration** (_float_ _or_ _Tensor_) – concentration/shape parameter of the distribution (often referred to as eta)


**References**
[1] Generating random correlation matrices based on vines and extended onion method (2009), Daniel Lewandowski, Dorota Kurowicka, Harry Joe. Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008 

arg_constraints _={'concentration': GreaterThan(lower_bound=0.0)}_


expand(_batch_shape_ , __instance =None_)[source][source]


log_prob(_value_)[source][source]


sample(_sample_shape =torch.Size([])_)[source][source]


support _= CorrCholesky()_

## Laplace 

_class_ torch.distributions.laplace.Laplace(_loc_ , _scale_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a Laplace distribution parameterized by `loc` and `scale`.
Example:
```
>>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample() # Laplace distributed with loc=0, scale=1
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
  * **loc** (_float_ _or_ _Tensor_) – mean of the distribution
  * **scale** (_float_ _or_ _Tensor_) – scale of the distribution



arg_constraints _={'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_value_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

_property_ stddev _: Tensor_


support _= Real()_


_property_ variance _: Tensor_

## LogNormal 

_class_ torch.distributions.log_normal.LogNormal(_loc_ , _scale_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Creates a log-normal distribution parameterized by `loc` and `scale` where:
```
X ~ Normal(loc, scale)
Y = exp(X) ~ LogNormal(loc, scale)

```
Copy to clipboard
Example:
```
>>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample() # log-normal distributed with mean=0 and stddev=1
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
  * **loc** (_float_ _or_ _Tensor_) – mean of log of distribution
  * **scale** (_float_ _or_ _Tensor_) – standard deviation of log of the distribution



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


_property_ loc _: Tensor_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ scale _: Tensor_


support _= GreaterThan(lower_bound=0.0)_


_property_ variance _: Tensor_

## LowRankMultivariateNormal 

_class_ torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(_loc_ , _cov_factor_ , _cov_diag_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by `cov_factor` and `cov_diag`:
```
covariance_matrix = cov_factor @ cov_factor.T + cov_diag

```
Copy to clipboard
Example
```
>>> m = LowRankMultivariateNormal(
...   torch.zeros(2), torch.tensor([[1.0], [0.0]]), torch.ones(2)
... )
>>> m.sample() # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`
tensor([-0.2102, -0.5429])

```
Copy to clipboard 

Parameters
    
  * **loc** (_Tensor_) – mean of the distribution with shape batch_shape + event_shape
  * **cov_factor** (_Tensor_) – factor part of low-rank form of covariance matrix with shape batch_shape + event_shape + (rank,)
  * **cov_diag** (_Tensor_) – diagonal part of low-rank form of covariance matrix with shape batch_shape + event_shape


Note
The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size “capacitance” matrix:
```
capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor

```
Copy to clipboard 

arg_constraints _={'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)}_


_property_ covariance_matrix _: Tensor_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ precision_matrix _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

_property_ scale_tril _: Tensor_


support _= IndependentConstraint(Real(), 1)_


_property_ variance _: Tensor_

## MixtureSameFamily 

_class_ torch.distributions.mixture_same_family.MixtureSameFamily(_mixture_distribution_ , _component_distribution_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical “selecting distribution” (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component.
Examples:
```
>>> # Construct Gaussian Mixture Model in 1D consisting of 5 equally
>>> # weighted normal distributions
>>> mix = D.Categorical(torch.ones(5,))
>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))
>>> gmm = MixtureSameFamily(mix, comp)
>>> # Construct Gaussian Mixture Model in 2D consisting of 5 equally
>>> # weighted bivariate normal distributions
>>> mix = D.Categorical(torch.ones(5,))
>>> comp = D.Independent(D.Normal(
...      torch.randn(5,2), torch.rand(5,2)), 1)
>>> gmm = MixtureSameFamily(mix, comp)
>>> # Construct a batch of 3 Gaussian Mixture Models in 2D each
>>> # consisting of 5 random weighted bivariate normal distributions
>>> mix = D.Categorical(torch.rand(3,5))
>>> comp = D.Independent(D.Normal(
...     torch.randn(3,5,2), torch.rand(3,5,2)), 1)
>>> gmm = MixtureSameFamily(mix, comp)

```
Copy to clipboard 

Parameters
    
  * **mixture_distribution** (_Categorical_) – torch.distributions.Categorical-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the component_distribution. Must have either scalar batch_shape or batch_shape matching component_distribution.batch_shape[:-1]
  * **component_distribution** (_Distribution_) – torch.distributions.Distribution-like instance. Right-most batch dimension indexes component.



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={}_


cdf(_x_)[source][source]


_property_ component_distribution _: Distribution_


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= False_


log_prob(_x_)[source][source]


_property_ mean _: Tensor_


_property_ mixture_distribution _: Categorical_


sample(_sample_shape =torch.Size([])_)[source][source]


_property_ support
     

Return type
    
_DependentProperty 

_property_ variance _: Tensor_

## Multinomial 

_class_ torch.distributions.multinomial.Multinomial(_total_count =1_, _probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a Multinomial distribution parameterized by `total_count` and either `probs` or `logits` (but not both). The innermost dimension of `probs` indexes over categories. All other dimensions index over batches.
Note that `total_count` need not be specified if only `log_prob()` is called (see example below)
Note
The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. `probs` will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. `logits` will return this normalized value.
  * `sample()` requires a single shared total_count for all parameters and samples.
  * `log_prob()` allows different total_count for each parameter and sample.


Example:
```
>>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))
>>> x = m.sample() # equal probability of 0, 1, 2, 3
tensor([ 21., 24., 30., 25.])
>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)
tensor([-4.1338])

```
Copy to clipboard 

Parameters
    
  * **total_count** (_int_) – number of trials
  * **probs** (_Tensor_) – event probabilities
  * **logits** (_Tensor_) – event log probabilities (unnormalized)



arg_constraints _={'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


_property_ support
     

Return type
    
_DependentProperty 

total_count _: int_


_property_ variance _: Tensor_

## MultivariateNormal 

_class_ torch.distributions.multivariate_normal.MultivariateNormal(_loc_ , _covariance_matrix =None_, _precision_matrix =None_, _scale_tril =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.
The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix Σ\mathbf{\Sigma}Σ or a positive definite precision matrix Σ−1\mathbf{\Sigma}^{-1}Σ−1 or a lower-triangular matrix L\mathbf{L}L with positive-valued diagonal entries, such that Σ=LL⊤\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\topΣ=LL⊤. This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.
Example
```
>>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))
>>> m.sample() # normally distributed with mean=`[0,0]` and covariance_matrix=`I`
tensor([-0.2102, -0.5429])

```
Copy to clipboard 

Parameters
    
  * **loc** (_Tensor_) – mean of the distribution
  * **covariance_matrix** (_Tensor_) – positive-definite covariance matrix
  * **precision_matrix** (_Tensor_) – positive-definite precision matrix
  * **scale_tril** (_Tensor_) – lower-triangular factor of covariance, with positive-valued diagonal


Note
Only one of `covariance_matrix` or `precision_matrix` or `scale_tril` can be specified.
Using `scale_tril` will be more efficient: all computations internally are based on `scale_tril`. If `covariance_matrix` or `precision_matrix` is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition. 

arg_constraints _={'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}_


_property_ covariance_matrix _: Tensor_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ precision_matrix _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

_property_ scale_tril _: Tensor_


support _= IndependentConstraint(Real(), 1)_


_property_ variance _: Tensor_

## NegativeBinomial 

_class_ torch.distributions.negative_binomial.NegativeBinomial(_total_count_ , _probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before `total_count` failures are achieved. The probability of success of each Bernoulli trial is `probs`. 

Parameters
    
  * **total_count** (_float_ _or_ _Tensor_) – non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count
  * **probs** (_Tensor_) – Event probabilities of success in the half open interval [0, 1)
  * **logits** (_Tensor_) – Event log-odds for probabilities of success



arg_constraints _={'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)}_


expand(_batch_shape_ , __instance =None_)[source][source]


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


support _= IntegerGreaterThan(lower_bound=0)_


_property_ variance _: Tensor_

## Normal 

_class_ torch.distributions.normal.Normal(_loc_ , _scale_ , _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a normal (also called Gaussian) distribution parameterized by `loc` and `scale`.
Example:
```
>>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample() # normally distributed with loc=0 and scale=1
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
  * **loc** (_float_ _or_ _Tensor_) – mean of the distribution (often referred to as mu)
  * **scale** (_float_ _or_ _Tensor_) – standard deviation of the distribution (often referred to as sigma)



arg_constraints _={'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_value_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

sample(_sample_shape =torch.Size([])_)[source][source]


_property_ stddev _: Tensor_


support _= Real()_


_property_ variance _: Tensor_

## OneHotCategorical 

_class_ torch.distributions.one_hot_categorical.OneHotCategorical(_probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a one-hot categorical distribution parameterized by `probs` or `logits`.
Samples are one-hot coded vectors of size `probs.size(-1)`.
Note
The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. `probs` will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. `logits` will return this normalized value.
See also: `torch.distributions.Categorical()` for specifications of `probs` and `logits`.
Example:
```
>>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
>>> m.sample() # equal probability of 0, 1, 2, 3
tensor([ 0., 0., 0., 1.])

```
Copy to clipboard 

Parameters
    
  * **probs** (_Tensor_) – event probabilities
  * **logits** (_Tensor_) – event log probabilities (unnormalized)



arg_constraints _={'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}_


entropy()[source][source]


enumerate_support(_expand =True_)[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_enumerate_support _= True_


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


support _= OneHot()_


_property_ variance _: Tensor_

## Pareto 

_class_ torch.distributions.pareto.Pareto(_scale_ , _alpha_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Samples from a Pareto Type 1 distribution.
Example:
```
>>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample() # sample from a Pareto distribution with scale=1 and alpha=1
tensor([ 1.5623])

```
Copy to clipboard 

Parameters
    
  * **scale** (_float_ _or_ _Tensor_) – Scale parameter of the distribution
  * **alpha** (_float_ _or_ _Tensor_) – Shape parameter of the distribution



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}_


entropy()[source][source]
     

Return type
    
_Tensor_ 

expand(_batch_shape_ , __instance =None_)[source][source]
     

Return type
    
_Pareto_ 

_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ support _: Constraint_
     

Return type
    
_DependentProperty 

_property_ variance _: Tensor_

## Poisson 

_class_ torch.distributions.poisson.Poisson(_rate_ , _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a Poisson distribution parameterized by `rate`, the rate parameter.
Samples are nonnegative integers, with a pmf given by
rateke−ratek!\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!} ratekk!e−rate​
Example:
```
>>> m = Poisson(torch.tensor([4]))
>>> m.sample()
tensor([ 3.])

```
Copy to clipboard 

Parameters
    
**rate** (_Number_ _,__Tensor_) – the rate parameter 

arg_constraints _={'rate': GreaterThanEq(lower_bound=0.0)}_


expand(_batch_shape_ , __instance =None_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]


support _= IntegerGreaterThan(lower_bound=0)_


_property_ variance _: Tensor_

## RelaxedBernoulli 

_class_ torch.distributions.relaxed_bernoulli.RelaxedBernoulli(_temperature_ , _probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Creates a RelaxedBernoulli distribution, parametrized by `temperature`, and either `probs` or `logits` (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples.
Example:
```
>>> m = RelaxedBernoulli(torch.tensor([2.2]),
...            torch.tensor([0.1, 0.2, 0.3, 0.99]))
>>> m.sample()
tensor([ 0.2951, 0.3442, 0.8918, 0.9021])

```
Copy to clipboard 

Parameters
    
  * **temperature** (_Tensor_) – relaxation temperature
  * **probs** (_Number_ _,__Tensor_) – the probability of sampling 1
  * **logits** (_Number_ _,__Tensor_) – the log-odds of sampling 1



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}_


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


_property_ logits _: Tensor_


_property_ probs _: Tensor_


support _= Interval(lower_bound=0.0, upper_bound=1.0)_


_property_ temperature _: Tensor_

## LogitRelaxedBernoulli 

_class_ torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(_temperature_ , _probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a LogitRelaxedBernoulli distribution parameterized by `probs` or `logits` (but not both), which is the logit of a RelaxedBernoulli distribution.
Samples are logits of values in (0, 1). See [1] for more details. 

Parameters
    
  * **temperature** (_Tensor_) – relaxation temperature
  * **probs** (_Number_ _,__Tensor_) – the probability of sampling 1
  * **logits** (_Number_ _,__Tensor_) – the log-odds of sampling 1


[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al., 2017)
[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al., 2017) 

arg_constraints _={'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}_


expand(_batch_shape_ , __instance =None_)[source][source]


log_prob(_value_)[source][source]


_property_ logits _: Tensor_


_property_ param_shape _: Size_


_property_ probs _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

support _= Real()_

## RelaxedOneHotCategorical 

_class_ torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(_temperature_ , _probs =None_, _logits =None_, _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Creates a RelaxedOneHotCategorical distribution parametrized by `temperature`, and either `probs` or `logits`. This is a relaxed version of the `OneHotCategorical` distribution, so its samples are on simplex, and are reparametrizable.
Example:
```
>>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),
...                torch.tensor([0.1, 0.2, 0.3, 0.4]))
>>> m.sample()
tensor([ 0.1294, 0.2324, 0.3859, 0.2523])

```
Copy to clipboard 

Parameters
    
  * **temperature** (_Tensor_) – relaxation temperature
  * **probs** (_Tensor_) – event probabilities
  * **logits** (_Tensor_) – unnormalized log probability for each event



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}_


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


_property_ logits _: Tensor_


_property_ probs _: Tensor_


support _= Simplex()_


_property_ temperature _: Tensor_

## StudentT 

_class_ torch.distributions.studentT.StudentT(_df_ , _loc =0.0_, _scale =1.0_, _validate_args =None_)[source][source]
    
Bases: `Distribution`
Creates a Student’s t-distribution parameterized by degree of freedom `df`, mean `loc` and scale `scale`.
Example:
```
>>> m = StudentT(torch.tensor([2.0]))
>>> m.sample() # Student's t-distributed with degrees of freedom=2
tensor([ 0.1046])

```
Copy to clipboard 

Parameters
    
  * **df** (_float_ _or_ _Tensor_) – degrees of freedom
  * **loc** (_float_ _or_ _Tensor_) – mean of the distribution
  * **scale** (_float_ _or_ _Tensor_) – scale of the distribution



arg_constraints _={'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

support _= Real()_


_property_ variance _: Tensor_

## TransformedDistribution 

_class_ torch.distributions.transformed_distribution.TransformedDistribution(_base_distribution_ , _transforms_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:
```
X ~ BaseDistribution
Y = f(X) ~ TransformedDistribution(BaseDistribution, f)
log p(Y) = log p(X) + log |det (dX/dY)|

```
Copy to clipboard
Note that the `.event_shape` of a `TransformedDistribution` is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.
An example for the usage of `TransformedDistribution` would be:
```
# Building a Logistic Distribution
# X ~ Uniform(0, 1)
# f = a + b * logit(X)
# Y ~ f(X) ~ Logistic(a, b)
base_distribution = Uniform(0, 1)
transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]
logistic = TransformedDistribution(base_distribution, transforms)

```
Copy to clipboard
For more examples, please look at the implementations of `Gumbel`, `HalfCauchy`, `HalfNormal`, `LogNormal`, `Pareto`, `Weibull`, `RelaxedBernoulli` and `RelaxedOneHotCategorical` 

arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={}_


cdf(_value_)[source][source]
    
Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. 

expand(_batch_shape_ , __instance =None_)[source][source]


_property_ has_rsample _: bool_


icdf(_value_)[source][source]
    
Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. 

log_prob(_value_)[source][source]
    
Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. 

rsample(_sample_shape =torch.Size([])_)[source][source]
    
Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. 

Return type
    
_Tensor_ 

sample(_sample_shape =torch.Size([])_)[source][source]
    
Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. 

_property_ support
     

Return type
    
_DependentProperty
## Uniform 

_class_ torch.distributions.uniform.Uniform(_low_ , _high_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
Generates uniformly distributed random samples from the half-open interval `[low, high)`.
Example:
```
>>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))
>>> m.sample() # uniformly distributed in the range [0.0, 5.0)
tensor([ 2.3418])

```
Copy to clipboard 

Parameters
    
  * **low** (_float_ _or_ _Tensor_) – lower range (inclusive).
  * **high** (_float_ _or_ _Tensor_) – upper range (exclusive).



arg_constraints _={'high': Dependent(), 'low': Dependent()}_


cdf(_value_)[source][source]


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


icdf(_value_)[source][source]


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


rsample(_sample_shape =torch.Size([])_)[source][source]
     

Return type
    
_Tensor_ 

_property_ stddev _: Tensor_


_property_ support
     

Return type
    
_DependentProperty 

_property_ variance _: Tensor_

## VonMises 

_class_ torch.distributions.von_mises.VonMises(_loc_ , _concentration_ , _validate_args =None_)[source][source]
    
Bases: `Distribution`
A circular von Mises distribution.
This implementation uses polar coordinates. The `loc` and `value` args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi. 

Example::
    
```
>>> m = VonMises(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample() # von Mises distributed with loc=1 and concentration=1
tensor([1.9777])

```
Copy to clipboard 

Parameters
    
  * **loc** (_torch.Tensor_) – an angle in radians.
  * **concentration** (_torch.Tensor_) – concentration parameter



arg_constraints _={'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()}_


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= False_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_
    
The provided mean is the circular one. 

_property_ mode _: Tensor_


sample(_sample_shape =torch.Size([])_)[source][source]
    
The sampling algorithm for the von Mises distribution is based on the following paper: D.J. Best and N.I. Fisher, “Efficient simulation of the von Mises distribution.” Applied Statistics (1979): 152-157.
Sampling is always done in double precision internally to avoid a hang in _rejection_sample() for small values of the concentration, which starts to happen for single precision around 1e-4 (see issue #88443). 

support _= Real()_


_property_ variance _: Tensor_
    
The provided variance is the circular one.
## Weibull 

_class_ torch.distributions.weibull.Weibull(_scale_ , _concentration_ , _validate_args =None_)[source][source]
    
Bases: `TransformedDistribution`
Samples from a two-parameter Weibull distribution.
Example
```
>>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample() # sample from a Weibull distribution with scale=1, concentration=1
tensor([ 0.4784])

```
Copy to clipboard 

Parameters
    
  * **scale** (_float_ _or_ _Tensor_) – Scale parameter of distribution (lambda).
  * **concentration** (_float_ _or_ _Tensor_) – Concentration parameter of distribution (k/shape).



arg_constraints _: dict[str,torch.distributions.constraints.Constraint]__={'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


support _= GreaterThan(lower_bound=0.0)_


_property_ variance _: Tensor_

## Wishart 

_class_ torch.distributions.wishart.Wishart(_df_ , _covariance_matrix =None_, _precision_matrix =None_, _scale_tril =None_, _validate_args =None_)[source][source]
    
Bases: `ExponentialFamily`
Creates a Wishart distribution parameterized by a symmetric positive definite matrix Σ\SigmaΣ, or its Cholesky decomposition Σ=LL⊤\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\topΣ=LL⊤
Example
```
>>> m = Wishart(torch.Tensor([2]), covariance_matrix=torch.eye(2))
>>> m.sample() # Wishart distributed with mean=`df * I` and
>>> # variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j

```
Copy to clipboard 

Parameters
    
  * **df** (_float_ _or_ _Tensor_) – real-valued parameter larger than the (dimension of Square matrix) - 1
  * **covariance_matrix** (_Tensor_) – positive-definite covariance matrix
  * **precision_matrix** (_Tensor_) – positive-definite precision matrix
  * **scale_tril** (_Tensor_) – lower-triangular factor of covariance, with positive-valued diagonal


Note
Only one of `covariance_matrix` or `precision_matrix` or `scale_tril` can be specified. Using `scale_tril` will be more efficient: all computations internally are based on `scale_tril`. If `covariance_matrix` or `precision_matrix` is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition. ‘torch.distributions.LKJCholesky’ is a restricted Wishart distribution.[1]
**References**
[1] Wang, Z., Wu, Y. and Chu, H., 2018. On equivalence of the LKJ distribution and the restricted Wishart distribution. [2] Sawyer, S., 2007. Wishart Distributions and Inverse-Wishart Sampling. [3] Anderson, T. W., 2003. An Introduction to Multivariate Statistical Analysis (3rd ed.). [4] Odell, P. L. & Feiveson, A. H., 1966. A Numerical Procedure to Generate a SampleCovariance Matrix. JASA, 61(313):199-203. [5] Ku, Y.-C. & Bloomfield, P., 2010. Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX. 

arg_constraints _={'covariance_matrix': PositiveDefinite(), 'df': GreaterThan(lower_bound=0), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}_


_property_ covariance_matrix _: Tensor_


entropy()[source][source]


expand(_batch_shape_ , __instance =None_)[source][source]


has_rsample _= True_


log_prob(_value_)[source][source]


_property_ mean _: Tensor_


_property_ mode _: Tensor_


_property_ precision_matrix _: Tensor_


rsample(_sample_shape =torch.Size([])_, _max_try_correction =None_)[source][source]
    
Warning
In some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return -inf values in .log_prob(). In those cases, the user should validate the samples and either fix the value of df or adjust max_try_correction value for argument in .rsample accordingly. 

Return type
    
_Tensor_ 

_property_ scale_tril _: Tensor_


support _= PositiveDefinite()_


_property_ variance _: Tensor_

## KL Divergence 

torch.distributions.kl.kl_divergence(_p_ , _q_)[source][source]
    
Compute Kullback-Leibler divergence KL(p∥q)KL(p \| q)KL(p∥q) between two distributions.
KL(p∥q)=∫p(x)log⁡p(x)q(x)dxKL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dxKL(p∥q)=∫p(x)logq(x)p(x)​dx 

Parameters
    
  * **p** (_Distribution_) – A `Distribution` object.
  * **q** (_Distribution_) – A `Distribution` object.



Returns
    
A batch of KL divergences of shape batch_shape. 

Return type
    
Tensor 

Raises
    
**NotImplementedError** – If the distribution types have not been registered via `register_kl()`. 

KL divergence is currently implemented for the following distribution pairs:
    
  * `Bernoulli` and `Bernoulli`
  * `Bernoulli` and `Poisson`
  * `Beta` and `Beta`
  * `Beta` and `ContinuousBernoulli`
  * `Beta` and `Exponential`
  * `Beta` and `Gamma`
  * `Beta` and `Normal`
  * `Beta` and `Pareto`
  * `Beta` and `Uniform`
  * `Binomial` and `Binomial`
  * `Categorical` and `Categorical`
  * `Cauchy` and `Cauchy`
  * `ContinuousBernoulli` and `ContinuousBernoulli`
  * `ContinuousBernoulli` and `Exponential`
  * `ContinuousBernoulli` and `Normal`
  * `ContinuousBernoulli` and `Pareto`
  * `ContinuousBernoulli` and `Uniform`
  * `Dirichlet` and `Dirichlet`
  * `Exponential` and `Beta`
  * `Exponential` and `ContinuousBernoulli`
  * `Exponential` and `Exponential`
  * `Exponential` and `Gamma`
  * `Exponential` and `Gumbel`
  * `Exponential` and `Normal`
  * `Exponential` and `Pareto`
  * `Exponential` and `Uniform`
  * `ExponentialFamily` and `ExponentialFamily`
  * `Gamma` and `Beta`
  * `Gamma` and `ContinuousBernoulli`
  * `Gamma` and `Exponential`
  * `Gamma` and `Gamma`
  * `Gamma` and `Gumbel`
  * `Gamma` and `Normal`
  * `Gamma` and `Pareto`
  * `Gamma` and `Uniform`
  * `Geometric` and `Geometric`
  * `Gumbel` and `Beta`
  * `Gumbel` and `ContinuousBernoulli`
  * `Gumbel` and `Exponential`
  * `Gumbel` and `Gamma`
  * `Gumbel` and `Gumbel`
  * `Gumbel` and `Normal`
  * `Gumbel` and `Pareto`
  * `Gumbel` and `Uniform`
  * `HalfNormal` and `HalfNormal`
  * `Independent` and `Independent`
  * `Laplace` and `Beta`
  * `Laplace` and `ContinuousBernoulli`
  * `Laplace` and `Exponential`
  * `Laplace` and `Gamma`
  * `Laplace` and `Laplace`
  * `Laplace` and `Normal`
  * `Laplace` and `Pareto`
  * `Laplace` and `Uniform`
  * `LowRankMultivariateNormal` and `LowRankMultivariateNormal`
  * `LowRankMultivariateNormal` and `MultivariateNormal`
  * `MultivariateNormal` and `LowRankMultivariateNormal`
  * `MultivariateNormal` and `MultivariateNormal`
  * `Normal` and `Beta`
  * `Normal` and `ContinuousBernoulli`
  * `Normal` and `Exponential`
  * `Normal` and `Gamma`
  * `Normal` and `Gumbel`
  * `Normal` and `Laplace`
  * `Normal` and `Normal`
  * `Normal` and `Pareto`
  * `Normal` and `Uniform`
  * `OneHotCategorical` and `OneHotCategorical`
  * `Pareto` and `Beta`
  * `Pareto` and `ContinuousBernoulli`
  * `Pareto` and `Exponential`
  * `Pareto` and `Gamma`
  * `Pareto` and `Normal`
  * `Pareto` and `Pareto`
  * `Pareto` and `Uniform`
  * `Poisson` and `Bernoulli`
  * `Poisson` and `Binomial`
  * `Poisson` and `Poisson`
  * `TransformedDistribution` and `TransformedDistribution`
  * `Uniform` and `Beta`
  * `Uniform` and `ContinuousBernoulli`
  * `Uniform` and `Exponential`
  * `Uniform` and `Gamma`
  * `Uniform` and `Gumbel`
  * `Uniform` and `Normal`
  * `Uniform` and `Pareto`
  * `Uniform` and `Uniform`



torch.distributions.kl.register_kl(_type_p_ , _type_q_)[source][source]
    
Decorator to register a pairwise function with `kl_divergence()`. Usage:
```
@register_kl(Normal, Normal)
def kl_normal_normal(p, q):
  # insert implementation here

```
Copy to clipboard
Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation:
```
@register_kl(BaseP, DerivedQ)
def kl_version1(p, q): ...
@register_kl(DerivedP, BaseQ)
def kl_version2(p, q): ...

```
Copy to clipboard
you should register a third most-specific implementation, e.g.:
```
register_kl(DerivedP, DerivedQ)(kl_version1) # Break the tie.

```
Copy to clipboard 

Parameters
    
  * **type_p** (_type_) – A subclass of `Distribution`.
  * **type_q** (_type_) – A subclass of `Distribution`.


## Transforms 

_class_ torch.distributions.transforms.AbsTransform(_cache_size =0_)[source][source]
    
Transform via the mapping y=∣x∣y = |x|y=∣x∣. 

_class_ torch.distributions.transforms.AffineTransform(_loc_ , _scale_ , _event_dim =0_, _cache_size =0_)[source][source]
    
Transform via the pointwise affine mapping y=loc+scale×xy = \text{loc} + \text{scale} \times xy=loc+scale×x. 

Parameters
    
  * **loc** (_Tensor_ _or_ _float_) – Location parameter.
  * **scale** (_Tensor_ _or_ _float_) – Scale parameter.
  * **event_dim** (_int_) – Optional size of event_shape. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.



_class_ torch.distributions.transforms.CatTransform(_tseq_ , _dim =0_, _lengths =None_, _cache_size =0_)[source][source]
    
Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim, of length lengths[dim], in a way compatible with `torch.cat()`.
Example:
```
x0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)
x = torch.cat([x0, x0], dim=0)
t0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])
t = CatTransform([t0, t0], dim=0, lengths=[20, 20])
y = t(x)

```
Copy to clipboard 

_class_ torch.distributions.transforms.ComposeTransform(_parts_ , _cache_size =0_)[source][source]
    
Composes multiple transforms in a chain. The transforms being composed are responsible for caching. 

Parameters
    
  * **parts** (list of `Transform`) – A list of transforms to compose.
  * **cache_size** (_int_) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.



_class_ torch.distributions.transforms.CorrCholeskyTransform(_cache_size =0_)[source][source]
    
Transforms an uncontrained real vector xxx with length D∗(D−1)/2D*(D-1)/2D∗(D−1)/2 into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:
>   1. First we convert x into a lower triangular matrix in row order.
>   2. For each row XiX_iXi​ of the lower triangular part, we apply a _signed_ version of class `StickBreakingTransform` to transform XiX_iXi​ into a unit Euclidean length vector using the following steps: - Scales into the interval (−1,1)(-1, 1)(−1,1) domain: ri=tanh⁡(Xi)r_i = \tanh(X_i)ri​=tanh(Xi​). - Transforms into an unsigned domain: zi=ri2z_i = r_i^2zi​=ri2​. - Applies si=StickBreakingTransform(zi)s_i = StickBreakingTransform(z_i)si​=StickBreakingTransform(zi​). - Transforms back into signed domain: yi=sign(ri)∗siy_i = sign(r_i) * \sqrt{s_i}yi​=sign(ri​)∗si​​.
> 


_class_ torch.distributions.transforms.CumulativeDistributionTransform(_distribution_ , _cache_size =0_)[source][source]
    
Transform via the cumulative distribution function of a probability distribution. 

Parameters
    
**distribution** (_Distribution_) – Distribution whose cumulative distribution function to use for the transformation.
Example:
```
# Construct a Gaussian copula from a multivariate normal.
base_dist = MultivariateNormal(
  loc=torch.zeros(2),
  scale_tril=LKJCholesky(2).sample(),
)
transform = CumulativeDistributionTransform(Normal(0, 1))
copula = TransformedDistribution(base_dist, [transform])

```
Copy to clipboard 

_class_ torch.distributions.transforms.ExpTransform(_cache_size =0_)[source][source]
    
Transform via the mapping y=exp⁡(x)y = \exp(x)y=exp(x). 

_class_ torch.distributions.transforms.IndependentTransform(_base_transform_ , _reinterpreted_batch_ndims_ , _cache_size =0_)[source][source]
    
Wrapper around another transform to treat `reinterpreted_batch_ndims`-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out `reinterpreted_batch_ndims`-many of the rightmost dimensions in `log_abs_det_jacobian()`. 

Parameters
    
  * **base_transform** (`Transform`) – A base transform.
  * **reinterpreted_batch_ndims** (_int_) – The number of extra rightmost dimensions to treat as dependent.



_class_ torch.distributions.transforms.LowerCholeskyTransform(_cache_size =0_)[source][source]
    
Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.
This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization. 

_class_ torch.distributions.transforms.PositiveDefiniteTransform(_cache_size =0_)[source][source]
    
Transform from unconstrained matrices to positive-definite matrices. 

_class_ torch.distributions.transforms.PowerTransform(_exponent_ , _cache_size =0_)[source][source]
    
Transform via the mapping y=xexponenty = x^{\text{exponent}}y=xexponent. 

_class_ torch.distributions.transforms.ReshapeTransform(_in_shape_ , _out_shape_ , _cache_size =0_)[source][source]
    
Unit Jacobian transform to reshape the rightmost part of a tensor.
Note that `in_shape` and `out_shape` must have the same number of elements, just as for `torch.Tensor.reshape()`. 

Parameters
    
  * **in_shape** (_torch.Size_) – The input event shape.
  * **out_shape** (_torch.Size_) – The output event shape.
  * **cache_size** (_int_) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported. (Default 0.)



_class_ torch.distributions.transforms.SigmoidTransform(_cache_size =0_)[source][source]
    
Transform via the mapping y=11+exp⁡(−x)y = \frac{1}{1 + \exp(-x)}y=1+exp(−x)1​ and x=logit(y)x = \text{logit}(y)x=logit(y). 

_class_ torch.distributions.transforms.SoftplusTransform(_cache_size =0_)[source][source]
    
Transform via the mapping Softplus(x)=log⁡(1+exp⁡(x))\text{Softplus}(x) = \log(1 + \exp(x))Softplus(x)=log(1+exp(x)). The implementation reverts to the linear function when x>20x > 20x>20. 

_class_ torch.distributions.transforms.TanhTransform(_cache_size =0_)[source][source]
    
Transform via the mapping y=tanh⁡(x)y = \tanh(x)y=tanh(x).
It is equivalent to
```
ComposeTransform(
  [
    AffineTransform(0.0, 2.0),
    SigmoidTransform(),
    AffineTransform(-1.0, 2.0),
  ]
)

```
Copy to clipboard
However this might not be numerically stable, thus it is recommended to use TanhTransform instead.
Note that one should use cache_size=1 when it comes to NaN/Inf values. 

_class_ torch.distributions.transforms.SoftmaxTransform(_cache_size =0_)[source][source]
    
Transform from unconstrained space to the simplex via y=exp⁡(x)y = \exp(x)y=exp(x) then normalizing.
This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms. 

_class_ torch.distributions.transforms.StackTransform(_tseq_ , _dim =0_, _cache_size =0_)[source][source]
    
Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with `torch.stack()`.
Example:
```
x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)
t = StackTransform([ExpTransform(), identity_transform], dim=1)
y = t(x)

```
Copy to clipboard 

_class_ torch.distributions.transforms.StickBreakingTransform(_cache_size =0_)[source][source]
    
Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.
This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.
This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization. 

_class_ torch.distributions.transforms.Transform(_cache_size =0_)[source][source]
    
Abstract class for invertable transformations with computable log det jacobians. They are primarily used in `torch.distributions.TransformedDistribution`.
Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:
```
y = t(x)
t.log_abs_det_jacobian(x, y).backward() # x will receive gradients.

```
Copy to clipboard
However the following will error when caching due to dependency reversal:
```
y = t(x)
z = t.inv(y)
grad(z.sum(), [y]) # error because z is x

```
Copy to clipboard
Derived classes should implement one or both of `_call()` or `_inverse()`. Derived classes that set bijective=True should also implement `log_abs_det_jacobian()`. 

Parameters
    
**cache_size** (_int_) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported. 

Variables
    
  * **domain** (`Constraint`) – The constraint representing valid inputs to this transform.
  * **codomain** (`Constraint`) – The constraint representing valid outputs to this transform which are inputs to the inverse transform.
  * **bijective** (_bool_) – Whether this transform is bijective. A transform `t` is bijective iff `t.inv(t(x)) == x` and `t(t.inv(y)) == y` for every `x` in the domain and `y` in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties `t(t.inv(t(x)) == t(x)` and `t.inv(t(t.inv(y))) == t.inv(y)`.
  * **sign** (_int_ _or_ _Tensor_) – For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.



_property_ inv _: Transform_
    
Returns the inverse `Transform` of this transform. This should satisfy `t.inv.inv is t`. 

_property_ sign _: int_
    
Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. 

log_abs_det_jacobian(_x_ , _y_)[source][source]
    
Computes the log det jacobian log |dy/dx| given input and output. 

forward_shape(_shape_)[source][source]
    
Infers the shape of the forward computation, given the input shape. Defaults to preserving shape. 

inverse_shape(_shape_)[source][source]
    
Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape.
## Constraints 

_class_ torch.distributions.constraints.Constraint[source][source]
    
Abstract base class for constraints.
A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized. 

Variables
    
  * **is_discrete** (_bool_) – Whether constrained space is discrete. Defaults to False.
  * **event_dim** (_int_) – Number of rightmost dimensions that together define an event. The `check()` method will remove this many dimensions when computing validity.



check(_value_)[source][source]
    
Returns a byte tensor of `sample_shape + batch_shape` indicating whether each event in value satisfies this constraint. 

torch.distributions.constraints.cat[source]
    
alias of `_Cat` 

torch.distributions.constraints.dependent_property[source]
    
alias of `_DependentProperty` 

torch.distributions.constraints.greater_than[source]
    
alias of `_GreaterThan` 

torch.distributions.constraints.greater_than_eq[source]
    
alias of `_GreaterThanEq` 

torch.distributions.constraints.independent[source]
    
alias of `_IndependentConstraint` 

torch.distributions.constraints.integer_interval[source]
    
alias of `_IntegerInterval` 

torch.distributions.constraints.interval[source]
    
alias of `_Interval` 

torch.distributions.constraints.half_open_interval[source]
    
alias of `_HalfOpenInterval` 

torch.distributions.constraints.is_dependent(_constraint_)[source][source]
    
Checks if `constraint` is a `_Dependent` object. 

Parameters
    
**constraint** – A `Constraint` object. 

Returns
    
True if `constraint` can be refined to the type `_Dependent`, False otherwise. 

Return type
    
`bool`
Examples
```
>>> import torch
>>> from torch.distributions import Bernoulli
>>> from torch.distributions.constraints import is_dependent

```
Copy to clipboard
```
>>> dist = Bernoulli(probs=torch.tensor([0.6], requires_grad=True))
>>> constraint1 = dist.arg_constraints["probs"]
>>> constraint2 = dist.arg_constraints["logits"]

```
Copy to clipboard
```
>>> for constraint in [constraint1, constraint2]:
>>>   if is_dependent(constraint):
>>>     continue

```
Copy to clipboard 

torch.distributions.constraints.less_than[source]
    
alias of `_LessThan` 

torch.distributions.constraints.multinomial[source]
    
alias of `_Multinomial` 

torch.distributions.constraints.stack[source]
    
alias of `_Stack`
## Constraint Registry
PyTorch provides two global `ConstraintRegistry` objects that link `Constraint` objects to `Transform` objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.
  1. `biject_to(constraint)` looks up a bijective `Transform` from `constraints.real` to the given `constraint`. The returned transform is guaranteed to have `.bijective = True` and should implement `.log_abs_det_jacobian()`.
  2. `transform_to(constraint)` looks up a not-necessarily bijective `Transform` from `constraints.real` to the given `constraint`. The returned transform is not guaranteed to implement `.log_abs_det_jacobian()`.


The `transform_to()` registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution’s `.arg_constraints` dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:
```
loc = torch.zeros(100, requires_grad=True)
unconstrained = torch.zeros(100, requires_grad=True)
scale = transform_to(Normal.arg_constraints["scale"])(unconstrained)
loss = -Normal(loc, scale).log_prob(data).sum()

```
Copy to clipboard
The `biject_to()` registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained `.support` are propagated in an unconstrained space, and algorithms are typically rotation invariant.:
```
dist = Exponential(rate)
unconstrained = torch.zeros(100, requires_grad=True)
sample = biject_to(dist.support)(unconstrained)
potential_energy = -dist.log_prob(sample).sum()

```
Copy to clipboard
Note
An example where `transform_to` and `biject_to` differ is `constraints.simplex`: `transform_to(constraints.simplex)` returns a `SoftmaxTransform` that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, `biject_to(constraints.simplex)` returns a `StickBreakingTransform` that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.
The `biject_to` and `transform_to` objects can be extended by user-defined constraints and transforms using their `.register()` method either as a function on singleton constraints:
```
transform_to.register(my_constraint, my_transform)

```
Copy to clipboard
or as a decorator on parameterized constraints:
```
@transform_to.register(MyConstraintClass)
def my_factory(constraint):
  assert isinstance(constraint, MyConstraintClass)
  return MyTransform(constraint.param1, constraint.param2)

```
Copy to clipboard
You can create your own registry by creating a new `ConstraintRegistry` object. 

_class_ torch.distributions.constraint_registry.ConstraintRegistry[source][source]
    
Registry to link constraints to transforms. 

register(_constraint_ , _factory =None_)[source][source]
    
Registers a `Constraint` subclass in this registry. Usage:
```
@my_registry.register(MyConstraintClass)
def construct_transform(constraint):
  assert isinstance(constraint, MyConstraint)
  return MyTransform(constraint.arg_constraints)

```
Copy to clipboard 

Parameters
    
  * **constraint** (subclass of `Constraint`) – A subclass of `Constraint`, or a singleton object of the desired class.
  * **factory** (_Callable_) – A callable that inputs a constraint object and returns a `Transform` object.


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Probability distributions - torch.distributions
    * Score function
    * Pathwise derivative
    * Distribution
      * `Distribution`
        * `Distribution.arg_constraints`
        * `Distribution.batch_shape`
        * `Distribution.cdf()`
        * `Distribution.entropy()`
        * `Distribution.enumerate_support()`
        * `Distribution.event_shape`
        * `Distribution.expand()`
        * `Distribution.icdf()`
        * `Distribution.log_prob()`
        * `Distribution.mean`
        * `Distribution.mode`
        * `Distribution.perplexity()`
        * `Distribution.rsample()`
        * `Distribution.sample()`
        * `Distribution.sample_n()`
        * `Distribution.set_default_validate_args()`
        * `Distribution.stddev`
        * `Distribution.support`
        * `Distribution.variance`
    * ExponentialFamily
      * `ExponentialFamily`
        * `ExponentialFamily.entropy()`
    * Bernoulli
      * `Bernoulli`
        * `Bernoulli.arg_constraints`
        * `Bernoulli.entropy()`
        * `Bernoulli.enumerate_support()`
        * `Bernoulli.expand()`
        * `Bernoulli.has_enumerate_support`
        * `Bernoulli.log_prob()`
        * `Bernoulli.logits`
        * `Bernoulli.mean`
        * `Bernoulli.mode`
        * `Bernoulli.param_shape`
        * `Bernoulli.probs`
        * `Bernoulli.sample()`
        * `Bernoulli.support`
        * `Bernoulli.variance`
    * Beta
      * `Beta`
        * `Beta.arg_constraints`
        * `Beta.concentration0`
        * `Beta.concentration1`
        * `Beta.entropy()`
        * `Beta.expand()`
        * `Beta.has_rsample`
        * `Beta.log_prob()`
        * `Beta.mean`
        * `Beta.mode`
        * `Beta.rsample()`
        * `Beta.support`
        * `Beta.variance`
    * Binomial
      * `Binomial`
        * `Binomial.arg_constraints`
        * `Binomial.entropy()`
        * `Binomial.enumerate_support()`
        * `Binomial.expand()`
        * `Binomial.has_enumerate_support`
        * `Binomial.log_prob()`
        * `Binomial.logits`
        * `Binomial.mean`
        * `Binomial.mode`
        * `Binomial.param_shape`
        * `Binomial.probs`
        * `Binomial.sample()`
        * `Binomial.support`
        * `Binomial.variance`
    * Categorical
      * `Categorical`
        * `Categorical.arg_constraints`
        * `Categorical.entropy()`
        * `Categorical.enumerate_support()`
        * `Categorical.expand()`
        * `Categorical.has_enumerate_support`
        * `Categorical.log_prob()`
        * `Categorical.logits`
        * `Categorical.mean`
        * `Categorical.mode`
        * `Categorical.param_shape`
        * `Categorical.probs`
        * `Categorical.sample()`
        * `Categorical.support`
        * `Categorical.variance`
    * Cauchy
      * `Cauchy`
        * `Cauchy.arg_constraints`
        * `Cauchy.cdf()`
        * `Cauchy.entropy()`
        * `Cauchy.expand()`
        * `Cauchy.has_rsample`
        * `Cauchy.icdf()`
        * `Cauchy.log_prob()`
        * `Cauchy.mean`
        * `Cauchy.mode`
        * `Cauchy.rsample()`
        * `Cauchy.support`
        * `Cauchy.variance`
    * Chi2
      * `Chi2`
        * `Chi2.arg_constraints`
        * `Chi2.df`
        * `Chi2.expand()`
    * ContinuousBernoulli
      * `ContinuousBernoulli`
        * `ContinuousBernoulli.arg_constraints`
        * `ContinuousBernoulli.cdf()`
        * `ContinuousBernoulli.entropy()`
        * `ContinuousBernoulli.expand()`
        * `ContinuousBernoulli.has_rsample`
        * `ContinuousBernoulli.icdf()`
        * `ContinuousBernoulli.log_prob()`
        * `ContinuousBernoulli.logits`
        * `ContinuousBernoulli.mean`
        * `ContinuousBernoulli.param_shape`
        * `ContinuousBernoulli.probs`
        * `ContinuousBernoulli.rsample()`
        * `ContinuousBernoulli.sample()`
        * `ContinuousBernoulli.stddev`
        * `ContinuousBernoulli.support`
        * `ContinuousBernoulli.variance`
    * Dirichlet
      * `Dirichlet`
        * `Dirichlet.arg_constraints`
        * `Dirichlet.entropy()`
        * `Dirichlet.expand()`
        * `Dirichlet.has_rsample`
        * `Dirichlet.log_prob()`
        * `Dirichlet.mean`
        * `Dirichlet.mode`
        * `Dirichlet.rsample()`
        * `Dirichlet.support`
        * `Dirichlet.variance`
    * Exponential
      * `Exponential`
        * `Exponential.arg_constraints`
        * `Exponential.cdf()`
        * `Exponential.entropy()`
        * `Exponential.expand()`
        * `Exponential.has_rsample`
        * `Exponential.icdf()`
        * `Exponential.log_prob()`
        * `Exponential.mean`
        * `Exponential.mode`
        * `Exponential.rsample()`
        * `Exponential.stddev`
        * `Exponential.support`
        * `Exponential.variance`
    * FisherSnedecor
      * `FisherSnedecor`
        * `FisherSnedecor.arg_constraints`
        * `FisherSnedecor.expand()`
        * `FisherSnedecor.has_rsample`
        * `FisherSnedecor.log_prob()`
        * `FisherSnedecor.mean`
        * `FisherSnedecor.mode`
        * `FisherSnedecor.rsample()`
        * `FisherSnedecor.support`
        * `FisherSnedecor.variance`
    * Gamma
      * `Gamma`
        * `Gamma.arg_constraints`
        * `Gamma.cdf()`
        * `Gamma.entropy()`
        * `Gamma.expand()`
        * `Gamma.has_rsample`
        * `Gamma.log_prob()`
        * `Gamma.mean`
        * `Gamma.mode`
        * `Gamma.rsample()`
        * `Gamma.support`
        * `Gamma.variance`
    * Geometric
      * `Geometric`
        * `Geometric.arg_constraints`
        * `Geometric.entropy()`
        * `Geometric.expand()`
        * `Geometric.log_prob()`
        * `Geometric.logits`
        * `Geometric.mean`
        * `Geometric.mode`
        * `Geometric.probs`
        * `Geometric.sample()`
        * `Geometric.support`
        * `Geometric.variance`
    * Gumbel
      * `Gumbel`
        * `Gumbel.arg_constraints`
        * `Gumbel.entropy()`
        * `Gumbel.expand()`
        * `Gumbel.log_prob()`
        * `Gumbel.mean`
        * `Gumbel.mode`
        * `Gumbel.stddev`
        * `Gumbel.support`
        * `Gumbel.variance`
    * HalfCauchy
      * `HalfCauchy`
        * `HalfCauchy.arg_constraints`
        * `HalfCauchy.cdf()`
        * `HalfCauchy.entropy()`
        * `HalfCauchy.expand()`
        * `HalfCauchy.has_rsample`
        * `HalfCauchy.icdf()`
        * `HalfCauchy.log_prob()`
        * `HalfCauchy.mean`
        * `HalfCauchy.mode`
        * `HalfCauchy.scale`
        * `HalfCauchy.support`
        * `HalfCauchy.variance`
    * HalfNormal
      * `HalfNormal`
        * `HalfNormal.arg_constraints`
        * `HalfNormal.cdf()`
        * `HalfNormal.entropy()`
        * `HalfNormal.expand()`
        * `HalfNormal.has_rsample`
        * `HalfNormal.icdf()`
        * `HalfNormal.log_prob()`
        * `HalfNormal.mean`
        * `HalfNormal.mode`
        * `HalfNormal.scale`
        * `HalfNormal.support`
        * `HalfNormal.variance`
    * Independent
      * `Independent`
        * `Independent.arg_constraints`
        * `Independent.entropy()`
        * `Independent.enumerate_support()`
        * `Independent.expand()`
        * `Independent.has_enumerate_support`
        * `Independent.has_rsample`
        * `Independent.log_prob()`
        * `Independent.mean`
        * `Independent.mode`
        * `Independent.rsample()`
        * `Independent.sample()`
        * `Independent.support`
        * `Independent.variance`
    * InverseGamma
      * `InverseGamma`
        * `InverseGamma.arg_constraints`
        * `InverseGamma.concentration`
        * `InverseGamma.entropy()`
        * `InverseGamma.expand()`
        * `InverseGamma.has_rsample`
        * `InverseGamma.mean`
        * `InverseGamma.mode`
        * `InverseGamma.rate`
        * `InverseGamma.support`
        * `InverseGamma.variance`
    * Kumaraswamy
      * `Kumaraswamy`
        * `Kumaraswamy.arg_constraints`
        * `Kumaraswamy.entropy()`
        * `Kumaraswamy.expand()`
        * `Kumaraswamy.has_rsample`
        * `Kumaraswamy.mean`
        * `Kumaraswamy.mode`
        * `Kumaraswamy.support`
        * `Kumaraswamy.variance`
    * LKJCholesky
      * `LKJCholesky`
        * `LKJCholesky.arg_constraints`
        * `LKJCholesky.expand()`
        * `LKJCholesky.log_prob()`
        * `LKJCholesky.sample()`
        * `LKJCholesky.support`
    * Laplace
      * `Laplace`
        * `Laplace.arg_constraints`
        * `Laplace.cdf()`
        * `Laplace.entropy()`
        * `Laplace.expand()`
        * `Laplace.has_rsample`
        * `Laplace.icdf()`
        * `Laplace.log_prob()`
        * `Laplace.mean`
        * `Laplace.mode`
        * `Laplace.rsample()`
        * `Laplace.stddev`
        * `Laplace.support`
        * `Laplace.variance`
    * LogNormal
      * `LogNormal`
        * `LogNormal.arg_constraints`
        * `LogNormal.entropy()`
        * `LogNormal.expand()`
        * `LogNormal.has_rsample`
        * `LogNormal.loc`
        * `LogNormal.mean`
        * `LogNormal.mode`
        * `LogNormal.scale`
        * `LogNormal.support`
        * `LogNormal.variance`
    * LowRankMultivariateNormal
      * `LowRankMultivariateNormal`
        * `LowRankMultivariateNormal.arg_constraints`
        * `LowRankMultivariateNormal.covariance_matrix`
        * `LowRankMultivariateNormal.entropy()`
        * `LowRankMultivariateNormal.expand()`
        * `LowRankMultivariateNormal.has_rsample`
        * `LowRankMultivariateNormal.log_prob()`
        * `LowRankMultivariateNormal.mean`
        * `LowRankMultivariateNormal.mode`
        * `LowRankMultivariateNormal.precision_matrix`
        * `LowRankMultivariateNormal.rsample()`
        * `LowRankMultivariateNormal.scale_tril`
        * `LowRankMultivariateNormal.support`
        * `LowRankMultivariateNormal.variance`
    * MixtureSameFamily
      * `MixtureSameFamily`
        * `MixtureSameFamily.arg_constraints`
        * `MixtureSameFamily.cdf()`
        * `MixtureSameFamily.component_distribution`
        * `MixtureSameFamily.expand()`
        * `MixtureSameFamily.has_rsample`
        * `MixtureSameFamily.log_prob()`
        * `MixtureSameFamily.mean`
        * `MixtureSameFamily.mixture_distribution`
        * `MixtureSameFamily.sample()`
        * `MixtureSameFamily.support`
        * `MixtureSameFamily.variance`
    * Multinomial
      * `Multinomial`
        * `Multinomial.arg_constraints`
        * `Multinomial.entropy()`
        * `Multinomial.expand()`
        * `Multinomial.log_prob()`
        * `Multinomial.logits`
        * `Multinomial.mean`
        * `Multinomial.param_shape`
        * `Multinomial.probs`
        * `Multinomial.sample()`
        * `Multinomial.support`
        * `Multinomial.total_count`
        * `Multinomial.variance`
    * MultivariateNormal
      * `MultivariateNormal`
        * `MultivariateNormal.arg_constraints`
        * `MultivariateNormal.covariance_matrix`
        * `MultivariateNormal.entropy()`
        * `MultivariateNormal.expand()`
        * `MultivariateNormal.has_rsample`
        * `MultivariateNormal.log_prob()`
        * `MultivariateNormal.mean`
        * `MultivariateNormal.mode`
        * `MultivariateNormal.precision_matrix`
        * `MultivariateNormal.rsample()`
        * `MultivariateNormal.scale_tril`
        * `MultivariateNormal.support`
        * `MultivariateNormal.variance`
    * NegativeBinomial
      * `NegativeBinomial`
        * `NegativeBinomial.arg_constraints`
        * `NegativeBinomial.expand()`
        * `NegativeBinomial.log_prob()`
        * `NegativeBinomial.logits`
        * `NegativeBinomial.mean`
        * `NegativeBinomial.mode`
        * `NegativeBinomial.param_shape`
        * `NegativeBinomial.probs`
        * `NegativeBinomial.sample()`
        * `NegativeBinomial.support`
        * `NegativeBinomial.variance`
    * Normal
      * `Normal`
        * `Normal.arg_constraints`
        * `Normal.cdf()`
        * `Normal.entropy()`
        * `Normal.expand()`
        * `Normal.has_rsample`
        * `Normal.icdf()`
        * `Normal.log_prob()`
        * `Normal.mean`
        * `Normal.mode`
        * `Normal.rsample()`
        * `Normal.sample()`
        * `Normal.stddev`
        * `Normal.support`
        * `Normal.variance`
    * OneHotCategorical
      * `OneHotCategorical`
        * `OneHotCategorical.arg_constraints`
        * `OneHotCategorical.entropy()`
        * `OneHotCategorical.enumerate_support()`
        * `OneHotCategorical.expand()`
        * `OneHotCategorical.has_enumerate_support`
        * `OneHotCategorical.log_prob()`
        * `OneHotCategorical.logits`
        * `OneHotCategorical.mean`
        * `OneHotCategorical.mode`
        * `OneHotCategorical.param_shape`
        * `OneHotCategorical.probs`
        * `OneHotCategorical.sample()`
        * `OneHotCategorical.support`
        * `OneHotCategorical.variance`
    * Pareto
      * `Pareto`
        * `Pareto.arg_constraints`
        * `Pareto.entropy()`
        * `Pareto.expand()`
        * `Pareto.mean`
        * `Pareto.mode`
        * `Pareto.support`
        * `Pareto.variance`
    * Poisson
      * `Poisson`
        * `Poisson.arg_constraints`
        * `Poisson.expand()`
        * `Poisson.log_prob()`
        * `Poisson.mean`
        * `Poisson.mode`
        * `Poisson.sample()`
        * `Poisson.support`
        * `Poisson.variance`
    * RelaxedBernoulli
      * `RelaxedBernoulli`
        * `RelaxedBernoulli.arg_constraints`
        * `RelaxedBernoulli.expand()`
        * `RelaxedBernoulli.has_rsample`
        * `RelaxedBernoulli.logits`
        * `RelaxedBernoulli.probs`
        * `RelaxedBernoulli.support`
        * `RelaxedBernoulli.temperature`
    * LogitRelaxedBernoulli
      * `LogitRelaxedBernoulli`
        * `LogitRelaxedBernoulli.arg_constraints`
        * `LogitRelaxedBernoulli.expand()`
        * `LogitRelaxedBernoulli.log_prob()`
        * `LogitRelaxedBernoulli.logits`
        * `LogitRelaxedBernoulli.param_shape`
        * `LogitRelaxedBernoulli.probs`
        * `LogitRelaxedBernoulli.rsample()`
        * `LogitRelaxedBernoulli.support`
    * RelaxedOneHotCategorical
      * `RelaxedOneHotCategorical`
        * `RelaxedOneHotCategorical.arg_constraints`
        * `RelaxedOneHotCategorical.expand()`
        * `RelaxedOneHotCategorical.has_rsample`
        * `RelaxedOneHotCategorical.logits`
        * `RelaxedOneHotCategorical.probs`
        * `RelaxedOneHotCategorical.support`
        * `RelaxedOneHotCategorical.temperature`
    * StudentT
      * `StudentT`
        * `StudentT.arg_constraints`
        * `StudentT.entropy()`
        * `StudentT.expand()`
        * `StudentT.has_rsample`
        * `StudentT.log_prob()`
        * `StudentT.mean`
        * `StudentT.mode`
        * `StudentT.rsample()`
        * `StudentT.support`
        * `StudentT.variance`
    * TransformedDistribution
      * `TransformedDistribution`
        * `TransformedDistribution.arg_constraints`
        * `TransformedDistribution.cdf()`
        * `TransformedDistribution.expand()`
        * `TransformedDistribution.has_rsample`
        * `TransformedDistribution.icdf()`
        * `TransformedDistribution.log_prob()`
        * `TransformedDistribution.rsample()`
        * `TransformedDistribution.sample()`
        * `TransformedDistribution.support`
    * Uniform
      * `Uniform`
        * `Uniform.arg_constraints`
        * `Uniform.cdf()`
        * `Uniform.entropy()`
        * `Uniform.expand()`
        * `Uniform.has_rsample`
        * `Uniform.icdf()`
        * `Uniform.log_prob()`
        * `Uniform.mean`
        * `Uniform.mode`
        * `Uniform.rsample()`
        * `Uniform.stddev`
        * `Uniform.support`
        * `Uniform.variance`
    * VonMises
      * `VonMises`
        * `VonMises.arg_constraints`
        * `VonMises.expand()`
        * `VonMises.has_rsample`
        * `VonMises.log_prob()`
        * `VonMises.mean`
        * `VonMises.mode`
        * `VonMises.sample()`
        * `VonMises.support`
        * `VonMises.variance`
    * Weibull
      * `Weibull`
        * `Weibull.arg_constraints`
        * `Weibull.entropy()`
        * `Weibull.expand()`
        * `Weibull.mean`
        * `Weibull.mode`
        * `Weibull.support`
        * `Weibull.variance`
    * Wishart
      * `Wishart`
        * `Wishart.arg_constraints`
        * `Wishart.covariance_matrix`
        * `Wishart.entropy()`
        * `Wishart.expand()`
        * `Wishart.has_rsample`
        * `Wishart.log_prob()`
        * `Wishart.mean`
        * `Wishart.mode`
        * `Wishart.precision_matrix`
        * `Wishart.rsample()`
        * `Wishart.scale_tril`
        * `Wishart.support`
        * `Wishart.variance`
    * KL Divergence
      * `kl_divergence()`
      * `register_kl()`
    * Transforms
      * `AbsTransform`
      * `AffineTransform`
      * `CatTransform`
      * `ComposeTransform`
      * `CorrCholeskyTransform`
      * `CumulativeDistributionTransform`
      * `ExpTransform`
      * `IndependentTransform`
      * `LowerCholeskyTransform`
      * `PositiveDefiniteTransform`
      * `PowerTransform`
      * `ReshapeTransform`
      * `SigmoidTransform`
      * `SoftplusTransform`
      * `TanhTransform`
      * `SoftmaxTransform`
      * `StackTransform`
      * `StickBreakingTransform`
      * `Transform`
        * `Transform.inv`
        * `Transform.sign`
        * `Transform.log_abs_det_jacobian()`
        * `Transform.forward_shape()`
        * `Transform.inverse_shape()`
    * Constraints
      * `Constraint`
        * `Constraint.check()`
      * `cat`
      * `dependent_property`
      * `greater_than`
      * `greater_than_eq`
      * `independent`
      * `integer_interval`
      * `interval`
      * `half_open_interval`
      * `is_dependent()`
      * `less_than`
      * `multinomial`
      * `stack`
    * Constraint Registry
      * `ConstraintRegistry`
        * `ConstraintRegistry.register()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.__future__
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.__future__ 

torch.__future__.set_overwrite_module_params_on_conversion(_value_)[source][source]
    
Sets whether to assign new tensors to the parameters instead of changing the existing parameters in-place when converting an `nn.Module`.
When enabled, the following methods will assign new parameters to the module:
  1. `module.{device}()` (e.g. `nn.Module.cuda()`) for moving a module between devices
  2. `module.{dtype}()` (e.g. `nn.Module.float()`) for converting a module to a different dtype
  3. `nn.Module.to()`
  4. `nn.Module.to_empty()`



Parameters
    
**value** (_bool_) – Whether to assign new tensors or not. 

torch.__future__.get_overwrite_module_params_on_conversion()[source][source]
    
Returns whether to assign new tensors to the parameters instead of changing the existing parameters in-place when converting an `torch.nn.Module`. Defaults to `False`.
See `set_overwrite_module_params_on_conversion()` for more information. 

Return type
    
bool 

torch.__future__.set_swap_module_params_on_conversion(_value_)[source][source]
    
Sets whether to use `swap_tensors()` instead of setting `.data` to change the existing parameters in-place when converting an `nn.Module` and instead of `param.copy_(state_dict[key])` when loading a state dict into an `nn.Module`.
Note
This function takes precedence over `get_overwrite_module_params_on_conversion()`
When enabled, the following methods will swap the existing parameters in-place:
  1. `module.{device}()` (e.g. `nn.Module.cuda()`) for moving a module between devices
  2. `module.{dtype}()` (e.g. `nn.Module.float()`) for converting a module to a different dtype
  3. `nn.Module.to()`
  4. `nn.Module.to_empty()`
  5. `nn.Module.load_state_dict()`


The semantics for `load_state_dict()` when this is set are as follows:
  1. For each parameter/buffer, its corresponding `state_dict['key']` is transformed via `module_load()` (i.e. `res = param.module_load(state_dict['key'])`)
  2. If necessary, `res` will be wrapped in an `Parameter`
  3. The parameter/buffer in the module will be swapped via `swap_tensors()` with `res`



Parameters
    
**value** (_bool_) – Whether to use `swap_tensors()` or not. 

torch.__future__.get_swap_module_params_on_conversion()[source][source]
    
Returns whether to use `swap_tensors()` instead of setting .data to change the existing parameters in-place when converting an `nn.Module`. Defaults to `False`.
See `set_swap_module_params_on_conversion()` for more information. 

Return type
    
bool
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.__future__
    * `set_overwrite_module_params_on_conversion()`
    * `get_overwrite_module_params_on_conversion()`
    * `set_swap_module_params_on_conversion()`
    * `get_swap_module_params_on_conversion()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.library
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.library
torch.library is a collection of APIs for extending PyTorch’s core library of operators. It contains utilities for testing custom operators, creating new custom operators, and extending operators defined with PyTorch’s C++ operator registration APIs (e.g. aten operators).
For a detailed guide on effectively using these APIs, please see PyTorch Custom Operators Landing Page for more details on how to effectively use these APIs.
## Testing custom ops
Use `torch.library.opcheck()` to test custom ops for incorrect usage of the Python torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports training, use `torch.autograd.gradcheck()` to test that the gradients are mathematically correct. 

torch.library.opcheck(_op_ , _args_ , _kwargs =None_, _*_ , _test_utils =('test_schema', 'test_autograd_registration', 'test_faketensor', 'test_aot_dispatch_dynamic')_, _raise_exception =True_, _atol =None_, _rtol =None_)[source][source]
    
Given an operator and some sample arguments, tests if the operator is registered correctly.
That is, when you use the torch.library/TORCH_LIBRARY APIs to create a custom op, you specified metadata (e.g. mutability info) about the custom op and these APIs require that the functions you pass them satisfy certain properties (e.g. no data pointer access in the fake/meta/abstract kernel) `opcheck` tests these metadata and properties.
Concretely, we test the following:
  * test_schema: If the schema matches the implementation of the operator. For example: if the schema specifies a Tensor is mutated, then we check the implementation mutates the Tensor. If the schema specifies that we return a new Tensor, then we check that the implementation returns a new Tensor (instead of an existing one or a view of an existing one).
  * test_autograd_registration: If the operator supports training (autograd): we check that its autograd formula is registered via torch.library.register_autograd or a manual registration to one or more DispatchKey::Autograd keys. Any other DispatchKey-based registrations may lead to undefined behavior.
  * test_faketensor: If the operator has a FakeTensor kernel (and if it is correct). The FakeTensor kernel is necessary ( but not sufficient) for the operator to work with PyTorch compilation APIs (torch.compile/export/FX). We check that a FakeTensor kernel (also sometimes known as a meta kernel) was registered for the operator and that it is correct. This test takes the result of running the operator on real tensors and the result of running the operator on FakeTensors and checks that they have the same Tensor metadata (sizes/strides/dtype/device/etc).
  * test_aot_dispatch_dynamic: If the operator has correct behavior with PyTorch compilation APIs (torch.compile/export/FX). This checks that the outputs (and gradients, if applicable) are the same under eager-mode PyTorch and torch.compile. This test is a superset of `test_faketensor` and is an e2e test; other things it tests are that the operator supports functionalization and that the backward pass (if it exists) also supports FakeTensor and functionalization.


For best results, please call `opcheck` multiple times with a representative set of inputs. If your operator supports autograd, please use `opcheck` with inputs with `requires_grad = True`; if your operator supports multiple devices (e.g. CPU and CUDA), please use `opcheck` with inputs on all supported devices. 

Parameters
    
  * **op** (_Union_ _[__OpOverload_ _,__OpOverloadPacket_ _,__CustomOpDef_ _]_) – The operator. Must either be a function decorated with `torch.library.custom_op()` or an OpOverload/OpOverloadPacket found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)
  * **args** (_tuple_ _[__Any_ _,__...__]_) – The args to the operator
  * **kwargs** (_Optional_ _[__dict_ _[__str_ _,__Any_ _]__]_) – The kwargs to the operator
  * **test_utils** (_Union_ _[__str_ _,__Sequence_ _[__str_ _]__]_) – Tests that we should run. Default: all of them. Example: (“test_schema”, “test_faketensor”)
  * **raise_exception** (_bool_) – If we should raise an exception on the first error. If False, we will return a dict with information on if each test passed or not.
  * **rtol** (_Optional_ _[__float_ _]_) – Relative tolerance for floating point comparisons. If specified `atol` must also be specified. If omitted, default values based on the `dtype` are selected (see the table in `torch.testing.assert_close()`).
  * **atol** (_Optional_ _[__float_ _]_) – Absolute tolerance for floating point comparisons. If specified `rtol` must also be specified. If omitted, default values based on the `dtype` are selected (see the table in `torch.testing.assert_close()`).



Return type
    
dict[str, str]
Warning
opcheck and `torch.autograd.gradcheck()` test different things; opcheck tests if your usage of torch.library APIs is correct while `torch.autograd.gradcheck()` tests if your autograd formula is mathematically correct. Use both to test custom ops that support gradient computation.
Example
```
>>> @torch.library.custom_op("mylib::numpy_mul", mutates_args=())
>>> def numpy_mul(x: Tensor, y: float) -> Tensor:
>>>   x_np = x.numpy(force=True)
>>>   z_np = x_np * y
>>>   return torch.from_numpy(z_np).to(x.device)
>>>
>>> @numpy_mul.register_fake
>>> def _(x, y):
>>>   return torch.empty_like(x)
>>>
>>> def setup_context(ctx, inputs, output):
>>>   y, = inputs
>>>   ctx.y = y
>>>
>>> def backward(ctx, grad):
>>>   return grad * ctx.y, None
>>>
>>> numpy_mul.register_autograd(backward, setup_context=setup_context)
>>>
>>> sample_inputs = [
>>>   (torch.randn(3), 3.14),
>>>   (torch.randn(2, 3, device='cuda'), 2.718),
>>>   (torch.randn(1, 10, requires_grad=True), 1.234),
>>>   (torch.randn(64, 64, device='cuda', requires_grad=True), 90.18),
>>> ]
>>>
>>> for args in sample_inputs:
>>>   torch.library.opcheck(numpy_mul, args)

```
Copy to clipboard
## Creating new custom ops in Python
Use `torch.library.custom_op()` to create new custom ops. 

torch.library.custom_op(_name_ , _fn =None_, _/_ , _*_ , _mutates_args_ , _device_types =None_, _schema =None_)[source]
    
Wraps a function into custom operator.
Reasons why you may want to create a custom op include: - Wrapping a third-party library or custom kernel to work with PyTorch subsystems like Autograd. - Preventing torch.compile/export/FX tracing from peeking inside your function.
This API is used as a decorator around a function (please see examples). The provided function must have type hints; these are needed to interface with PyTorch’s various subsystems. 

Parameters
    
  * **name** (_str_) – A name for the custom op that looks like “{namespace}::{name}”, e.g. “mylib::my_linear”. The name is used as the op’s stable identifier in PyTorch subsystems (e.g. torch.export, FX graphs). To avoid name collisions, please use your project name as the namespace; e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
  * **mutates_args** (_Iterable_ _[__str_ _] or_ _"unknown"_) – The names of args that the function mutates. This MUST be accurate, otherwise, the behavior is undefined. If “unknown”, it pessimistically assumes that all inputs to the operator are being mutated.
  * **device_types** (_None_ _|__str_ _|__Sequence_ _[__str_ _]_) – The device type(s) the function is valid for. If no device type is provided, then the function is used as the default implementation for all device types. Examples: “cpu”, “cuda”. When registering a device-specific implementation for an operator that accepts no Tensors, we require the operator to have a “device: torch.device argument”.
  * **schema** (_None_ _|__str_) – A schema string for the operator. If None (recommended) we’ll infer a schema for the operator from its type annotations. We recommend letting us infer a schema unless you have a specific reason not to. Example: “(Tensor x, int y) -> (Tensor, Tensor)”.



Return type
    
_Union_[_Callable_[[_Callable_[[…], object]], _CustomOpDef_], _CustomOpDef_]
Note
We recommend not passing in a `schema` arg and instead letting us infer it from the type annotations. It is error-prone to write your own schema. You may wish to provide your own schema if our interpretation of the type annotation is not what you want. For more info on how to write a schema string, see here 

Examples::
    
```
>>> import torch
>>> from torch import Tensor
>>> from torch.library import custom_op
>>> import numpy as np
>>>
>>> @custom_op("mylib::numpy_sin", mutates_args=())
>>> def numpy_sin(x: Tensor) -> Tensor:
>>>   x_np = x.cpu().numpy()
>>>   y_np = np.sin(x_np)
>>>   return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> x = torch.randn(3)
>>> y = numpy_sin(x)
>>> assert torch.allclose(y, x.sin())
>>>
>>> # Example of a custom op that only works for one device type.
>>> @custom_op("mylib::numpy_sin_cpu", mutates_args=(), device_types="cpu")
>>> def numpy_sin_cpu(x: Tensor) -> Tensor:
>>>   x_np = x.numpy()
>>>   y_np = np.sin(x_np)
>>>   return torch.from_numpy(y_np)
>>>
>>> x = torch.randn(3)
>>> y = numpy_sin_cpu(x)
>>> assert torch.allclose(y, x.sin())
>>>
>>> # Example of a custom op that mutates an input
>>> @custom_op("mylib::numpy_sin_inplace", mutates_args={"x"}, device_types="cpu")
>>> def numpy_sin_inplace(x: Tensor) -> None:
>>>   x_np = x.numpy()
>>>   np.sin(x_np, out=x_np)
>>>
>>> x = torch.randn(3)
>>> expected = x.sin()
>>> numpy_sin_inplace(x)
>>> assert torch.allclose(x, expected)
>>>
>>> # Example of a factory function
>>> @torch.library.custom_op("mylib::bar", mutates_args={}, device_types="cpu")
>>> def bar(device: torch.device) -> Tensor:
>>>   return torch.ones(3)
>>>
>>> bar("cpu")

```
Copy to clipboard 

torch.library.triton_op(_name_ , _fn =None_, _/_ , _*_ , _mutates_args_ , _schema =None_)[source]
    
Create a custom operator whose implementation is backed by 1+ triton kernels.
This is a more structured way of using triton kernels with PyTorch. Prefer using triton kernels with no `torch.library` custom operator wrappers (like `torch.library.custom_op()`, `torch.library.triton_op()`) because that is simpler; only use `torch.library.custom_op()`/`torch.library.triton_op()` if you want to create an operator that behaves like PyTorch built-in operators. For example, you may use a `torch.library` wrapper API to define the behavior of the triton kernel when passed a tensor subclass or under a TorchDispatchMode.
Use `torch.library.triton_op()` instead of `torch.library.custom_op()` when the implementation consists of 1+ triton kernels. `torch.library.custom_op()` treats custom operators as opaque (`torch.compile()` and `torch.export.export()` will never trace into them), but `triton_op` makes the implementation visible to these subsystems, allowing them to optimize the triton kernel(s).
Note that `fn` must only consist of calls to PyTorch-understood operators and triton kernels. Any triton kernels called inside `fn` must be wrapped in a call to `torch.library.wrap_triton()`. 

Parameters
    
  * **name** (_str_) – A name for the custom op that looks like “{namespace}::{name}”, e.g. “mylib::my_linear”. The name is used as the op’s stable identifier in PyTorch subsystems (e.g. torch.export, FX graphs). To avoid name collisions, please use your project name as the namespace; e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
  * **mutates_args** (_Iterable_ _[__str_ _] or_ _"unknown"_) – The names of args that the function mutates. This MUST be accurate, otherwise, the behavior is undefined. If “unknown”, it pessimistically assumes that all inputs to the operator are being mutated.
  * **schema** (_None_ _|__str_) – A schema string for the operator. If None (recommended) we’ll infer a schema for the operator from its type annotations. We recommend letting us infer a schema unless you have a specific reason not to. Example: “(Tensor x, int y) -> (Tensor, Tensor)”.



Return type
    
_Callable_
Example:
```
>>> import torch
>>> from torch.library import triton_op, wrap_triton
>>>
>>> import triton
>>> from triton import language as tl
>>>
>>> @triton.jit
>>> def add_kernel(
>>>   in_ptr0,
>>>   in_ptr1,
>>>   out_ptr,
>>>   n_elements,
>>>   BLOCK_SIZE: "tl.constexpr",
>>> ):
>>>   pid = tl.program_id(axis=0)
>>>   block_start = pid * BLOCK_SIZE
>>>   offsets = block_start + tl.arange(0, BLOCK_SIZE)
>>>   mask = offsets < n_elements
>>>   x = tl.load(in_ptr0 + offsets, mask=mask)
>>>   y = tl.load(in_ptr1 + offsets, mask=mask)
>>>   output = x + y
>>>   tl.store(out_ptr + offsets, output, mask=mask)
>>>
>>> @triton_op("mylib::add", mutates_args={})
>>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
>>>   output = torch.empty_like(x)
>>>   n_elements = output.numel()
>>>
>>>   def grid(meta):
>>>     return (triton.cdiv(n_elements, meta["BLOCK_SIZE"]),)
>>>
>>>   # NB: we need to wrap the triton kernel in a call to wrap_triton
>>>   wrap_triton(add_kernel)[grid](x, y, output, n_elements, 16)
>>>   return output
>>>
>>> @torch.compile
>>> def f(x, y):
>>>   return add(x, y)
>>>
>>> x = torch.randn(3, device="cuda")
>>> y = torch.randn(3, device="cuda")
>>>
>>> z = f(x, y)
>>> assert torch.allclose(z, x + y)

```
Copy to clipboard 

torch.library.wrap_triton(_triton_kernel_ , _/_)[source]
    
Allows capture of a triton kernel into a graph via make_fx or non-strict `torch.export`.
These technologies perform Dispatcher-based tracing (via `__torch_dispatch__`) and cannot see calls to raw triton kernels. The `wrap_triton` API wraps a triton kernel into a callable that can actually be traced into a graph.
Please use this API together with `torch.library.triton_op()`.
Examples
```
>>> import torch
>>> import triton
>>> from triton import language as tl
>>> from torch.fx.experimental.proxy_tensor import make_fx
>>> from torch.library import wrap_triton
>>>
>>> @triton.jit
>>> def add_kernel(
>>>   in_ptr0,
>>>   in_ptr1,
>>>   out_ptr,
>>>   n_elements,
>>>   BLOCK_SIZE: "tl.constexpr",
>>> ):
>>>   pid = tl.program_id(axis=0)
>>>   block_start = pid * BLOCK_SIZE
>>>   offsets = block_start + tl.arange(0, BLOCK_SIZE)
>>>   mask = offsets < n_elements
>>>   x = tl.load(in_ptr0 + offsets, mask=mask)
>>>   y = tl.load(in_ptr1 + offsets, mask=mask)
>>>   output = x + y
>>>   tl.store(out_ptr + offsets, output, mask=mask)
>>>
>>> def add(x, y):
>>>   output = torch.empty_like(x)
>>>   n_elements = output.numel()
>>>
>>>   def grid_fn(meta):
>>>     return (triton.cdiv(n_elements, meta["BLOCK_SIZE"]),)
>>>
>>>   wrap_triton(add_kernel)[grid_fn](x, y, output, n_elements, 16)
>>>   return output
>>>
>>> x = torch.randn(3, device="cuda")
>>> y = torch.randn(3, device="cuda")
>>> gm = make_fx(add)(x, y)
>>> print(gm.code)
>>> # def forward(self, x_1, y_1):
>>> #   empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)
>>> #   triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(
>>> #     kernel_idx = 0, constant_args_idx = 0,
>>> #     grid = [(1, 1, 1)], kwargs = {
>>> #       'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,
>>> #       'n_elements': 3, 'BLOCK_SIZE': 16
>>> #     })
>>> #   return empty_like

```
Copy to clipboard 

Return type
    
_Any_
## Extending custom ops (created from Python or C++)
Use the register.* methods, such as `torch.library.register_kernel()` and `torch.library.register_fake()`, to add implementations for any operators (they may have been created using `torch.library.custom_op()` or via PyTorch’s C++ operator registration APIs). 

torch.library.register_kernel(_op_ , _device_types_ , _func =None_, _/_ , _*_ , _lib =None_)[source][source]
    
Register an implementation for a device type for this operator.
Some valid device_types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”. This API may be used as a decorator. 

Parameters
    
  * **op** (_str_ _|__OpOverload_) – The operator to register an impl to.
  * **device_types** (_None_ _|__str_ _|__Sequence_ _[__str_ _]_) – The device_types to register an impl to. If None, we will register to all device types – please only use this option if your implementation is truly device-type-agnostic.
  * **func** (_Callable_) – The function to register as the implementation for the given device types.
  * **lib** (_Optional_ _[__Library_ _]_) – If provided, the lifetime of this registration



Examples::
    
```
>>> import torch
>>> from torch import Tensor
>>> from torch.library import custom_op
>>> import numpy as np
>>>
>>> # Create a custom op that works on cpu
>>> @custom_op("mylib::numpy_sin", mutates_args=(), device_types="cpu")
>>> def numpy_sin(x: Tensor) -> Tensor:
>>>   x_np = x.numpy()
>>>   y_np = np.sin(x_np)
>>>   return torch.from_numpy(y_np)
>>>
>>> # Add implementations for the cuda device
>>> @torch.library.register_kernel("mylib::numpy_sin", "cuda")
>>> def _(x):
>>>   x_np = x.cpu().numpy()
>>>   y_np = np.sin(x_np)
>>>   return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> x_cpu = torch.randn(3)
>>> x_cuda = x_cpu.cuda()
>>> assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())
>>> assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())

```
Copy to clipboard 

torch.library.register_autocast(_op_ , _device_type_ , _cast_inputs_ , _/_ , _*_ , _lib =None_)[source][source]
    
Register an autocast dispatch rule for this custom op.
Valid device_type include: “cpu” and “cuda”. 

Parameters
    
  * **op** (_str_ _|__OpOverload_) – The operator to register an autocast dispatch rule to.
  * **device_type** (_str_) – Device type to use. ‘cuda’ or ‘cpu’. The type is the same as the type attribute of a `torch.device`. Thus, you may obtain the device type of a tensor using Tensor.device.type.
  * **cast_inputs** (`torch.dtype`) – When custom op runs in an autocast-enabled region, casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors are not affected), then executes custom op with autocast disabled.
  * **lib** (_Optional_ _[__Library_ _]_) – If provided, the lifetime of this registration



Examples::
    
```
>>> import torch
>>> from torch import Tensor
>>> from torch.library import custom_op
>>>
>>> # Create a custom op that works on cuda
>>> @torch.library.custom_op("mylib::my_sin", mutates_args=())
>>> def my_sin(x: Tensor) -> Tensor:
>>>   return torch.sin(x)
>>>
>>> # Register autocast dispatch rule for the cuda device
>>> torch.library.register_autocast("mylib::my_sin", "cuda", torch.float16)
>>>
>>> x = torch.randn(3, dtype=torch.float32, device="cuda")
>>> with torch.autocast("cuda", dtype=torch.float16):
>>>   y = torch.ops.mylib.my_sin(x)
>>> assert y.dtype == torch.float16

```
Copy to clipboard 

torch.library.register_autograd(_op_ , _backward_ , _/_ , _*_ , _setup_context =None_, _lib =None_)[source][source]
    
Register a backward formula for this custom op.
In order for an operator to work with autograd, you need to register a backward formula: 1. You must tell us how to compute gradients during the backward pass by providing us a “backward” function. 2. If you need any values from the forward to compute gradients, you can use setup_context to save values for backward.
`backward` runs during the backward pass. It accepts `(ctx, *grads)`: - `grads` is one or more gradients. The number of gradients matches the number of outputs of the operator. The `ctx` object is the same ctx object used by `torch.autograd.Function`. The semantics of `backward_fn` are the same as `torch.autograd.Function.backward()`.
`setup_context(ctx, inputs, output)` runs during the forward pass. Please save quantities needed for backward onto the `ctx` object via either `torch.autograd.function.FunctionCtx.save_for_backward()` or assigning them as attributes of `ctx`. If your custom op has kwarg-only arguments, we expect the signature of `setup_context` to be `setup_context(ctx, inputs, keyword_only_inputs, output)`.
Both `setup_context_fn` and `backward_fn` must be traceable. That is, they may not directly access `torch.Tensor.data_ptr()` and they must not depend on or mutate global state. If you need a non-traceable backward, you can make it a separate custom_op that you call inside `backward_fn`.
If you need different autograd behavior on different devices, then we recommend creating two different custom operators, one for each device that needs different behavior, and switching between them at runtime.
Examples
```
>>> import torch
>>> import numpy as np
>>> from torch import Tensor
>>>
>>> @torch.library.custom_op("mylib::numpy_sin", mutates_args=())
>>> def numpy_sin(x: Tensor) -> Tensor:
>>>   x_np = x.cpu().numpy()
>>>   y_np = np.sin(x_np)
>>>   return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> def setup_context(ctx, inputs, output) -> Tensor:
>>>   x, = inputs
>>>   ctx.save_for_backward(x)
>>>
>>> def backward(ctx, grad):
>>>   x, = ctx.saved_tensors
>>>   return grad * x.cos()
>>>
>>> torch.library.register_autograd(
...   "mylib::numpy_sin", backward, setup_context=setup_context
... )
>>>
>>> x = torch.randn(3, requires_grad=True)
>>> y = numpy_sin(x)
>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))
>>> assert torch.allclose(grad_x, x.cos())
>>>
>>> # Example with a keyword-only arg
>>> @torch.library.custom_op("mylib::numpy_mul", mutates_args=())
>>> def numpy_mul(x: Tensor, *, val: float) -> Tensor:
>>>   x_np = x.cpu().numpy()
>>>   y_np = x_np * val
>>>   return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> def setup_context(ctx, inputs, keyword_only_inputs, output) -> Tensor:
>>>   ctx.val = keyword_only_inputs["val"]
>>>
>>> def backward(ctx, grad):
>>>   return grad * ctx.val
>>>
>>> torch.library.register_autograd(
...   "mylib::numpy_mul", backward, setup_context=setup_context
... )
>>>
>>> x = torch.randn(3, requires_grad=True)
>>> y = numpy_mul(x, val=3.14)
>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))
>>> assert torch.allclose(grad_x, torch.full_like(x, 3.14))

```
Copy to clipboard 

torch.library.register_fake(_op_ , _func =None_, _/_ , _*_ , _lib =None_, __stacklevel =1_)[source][source]
    
Register a FakeTensor implementation (“fake impl”) for this operator.
Also sometimes known as a “meta kernel”, “abstract impl”.
An “FakeTensor implementation” specifies the behavior of this operator on Tensors that carry no data (“FakeTensor”). Given some input Tensors with certain properties (sizes/strides/storage_offset/device), it specifies what the properties of the output Tensors are.
The FakeTensor implementation has the same signature as the operator. It is run for both FakeTensors and meta tensors. To write a FakeTensor implementation, assume that all Tensor inputs to the operator are regular CPU/CUDA/Meta tensors, but they do not have storage, and you are trying to return regular CPU/CUDA/Meta tensor(s) as output. The FakeTensor implementation must consist of only PyTorch operations (and may not directly access the storage or data of any input or intermediate Tensors).
This API may be used as a decorator (see examples).
For a detailed guide on custom ops, please see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html
Examples
```
>>> import torch
>>> import numpy as np
>>> from torch import Tensor
>>>
>>> # Example 1: an operator without data-dependent output shape
>>> @torch.library.custom_op("mylib::custom_linear", mutates_args=())
>>> def custom_linear(x: Tensor, weight: Tensor, bias: Tensor) -> Tensor:
>>>   raise NotImplementedError("Implementation goes here")
>>>
>>> @torch.library.register_fake("mylib::custom_linear")
>>> def _(x, weight, bias):
>>>   assert x.dim() == 2
>>>   assert weight.dim() == 2
>>>   assert bias.dim() == 1
>>>   assert x.shape[1] == weight.shape[1]
>>>   assert weight.shape[0] == bias.shape[0]
>>>   assert x.device == weight.device
>>>
>>>   return (x @ weight.t()) + bias
>>>
>>> with torch._subclasses.fake_tensor.FakeTensorMode():
>>>   x = torch.randn(2, 3)
>>>   w = torch.randn(3, 3)
>>>   b = torch.randn(3)
>>>   y = torch.ops.mylib.custom_linear(x, w, b)
>>>
>>> assert y.shape == (2, 3)
>>>
>>> # Example 2: an operator with data-dependent output shape
>>> @torch.library.custom_op("mylib::custom_nonzero", mutates_args=())
>>> def custom_nonzero(x: Tensor) -> Tensor:
>>>   x_np = x.numpy(force=True)
>>>   res = np.stack(np.nonzero(x_np), axis=1)
>>>   return torch.tensor(res, device=x.device)
>>>
>>> @torch.library.register_fake("mylib::custom_nonzero")
>>> def _(x):
>>> # Number of nonzero-elements is data-dependent.
>>> # Since we cannot peek at the data in an fake impl,
>>> # we use the ctx object to construct a new symint that
>>> # represents the data-dependent size.
>>>   ctx = torch.library.get_ctx()
>>>   nnz = ctx.new_dynamic_size()
>>>   shape = [nnz, x.dim()]
>>>   result = x.new_empty(shape, dtype=torch.int64)
>>>   return result
>>>
>>> from torch.fx.experimental.proxy_tensor import make_fx
>>>
>>> x = torch.tensor([0, 1, 2, 3, 4, 0])
>>> trace = make_fx(torch.ops.mylib.custom_nonzero, tracing_mode="symbolic")(x)
>>> trace.print_readable()
>>>
>>> assert torch.allclose(trace(x), torch.ops.mylib.custom_nonzero(x))

```
Copy to clipboard 

torch.library.register_vmap(_op_ , _func =None_, _/_ , _*_ , _lib =None_)[source][source]
    
Register a vmap implementation to support `torch.vmap()` for this custom op.
This API may be used as a decorator (see examples).
In order for an operator to work with `torch.vmap()`, you may need to register a vmap implementation in the following signature:
> `vmap_func(info, in_dims: Tuple[Optional[int]], *args, **kwargs)`,
where `*args` and `**kwargs` are the arguments and kwargs for `op`. We do not support kwarg-only Tensor args.
It specifies how do we compute the batched version of `op` given inputs with an additional dimension (specified by `in_dims`).
For each arg in `args`, `in_dims` has a corresponding `Optional[int]`. It is `None` if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.
`info` is a collection of additional metadata that may be helpful: `info.batch_size` specifies the size of the dimension being vmapped over, while `info.randomness` is the `randomness` option that was passed to `torch.vmap()`.
The return of the function `func` is a tuple of `(output, out_dims)`. Similar to `in_dims`, `out_dims` should be of the same structure as `output` and contain one `out_dim` per output that specifies if the output has the vmapped dimension and what index it is in.
Examples
```
>>> import torch
>>> import numpy as np
>>> from torch import Tensor
>>> from typing import Tuple
>>>
>>> def to_numpy(tensor):
>>>   return tensor.cpu().numpy()
>>>
>>> lib = torch.library.Library("mylib", "FRAGMENT")
>>> @torch.library.custom_op("mylib::numpy_cube", mutates_args=())
>>> def numpy_cube(x: Tensor) -> Tuple[Tensor, Tensor]:
>>>   x_np = to_numpy(x)
>>>   dx = torch.tensor(3 * x_np ** 2, device=x.device)
>>>   return torch.tensor(x_np ** 3, device=x.device), dx
>>>
>>> def numpy_cube_vmap(info, in_dims, x):
>>>   result = numpy_cube(x)
>>>   return result, (in_dims[0], in_dims[0])
>>>
>>> torch.library.register_vmap(numpy_cube, numpy_cube_vmap)
>>>
>>> x = torch.randn(3)
>>> torch.vmap(numpy_cube)(x)
>>>
>>> @torch.library.custom_op("mylib::numpy_mul", mutates_args=())
>>> def numpy_mul(x: Tensor, y: Tensor) -> Tensor:
>>>   return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)
>>>
>>> @torch.library.register_vmap("mylib::numpy_mul")
>>> def numpy_mul_vmap(info, in_dims, x, y):
>>>   x_bdim, y_bdim = in_dims
>>>   x = x.movedim(x_bdim, -1) if x_bdim is not None else x.unsqueeze(-1)
>>>   y = y.movedim(y_bdim, -1) if y_bdim is not None else y.unsqueeze(-1)
>>>   result = x * y
>>>   result = result.movedim(-1, 0)
>>>   return result, 0
>>>
>>>
>>> x = torch.randn(3)
>>> y = torch.randn(3)
>>> torch.vmap(numpy_mul)(x, y)

```
Copy to clipboard
Note
The vmap function should aim to preserve the semantics of the entire custom operator. That is, `grad(vmap(op))` should be replaceable with a `grad(map(op))`.
If your custom operator has any custom behavior in the backward pass, please keep this in mind. 

torch.library.impl_abstract(_qualname_ , _func =None_, _*_ , _lib =None_, __stacklevel =1_)[source][source]
    
This API was renamed to `torch.library.register_fake()` in PyTorch 2.4. Please use that instead. 

torch.library.get_ctx()[source][source]
    
get_ctx() returns the current AbstractImplCtx object.
Calling `get_ctx()` is only valid inside of an fake impl (see `torch.library.register_fake()` for more usage details. 

Return type
    
_FakeImplCtx_ 

torch.library.register_torch_dispatch(_op_ , _torch_dispatch_class_ , _func =None_, _/_ , _*_ , _lib =None_)[source][source]
    
Registers a torch_dispatch rule for the given operator and `torch_dispatch_class`.
This allows for open registration to specify the behavior between the operator and the `torch_dispatch_class` without needing to modify the `torch_dispatch_class` or the operator directly.
The `torch_dispatch_class` is either a Tensor subclass with `__torch_dispatch__` or a TorchDispatchMode.
If it is a Tensor subclass, we expect `func` to have the following signature: `(cls, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any`
If it is a TorchDispatchMode, we expect `func` to have the following signature: `(mode, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any`
`args` and `kwargs` will have been normalized the same way they are in `__torch_dispatch__` (see __torch_dispatch__ calling convention).
Examples
```
>>> import torch
>>>
>>> @torch.library.custom_op("mylib::foo", mutates_args={})
>>> def foo(x: torch.Tensor) -> torch.Tensor:
>>>   return x.clone()
>>>
>>> class MyMode(torch.utils._python_dispatch.TorchDispatchMode):
>>>   def __torch_dispatch__(self, func, types, args=(), kwargs=None):
>>>     return func(*args, **kwargs)
>>>
>>> @torch.library.register_torch_dispatch("mylib::foo", MyMode)
>>> def _(mode, func, types, args, kwargs):
>>>   x, = args
>>>   return x + 1
>>>
>>> x = torch.randn(3)
>>> y = foo(x)
>>> assert torch.allclose(y, x)
>>>
>>> with MyMode():
>>>   y = foo(x)
>>> assert torch.allclose(y, x + 1)

```
Copy to clipboard 

torch.library.infer_schema(_prototype_function_ , _/_ , _*_ , _mutates_args_ , _op_name =None_)[source]
    
Parses the schema of a given function with type hints. The schema is inferred from the function’s type hints, and can be used to define a new operator.
We make the following assumptions:
  * None of the outputs alias any of the inputs or each other.
  * String type annotations “device, dtype, Tensor, types” without library specification are
assumed to be torch.*. Similarly, string type annotations “Optional, List, Sequence, Union”
without library specification are assumed to be typing.*.
  * Only the args listed in `mutates_args` are being mutated. If `mutates_args` is “unknown”,
it assumes that all inputs to the operator are being mutates.


Callers (e.g. the custom ops API) are responsible for checking these assumptions. 

Parameters
    
  * **prototype_function** (_Callable_) – The function from which to infer a schema for from its type annotations.
  * **op_name** (_Optional_ _[__str_ _]_) – The name of the operator in the schema. If `name` is None, then the name is not included in the inferred schema. Note that the input schema to `torch.library.Library.define` requires a operator name.
  * **mutates_args** (_"unknown"__|__Iterable_ _[__str_ _]_) – The arguments that are mutated in the function.



Returns
    
The inferred schema. 

Return type
    
str
Example
```
>>> def foo_impl(x: torch.Tensor) -> torch.Tensor:
>>>   return x.sin()
>>>
>>> infer_schema(foo_impl, op_name="foo", mutates_args={})
foo(Tensor x) -> Tensor
>>>
>>> infer_schema(foo_impl, mutates_args={})
(Tensor x) -> Tensor

```
Copy to clipboard 

_class_ torch._library.custom_ops.CustomOpDef(_namespace_ , _name_ , _schema_ , _fn_)[source][source]
    
CustomOpDef is a wrapper around a function that turns it into a custom op.
It has various methods for registering additional behavior for this custom op.
You should not instantiate CustomOpDef directly; instead, use the `torch.library.custom_op()` API. 

set_kernel_enabled(_device_type_ , _enabled =True_)[source][source]
    
Disable or re-enable an already registered kernel for this custom operator.
If the kernel is already disabled/enabled, this is a no-op.
Note
If a kernel is first disabled and then registered, it is disabled until enabled again. 

Parameters
    
  * **device_type** (_str_) – The device type to disable/enable the kernel for.
  * **disable** (_bool_) – Whether to disable or enable the kernel.


Example
```
>>> inp = torch.randn(1)
>>>
>>> # define custom op `f`.
>>> @custom_op("mylib::f", mutates_args=())
>>> def f(x: Tensor) -> Tensor:
>>>   return torch.zeros(1)
>>>
>>> print(f(inp)) # tensor([0.]), default kernel
>>>
>>> @f.register_kernel("cpu")
>>> def _(x):
>>>   return torch.ones(1)
>>>
>>> print(f(inp)) # tensor([1.]), CPU kernel
>>>
>>> # temporarily disable the CPU kernel
>>> with f.set_kernel_enabled("cpu", enabled = False):
>>>   print(f(inp)) # tensor([0.]) with CPU kernel disabled

```
Copy to clipboard
## Low-level APIs
The following APIs are direct bindings to PyTorch’s C++ low-level operator registration APIs.
Warning
The low-level operator registration APIs and the PyTorch Dispatcher are a complicated PyTorch concept. We recommend you use the higher level APIs above (that do not require a torch.library.Library object) when possible. This blog post <http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/>`_ is a good starting point to learn about the PyTorch Dispatcher.
A tutorial that walks you through some examples on how to use this API is available on Google Colab. 

_class_ torch.library.Library(_ns_ , _kind_ , _dispatch_key =''_)[source][source]
    
A class to create libraries that can be used to register new operators or override operators in existing libraries from Python. A user can optionally pass in a dispatch keyname if they only want to register kernels corresponding to only one specific dispatch key.
To create a library to override operators in an existing library (with name ns), set the kind to “IMPL”. To create a new library (with name ns) to register new operators, set the kind to “DEF”. To create a fragment of a possibly existing library to register operators (and bypass the limitation that there is only one library for a given namespace), set the kind to “FRAGMENT”. 

Parameters
    
  * **ns** – library name
  * **kind** – “DEF”, “IMPL” (default: “IMPL”), “FRAGMENT”
  * **dispatch_key** – PyTorch dispatch key (default: “”)



define(_schema_ , _alias_analysis =''_, _*_ , _tags =()_)[source][source]
    
Defines a new operator and its semantics in the ns namespace. 

Parameters
    
  * **schema** – function schema to define a new operator.
  * **alias_analysis** (_optional_) – Indicates if the aliasing properties of the operator arguments can be inferred from the schema (default behavior) or not (“CONSERVATIVE”).
  * **tags** (_Tag_ _|__Sequence_ _[__Tag_ _]_) – one or more torch.Tag to apply to this operator. Tagging an operator changes the operator’s behavior under various PyTorch subsystems; please read the docs for the torch.Tag carefully before applying it.



Returns
    
name of the operator as inferred from the schema. 

Example::
    
```
>>> my_lib = Library("mylib", "DEF")
>>> my_lib.define("sum(Tensor self) -> Tensor")

```
Copy to clipboard 

fallback(_fn_ , _dispatch_key =''_, _*_ , _with_keyset =False_)[source][source]
    
Registers the function implementation as the fallback for the given key.
This function only works for a library with global namespace (“_”). 

Parameters
    
  * **fn** – function used as fallback for the given dispatch key or `fallthrough_kernel()` to register a fallthrough.
  * **dispatch_key** – dispatch key that the input function should be registered for. By default, it uses the dispatch key that the library was created with.
  * **with_keyset** – flag controlling if the current dispatcher call keyset should be passed as the first argument to `fn` when calling. This should be used to create the appropriate keyset for redispatch calls.



Example::
    
```
>>> my_lib = Library("_", "IMPL")
>>> def fallback_kernel(op, *args, **kwargs):
>>>   # Handle all autocast ops generically
>>>   # ...
>>> my_lib.fallback(fallback_kernel, "Autocast")

```
Copy to clipboard 

impl(_op_name_ , _fn_ , _dispatch_key =''_, _*_ , _with_keyset =False_)[source][source]
    
Registers the function implementation for an operator defined in the library. 

Parameters
    
  * **op_name** – operator name (along with the overload) or OpOverload object.
  * **fn** – function that’s the operator implementation for the input dispatch key or `fallthrough_kernel()` to register a fallthrough.
  * **dispatch_key** – dispatch key that the input function should be registered for. By default, it uses the dispatch key that the library was created with.
  * **with_keyset** – flag controlling if the current dispatcher call keyset should be passed as the first argument to `fn` when calling. This should be used to create the appropriate keyset for redispatch calls.



Example::
    
```
>>> my_lib = Library("aten", "IMPL")
>>> def div_cpu(self, other):
>>>   return self * (1 / other)
>>> my_lib.impl("div.Tensor", div_cpu, "CPU")

```
Copy to clipboard 

torch.library.fallthrough_kernel()[source][source]
    
A dummy function to pass to `Library.impl` in order to register a fallthrough. 

torch.library.define(_qualname_ , _schema_ , _*_ , _lib =None_, _tags =()_)[source][source]


torch.library.define(_lib_ , _schema_ , _alias_analysis =''_)
    
Defines a new operator.
In PyTorch, defining an op (short for “operator”) is a two step-process: - we need to define the op (by providing an operator name and schema) - we need to implement behavior for how the operator interacts with various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.
This entrypoint defines the custom operator (the first step) you must then perform the second step by calling various `impl_*` APIs, like `torch.library.impl()` or `torch.library.register_fake()`. 

Parameters
    
  * **qualname** (_str_) – The qualified name for the operator. Should be a string that looks like “namespace::name”, e.g. “aten::sin”. Operators in PyTorch need a namespace to avoid name collisions; a given operator may only be created once. If you are writing a Python library, we recommend the namespace to be the name of your top-level module.
  * **schema** (_str_) – The schema of the operator. E.g. “(Tensor x) -> Tensor” for an op that accepts one Tensor and returns one Tensor. It does not contain the operator name (that is passed in `qualname`).
  * **lib** (_Optional_ _[__Library_ _]_) – If provided, the lifetime of this operator will be tied to the lifetime of the Library object.
  * **tags** (_Tag_ _|__Sequence_ _[__Tag_ _]_) – one or more torch.Tag to apply to this operator. Tagging an operator changes the operator’s behavior under various PyTorch subsystems; please read the docs for the torch.Tag carefully before applying it.



Example::
    
```
>>> import torch
>>> import numpy as np
>>>
>>> # Define the operator
>>> torch.library.define("mylib::sin", "(Tensor x) -> Tensor")
>>>
>>> # Add implementations for the operator
>>> @torch.library.impl("mylib::sin", "cpu")
>>> def f(x):
>>>   return torch.from_numpy(np.sin(x.numpy()))
>>>
>>> # Call the new operator from torch.ops.
>>> x = torch.randn(3)
>>> y = torch.ops.mylib.sin(x)
>>> assert torch.allclose(y, x.sin())

```
Copy to clipboard 

torch.library.impl(_lib_ , _name_ , _dispatch_key =''_)[source][source]


torch.library.impl(_qualname :str_, _types :Union[str,Sequence[str]]_, _func :Literal[None]=None_, _*_ , _lib :Optional[Library]=None_) → Callable[[Callable[...,object]],None]


torch.library.impl(_qualname :str_, _types :Union[str,Sequence[str]]_, _func :Callable[...,object]_, _*_ , _lib :Optional[Library]=None_) → None


torch.library.impl(_lib :Library_, _name :str_, _dispatch_key :str=''_) → Callable[[Callable[_P,_T]],Callable[_P,_T]]
    
Register an implementation for a device type for this operator.
You may pass “default” for `types` to register this implementation as the default implementation for ALL device types. Please only use this if the implementation truly supports all device types; for example, this is true if it is a composition of built-in PyTorch operators.
This API may be used as a decorator. You can use nested decorators with this API provided they return a function and are placed inside this API (see Example 2).
Some valid types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”. 

Parameters
    
  * **qualname** (_str_) – Should be a string that looks like “namespace::operator_name”.
  * **types** (_str_ _|__Sequence_ _[__str_ _]_) – The device types to register an impl to.
  * **lib** (_Optional_ _[__Library_ _]_) – If provided, the lifetime of this registration will be tied to the lifetime of the Library object.


Examples
```
>>> import torch
>>> import numpy as np
>>> # Example 1: Register function.
>>> # Define the operator
>>> torch.library.define("mylib::mysin", "(Tensor x) -> Tensor")
>>>
>>> # Add implementations for the cpu device
>>> @torch.library.impl("mylib::mysin", "cpu")
>>> def f(x):
>>>   return torch.from_numpy(np.sin(x.numpy()))
>>>
>>> x = torch.randn(3)
>>> y = torch.ops.mylib.mysin(x)
>>> assert torch.allclose(y, x.sin())
>>>
>>> # Example 2: Register function with decorator.
>>> def custom_decorator(func):
>>>   def wrapper(*args, **kwargs):
>>>     return func(*args, **kwargs) + 1
>>>   return wrapper
>>>
>>> # Define the operator
>>> torch.library.define("mylib::sin_plus_one", "(Tensor x) -> Tensor")
>>>
>>> # Add implementations for the operator
>>> @torch.library.impl("mylib::sin_plus_one", "cpu")
>>> @custom_decorator
>>> def f(x):
>>>   return torch.from_numpy(np.sin(x.numpy()))
>>>
>>> # Call the new operator from torch.ops.
>>> x = torch.randn(3)
>>>
>>> y1 = torch.ops.mylib.sin_plus_one(x)
>>> y2 = torch.sin(x) + 1
>>> assert torch.allclose(y1, y2)

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.library
    * Testing custom ops
      * `opcheck()`
    * Creating new custom ops in Python
      * `custom_op()`
      * `triton_op()`
      * `wrap_triton()`
    * Extending custom ops (created from Python or C++)
      * `register_kernel()`
      * `register_autocast()`
      * `register_autograd()`
      * `register_fake()`
      * `register_vmap()`
      * `impl_abstract()`
      * `get_ctx()`
      * `register_torch_dispatch()`
      * `infer_schema()`
      * `CustomOpDef`
        * `CustomOpDef.set_kernel_enabled()`
    * Low-level APIs
      * `Library`
        * `Library.define()`
        * `Library.fallback()`
        * `Library.impl()`
      * `fallthrough_kernel()`
      * `define()`
      * `impl()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.fx.experimental
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.fx.experimental
Warning
These APIs are experimental and subject to change without notice.
## torch.fx.experimental.symbolic_shapes
`ShapeEnv` |   
---|---  
`DimDynamic` | Controls how to perform symbol allocation for a dimension.  
`StrictMinMaxConstraint` | For clients: the size at this dimension must be within 'vr' (which specifies a lower and upper bound, inclusive-inclusive) AND it must be non-negative and should not be 0 or 1 (but see NB below).  
`RelaxedUnspecConstraint` | For clients: no explicit constraint; constraint is whatever is implicitly inferred by guards from tracing.  
`EqualityConstraint` | Represent and decide various kinds of equality constraints between input sources.  
`SymbolicContext` | Data structure specifying how we should create symbols in `create_symbolic_sizes_strides_storage_offset`; e.g., should they be static or dynamic.  
`StatelessSymbolicContext` | Create symbols in `create_symbolic_sizes_strides_storage_offset` via a symbolic_context determination as given by `DimDynamic` and `DimConstraint`.  
`StatefulSymbolicContext` | Create symbols in `create_symbolic_sizes_strides_storage_offset` via a symbolic_context determination as given by a cache of Source:Symbol.  
`SubclassSymbolicContext` | The correct symbolic context for a given inner tensor of a traceable tensor subclass may differ from that of the outer symbolic context.  
`DimConstraints` | Custom solver for a system of constraints on symbolic dimensions.  
`ShapeEnvSettings` | Encapsulates all shape env settings that could potentially affect FakeTensor dispatch.  
`ConvertIntKey` |   
`CallMethodKey` |   
`PropagateUnbackedSymInts` |   
`DivideByKey` |   
`InnerTensorKey` |   
`hint_int` | Retrieve the hint for an int (based on the underlying real values as observed at runtime).  
`is_concrete_int` | Utility to check if underlying object in SymInt is concrete value.  
`is_concrete_bool` | Utility to check if underlying object in SymBool is concrete value.  
`is_concrete_float` | Utility to check if underlying object in SymInt is concrete value.  
`has_free_symbols` | Faster version of bool(free_symbols(val))  
`has_free_unbacked_symbols` | Faster version of bool(free_unbacked_symbols(val))  
`definitely_true` | Returns True only if we can tell that a is True, possibly introducing a guard in the process.  
`definitely_false` | Returns True only if we can tell that a is False, possibly introducing a guard in the process.  
`guard_size_oblivious` | Perform a guard on a symbolic boolean expression in a size oblivious way.  
`sym_eq` | Like ==, but when run on list/tuple, it will recursively test equality and use sym_and to join the results together, without guarding.  
`constrain_range` | Applies a constraint that the passed in SymInt must lie between min-max inclusive-inclusive, WITHOUT introducing a guard on the SymInt (meaning that it can be used on unbacked SymInts).  
`constrain_unify` | Given two SymInts, constrain them so that they must be equal.  
`canonicalize_bool_expr` | Canonicalize a boolean expression by transforming it into a lt / le inequality and moving all the non-constant terms to the rhs.  
`statically_known_true` | Returns True if x can be simplified to a constant and is true.  
`lru_cache` |   
`check_consistent` | Test that two "meta" values (typically either Tensor or SymInt) have the same values, e.g., after retracing.  
`compute_unbacked_bindings` | After having run fake tensor propagation and producing example_value result, traverse example_value looking for freshly bound unbacked symbols and record their paths for later.  
`rebind_unbacked` | Suppose we are retracing a pre-existing FX graph that previously had fake tensor propagation (and therefore unbacked SymInts).  
`resolve_unbacked_bindings` |   
`is_accessor_node` |   
## torch.fx.experimental.proxy_tensor
`make_fx` | Given a function f, return a new function which when executed with valid arguments to f, returns an FX GraphModule representing the set of operations that were executed during the course of execution.  
---|---  
`handle_sym_dispatch` | Call into the currently active proxy tracing mode to do a SymInt/SymFloat/SymBool dispatch trace on a function that operates on these arguments.  
`get_proxy_mode` | Current the currently active proxy tracing mode, or None if we are not currently tracing.  
`maybe_enable_thunkify` | Within this context manager, if you are doing make_fx tracing, we will thunkify all SymNode compute and avoid tracing it into the graph unless it is actually needed.  
`maybe_disable_thunkify` | Within a context, disable thunkification.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.fx.experimental
    * torch.fx.experimental.symbolic_shapes
    * torch.fx.experimental.proxy_tensor


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.fx
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.fx
## Overview
FX is a toolkit for developers to use to transform `nn.Module` instances. FX consists of three main components: a **symbolic tracer,** an **intermediate representation** , and **Python code generation**. A demonstration of these components in action:
```
import torch

# Simple module for demonstration
class MyModule(torch.nn.Module):
  def __init__(self) -> None:
    super().__init__()
    self.param = torch.nn.Parameter(torch.rand(3, 4))
    self.linear = torch.nn.Linear(4, 5)
  def forward(self, x):
    return self.linear(x + self.param).clamp(min=0.0, max=1.0)

module = MyModule()
from torch.fx import symbolic_trace
# Symbolic tracing frontend - captures the semantics of the module
symbolic_traced: torch.fx.GraphModule = symbolic_trace(module)
# High-level intermediate representation (IR) - Graph representation
print(symbolic_traced.graph)
"""
graph():
  %x : [num_users=1] = placeholder[target=x]
  %param : [num_users=1] = get_attr[target=param]
  %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})
  %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})
  %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})
  return clamp
"""
# Code generation - valid Python code
print(symbolic_traced.code)
"""
def forward(self, x):
  param = self.param
  add = x + param; x = param = None
  linear = self.linear(add); add = None
  clamp = linear.clamp(min = 0.0, max = 1.0); linear = None
  return clamp
"""

```
Copy to clipboard
The **symbolic tracer** performs “symbolic execution” of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the `symbolic_trace()` and `Tracer` documentation.
The **intermediate representation** is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or `torch.nn.Module` instances), and return values. More information about the IR can be found in the documentation for `Graph`. The IR is the format on which transformations are applied.
**Python code generation** is what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph’s semantics. This functionality is wrapped up in `GraphModule`, which is a `torch.nn.Module` instance that holds a `Graph` as well as a `forward` method generated from the Graph.
Taken together, this pipeline of components (symbolic tracing -> intermediate representation -> transforms -> Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX!
Several example transformations can be found at the examples repository.
## Writing Transformations
What is an FX transform? Essentially, it’s a function that looks like this.
```
import torch
import torch.fx
def transform(m: nn.Module,
       tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:
  # Step 1: Acquire a Graph representing the code in `m`
  # NOTE: torch.fx.symbolic_trace is a wrapper around a call to
  # fx.Tracer.trace and constructing a GraphModule. We'll
  # split that out in our transform to allow the caller to
  # customize tracing behavior.
  graph : torch.fx.Graph = tracer_class().trace(m)
  # Step 2: Modify this Graph or create a new one
  graph = ...
  # Step 3: Construct a Module to return
  return torch.fx.GraphModule(m, graph)

```
Copy to clipboard
Your transform will take in a `torch.nn.Module`, acquire a `Graph` from it, do some modifications, and return a new `torch.nn.Module`. You should think of the `torch.nn.Module` that your FX transform returns as identical to a regular `torch.nn.Module` – you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a `torch.nn.Module` will allow for composability.
Note
It is also possible to modify an existing `GraphModule` instead of creating a new one, like so:
```
import torch
import torch.fx
def transform(m : nn.Module) -> nn.Module:
  gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)
  # Modify gm.graph
  # <...>
  # Recompile the forward() method of `gm` from its Graph
  gm.recompile()
  return gm

```
Copy to clipboard
Note that you MUST call `GraphModule.recompile()` to bring the generated `forward()` method on the `GraphModule` in sync with the modified `Graph`.
Given that you’ve passed in a `torch.nn.Module` that has been traced into a `Graph`, there are now two primary approaches you can take to building a new `Graph`.
### A Quick Primer on Graphs
Full treatment of the semantics of graphs can be found in the `Graph` documentation, but we are going to cover the basics here. A `Graph` is a data structure that represents a method on a `GraphModule`. The information that this requires is:
  * What are the inputs to the method?
  * What are the operations that run inside the method?
  * What is the output (i.e. return) value from the method?


All three of these concepts are represented with `Node` instances. Let’s see what we mean by that with a short example:
```
import torch
import torch.fx
class MyModule(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.param = torch.nn.Parameter(torch.rand(3, 4))
    self.linear = torch.nn.Linear(4, 5)
  def forward(self, x):
    return torch.topk(torch.sum(
      self.linear(x + self.linear.weight).relu(), dim=-1), 3)
m = MyModule()
gm = torch.fx.symbolic_trace(m)
gm.graph.print_tabular()

```
Copy to clipboard
Here we define a module `MyModule` for demonstration purposes, instantiate it, symbolically trace it, then call the `Graph.print_tabular()` method to print out a table showing the nodes of this `Graph`:
> opcode | name | target | args | kwargs  
> ---|---|---|---|---  
> placeholder | x | x | () | {}  
> get_attr | linear_weight | linear.weight | () | {}  
> call_function | add_1 | <built-in function add> | (x, linear_weight) | {}  
> call_module | linear_1 | linear | (add_1,) | {}  
> call_method | relu_1 | relu | (linear_1,) | {}  
> call_function | sum_1 | <built-in method sum …> | (relu_1,) | {‘dim’: -1}  
> call_function | topk_1 | <built-in method topk …> | (sum_1, 3) | {}  
> output | output | output | (topk_1,) | {}  
We can use this information to answer the questions we posed above.
  * What are the inputs to the method? In FX, method inputs are specified via special `placeholder` nodes. In this case, we have a single `placeholder` node with a `target` of `x`, meaning we have a single (non-self) argument named x.
  * What are the operations within the method? The `get_attr`, `call_function`, `call_module`, and `call_method` nodes represent the operations in the method. A full treatment of the semantics of all of these can be found in the `Node` documentation.
  * What is the return value of the method? The return value in a `Graph` is specified by a special `output` node.


Given that we now know the basics of how code is represented in FX, we can now explore how we would edit a `Graph`.
### Graph Manipulation
#### Direct Graph Manipulation
One approach to building this new `Graph` is to directly manipulate your old one. To aid in this, we can simply take the `Graph` we obtain from symbolic tracing and modify it. For example, let’s say we desire to replace `torch.add()` calls with `torch.mul()` calls.
```
import torch
import torch.fx
# Sample module
class M(torch.nn.Module):
  def forward(self, x, y):
    return torch.add(x, y)
def transform(m: torch.nn.Module,
       tracer_class : type = fx.Tracer) -> torch.nn.Module:
  graph : fx.Graph = tracer_class().trace(m)
  # FX represents its Graph as an ordered list of
  # nodes, so we can iterate through them.
  for node in graph.nodes:
    # Checks if we're calling a function (i.e:
    # torch.add)
    if node.op == 'call_function':
      # The target attribute is the function
      # that call_function calls.
      if node.target == torch.add:
        node.target = torch.mul
  graph.lint() # Does some checks to make sure the
         # Graph is well-formed.
  return fx.GraphModule(m, graph)

```
Copy to clipboard
We can also do more involved `Graph` rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the `Graph` documentation. An example of using these APIs to append a `torch.relu()` call can be found below.
```
# Specifies the insertion point. Any nodes added to the
# Graph within this scope will be inserted after `node`
with traced.graph.inserting_after(node):
  # Insert a new `call_function` node calling `torch.relu`
  new_node = traced.graph.call_function(
    torch.relu, args=(node,))
  # We want all places that used the value of `node` to
  # now use that value after the `relu` call we've added.
  # We use the `replace_all_uses_with` API to do this.
  node.replace_all_uses_with(new_node)

```
Copy to clipboard
For simple transformations that only consist of substitutions, you can also make use of the subgraph rewriter.
#### Subgraph Rewriting With replace_pattern()
FX also provides another level of automation on top of direct graph manipulation. The `replace_pattern()` API is essentially a “find/replace” tool for editing `Graph`s. It allows you to specify a `pattern` and `replacement` function and it will trace through those functions, find instances of the group of operations in the `pattern` graph, and replace those instances with copies of the `replacement` graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex.
#### Graph Manipulation Examples
  * Replace one op
  * Conv/Batch Norm fusion
  * replace_pattern: Basic usage
  * Quantization
  * Invert Transformation


### Proxy/Retracing
Another way of manipulating `Graph`s is by reusing the `Proxy` machinery used in symbolic tracing. For example, let’s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every `F.relu(x)` call into `(x > 0) * x`. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the `F.relu`, and then clean up the original `F.relu`. However, we can automate this process by using `Proxy` objects to automatically record operations into the `Graph`.
To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with `Proxy` objects as arguments. These `Proxy` objects will capture the operations that are performed on them and append them to the `Graph`.
```
# Note that this decomposition rule can be read as regular Python
def relu_decomposition(x):
  return (x > 0) * x
decomposition_rules = {}
decomposition_rules[F.relu] = relu_decomposition
def decompose(model: torch.nn.Module,
       tracer_class : type = fx.Tracer) -> torch.nn.Module:
"""
  Decompose `model` into smaller constituent operations.
  Currently,this only supports decomposing ReLU into its
  mathematical definition: (x > 0) * x
  """
  graph : fx.Graph = tracer_class().trace(model)
  new_graph = fx.Graph()
  env = {}
  tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)
  for node in graph.nodes:
    if node.op == 'call_function' and node.target in decomposition_rules:
      # By wrapping the arguments with proxies,
      # we can dispatch to the appropriate
      # decomposition rule and implicitly add it
      # to the Graph by symbolically tracing it.
      proxy_args = [
        fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]
      output_proxy = decomposition_rules[node.target](*proxy_args)
      # Operations on `Proxy` always yield new `Proxy`s, and the
      # return value of our decomposition rule is no exception.
      # We need to extract the underlying `Node` from the `Proxy`
      # to use it in subsequent iterations of this transform.
      new_node = output_proxy.node
      env[node.name] = new_node
    else:
      # Default case: we don't have a decomposition rule for this
      # node, so just copy the node over into the new graph.
      new_node = new_graph.node_copy(node, lambda x: env[x.name])
      env[node.name] = new_node
  return fx.GraphModule(model, new_graph)

```
Copy to clipboard
In addition to avoiding explicit graph manipulation, using `Proxy`s also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. Note that while calling `Proxy` we also passed a tracer pointing to the underlying variable graph. This is done so if in case the operations in graph are n-ary (e.g. add is a binary operator) the call to `Proxy` does not create multiple instances of a graph tracer which can lead to unexpected runtime errors. We recommend this method of using `Proxy` especially when the underlying operators can not be safely assumed to be unary.
A worked example of using `Proxy`s for `Graph` manipulation can be found here.
### The Interpreter Pattern
A useful code organizational pattern in FX is to loop over all the `Node`s in a `Graph` and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with `Proxy`s. For example, suppose we want to run a `GraphModule` and record the `torch.Tensor` shape and dtype properties on the nodes as we see them at runtime. That might look like:
```
import torch
import torch.fx
from torch.fx.node import Node
from typing import Dict
class ShapeProp:
"""
  Shape propagation. This class takes a `GraphModule`.
  Then, its `propagate` method executes the `GraphModule`
  node-by-node with the given arguments. As each operation
  executes, the ShapeProp class stores away the shape and
  element type for the output values of each operation on
  the `shape` and `dtype` attributes of the operation's
  `Node`.
  """
  def __init__(self, mod):
    self.mod = mod
    self.graph = mod.graph
    self.modules = dict(self.mod.named_modules())
  def propagate(self, *args):
    args_iter = iter(args)
    env : Dict[str, Node] = {}
    def load_arg(a):
      return torch.fx.graph.map_arg(a, lambda n: env[n.name])
    def fetch_attr(target : str):
      target_atoms = target.split('.')
      attr_itr = self.mod
      for i, atom in enumerate(target_atoms):
        if not hasattr(attr_itr, atom):
          raise RuntimeError(f"Node referenced nonexistent target {'.'.join(target_atoms[:i])}")
        attr_itr = getattr(attr_itr, atom)
      return attr_itr
    for node in self.graph.nodes:
      if node.op == 'placeholder':
        result = next(args_iter)
      elif node.op == 'get_attr':
        result = fetch_attr(node.target)
      elif node.op == 'call_function':
        result = node.target(*load_arg(node.args), **load_arg(node.kwargs))
      elif node.op == 'call_method':
        self_obj, *args = load_arg(node.args)
        kwargs = load_arg(node.kwargs)
        result = getattr(self_obj, node.target)(*args, **kwargs)
      elif node.op == 'call_module':
        result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))
      # This is the only code specific to shape propagation.
      # you can delete this `if` branch and this becomes
      # a generic GraphModule interpreter.
      if isinstance(result, torch.Tensor):
        node.shape = result.shape
        node.dtype = result.dtype
      env[node.name] = result
    return load_arg(self.graph.result)

```
Copy to clipboard
As you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the `Interpreter` class, which encompasses the above logic in a way that certain aspects of the interpreter’s execution can be overridden via method overrides.
In addition to executing operations, we can also generate a new Graph by feeding `Proxy` values through an interpreter. Similarly, we provide the `Transformer` class to encompass this pattern. `Transformer` behaves similarly to `Interpreter`, but instead of calling the `run` method to get a concrete output value from the Module, you would call the `Transformer.transform()` method to return a new `GraphModule` which was subject to any transformation rules you installed as overridden methods.
#### Examples of the Interpreter Pattern
  * Shape Propagation
  * Performance Profiler


## Debugging
### Introduction
Often in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code.
If you’re not familiar with debuggers, please see the auxiliary section Available Debuggers.
### Common Pitfalls in Transform Authoring
  * Nondeterministic `set` iteration order. In Python, the `set` datatype is unordered. Using `set` to contain collections of objects like `Node`s, for example, can cause unexpected nondeterminism. An example is iterating over a set of `Node`s to insert them into a `Graph`. Because the `set` data type is unordered, the ordering of the operations in the output program will be nondeterministic and can change across program invocations. The recommended alternative is to use a `dict` data type, which is insertion ordered as of Python 3.7 (and as of cPython 3.6). A `dict` can be used equivalently to a set by storing values to be deduplicated in the keys of the `dict`.


### Checking Correctness of Modules
Because the output of most deep learning modules consists of floating point `torch.Tensor` instances, checking for equivalence between the results of two `torch.nn.Module` is not as straightforward as doing a simple equality check. To motivate this, let’s use an example:
```
import torch
import torch.fx
import torchvision.models as models
def transform(m : torch.nn.Module) -> torch.nn.Module:
  gm = torch.fx.symbolic_trace(m)
  # Imagine we're doing some transforms here
  # <...>
  gm.recompile()
  return gm
resnet18 = models.resnet18()
transformed_resnet18 = transform(resnet18)
input_image = torch.randn(5, 3, 224, 224)
assert resnet18(input_image) == transformed_resnet18(input_image)
"""
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
"""

```
Copy to clipboard
Here, we’ve tried to check equality of the values of two deep learning models with the `==` equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see here for more details). We can use `torch.allclose()` instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold:
```
assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))

```
Copy to clipboard
This is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation.
### Debugging the Generated Code
Because FX generates the `forward()` function on `GraphModule`s, using traditional debugging techniques like `print` statements or `pdb` is not as straightforward. Luckily, we have several techniques we can use for debugging the generated code.
#### Use `pdb`
Invoke `pdb` to step into the running program. Although the code that represents the `Graph` is not in any source file, we can still step into it manually using `pdb` when the forward pass is invoked.
```
import torch
import torch.fx
import torchvision.models as models
def my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:
  graph = tracer_class().trace(inp)
  # Transformation logic here
  # <...>
  # Return new Module
  return fx.GraphModule(inp, graph)
my_module = models.resnet18()
my_module_transformed = my_pass(my_module)
input_value = torch.randn(5, 3, 224, 224)
# When this line is executed at runtime, we will be dropped into an
# interactive `pdb` prompt. We can use the `step` or `s` command to
# step into the execution of the next line
import pdb; pdb.set_trace()
my_module_transformed(input_value)

```
Copy to clipboard
#### Print the Generated Code
If you’d like to run the same code multiple times, then it can be a bit tedious to step to the right code with `pdb`. In that case, one approach is to simply copy-paste the generated `forward` pass into your code and examine it from there.
```
# Assume that `traced` is a GraphModule that has undergone some
# number of transforms
# Copy this code for later
print(traced)
# Print the code generated from symbolic tracing. This outputs:
"""
def forward(self, y):
  x = self.x
  add_1 = x + y; x = y = None
  return add_1
"""
# Subclass the original Module
class SubclassM(M):
  def __init__(self):
    super().__init__()
  # Paste the generated `forward` function (the one we printed and
  # copied above) here
  def forward(self, y):
    x = self.x
    add_1 = x + y; x = y = None
    return add_1
# Create an instance of the original, untraced Module. Then, create an
# instance of the Module with the copied `forward` function. We can
# now compare the output of both the original and the traced version.
pre_trace = M()
post_trace = SubclassM()

```
Copy to clipboard
#### Use the `to_folder` Function From `GraphModule`
`GraphModule.to_folder()` is a method in `GraphModule` that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in Print the Generated Code, it may be easier to examine modules and parameters using `to_folder`.
```
m = symbolic_trace(M())
m.to_folder("foo", "Bar")
from foo import Bar
y = Bar()

```
Copy to clipboard
After running the above example, we can then look at the code within `foo/module.py` and modify it as desired (e.g. adding `print` statements or using `pdb`) to debug the generated code.
### Debugging the Transformation
Now that we’ve identified that a transformation is creating incorrect code, it’s time to debug the transformation itself. First, we’ll check the Limitations of Symbolic Tracing section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our `GraphModule` transformation. There may be a quick answer in Writing Transformations, but, if not, there are several ways to examine our traced module:
```
# Sample Module
class M(torch.nn.Module):
  def forward(self, x, y):
    return x + y
# Create an instance of `M`
m = M()
# Symbolically trace an instance of `M` (returns a GraphModule). In
# this example, we'll only be discussing how to inspect a
# GraphModule, so we aren't showing any sample transforms for the
# sake of brevity.
traced = symbolic_trace(m)
# Print the code produced by tracing the module.
print(traced)
# The generated `forward` function is:
"""
def forward(self, x, y):
  add = x + y; x = y = None
  return add
"""
# Print the internal Graph.
print(traced.graph)
# This print-out returns:
"""
graph():
  %x : [num_users=1] = placeholder[target=x]
  %y : [num_users=1] = placeholder[target=y]
  %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})
  return add
"""
# Print a tabular representation of the internal Graph.
traced.graph.print_tabular()
# This gives us:
"""
opcode     name  target          args  kwargs
------------- ------ ----------------------- ------ --------
placeholder  x    x            ()   {}
placeholder  y    y            ()   {}
call_function add   <built-in function add> (x, y) {}
output     output output          (add,) {}
"""

```
Copy to clipboard
Using the utility functions above, we can compare our traced Module before and after we’ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it’s still not clear what’s going wrong, a debugger like `pdb` can be a good next step.
Going off of the example above, consider the following code:
```
# Sample user-defined function
def transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:
  # Get the Graph from our traced Module
  g = tracer_class().trace(module)
"""
  Transformations on `g` go here
  """
  return fx.GraphModule(module, g)
# Transform the Graph
transformed = transform_graph(traced)
# Print the new code after our transforms. Check to see if it was
# what we expected
print(transformed)

```
Copy to clipboard
Using the above example, let’s say that the call to `print(traced)` showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a `pdb` session. We can see what’s happening during the transform by breaking on `transform_graph(traced)`, then pressing `s` to “step into” the call to `transform_graph(traced)`.
We may also have good luck by editing the `print_tabular` method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node’s `input_nodes` and `users`.)
### Available Debuggers
The most common Python debugger is pdb. You can start your program in “debug mode” with `pdb` by typing `python -m pdb FILENAME.py` into the command line, where `FILENAME` is the name of the file you want to debug. After that, you can use the `pdb` debugger commands to move through your running program stepwise. It’s common to set a breakpoint (`b LINE-NUMBER`) when you start `pdb`, then call `c` to run the program until that point. This prevents you from having to step through each line of execution (using `s` or `n`) to get to the part of the code you want to examine. Alternatively, you can write `import pdb; pdb.set_trace()` before the line you want to break at. If you add `pdb.set_trace()`, your program will automatically start in debug mode when you run it. (In other words, you can just type `python FILENAME.py` into the command line instead of `python -m pdb FILENAME.py`.) Once you’re running your file in debug mode, you can step through the code and examine your program’s internal state using certain commands. There are many excellent tutorials on `pdb` online, including RealPython’s “Python Debugging With Pdb”.
IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use `pdb` by pulling up a terminal window in your IDE (e.g. View → Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around `pdb`).
## Limitations of Symbolic Tracing
FX uses a system of **symbolic tracing** (a.k.a symbolic execution) to capture the semantics of programs in a transformable/analyzable form. The system is **tracing** in that it executes the program (really a `torch.nn.Module` or function) to record operations. It is **symbolic** in that the data flowing through the program during this execution is not real data, but rather symbols (`Proxy` in FX parlance).
Although symbolic tracing works for most neural net code, it has some limitations.
### Dynamic Control Flow
The main limitation of symbolic tracing is it does not currently support _dynamic control flow_. That is, loops or `if` statements where the condition may depend on the input values of the program.
For example, let’s examine the following program:
```
def func_to_trace(x):
  if x.sum() > 0:
    return torch.relu(x)
  else:
    return torch.neg(x)
traced = torch.fx.symbolic_trace(func_to_trace)
"""
 <...>
 File "dyn.py", line 6, in func_to_trace
  if x.sum() > 0:
 File "pytorch/torch/fx/proxy.py", line 155, in __bool__
  return self.tracer.to_bool(self)
 File "pytorch/torch/fx/proxy.py", line 85, in to_bool
  raise TraceError('symbolically traced variables cannot be used as inputs to control flow')
torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow
"""

```
Copy to clipboard
The condition to the `if` statement relies on the value of `x.sum()`, which relies on the value of `x`, a function input. Since `x` can change (i.e. if you pass a new input tensor to the traced function), this is _dynamic control flow_. The traceback walks back up through your code to show you where this situation happens.
#### Static Control Flow
On the other hand, so-called _static control flow_ is supported. Static control flow is loops or `if` statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model’s architecture based on hyper-parameters. As a concrete example:
```
import torch
import torch.fx
class MyModule(torch.nn.Module):
  def __init__(self, do_activation : bool = False):
    super().__init__()
    self.do_activation = do_activation
    self.linear = torch.nn.Linear(512, 512)
  def forward(self, x):
    x = self.linear(x)
    # This if-statement is so-called static control flow.
    # Its condition does not depend on any input values
    if self.do_activation:
      x = torch.relu(x)
    return x
without_activation = MyModule(do_activation=False)
with_activation = MyModule(do_activation=True)
traced_without_activation = torch.fx.symbolic_trace(without_activation)
print(traced_without_activation.code)
"""
def forward(self, x):
  linear_1 = self.linear(x); x = None
  return linear_1
"""
traced_with_activation = torch.fx.symbolic_trace(with_activation)
print(traced_with_activation.code)
"""
import torch
def forward(self, x):
  linear_1 = self.linear(x); x = None
  relu_1 = torch.relu(linear_1); linear_1 = None
  return relu_1
"""

```
Copy to clipboard
The if-statement `if self.do_activation` does not depend on any function inputs, thus it is static. `do_activation` can be considered to be a hyper-parameter, and the traces of different instances of `MyModule` with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing.
Many instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to `Module` attributes or by binding concrete values to arguments during symbolic tracing:
```
def f(x, flag):
  if flag: return x
  else: return x*2
fx.symbolic_trace(f) # Fails!
fx.symbolic_trace(f, concrete_args={'flag': True})

```
Copy to clipboard
In the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see `wrap()`) rather than tracing through them.
### Non-`torch` Functions
FX uses `__torch_function__` as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the `math` module, are not covered by `__torch_function__`, but we would still like to capture them in symbolic tracing. For example:
```
import torch
import torch.fx
from math import sqrt
def normalize(x):
"""
  Normalize `x` by the size of the batch dimension
  """
  return x / sqrt(len(x))
# It's valid Python code
normalize(torch.rand(3, 4))
traced = torch.fx.symbolic_trace(normalize)
"""
 <...>
 File "sqrt.py", line 9, in normalize
  return x / sqrt(len(x))
 File "pytorch/torch/fx/proxy.py", line 161, in __len__
  raise RuntimeError("'len' is not supported in symbolic tracing by default. If you want "
RuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope
"""

```
Copy to clipboard
The error tells us that the built-in function `len` is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the `wrap()` API:
```
torch.fx.wrap('len')
torch.fx.wrap('sqrt')
traced = torch.fx.symbolic_trace(normalize)
print(traced.code)
"""
import math
def forward(self, x):
  len_1 = len(x)
  sqrt_1 = math.sqrt(len_1); len_1 = None
  truediv = x / sqrt_1; x = sqrt_1 = None
  return truediv
"""

```
Copy to clipboard
### Customizing Tracing with the `Tracer` class
The `Tracer` class is the class that underlies the implementation of `symbolic_trace`. The behavior of tracing can be customized by subclassing Tracer, like so:
```
class MyCustomTracer(torch.fx.Tracer):
  # Inside here you can override various methods
  # to customize tracing. See the `Tracer` API
  # reference
  pass

# Let's use this custom tracer to trace through this module
class MyModule(torch.nn.Module):
  def forward(self, x):
    return torch.relu(x) + torch.ones(3, 4)
mod = MyModule()
traced_graph = MyCustomTracer().trace(mod)
# trace() returns a Graph. Let's wrap it up in a
# GraphModule to make it runnable
traced = torch.fx.GraphModule(mod, traced_graph)

```
Copy to clipboard
#### Leaf Modules
Leaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard `torch.nn` module instances. For example:
```
class MySpecialSubmodule(torch.nn.Module):
  def forward(self, x):
    return torch.neg(x)
class MyModule(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = torch.nn.Linear(3, 4)
    self.submod = MySpecialSubmodule()
  def forward(self, x):
    return self.submod(self.linear(x))
traced = torch.fx.symbolic_trace(MyModule())
print(traced.code)
# `linear` is preserved as a call, yet `submod` is traced though.
# This is because the default set of "Leaf Modules" includes all
# standard `torch.nn` modules.
"""
import torch
def forward(self, x):
  linear_1 = self.linear(x); x = None
  neg_1 = torch.neg(linear_1); linear_1 = None
  return neg_1
"""

```
Copy to clipboard
The set of leaf modules can be customized by overriding `Tracer.is_leaf_module()`.
### Miscellanea
  * Tensor constructors (e.g. `torch.zeros`, `torch.ones`, `torch.rand`, `torch.randn`, `torch.sparse_coo_tensor`) are currently not traceable.
    * The deterministic constructors (`zeros`, `ones`) can be used and the value they produce will be embedded in the trace as a constant. This is only problematic if the arguments to these constructors refers to dynamic input sizes. In this case, `ones_like` or `zeros_like` may be a viable substitute.
    * Nondeterministic constructors (`rand`, `randn`) will have a single random value embedded in the trace. This is likely not the intended behavior. One workaround is to wrap `torch.randn` in a `torch.fx.wrap` function and call that instead.
> ```
@torch.fx.wrap
def torch_randn(x, shape):
  return torch.randn(shape)
def f(x):
  return x + torch_randn(x, 5)
fx.symbolic_trace(f)

```
Copy to clipboard
    * This behavior may be fixed in a future release.
  * Type annotations
    * Python 3-style type annotations (e.g. `func(x : torch.Tensor, y : int) -> torch.Tensor`) are supported and will be preserved by symbolic tracing.
    * Python 2-style comment type annotations `# type: (torch.Tensor, int) -> torch.Tensor` are not currently supported.
    * Annotations on local names within a function are not currently supported.
  * Gotcha around `training` flag and submodules
    * When using functionals like `torch.nn.functional.dropout`, it will be common for the training argument to be passed in as `self.training`. During FX tracing, this will likely be baked in as a constant value.
> ```
import torch
import torch.fx
class DropoutRepro(torch.nn.Module):
 def forward(self, x):
  return torch.nn.functional.dropout(x, training=self.training)

traced = torch.fx.symbolic_trace(DropoutRepro())
print(traced.code)
"""
def forward(self, x):
 dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False); x = None
 return dropout
"""
traced.eval()
x = torch.randn(5, 3)
torch.testing.assert_close(traced(x), x)
"""
AssertionError: Tensor-likes are not close!
Mismatched elements: 15 / 15 (100.0%)
Greatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)
Greatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)
"""

```
Copy to clipboard
    * However, when the standard `nn.Dropout()` submodule is used, the training flag is encapsulated and–because of the preservation of the `nn.Module` object model–can be changed.
> ```
class DropoutRepro2(torch.nn.Module):
 def __init__(self):
  super().__init__()
  self.drop = torch.nn.Dropout()
 def forward(self, x):
  return self.drop(x)
traced = torch.fx.symbolic_trace(DropoutRepro2())
print(traced.code)
"""
def forward(self, x):
 drop = self.drop(x); x = None
 return drop
"""
traced.eval()
x = torch.randn(5, 3)
torch.testing.assert_close(traced(x), x)

```
Copy to clipboard


>   * Because of this difference, consider marking modules that interact with the `training` flag dynamically as leaf modules.
> 

## API Reference 

torch.fx.symbolic_trace(_root_ , _concrete_args =None_)[source][source]
    
Symbolic tracing API
Given an `nn.Module` or function instance `root`, this function will return a `GraphModule` constructed by recording operations seen while tracing through `root`.
`concrete_args` allows you to partially specialize your function, whether it’s to remove control flow or data structures.
For example:
```
def f(a, b):
  if b == True:
    return a
  else:
    return a * 2

```
Copy to clipboard
FX can typically not trace through this due to the presence of control flow. However, we can use concrete_args to specialize on the value of b to trace through this:
```
f = fx.symbolic_trace(f, concrete_args={"b": False})
assert f(3, False) == 6

```
Copy to clipboard
Note that although you can still pass in different values of b, they will be ignored.
We can also use concrete_args to eliminate data-structure handling from our function. This will use pytrees to flatten your input. To avoid overspecializing, pass in fx.PH for values that shouldn’t be specialized. For example:
```
def f(x):
  out = 0
  for v in x.values():
    out += v
  return out

f = fx.symbolic_trace(f, concrete_args={"x": {"a": fx.PH, "b": fx.PH, "c": fx.PH}})
assert f({"a": 1, "b": 2, "c": 4}) == 7

```
Copy to clipboard 

Parameters
    
  * **root** (_Union_ _[__torch.nn.Module_ _,__Callable_ _]_) – Module or function to be traced and converted into a Graph representation.
  * **concrete_args** (_Optional_ _[__Dict_ _[__str_ _,__any_ _]__]_) – Inputs to be partially specialized



Returns
    
a Module created from the recorded operations from `root`. 

Return type
    
GraphModule
Note
Backwards-compatibility for this API is guaranteed. 

torch.fx.wrap(_fn_or_name_)[source][source]
    
This function can be called at module-level scope to register fn_or_name as a “leaf function”. A “leaf function” will be preserved as a CallFunction node in the FX trace instead of being traced through:
```
# foo/bar/baz.py
def my_custom_function(x, y):
  return x * x + y * y

torch.fx.wrap("my_custom_function")

def fn_to_be_traced(x, y):
  # When symbolic tracing, the below call to my_custom_function will be inserted into
  # the graph rather than tracing it.
  return my_custom_function(x, y)

```
Copy to clipboard
This function can also equivalently be used as a decorator:
```
# foo/bar/baz.py
@torch.fx.wrap
def my_custom_function(x, y):
  return x * x + y * y

```
Copy to clipboard
A wrapped function can be thought of a “leaf function”, analogous to the concept of “leaf modules”, that is, they are functions that are left as calls in the FX trace rather than traced through. 

Parameters
    
**fn_or_name** (_Union_ _[__str_ _,__Callable_ _]_) – The function or name of the global function to insert into the graph when it’s called
Note
Backwards-compatibility for this API is guaranteed. 

_class_ torch.fx.GraphModule(_* args_, _** kwargs_)[source][source]
    
GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a `graph` attribute, as well as `code` and `forward` attributes generated from that `graph`.
Warning
When `graph` is reassigned, `code` and `forward` will be automatically regenerated. However, if you edit the contents of the `graph` without reassigning the `graph` attribute itself, you must call `recompile()` to update the generated code.
Note
Backwards-compatibility for this API is guaranteed. 

__init__(_root_ , _graph_ , _class_name ='GraphModule'_)[source][source]
    
Construct a GraphModule. 

Parameters
    
  * **root** (_Union_ _[__torch.nn.Module_ _,__Dict_ _[__str_ _,__Any_ _]_) – `root` can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that `root` is a Module, any references to Module-based objects (via qualified name) in the Graph’s Nodes’ `target` field will be copied over from the respective place within `root`’s Module hierarchy into the GraphModule’s module hierarchy. In the case that `root` is a dict, the qualified name found in a Node’s `target` will be looked up directly in the dict’s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule’s module hierarchy.
  * **graph** (_Graph_) – `graph` contains the nodes this GraphModule should use for code generation
  * **class_name** (_str_) – `name` denotes the name of this GraphModule for debugging purposes. If it’s unset, all error messages will report as originating from `GraphModule`. It may be helpful to set this to `root`’s original name or a name that makes sense within the context of your transform.


Note
Backwards-compatibility for this API is guaranteed. 

add_submodule(_target_ , _m_)[source][source]
    
Adds the given submodule to `self`.
This installs empty Modules where none exist yet if they are subpaths of `target`. 

Parameters
    
  * **target** (_str_) – The fully-qualified string name of the new submodule (See example in `nn.Module.get_submodule` for how to specify a fully-qualified string.)
  * **m** (_Module_) – The submodule itself; the actual object we want to install in the current Module



Returns
     

Whether or not the submodule could be inserted. For
    
this method to return True, each object in the chain denoted by `target` must either a) not exist yet, or b) reference an `nn.Module` (not a parameter or other attribute) 

Return type
    
bool
Note
Backwards-compatibility for this API is guaranteed. 

_property_ code _: str_
    
Return the Python code generated from the `Graph` underlying this `GraphModule`. 

delete_all_unused_submodules()[source][source]
    
Deletes all unused submodules from `self`.
A Module is considered “used” if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a `call_module` node 3. It has a non-Module attribute that is used from a `get_attr` node
This method can be called to clean up an `nn.Module` without manually calling `delete_submodule` on each unused submodule.
Note
Backwards-compatibility for this API is guaranteed. 

delete_submodule(_target_)[source][source]
    
Deletes the given submodule from `self`.
The module will not be deleted if `target` is not a valid target. 

Parameters
    
**target** (_str_) – The fully-qualified string name of the new submodule (See example in `nn.Module.get_submodule` for how to specify a fully-qualified string.) 

Returns
     

Whether or not the target string referenced a
    
submodule we want to delete. A return value of `False` means that the `target` was not a valid reference to a submodule. 

Return type
    
bool
Note
Backwards-compatibility for this API is guaranteed. 

_property_ graph _: Graph_
    
Return the `Graph` underlying this `GraphModule` 

print_readable(_print_output =True_, _include_stride =False_, _include_device =False_, _colored =False_)[source][source]
    
Return the Python code generated for current GraphModule and its children GraphModules
Warning
This API is experimental and is _NOT_ backward-compatible. 

recompile()[source][source]
    
Recompile this GraphModule from its `graph` attribute. This should be called after editing the contained `graph`, otherwise the generated code of this `GraphModule` will be out of date.
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_PythonCode_ 

to_folder(_folder_ , _module_name ='FxModule'_)[source][source]
     

Dumps out module to `folder` with `module_name` so that it can be
    
imported with `from <folder> import <module_name>`
Args:
> folder (Union[str, os.PathLike]): The folder to write the code out to 

module_name (str): Top-level name to use for the `Module` while
    
> writing out the code
Warning
This API is experimental and is _NOT_ backward-compatible. 

_class_ torch.fx.Graph(_owning_module =None_, _tracer_cls =None_, _tracer_extras =None_)[source][source]
    
`Graph` is the main data structure used in the FX Intermediate Representation. It consists of a series of `Node` s, each representing callsites (or other syntactic constructs). The list of `Node` s, taken together, constitute a valid Python function.
For example, the following code
```
import torch
import torch.fx

class MyModule(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.param = torch.nn.Parameter(torch.rand(3, 4))
    self.linear = torch.nn.Linear(4, 5)
  def forward(self, x):
    return torch.topk(
      torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3
    )

m = MyModule()
gm = torch.fx.symbolic_trace(m)

```
Copy to clipboard
Will produce the following Graph:
```
print(gm.graph)

```
Copy to clipboard
```
graph(x):
  %linear_weight : [num_users=1] = self.linear.weight
  %add_1 : [num_users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})
  %linear_1 : [num_users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})
  %relu_1 : [num_users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})
  %sum_1 : [num_users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})
  %topk_1 : [num_users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})
  return topk_1

```
Copy to clipboard
For the semantics of operations represented in the `Graph`, please see `Node`.
Note
Backwards-compatibility for this API is guaranteed. 

__init__(_owning_module =None_, _tracer_cls =None_, _tracer_extras =None_)[source][source]
    
Construct an empty Graph.
Note
Backwards-compatibility for this API is guaranteed. 

call_function(_the_function_ , _args =None_, _kwargs =None_, _type_expr =None_)[source][source]
    
Insert a `call_function` `Node` into the `Graph`. A `call_function` node represents a call to a Python callable, specified by `the_function`. 

Parameters
    
  * **the_function** (_Callable_ _[__...__,__Any_ _]_) – The function to be called. Can be any PyTorch operator, Python function, or member of the `builtins` or `operator` namespaces.
  * **args** (_Optional_ _[__Tuple_ _[__Argument_ _,__...__]__]_) – The positional arguments to be passed to the called function.
  * **kwargs** (_Optional_ _[__Dict_ _[__str_ _,__Argument_ _]__]_) – The keyword arguments to be passed to the called function
  * **type_expr** (_Optional_ _[__Any_ _]_) – an optional type annotation representing the Python type the output of this node will have.



Returns
    
The newly created and inserted `call_function` node. 

Return type
    
_Node_
Note
The same insertion point and type expression rules apply for this method as `Graph.create_node()`.
Note
Backwards-compatibility for this API is guaranteed. 

call_method(_method_name_ , _args =None_, _kwargs =None_, _type_expr =None_)[source][source]
    
Insert a `call_method` `Node` into the `Graph`. A `call_method` node represents a call to a given method on the 0th element of `args`. 

Parameters
    
  * **method_name** (_str_) – The name of the method to apply to the self argument. For example, if args[0] is a `Node` representing a `Tensor`, then to call `relu()` on that `Tensor`, pass `relu` to `method_name`.
  * **args** (_Optional_ _[__Tuple_ _[__Argument_ _,__...__]__]_) – The positional arguments to be passed to the called method. Note that this _should_ include a `self` argument.
  * **kwargs** (_Optional_ _[__Dict_ _[__str_ _,__Argument_ _]__]_) – The keyword arguments to be passed to the called method
  * **type_expr** (_Optional_ _[__Any_ _]_) – an optional type annotation representing the Python type the output of this node will have.



Returns
    
The newly created and inserted `call_method` node. 

Return type
    
_Node_
Note
The same insertion point and type expression rules apply for this method as `Graph.create_node()`.
Note
Backwards-compatibility for this API is guaranteed. 

call_module(_module_name_ , _args =None_, _kwargs =None_, _type_expr =None_)[source][source]
    
Insert a `call_module` `Node` into the `Graph`. A `call_module` node represents a call to the forward() function of a `Module` in the `Module` hierarchy. 

Parameters
    
  * **module_name** (_str_) – The qualified name of the `Module` in the `Module` hierarchy to be called. For example, if the traced `Module` has a submodule named `foo`, which has a submodule named `bar`, the qualified name `foo.bar` should be passed as `module_name` to call that module.
  * **args** (_Optional_ _[__Tuple_ _[__Argument_ _,__...__]__]_) – The positional arguments to be passed to the called method. Note that this should _not_ include a `self` argument.
  * **kwargs** (_Optional_ _[__Dict_ _[__str_ _,__Argument_ _]__]_) – The keyword arguments to be passed to the called method
  * **type_expr** (_Optional_ _[__Any_ _]_) – an optional type annotation representing the Python type the output of this node will have.



Returns
    
The newly-created and inserted `call_module` node. 

Return type
    
_Node_
Note
The same insertion point and type expression rules apply for this method as `Graph.create_node()`.
Note
Backwards-compatibility for this API is guaranteed. 

create_node(_op_ , _target_ , _args =None_, _kwargs =None_, _name =None_, _type_expr =None_)[source][source]
    
Create a `Node` and add it to the `Graph` at the current insert-point. Note that the current insert-point can be set via `Graph.inserting_before()` and `Graph.inserting_after()`. 

Parameters
    
  * **op** (_str_) – the opcode for this Node. One of ‘call_function’, ‘call_method’, ‘get_attr’, ‘call_module’, ‘placeholder’, or ‘output’. The semantics of these opcodes are described in the `Graph` docstring.
  * **args** (_Optional_ _[__Tuple_ _[__Argument_ _,__...__]__]_) – is a tuple of arguments to this node.
  * **kwargs** (_Optional_ _[__Dict_ _[__str_ _,__Argument_ _]__]_) – the kwargs of this Node
  * **name** (_Optional_ _[__str_ _]_) – an optional string name for the `Node`. This will influence the name of the value assigned to in the Python generated code.
  * **type_expr** (_Optional_ _[__Any_ _]_) – an optional type annotation representing the Python type the output of this node will have.



Returns
    
The newly-created and inserted node. 

Return type
    
_Node_
Note
Backwards-compatibility for this API is guaranteed. 

eliminate_dead_code(_is_impure_node =None_)[source][source]
    
Remove all dead code from the graph, based on each node’s number of users, and whether the nodes have any side effects. The graph must be topologically sorted before calling. 

Parameters
    
  * **is_impure_node** (_Optional_ _[__Callable_ _[__[__Node_ _]__,__bool_ _]__]_) – A function that returns
  * **None** (_whether a node is impure. If this is_) – 
  * **to** (_then the default behavior is_) – 
  * **Node.is_impure.** (_use_) – 



Returns
    
Whether the graph was changed as a result of the pass. 

Return type
    
bool
Example:
Before dead code is eliminated, a from a = x + 1 below has no users and thus can be eliminated from the graph without having an effect.
```
def forward(self, x):
  a = x + 1
  return x + self.attr_1

```
Copy to clipboard
After dead code is eliminated, a = x + 1 has been removed, and the rest of forward remains.
```
def forward(self, x):
  return x + self.attr_1

```
Copy to clipboard
Warning
Dead code elimination has some heuristics to avoid removing side-effectful nodes (see Node.is_impure) but in general coverage is very bad, so you should assume that this method is not sound to call unless you know that your FX graph consists entirely of functional operations or you supply your own custom function for detecting side-effectful nodes.
Note
Backwards-compatibility for this API is guaranteed. 

erase_node(_to_erase_)[source][source]
    
Erases a `Node` from the `Graph`. Throws an exception if there are still users of that node in the `Graph`. 

Parameters
    
**to_erase** (_Node_) – The `Node` to erase from the `Graph`.
Note
Backwards-compatibility for this API is guaranteed. 

find_nodes(_*_ , _op_ , _target =None_, _sort =True_)[source][source]
    
Allows for fast query of nodes 

Parameters
    
  * **op** (_str_) – the name of the operation
  * **target** (_Optional_ _[__Target_ _]_) – the target of the node. For call_function, the target is required. For other ops, the target is optional.
  * **sort** (_bool_) – whether to return nodes in the order they appear on on the graph.



Returns
    
Iteratable of nodes with the requested op and target.
Warning
This API is experimental and is _NOT_ backward-compatible. 

get_attr(_qualified_name_ , _type_expr =None_)[source][source]
    
Insert a `get_attr` node into the Graph. A `get_attr` `Node` represents the fetch of an attribute from the `Module` hierarchy. 

Parameters
    
  * **qualified_name** (_str_) – the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named `foo`, which has a submodule named `bar`, which has an attribute named `baz`, the qualified name `foo.bar.baz` should be passed as `qualified_name`.
  * **type_expr** (_Optional_ _[__Any_ _]_) – an optional type annotation representing the Python type the output of this node will have.



Returns
    
The newly-created and inserted `get_attr` node. 

Return type
    
_Node_
Note
The same insertion point and type expression rules apply for this method as `Graph.create_node`.
Note
Backwards-compatibility for this API is guaranteed. 

graph_copy(_g_ , _val_map_ , _return_output_node =False_)[source][source]
    
Copy all nodes from a given graph into `self`. 

Parameters
    
  * **g** (_Graph_) – The source graph from which to copy Nodes.
  * **val_map** (_Dict_ _[__Node_ _,__Node_ _]_) – a dictionary that will be populated with a mapping from nodes in `g` to nodes in `self`. Note that `val_map` can be passed in with values in it already to override copying of certain values.



Returns
    
The value in `self` that is now equivalent to the output value in `g`, if `g` had an `output` node. `None` otherwise. 

Return type
    
_Optional_[_Union_[tuple[‘Argument’, …], _Sequence_[Argument], _Mapping_[str, Argument], slice, range, _Node_, str, int, float, bool, complex, _dtype_, _Tensor_, _device_, _memory_format_, _layout_, _OpOverload_ , _SymInt_, _SymBool_, _SymFloat_]]
Note
Backwards-compatibility for this API is guaranteed. 

inserting_after(_n =None_)[source][source]
     

Set the point at which create_node and companion methods will insert into the graph.
    
When used within a ‘with’ statement, this will temporary set the insert point and then restore it when the with statement exits:
```
with g.inserting_after(n):
  ... # inserting after node n
... # insert point restored to what it was previously
g.inserting_after(n) # set the insert point permanently

```
Copy to clipboard
Args:
> 

n (Optional[Node]): The node before which to insert. If None this will insert after
    
> the beginning of the entire graph. 

Returns:
    
A resource manager that will restore the insert point on `__exit__`.
Note
Backwards-compatibility for this API is guaranteed. 

inserting_before(_n =None_)[source][source]
     

Set the point at which create_node and companion methods will insert into the graph.
    
When used within a ‘with’ statement, this will temporary set the insert point and then restore it when the with statement exits:
```
with g.inserting_before(n):
  ... # inserting before node n
... # insert point restored to what it was previously
g.inserting_before(n) # set the insert point permanently

```
Copy to clipboard
Args:
> 

n (Optional[Node]): The node before which to insert. If None this will insert before
    
> the beginning of the entire graph. 

Returns:
    
A resource manager that will restore the insert point on `__exit__`.
Note
Backwards-compatibility for this API is guaranteed. 

lint()[source][source]
    
Runs various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If this Graph has an owning GraphModule, checks that targets exist in that GraphModule
Note
Backwards-compatibility for this API is guaranteed. 

node_copy(_node_ , _arg_transform= <function Graph.<lambda>>_)[source][source]
    
Copy a node from one graph into another. `arg_transform` needs to transform arguments from the graph of node to the graph of self. Example:
```
# Copying all the nodes in `g` into `new_graph`
g: torch.fx.Graph = ...
new_graph = torch.fx.graph()
value_remap = {}
for node in g.nodes:
  value_remap[node] = new_graph.node_copy(node, lambda n: value_remap[n])

```
Copy to clipboard 

Parameters
    
  * **node** (_Node_) – The node to copy into `self`.
  * **arg_transform** (_Callable_ _[__[__Node_ _]__,__Argument_ _]_) – A function that transforms `Node` arguments in node’s `args` and `kwargs` into the equivalent argument in `self`. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to `self`.



Return type
    
_Node_
Note
Backwards-compatibility for this API is guaranteed. 

_property_ nodes _: _node_list_
    
Get the list of Nodes that constitute this Graph.
Note that this `Node` list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe. 

Returns
    
A doubly-linked list of Nodes. Note that `reversed` can be called on this list to switch iteration order. 

on_generate_code(_make_transformer_)[source][source]
    
Register a transformer function when python code is generated
> 

Args:
     

make_transformer (Callable[[Optional[TransformCodeFunc]], TransformCodeFunc]):
    
> a function that returns a code transformer to be registered. This function is called by on_generate_code to obtain the code transformer.
> This function is also given as its input the currently registered code transformer (or None if nothing is registered), in case it is not desirable to overwrite it. This is useful to chain code transformers together. 

Returns:
    
> a context manager that when used in a with statement, to automatically restore the previously registered code transformer.
> Example:
> ```
gm: fx.GraphModule = ...

# This is a code transformer we want to register. This code
# transformer prepends a pdb import and trace statement at the very
# beginning of the generated torch.fx code to allow for manual
# debugging with the PDB library.
def insert_pdb(body):
  return ["import pdb; pdb.set_trace()\n", *body]

# Registers `insert_pdb`, and overwrites the current registered
# code transformer (given by `_` to the lambda):
gm.graph.on_generate_code(lambda _: insert_pdb)
# Or alternatively, registers a code transformer which first
# runs `body` through existing registered transformer, then
# through `insert_pdb`:
gm.graph.on_generate_code(
  lambda current_trans: (
    lambda body: insert_pdb(current_trans(body) if current_trans else body)
  )
)
gm.recompile()
gm(*inputs) # drops into pdb

```
Copy to clipboard
> This function can also be used as a context manager, with the benefit to automatically restores the previously registered code transformer:
> ```
# ... continue from previous example
with gm.graph.on_generate_code(lambda _: insert_pdb):
  # do more stuff with `gm`...
  gm.recompile()
  gm(*inputs) # drops into pdb
# now previous code transformer is restored (but `gm`'s code with pdb
# remains - that means you can run `gm` with pdb here too, until you
# run next `recompile()`).

```
Copy to clipboard
Warning
This API is experimental and is _NOT_ backward-compatible. 

output(_result_ , _type_expr =None_)[source][source]
    
Insert an `output` `Node` into the `Graph`. An `output` node represents a `return` statement in Python code. `result` is the value that should be returned. 

Parameters
    
  * **result** (_Argument_) – The value to be returned.
  * **type_expr** (_Optional_ _[__Any_ _]_) – an optional type annotation representing the Python type the output of this node will have.


Note
The same insertion point and type expression rules apply for this method as `Graph.create_node`.
Note
Backwards-compatibility for this API is guaranteed. 

output_node()[source][source]
    
Warning
This API is experimental and is _NOT_ backward-compatible. 

Return type
    
_Node_ 

placeholder(_name_ , _type_expr=None_ , _default_value_)[source][source]
    
Insert a `placeholder` node into the Graph. A `placeholder` represents a function input. 

Parameters
    
  * **name** (_str_) – A name for the input value. This corresponds to the name of the positional argument to the function this `Graph` represents.
  * **type_expr** (_Optional_ _[__Any_ _]_) – an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).
  * **default_value** (_Any_) – The default value this function argument should take on. NOTE: to allow for None as a default value, inspect.Signature.empty should be passed as this argument to specify that the parameter does _not_ have a default value.



Return type
    
_Node_
Note
The same insertion point and type expression rules apply for this method as `Graph.create_node`.
Note
Backwards-compatibility for this API is guaranteed. 

print_tabular()[source][source]
    
Prints the intermediate representation of the graph in tabular format. Note that this API requires the `tabulate` module to be installed.
Note
Backwards-compatibility for this API is guaranteed. 

process_inputs(_* args_)[source][source]
    
Processes args so that they can be passed to the FX graph.
Warning
This API is experimental and is _NOT_ backward-compatible. 

process_outputs(_out_)[source][source]
    
Warning
This API is experimental and is _NOT_ backward-compatible. 

python_code(_root_module_ , _*_ , _verbose =False_, _include_stride =False_, _include_device =False_, _colored =False_)[source][source]
    
Turn this `Graph` into valid Python code. 

Parameters
    
**root_module** (_str_) – The name of the root module on which to look-up qualified name targets. This is usually ‘self’. 

Returns
    
src: the Python source code representing the object globals: a dictionary of global names in src -> the objects that they reference. 

Return type
    
A PythonCode object, consisting of two fields
Note
Backwards-compatibility for this API is guaranteed. 

set_codegen(_codegen_)[source][source]
    
Warning
This API is experimental and is _NOT_ backward-compatible. 

_class_ torch.fx.Node(_graph_ , _name_ , _op_ , _target_ , _args_ , _kwargs_ , _return_type =None_)[source][source]
    
`Node` is the data structure that represents individual operations within a `Graph`. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each `Node` has a function specified by its `op` property. The `Node` semantics for each value of `op` are as follows:
  * `placeholder` represents a function input. The `name` attribute specifies the name this value will take on. `target` is similarly the name of the argument. `args` holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. `kwargs` is don’t-care. Placeholders correspond to the function parameters (e.g. `x`) in the graph printout.
  * `get_attr` retrieves a parameter from the module hierarchy. `name` is similarly the name the result of the fetch is assigned to. `target` is the fully-qualified name of the parameter’s position in the module hierarchy. `args` and `kwargs` are don’t-care
  * `call_function` applies a free function to some values. `name` is similarly the name of the value to assign to. `target` is the function to be applied. `args` and `kwargs` represent the arguments to the function, following the Python calling convention
  * `call_module` applies a module in the module hierarchy’s `forward()` method to given arguments. `name` is as previous. `target` is the fully-qualified name of the module in the module hierarchy to call. `args` and `kwargs` represent the arguments to invoke the module on, _excluding the self argument_.
  * `call_method` calls a method on a value. `name` is as similar. `target` is the string name of the method to apply to the `self` argument. `args` and `kwargs` represent the arguments to invoke the module on, _including the self argument_
  * `output` contains the output of the traced function in its `args[0]` attribute. This corresponds to the “return” statement in the Graph printout.


Note
Backwards-compatibility for this API is guaranteed. 

_property_ all_input_nodes _: list['Node']_
    
Return all Nodes that are inputs to this Node. This is equivalent to iterating over `args` and `kwargs` and only collecting the values that are Nodes. 

Returns
    
List of `Nodes` that appear in the `args` and `kwargs` of this `Node`, in that order. 

append(_x_)[source][source]
    
Insert `x` after this node in the list of nodes in the graph. Equivalent to `self.next.prepend(x)` 

Parameters
    
**x** (_Node_) – The node to put after this node. Must be a member of the same graph.
Note
Backwards-compatibility for this API is guaranteed. 

_property_ args _: tuple[Union[tuple['Argument',...],collections.abc.Sequence['Argument'],collections.abc.Mapping[str,'Argument'],slice,range,torch.fx.node.Node,str,int,float,bool,complex,torch.dtype,torch.Tensor,torch.device,torch.memory_format,torch.layout,torch._ops.OpOverload,torch.SymInt,torch.SymBool,torch.SymFloat,NoneType],...]_
    
The tuple of arguments to this `Node`. The interpretation of arguments depends on the node’s opcode. See the `Node` docstring for more information.
Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. 

format_node(_placeholder_names =None_, _maybe_return_typename =None_)[source][source]
    
Return a descriptive string representation of `self`.
This method can be used with no arguments as a debugging utility.
This function is also used internally in the `__str__` method of `Graph`. Together, the strings in `placeholder_names` and `maybe_return_typename` make up the signature of the autogenerated `forward` function in this Graph’s surrounding GraphModule. `placeholder_names` and `maybe_return_typename` should not be used otherwise. 

Parameters
    
  * **placeholder_names** (_Optional_ _[__list_ _[__str_ _]__]_) – A list that will store formatted strings representing the placeholders in the generated `forward` function. Internal use only.
  * **maybe_return_typename** (_Optional_ _[__list_ _[__str_ _]__]_) – A single-element list that will store a formatted string representing the output of the generated `forward` function. Internal use only.



Returns
     

If 1) we’re using `format_node` as an internal helper
    
in the `__str__` method of `Graph`, and 2) `self` is a placeholder Node, return `None`. Otherwise, return a descriptive string representation of the current Node. 

Return type
    
str
Note
Backwards-compatibility for this API is guaranteed. 

insert_arg(_idx_ , _arg_)[source][source]
    
Insert an positional argument to the argument list with given index. 

Parameters
    
  * **idx** (_int_) – The index of the element in `self.args` to be inserted before.
  * **arg** (_Argument_) – The new argument value to insert into `args`


Note
Backwards-compatibility for this API is guaranteed. 

is_impure()[source][source]
    
Returns whether this op is impure, i.e. if its op is a placeholder or output, or if a call_function or call_module which is impure. 

Returns
    
If the op is impure or not. 

Return type
    
bool
Warning
This API is experimental and is _NOT_ backward-compatible. 

_property_ kwargs _: dict[str,Union[tuple['Argument',...],collections.abc.Sequence['Argument'],collections.abc.Mapping[str,'Argument'],slice,range,torch.fx.node.Node,str,int,float,bool,complex,torch.dtype,torch.Tensor,torch.device,torch.memory_format,torch.layout,torch._ops.OpOverload,torch.SymInt,torch.SymBool,torch.SymFloat,NoneType]]_
    
The dict of keyword arguments to this `Node`. The interpretation of arguments depends on the node’s opcode. See the `Node` docstring for more information.
Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. 

_property_ next _: Node_
    
Returns the next `Node` in the linked list of Nodes. 

Returns
    
The next `Node` in the linked list of Nodes. 

normalized_arguments(_root_ , _arg_types =None_, _kwarg_types =None_, _normalize_to_only_use_kwargs =False_)[source][source]
    
Returns normalized arguments to Python targets. This means that args/kwargs will be matched up to the module/functional’s signature and return exclusively kwargs in positional order if normalize_to_only_use_kwargs is true. Also populates default values. Does not support positional-only parameters or varargs parameters.
Supports module calls.
May require arg_types and kwarg_types in order to disambiguate overloads. 

Parameters
    
  * **root** (_torch.nn.Module_) – Module upon which to resolve module targets.
  * **arg_types** (_Optional_ _[__Tuple_ _[__Any_ _]__]_) – Tuple of arg types for the args
  * **kwarg_types** (_Optional_ _[__Dict_ _[__str_ _,__Any_ _]__]_) – Dict of arg types for the kwargs
  * **normalize_to_only_use_kwargs** (_bool_) – Whether to normalize to only use kwargs.



Returns
    
Returns NamedTuple ArgsKwargsPair, or None if not successful. 

Return type
    
_Optional_[_ArgsKwargsPair_]
Warning
This API is experimental and is _NOT_ backward-compatible. 

prepend(_x_)[source][source]
    
Insert x before this node in the list of nodes in the graph. Example:
```
Before: p -> self
    bx -> x -> ax
After: p -> x -> self
    bx -> ax

```
Copy to clipboard 

Parameters
    
**x** (_Node_) – The node to put before this node. Must be a member of the same graph.
Note
Backwards-compatibility for this API is guaranteed. 

_property_ prev _: Node_
    
Returns the previous `Node` in the linked list of Nodes. 

Returns
    
The previous `Node` in the linked list of Nodes. 

replace_all_uses_with(_replace_with_ , _delete_user_cb= <function Node.<lambda>>_, _*_ , _propagate_meta=False_)[source][source]
    
Replace all uses of `self` in the Graph with the Node `replace_with`. 

Parameters
    
  * **replace_with** (_Node_) – The node to replace all uses of `self` with.
  * **delete_user_cb** (_Callable_) – Callback that is called to determine whether a given user of the self node should be removed.
  * **propagate_meta** (_bool_) – Whether or not to copy all properties on the .meta field of the original node onto the replacement node. For safety, this is only valid to do if the replacement node doesn’t already have an existing .meta field.



Returns
    
The list of Nodes on which this change was made. 

Return type
    
list[‘Node’]
Note
Backwards-compatibility for this API is guaranteed. 

replace_input_with(_old_input_ , _new_input_)[source][source]
    
Loop through input nodes of `self`, and replace all instances of `old_input` with `new_input`. 

Parameters
    
  * **old_input** (_Node_) – The old input node to be replaced.
  * **new_input** (_Node_) – The new input node to replace `old_input`.


Note
Backwards-compatibility for this API is guaranteed. 

_property_ stack_trace _: Optional[str]_
    
Return the Python stack trace that was recorded during tracing, if any. When traced with fx.Tracer, this property is usually populated by Tracer.create_proxy. To record stack traces during tracing for debug purposes, set record_stack_traces = True on the Tracer instance. When traced with dynamo, this property will be populated by default by OutputGraph.create_proxy.
stack_trace would have the innermost frame at the end of the string. 

update_arg(_idx_ , _arg_)[source][source]
    
Update an existing positional argument to contain the new value `arg`. After calling, `self.args[idx] == arg`. 

Parameters
    
  * **idx** (_int_) – The index into `self.args` of the element to update
  * **arg** (_Argument_) – The new argument value to write into `args`


Note
Backwards-compatibility for this API is guaranteed. 

update_kwarg(_key_ , _arg_)[source][source]
    
Update an existing keyword argument to contain the new value `arg`. After calling, `self.kwargs[key] == arg`. 

Parameters
    
  * **key** (_str_) – The key in `self.kwargs` of the element to update
  * **arg** (_Argument_) – The new argument value to write into `kwargs`


Note
Backwards-compatibility for this API is guaranteed. 

_class_ torch.fx.Tracer(_autowrap_modules =(math,)_, _autowrap_functions =()_)[source][source]
    
> `Tracer` is the class that implements the symbolic tracing functionality of `torch.fx.symbolic_trace`. A call to `symbolic_trace(m)` is equivalent to `Tracer().trace(m)`.
> Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.
Note
Backwards-compatibility for this API is guaranteed. 

call_module(_m_ , _forward_ , _args_ , _kwargs_)[source][source]
    
Method that specifies the behavior of this `Tracer` when it encounters a call to an `nn.Module` instance.
By default, the behavior is to check if the called module is a leaf module via `is_leaf_module`. If it is, emit a `call_module` node referring to `m` in the `Graph`. Otherwise, call the `Module` normally, tracing through the operations in its `forward` function.
This method can be overridden to–for example–create nested traced GraphModules, or any other behavior you would want while tracing across `Module` boundaries. 

Parameters
    
  * **m** (_Module_) – The module for which a call is being emitted
  * **forward** (_Callable_) – The forward() method of the `Module` to be invoked
  * **args** (_Tuple_) – args of the module callsite
  * **kwargs** (_Dict_) – kwargs of the module callsite



Returns
    
The return value from the Module call. In the case that a `call_module` node was emitted, this is a `Proxy` value. Otherwise, it is whatever value was returned from the `Module` invocation. 

Return type
    
_Any_
Note
Backwards-compatibility for this API is guaranteed. 

create_arg(_a_)[source][source]
    
A method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the `Graph`.
By default, the behavior includes:
  1. Iterate through collection types (e.g. tuple, list, dict) and recursively call `create_args` on the elements.
  2. Given a Proxy object, return a reference to the underlying IR `Node`
  3. Given a non-Proxy Tensor object, emit IR for various cases:
>      * For a Parameter, emit a `get_attr` node referring to that Parameter
>      * For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.


This method can be overridden to support more types. 

Parameters
    
**a** (_Any_) – The value to be emitted as an `Argument` in the `Graph`. 

Returns
    
The value `a` converted into the appropriate `Argument` 

Return type
    
Argument
Note
Backwards-compatibility for this API is guaranteed. 

create_args_for_root(_root_fn_ , _is_module_ , _concrete_args =None_)[source][source]
    
Create `placeholder` nodes corresponding to the signature of the `root` Module. This method introspects root’s signature and emits those nodes accordingly, also supporting `*args` and `**kwargs`.
Warning
This API is experimental and is _NOT_ backward-compatible. 

create_node(_kind_ , _target_ , _args_ , _kwargs_ , _name =None_, _type_expr =None_)[source]
    
Inserts a graph node given target, args, kwargs, and name.
This method can be overridden to do extra checking, validation, or modification of values used in node creation. For example, one might want to disallow in-place operations from being recorded.
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_Node_ 

create_proxy(_kind_ , _target_ , _args_ , _kwargs_ , _name =None_, _type_expr =None_, _proxy_factory_fn =None_)[source]
    
Create a Node from the given arguments, then return the Node wrapped in a Proxy object.
If kind = ‘placeholder’, then we’re creating a Node that represents the parameter of a function. If we need to encode a default parameter, we use the `args` tuple. `args` is otherwise empty for `placeholder` Nodes.
Note
Backwards-compatibility for this API is guaranteed. 

get_fresh_qualname(_prefix_)[source][source]
    
Gets a fresh name for a prefix and returns it. This function ensures that it will not clash with an existing attribute on the graph.
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
str 

getattr(_attr_ , _attr_val_ , _parameter_proxy_cache_)[source][source]
    
Method that specifies the behavior of this `Tracer` when we call getattr on a call to an `nn.Module` instance.
By default, the behavior is to return a proxy value for the attribute. It also stores the proxy value in the `parameter_proxy_cache`, so that future calls will reuse the proxy rather than creating a new one.
This method can be overridden to –for example– not return proxies when querying parameters. 

Parameters
    
  * **attr** (_str_) – The name of the attribute being queried
  * **attr_val** (_Any_) – The value of the attribute
  * **parameter_proxy_cache** (_Dict_ _[__str_ _,__Any_ _]_) – A cache of attr names to proxies



Returns
    
The return value from the getattr call.
Warning
This API is experimental and is _NOT_ backward-compatible. 

is_leaf_module(_m_ , _module_qualified_name_)[source][source]
    
A method to specify whether a given `nn.Module` is a “leaf” module.
Leaf modules are the atomic units that appear in the IR, referenced by `call_module` calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter. 

Parameters
    
  * **m** (_Module_) – The module being queried about
  * **module_qualified_name** (_str_) – The path to root of this module. For example, if you have a module hierarchy where submodule `foo` contains submodule `bar`, which contains submodule `baz`, that module will appear with the qualified name `foo.bar.baz` here.



Return type
    
bool
Note
Backwards-compatibility for this API is guaranteed. 

iter(_obj_)[source]
     

Called when a proxy object is being iterated over, such as
    
when used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return an iterator.
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_Iterator_ 

keys(_obj_)[source]
     

Called when a proxy object is has the keys() method called.
    
This is what happens when ** is called on a proxy. This should return an iterator it ** is suppose to work in your custom tracer.
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_Any_ 

path_of_module(_mod_)[source][source]
    
Helper method to find the qualified name of `mod` in the Module hierarchy of `root`. For example, if `root` has a submodule named `foo`, which has a submodule named `bar`, passing `bar` into this function will return the string “foo.bar”. 

Parameters
    
**mod** (_str_) – The `Module` to retrieve the qualified name for. 

Return type
    
str
Note
Backwards-compatibility for this API is guaranteed. 

proxy(_node_)[source]
    
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_Proxy_ 

to_bool(_obj_)[source]
     

Called when a proxy object is being converted to a boolean, such as
    
when used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return a value.
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
bool 

trace(_root_ , _concrete_args =None_)[source][source]
    
Trace `root` and return the corresponding FX `Graph` representation. `root` can either be an `nn.Module` instance or a Python callable.
Note that after this call, `self.root` may be different from the `root` passed in here. For example, when a free function is passed to `trace()`, we will create an `nn.Module` instance to use as the root and add embedded constants to. 

Parameters
    
  * **root** (_Union_ _[__Module_ _,__Callable_ _]_) – Either a `Module` or a function to be traced through. Backwards-compatibility for this parameter is guaranteed.
  * **concrete_args** (_Optional_ _[__Dict_ _[__str_ _,__any_ _]__]_) – Concrete arguments that should not be treated as Proxies. This parameter is experimental and its backwards-compatibility is _NOT_ guaranteed.



Returns
    
A `Graph` representing the semantics of the passed-in `root`. 

Return type
    
_Graph_
Note
Backwards-compatibility for this API is guaranteed. 

_class_ torch.fx.Proxy(_node_ , _tracer =None_)[source][source]
    
`Proxy` objects are `Node` wrappers that flow through the program during symbolic tracing and record all the operations (`torch` function calls, method calls, operators) that they touch into the growing FX Graph.
If you’re doing graph transforms, you can wrap your own `Proxy` method around a raw `Node` so that you can use the overloaded operators to add additional things to a `Graph`.
`Proxy` objects cannot be iterated. In other words, the symbolic tracer will throw an error if a `Proxy` is used in a loop or as an `*args`/`**kwargs` function argument.
There are two main ways around this: 1. Factor out the untraceable logic into a top-level function and use `fx.wrap` on it. 2. If the control flow is static (i.e. the loop trip count is based on some hyperparameter), the code can be kept in its original position and refactored into something like:
```
for i in range(self.some_hyperparameter):
  indexed_item = proxied_value[i]

```
Copy to clipboard
For a more detailed description into the Proxy internals, check out the “Proxy” section in torch/fx/README.md
Note
Backwards-compatibility for this API is guaranteed. 

_class_ torch.fx.Interpreter(_module_ , _garbage_collect_values =True_, _graph =None_)[source][source]
    
An Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes.
Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy:
```
run()
  +-- run_node
    +-- placeholder()
    +-- get_attr()
    +-- call_function()
    +-- call_method()
    +-- call_module()
    +-- output()

```
Copy to clipboard
Example
Suppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and vice versa (including their `Tensor` method equivalents). We could subclass Interpreter like so:
```
class NegSigmSwapInterpreter(Interpreter):
  def call_function(self, target: Target, args: Tuple, kwargs: Dict) -> Any:
    if target == torch.sigmoid:
      return torch.neg(*args, **kwargs)
    return super().call_function(target, args, kwargs)
  def call_method(self, target: Target, args: Tuple, kwargs: Dict) -> Any:
    if target == "neg":
      call_self, *args_tail = args
      return call_self.sigmoid(*args_tail, **kwargs)
    return super().call_method(target, args, kwargs)

def fn(x):
  return torch.sigmoid(x).neg()

gm = torch.fx.symbolic_trace(fn)
input = torch.randn(3, 4)
result = NegSigmSwapInterpreter(gm).run(input)
torch.testing.assert_close(result, torch.neg(input).sigmoid())

```
Copy to clipboard 

Parameters
    
  * **module** (_torch.nn.Module_) – The module to be executed
  * **garbage_collect_values** (_bool_) – Whether to delete values after their last use within the Module’s execution. This ensures optimal memory usage during execution. This can be disabled to, for example, examine all of the intermediate values in the execution by looking at the `Interpreter.env` attribute.
  * **graph** (_Optional_ _[__Graph_ _]_) – If passed, the interpreter will execute this graph instead of module.graph, using the provided module argument to satisfy any requests for state.


Note
Backwards-compatibility for this API is guaranteed. 

boxed_run(_args_list_)[source][source]
    
Run module via interpretation and return the result. This uses the “boxed” calling convention, where you pass a list of arguments, which will be cleared by the interpreter. This ensures that input tensors are promptly deallocated.
Note
Backwards-compatibility for this API is guaranteed. 

call_function(_target_ , _args_ , _kwargs_)[source][source]
    
Execute a `call_function` node and return the result. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Return type
    
_Any_ 

Return
    
Any: The value returned by the function invocation
Note
Backwards-compatibility for this API is guaranteed. 

call_method(_target_ , _args_ , _kwargs_)[source][source]
    
Execute a `call_method` node and return the result. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Return type
    
_Any_ 

Return
    
Any: The value returned by the method invocation
Note
Backwards-compatibility for this API is guaranteed. 

call_module(_target_ , _args_ , _kwargs_)[source][source]
    
Execute a `call_module` node and return the result. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Return type
    
_Any_ 

Return
    
Any: The value returned by the module invocation
Note
Backwards-compatibility for this API is guaranteed. 

fetch_args_kwargs_from_env(_n_)[source][source]
    
Fetch the concrete values of `args` and `kwargs` of node `n` from the current execution environment. 

Parameters
    
**n** (_Node_) – The node for which `args` and `kwargs` should be fetched. 

Returns
    
`args` and `kwargs` with concrete values for `n`. 

Return type
    
Tuple[Tuple, Dict]
Note
Backwards-compatibility for this API is guaranteed. 

fetch_attr(_target_)[source][source]
    
Fetch an attribute from the `Module` hierarchy of `self.module`. 

Parameters
    
**target** (_str_) – The fully-qualified name of the attribute to fetch 

Returns
    
The value of the attribute. 

Return type
    
Any
Note
Backwards-compatibility for this API is guaranteed. 

get_attr(_target_ , _args_ , _kwargs_)[source][source]
    
Execute a `get_attr` node. Will retrieve an attribute value from the `Module` hierarchy of `self.module`. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Returns
    
The value of the attribute that was retrieved 

Return type
    
Any
Note
Backwards-compatibility for this API is guaranteed. 

map_nodes_to_values(_args_ , _n_)[source][source]
    
Recursively descend through `args` and look up the concrete value for each `Node` in the current execution environment. 

Parameters
    
  * **args** (_Argument_) – Data structure within which to look up concrete values
  * **n** (_Node_) – Node to which `args` belongs. This is only used for error reporting.



Return type
    
_Optional_[_Union_[tuple[‘Argument’, …], _Sequence_[Argument], _Mapping_[str, Argument], slice, range, _Node_, str, int, float, bool, complex, _dtype_, _Tensor_, _device_, _memory_format_, _layout_, _OpOverload_ , _SymInt_, _SymBool_, _SymFloat_]]
Note
Backwards-compatibility for this API is guaranteed. 

output(_target_ , _args_ , _kwargs_)[source][source]
    
Execute an `output` node. This really just retrieves the value referenced by the `output` node and returns it. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Returns
    
The return value referenced by the output node 

Return type
    
Any
Note
Backwards-compatibility for this API is guaranteed. 

placeholder(_target_ , _args_ , _kwargs_)[source][source]
    
Execute a `placeholder` node. Note that this is stateful: `Interpreter` maintains an internal iterator over arguments passed to `run` and this method returns next() on that iterator. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Returns
    
The argument value that was retrieved. 

Return type
    
Any
Note
Backwards-compatibility for this API is guaranteed. 

run(_* args_, _initial_env =None_, _enable_io_processing =True_)[source][source]
    
Run module via interpretation and return the result. 

Parameters
    
  * ***args** – The arguments to the Module to run, in positional order
  * **initial_env** (_Optional_ _[__Dict_ _[__Node_ _,__Any_ _]__]_) – An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.
  * **enable_io_processing** (_bool_) – If true, we process the inputs and outputs with graph’s process_inputs and process_outputs function first before using them.



Returns
    
The value returned from executing the Module 

Return type
    
Any
Note
Backwards-compatibility for this API is guaranteed. 

run_node(_n_)[source][source]
    
Run a specific node `n` and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on `node.op` 

Parameters
    
**n** (_Node_) – The Node to execute 

Returns
    
The result of executing `n` 

Return type
    
Any
Note
Backwards-compatibility for this API is guaranteed. 

_class_ torch.fx.Transformer(_module_)[source][source]
    
`Transformer` is a special type of interpreter that produces a new `Module`. It exposes a `transform()` method that returns the transformed `Module`. `Transformer` does not require arguments to run, as `Interpreter` does. `Transformer` works entirely symbolically.
Example
Suppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and vice versa (including their `Tensor` method equivalents). We could subclass `Transformer` like so:
```
class NegSigmSwapXformer(Transformer):
  def call_function(
    self, target: "Target", args: Tuple[Argument, ...], kwargs: Dict[str, Any]
  ) -> Any:
    if target == torch.sigmoid:
      return torch.neg(*args, **kwargs)
    return super().call_function(target, args, kwargs)
  def call_method(
    self, target: "Target", args: Tuple[Argument, ...], kwargs: Dict[str, Any]
  ) -> Any:
    if target == "neg":
      call_self, *args_tail = args
      return call_self.sigmoid(*args_tail, **kwargs)
    return super().call_method(target, args, kwargs)

def fn(x):
  return torch.sigmoid(x).neg()

gm = torch.fx.symbolic_trace(fn)
transformed: torch.nn.Module = NegSigmSwapXformer(gm).transform()
input = torch.randn(3, 4)
torch.testing.assert_close(transformed(input), torch.neg(input).sigmoid())

```
Copy to clipboard 

Parameters
    
**module** (_GraphModule_) – The `Module` to be transformed.
Note
Backwards-compatibility for this API is guaranteed. 

call_function(_target_ , _args_ , _kwargs_)[source][source]
    
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_Any_ 

call_module(_target_ , _args_ , _kwargs_)[source][source]
    
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_Any_ 

get_attr(_target_ , _args_ , _kwargs_)[source][source]
    
Execute a `get_attr` node. In `Transformer`, this is overridden to insert a new `get_attr` node into the output graph. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Return type
    
_Proxy_
Note
Backwards-compatibility for this API is guaranteed. 

placeholder(_target_ , _args_ , _kwargs_)[source][source]
    
Execute a `placeholder` node. In `Transformer`, this is overridden to insert a new `placeholder` into the output graph. 

Parameters
    
  * **target** (_Target_) – The call target for this node. See Node for details on semantics
  * **args** (_Tuple_) – Tuple of positional args for this invocation
  * **kwargs** (_Dict_) – Dict of keyword arguments for this invocation



Return type
    
_Proxy_
Note
Backwards-compatibility for this API is guaranteed. 

transform()[source][source]
    
Transform `self.module` and return the transformed `GraphModule`.
Note
Backwards-compatibility for this API is guaranteed. 

Return type
    
_GraphModule_ 

torch.fx.replace_pattern(_gm_ , _pattern_ , _replacement_)[source][source]
    
Matches all possible non-overlapping sets of operators and their data dependencies (`pattern`) in the Graph of a GraphModule (`gm`), then replaces each of these matched subgraphs with another subgraph (`replacement`). 

Parameters
    
  * **gm** (_GraphModule_) – The GraphModule that wraps the Graph to operate on
  * **pattern** (_Union_ _[__Callable_ _,__GraphModule_ _]_) – The subgraph to match in `gm` for replacement
  * **replacement** (_Union_ _[__Callable_ _,__GraphModule_ _]_) – The subgraph to replace `pattern` with



Returns
    
A list of `Match` objects representing the places in the original graph that `pattern` was matched to. The list is empty if there are no matches. `Match` is defined as:
```
class Match(NamedTuple):
  # Node from which the match was found
  anchor: Node
  # Maps nodes in the pattern subgraph to nodes in the larger graph
  nodes_map: Dict[Node, Node]

```
Copy to clipboard 

Return type
    
List[Match]
Examples:
```
import torch
from torch.fx import symbolic_trace, subgraph_rewriter

class M(torch.nn.Module):
  def __init__(self) -> None:
    super().__init__()
  def forward(self, x, w1, w2):
    m1 = torch.cat([w1, w2]).sum()
    m2 = torch.cat([w1, w2]).sum()
    return x + torch.max(m1) + torch.max(m2)

def pattern(w1, w2):
  return torch.cat([w1, w2])

def replacement(w1, w2):
  return torch.stack([w1, w2])

traced_module = symbolic_trace(M())
subgraph_rewriter.replace_pattern(traced_module, pattern, replacement)

```
Copy to clipboard
The above code will first match `pattern` in the `forward` method of `traced_module`. Pattern-matching is done based on use-def relationships, not node names. For example, if you had `p = torch.cat([a, b])` in `pattern`, you could match `m = torch.cat([a, b])` in the original `forward` function, despite the variable names being different (`p` vs `m`).
The `return` statement in `pattern` is matched based on its value only; it may or may not match to the `return` statement in the larger graph. In other words, the pattern doesn’t have to extend to the end of the larger graph.
When the pattern is matched, it will be removed from the larger function and replaced by `replacement`. If there are multiple matches for `pattern` in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (“First” here being defined as the first in a topological ordering of the Nodes’ use-def relationships. In most cases, the first Node is the parameter that appears directly after `self`, while the last Node is whatever the function returns.)
One important thing to note is that the parameters of the `pattern` Callable must be used in the Callable itself, and the parameters of the `replacement` Callable must match the pattern. The first rule is why, in the above code block, the `forward` function has parameters `x, w1, w2`, but the `pattern` function only has parameters `w1, w2`. `pattern` doesn’t use `x`, so it shouldn’t specify `x` as a parameter. As an example of the second rule, consider replacing
```
def pattern(x, y):
  return torch.neg(x) + torch.relu(y)

```
Copy to clipboard
with
```
def replacement(x, y):
  return torch.relu(x)

```
Copy to clipboard
In this case, `replacement` needs the same number of parameters as `pattern` (both `x` and `y`), even though the parameter `y` isn’t used in `replacement`.
After calling `subgraph_rewriter.replace_pattern`, the generated Python code looks like this:
```
def forward(self, x, w1, w2):
  stack_1 = torch.stack([w1, w2])
  sum_1 = stack_1.sum()
  stack_2 = torch.stack([w1, w2])
  sum_2 = stack_2.sum()
  max_1 = torch.max(sum_1)
  add_1 = x + max_1
  max_2 = torch.max(sum_2)
  add_2 = add_1 + max_2
  return add_2

```
Copy to clipboard
Note
Backwards-compatibility for this API is guaranteed.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.fx
    * Overview
    * Writing Transformations
      * A Quick Primer on Graphs
      * Graph Manipulation
        * Direct Graph Manipulation
        * Subgraph Rewriting With replace_pattern()
        * Graph Manipulation Examples
      * Proxy/Retracing
      * The Interpreter Pattern
        * Examples of the Interpreter Pattern
    * Debugging
      * Introduction
      * Common Pitfalls in Transform Authoring
      * Checking Correctness of Modules
      * Debugging the Generated Code
        * Use `pdb`
        * Print the Generated Code
        * Use the `to_folder` Function From `GraphModule`
      * Debugging the Transformation
      * Available Debuggers
    * Limitations of Symbolic Tracing
      * Dynamic Control Flow
        * Static Control Flow
      * Non-`torch` Functions
      * Customizing Tracing with the `Tracer` class
        * Leaf Modules
      * Miscellanea
    * API Reference
      * `symbolic_trace()`
      * `wrap()`
      * `GraphModule`
        * `GraphModule.__init__()`
        * `GraphModule.add_submodule()`
        * `GraphModule.code`
        * `GraphModule.delete_all_unused_submodules()`
        * `GraphModule.delete_submodule()`
        * `GraphModule.graph`
        * `GraphModule.print_readable()`
        * `GraphModule.recompile()`
        * `GraphModule.to_folder()`
      * `Graph`
        * `Graph.__init__()`
        * `Graph.call_function()`
        * `Graph.call_method()`
        * `Graph.call_module()`
        * `Graph.create_node()`
        * `Graph.eliminate_dead_code()`
        * `Graph.erase_node()`
        * `Graph.find_nodes()`
        * `Graph.get_attr()`
        * `Graph.graph_copy()`
        * `Graph.inserting_after()`
        * `Graph.inserting_before()`
        * `Graph.lint()`
        * `Graph.node_copy()`
        * `Graph.nodes`
        * `Graph.on_generate_code()`
        * `Graph.output()`
        * `Graph.output_node()`
        * `Graph.placeholder()`
        * `Graph.print_tabular()`
        * `Graph.process_inputs()`
        * `Graph.process_outputs()`
        * `Graph.python_code()`
        * `Graph.set_codegen()`
      * `Node`
        * `Node.all_input_nodes`
        * `Node.append()`
        * `Node.args`
        * `Node.format_node()`
        * `Node.insert_arg()`
        * `Node.is_impure()`
        * `Node.kwargs`
        * `Node.next`
        * `Node.normalized_arguments()`
        * `Node.prepend()`
        * `Node.prev`
        * `Node.replace_all_uses_with()`
        * `Node.replace_input_with()`
        * `Node.stack_trace`
        * `Node.update_arg()`
        * `Node.update_kwarg()`
      * `Tracer`
        * `Tracer.call_module()`
        * `Tracer.create_arg()`
        * `Tracer.create_args_for_root()`
        * `Tracer.create_node()`
        * `Tracer.create_proxy()`
        * `Tracer.get_fresh_qualname()`
        * `Tracer.getattr()`
        * `Tracer.is_leaf_module()`
        * `Tracer.iter()`
        * `Tracer.keys()`
        * `Tracer.path_of_module()`
        * `Tracer.proxy()`
        * `Tracer.to_bool()`
        * `Tracer.trace()`
      * `Proxy`
      * `Interpreter`
        * `Interpreter.boxed_run()`
        * `Interpreter.call_function()`
        * `Interpreter.call_method()`
        * `Interpreter.call_module()`
        * `Interpreter.fetch_args_kwargs_from_env()`
        * `Interpreter.fetch_attr()`
        * `Interpreter.get_attr()`
        * `Interpreter.map_nodes_to_values()`
        * `Interpreter.output()`
        * `Interpreter.placeholder()`
        * `Interpreter.run()`
        * `Interpreter.run_node()`
      * `Transformer`
        * `Transformer.call_function()`
        * `Transformer.call_module()`
        * `Transformer.get_attr()`
        * `Transformer.placeholder()`
        * `Transformer.transform()`
      * `replace_pattern()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * JIT Utils - torch.utils.jit
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# JIT Utils - torch.utils.jit
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * JIT Utils - torch.utils.jit


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch documentation
  * Edit on GitHub


Shortcuts 
# PyTorch documentation
PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
Features described in this documentation are classified by release status:
> _Stable:_ These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).
> _Beta:_ These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.
> _Prototype:_ These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.
Community
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings
  * C++
  * Javadoc
  * torch::deploy


Python API
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


# Indices and tables
  * Index
  * Module Index


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg)
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch documentation
  * Indices and tables


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.futures
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.futures
This package provides a `Future` type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on `Future` objects. Currently, the `Future` type is primarily used by the Distributed RPC Framework. 

_class_ torch.futures.Future(_*_ , _devices =None_)
    
Wrapper around a `torch._C.Future` which encapsulates an asynchronous execution of a callable, e.g. `rpc_async()`. It also exposes a set of APIs to add callback functions and set results.
Warning
GPU support is a beta feature, subject to changes. 

add_done_callback(_callback_)[source][source]
    
Append the given callback function to this `Future`, which will be run when the `Future` is completed. Multiple callbacks can be added to the same `Future`, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this `Future`. The callback function can use the `value()` method to get the value. Note that if this `Future` is already completed, the given callback will be run inline.
We recommend that you use the `then()` method as it provides a way to synchronize after your callback has completed. `add_done_callback` can be cheaper if your callback does not return anything. But both `then()` and `add_done_callback` use the same callback registration API under the hood.
With respect to GPU tensors, this method behaves in the same way as `then()`. 

Parameters
    
**callback** (`Future`) – a `Callable` that takes in one argument, which is the reference to this `Future`.
Note
Note that if the callback function throws, either through the original future being completed with an exception and calling `fut.wait()`, or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently. 

Example::
    
```
>>> def callback(fut):
...   print("This will run after the future has finished.")
...   print(fut.wait())
>>> fut = torch.futures.Future()
>>> fut.add_done_callback(callback)
>>> fut.set_result(5)
This will run after the future has finished.
5

```
Copy to clipboard 

done()[source][source]
    
Return `True` if this `Future` is done. A `Future` is done if it has a result or an exception.
If the value contains tensors that reside on GPUs, `Future.done()` will return `True` even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see `wait()`). 

Return type
    
bool 

set_exception(_result_)[source][source]
    
Set an exception for this `Future`, which will mark this `Future` as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this `Future`, the exception set here will be raised inline. 

Parameters
    
**result** (_BaseException_) – the exception for this `Future`. 

Example::
    
```
>>> fut = torch.futures.Future()
>>> fut.set_exception(ValueError("foo"))
>>> fut.wait()
Traceback (most recent call last):
...
ValueError: foo

```
Copy to clipboard 

set_result(_result_)[source][source]
    
Set the result for this `Future`, which will mark this `Future` as completed and trigger all attached callbacks. Note that a `Future` cannot be marked completed twice.
If the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it’s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn’t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this `Future`. 

Parameters
    
**result** (_object_) – the result object of this `Future`. 

Example::
    
```
>>> import threading
>>> import time
>>> def slow_set_future(fut, value):
...   time.sleep(0.5)
...   fut.set_result(value)
>>> fut = torch.futures.Future()
>>> t = threading.Thread(
...   target=slow_set_future,
...   args=(fut, torch.ones(2) * 3)
... )
>>> t.start()
>>> print(fut.wait())
tensor([3., 3.])
>>> t.join()

```
Copy to clipboard 

then(_callback_)[source][source]
    
Append the given callback function to this `Future`, which will be run when the `Future` is completed. Multiple callbacks can be added to the same `Future`, but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: `fut.then(cb1).then(cb2)`). The callback must take one argument, which is the reference to this `Future`. The callback function can use the `value()` method to get the value. Note that if this `Future` is already completed, the given callback will be run immediately inline.
If the `Future`’s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven’t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn’t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of `wait()`.
Similarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn’t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked. 

Parameters
    
**callback** (`Callable`) – a `Callable` that takes this `Future` as the only argument. 

Returns
    
A new `Future` object that holds the return value of the `callback` and will be marked as completed when the given `callback` finishes. 

Return type
    
_Future_[_S_]
Note
Note that if the callback function throws, either through the original future being completed with an exception and calling `fut.wait()`, or through other code in the callback, the future returned by `then` will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently. 

Example::
    
```
>>> def callback(fut):
...   print(f"RPC return value is {fut.wait()}.")
>>> fut = torch.futures.Future()
>>> # The inserted callback will print the return value when
>>> # receiving the response from "worker1"
>>> cb_fut = fut.then(callback)
>>> chain_cb_fut = cb_fut.then(
...   lambda x : print(f"Chained cb done. {x.wait()}")
... )
>>> fut.set_result(5)
RPC return value is 5.
Chained cb done. None

```
Copy to clipboard 

value()[source][source]
    
Obtain the value of an already-completed future.
This method should only be called after a call to `wait()` has completed, or inside a callback function passed to `then()`. In other cases this `Future` may not yet hold a value and calling `value()` could fail.
If the value contains tensors that reside on GPUs, then this method will _not_ perform any additional synchronization. This should be done beforehand, separately, through a call to `wait()` (except within callbacks, for which it’s already being taken care of by `then()`). 

Returns
    
The value held by this `Future`. If the function (callback or RPC) creating the value has thrown an error, this `value()` method will also throw an error. 

Return type
    
_T_ 

wait()[source][source]
    
Block until the value of this `Future` is ready.
If the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that `wait()` will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, `wait()` will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn’t change streams. 

Returns
    
The value held by this `Future`. If the function (callback or RPC) creating the value has thrown an error, this `wait` method will also throw an error. 

Return type
    
_T_ 

torch.futures.collect_all(_futures_)[source][source]
    
Collects the provided `Future` objects into a single combined `Future` that is completed when all of the sub-futures are completed. 

Parameters
    
**futures** (_list_) – a list of `Future` objects. 

Returns
    
Returns a `Future` object to a list of the passed in Futures. 

Return type
    
_Future_[list[torch.jit.Future]] 

Example::
    
```
>>> fut0 = torch.futures.Future()
>>> fut1 = torch.futures.Future()
>>> fut = torch.futures.collect_all([fut0, fut1])
>>> fut0.set_result(0)
>>> fut1.set_result(1)
>>> fut_list = fut.wait()
>>> print(f"fut0 result = {fut_list[0].wait()}")
fut0 result = 0
>>> print(f"fut1 result = {fut_list[1].wait()}")
fut1 result = 1

```
Copy to clipboard 

torch.futures.wait_all(_futures_)[source][source]
    
Waits for all provided futures to be complete, and returns the list of completed values. If any of the futures encounters an error, the method will exit early and report the error not waiting for other futures to complete. 

Parameters
    
**futures** (_list_) – a list of `Future` object. 

Returns
    
A list of the completed `Future` results. This method will throw an error if `wait` on any `Future` throws. 

Return type
    
list
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.futures
    * `Future`
      * `Future.add_done_callback()`
      * `Future.done()`
      * `Future.set_exception()`
      * `Future.set_result()`
      * `Future.then()`
      * `Future.value()`
      * `Future.wait()`
    * `collect_all()`
    * `wait_all()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.hub
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.hub
Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.
## Publishing models
Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a GitHub repository by adding a simple `hubconf.py` file;
`hubconf.py` can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish).
```
def entrypoint_name(*args, **kwargs):
  # args & kwargs are optional, for models which take positional/keyword arguments.
  ...

```
Copy to clipboard
### How to implement an entrypoint?
Here is a code snippet specifies an entrypoint for `resnet18` model if we expand the implementation in `pytorch/vision/hubconf.py`. In most case importing the right function in `hubconf.py` is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in pytorch/vision repo
```
dependencies = ['torch']
from torchvision.models.resnet import resnet18 as _resnet18
# resnet18 is the name of entrypoint
def resnet18(pretrained=False, **kwargs):
""" # This docstring shows up in hub.help()
  Resnet18 model
  pretrained (bool): kwargs, load pretrained weights into the model
  """
  # Call the model, load pretrained weights
  model = _resnet18(pretrained=pretrained, **kwargs)
  return model

```
Copy to clipboard
  * `dependencies` variable is a **list** of package names required to **load** the model. Note this might be slightly different from dependencies required for training a model.
  * `args` and `kwargs` are passed along to the real callable function.
  * Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It’s highly recommended to add a few examples here.
  * Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.
  * Callables prefixed with underscore are considered as helper functions which won’t show up in `torch.hub.list()`.
  * Pretrained weights can either be stored locally in the GitHub repo, or loadable by `torch.hub.load_state_dict_from_url()`. If less than 2GB, it’s recommended to attach it to a project release and use the url from the release. In the example above `torchvision.models.resnet.resnet18` handles `pretrained`, alternatively you can put the following logic in the entrypoint definition.


```
if pretrained:
  # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth
  dirname = os.path.dirname(__file__)
  checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)
  state_dict = torch.load(checkpoint)
  model.load_state_dict(state_dict)
  # For checkpoint saved elsewhere
  checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'
  model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))

```
Copy to clipboard
### Important Notice
  * The published models should be at least in a branch/tag. It can’t be a random commit.


## Loading models from Hub
Pytorch Hub provides convenient APIs to explore all available models in hub through `torch.hub.list()`, show docstring and examples through `torch.hub.help()` and load the pre-trained models using `torch.hub.load()`. 

torch.hub.list(_github_ , _force_reload =False_, _skip_validation =False_, _trust_repo =None_, _verbose =True_)[source][source]
    
List all callable entrypoints available in the repo specified by `github`. 

Parameters
    
  * **github** (_str_) – a string with format “repo_owner/repo_name[:ref]” with an optional ref (tag or branch). If `ref` is not specified, the default branch is assumed to be `main` if it exists, and otherwise `master`. Example: ‘pytorch/vision:0.10’
  * **force_reload** (_bool_ _,__optional_) – whether to discard the existing cache and force a fresh download. Default is `False`.
  * **skip_validation** (_bool_ _,__optional_) – if `False`, torchhub will check that the branch or commit specified by the `github` argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the `GITHUB_TOKEN` environment variable. Default is `False`.
  * **trust_repo** (_bool_ _,__str_ _or_ _None_) – 
`"check"`, `True`, `False` or `None`. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.
    * If `False`, a prompt will ask the user whether the repo should be trusted.
    * If `True`, the repo will be added to the trusted list and loaded without requiring explicit confirmation.
    * If `"check"`, the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the `trust_repo=False` option.
    * If `None`: this will raise a warning, inviting the user to set `trust_repo` to either `False`, `True` or `"check"`. This is only present for backward compatibility and will be removed in v2.0.
Default is `None` and will eventually change to `"check"` in v2.0.
  * **verbose** (_bool_ _,__optional_) – If `False`, mute messages about hitting local caches. Note that the message about first download cannot be muted. Default is `True`.



Returns
    
The available callables entrypoint 

Return type
    
list
Example
```
>>> entrypoints = torch.hub.list("pytorch/vision", force_reload=True)

```
Copy to clipboard 

torch.hub.help(_github_ , _model_ , _force_reload =False_, _skip_validation =False_, _trust_repo =None_)[source][source]
    
Show the docstring of entrypoint `model`. 

Parameters
    
  * **github** (_str_) – a string with format <repo_owner/repo_name[:ref]> with an optional ref (a tag or a branch). If `ref` is not specified, the default branch is assumed to be `main` if it exists, and otherwise `master`. Example: ‘pytorch/vision:0.10’
  * **model** (_str_) – a string of entrypoint name defined in repo’s `hubconf.py`
  * **force_reload** (_bool_ _,__optional_) – whether to discard the existing cache and force a fresh download. Default is `False`.
  * **skip_validation** (_bool_ _,__optional_) – if `False`, torchhub will check that the ref specified by the `github` argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the `GITHUB_TOKEN` environment variable. Default is `False`.
  * **trust_repo** (_bool_ _,__str_ _or_ _None_) – 
`"check"`, `True`, `False` or `None`. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.
    * If `False`, a prompt will ask the user whether the repo should be trusted.
    * If `True`, the repo will be added to the trusted list and loaded without requiring explicit confirmation.
    * If `"check"`, the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the `trust_repo=False` option.
    * If `None`: this will raise a warning, inviting the user to set `trust_repo` to either `False`, `True` or `"check"`. This is only present for backward compatibility and will be removed in v2.0.
Default is `None` and will eventually change to `"check"` in v2.0.


Example
```
>>> print(torch.hub.help("pytorch/vision", "resnet18", force_reload=True))

```
Copy to clipboard 

torch.hub.load(_repo_or_dir_ , _model_ , _* args_, _source ='github'_, _trust_repo =None_, _force_reload =False_, _verbose =True_, _skip_validation =False_, _** kwargs_)[source][source]
    
Load a model from a github repo or a local directory.
Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.
If `source` is ‘github’, `repo_or_dir` is expected to be of the form `repo_owner/repo_name[:ref]` with an optional ref (a tag or a branch).
If `source` is ‘local’, `repo_or_dir` is expected to be a path to a local directory. 

Parameters
    
  * **repo_or_dir** (_str_) – If `source` is ‘github’, this should correspond to a github repo with format `repo_owner/repo_name[:ref]` with an optional ref (tag or branch), for example ‘pytorch/vision:0.10’. If `ref` is not specified, the default branch is assumed to be `main` if it exists, and otherwise `master`. If `source` is ‘local’ then it should be a path to a local directory.
  * **model** (_str_) – the name of a callable (entrypoint) defined in the repo/dir’s `hubconf.py`.
  * ***args** (_optional_) – the corresponding args for callable `model`.
  * **source** (_str_ _,__optional_) – ‘github’ or ‘local’. Specifies how `repo_or_dir` is to be interpreted. Default is ‘github’.
  * **trust_repo** (_bool_ _,__str_ _or_ _None_) – 
`"check"`, `True`, `False` or `None`. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.
    * If `False`, a prompt will ask the user whether the repo should be trusted.
    * If `True`, the repo will be added to the trusted list and loaded without requiring explicit confirmation.
    * If `"check"`, the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the `trust_repo=False` option.
    * If `None`: this will raise a warning, inviting the user to set `trust_repo` to either `False`, `True` or `"check"`. This is only present for backward compatibility and will be removed in v2.0.
Default is `None` and will eventually change to `"check"` in v2.0.
  * **force_reload** (_bool_ _,__optional_) – whether to force a fresh download of the github repo unconditionally. Does not have any effect if `source = 'local'`. Default is `False`.
  * **verbose** (_bool_ _,__optional_) – If `False`, mute messages about hitting local caches. Note that the message about first download cannot be muted. Does not have any effect if `source = 'local'`. Default is `True`.
  * **skip_validation** (_bool_ _,__optional_) – if `False`, torchhub will check that the branch or commit specified by the `github` argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the `GITHUB_TOKEN` environment variable. Default is `False`.
  * ****kwargs** (_optional_) – the corresponding kwargs for callable `model`.



Returns
    
The output of the `model` callable when called with the given `*args` and `**kwargs`.
Example
```
>>> # from a github repo
>>> repo = "pytorch/vision"
>>> model = torch.hub.load(
...   repo, "resnet50", weights="ResNet50_Weights.IMAGENET1K_V1"
... )
>>> # from a local directory
>>> path = "/some/local/path/pytorch/vision"
>>> model = torch.hub.load(path, "resnet50", weights="ResNet50_Weights.DEFAULT")

```
Copy to clipboard 

torch.hub.download_url_to_file(_url_ , _dst_ , _hash_prefix =None_, _progress =True_)[source][source]
    
Download object at the given URL to a local path. 

Parameters
    
  * **url** (_str_) – URL of the object to download
  * **dst** (_str_) – Full path where object will be saved, e.g. `/tmp/temporary_file`
  * **hash_prefix** (_str_ _,__optional_) – If not None, the SHA256 downloaded file should start with `hash_prefix`. Default: None
  * **progress** (_bool_ _,__optional_) – whether or not to display a progress bar to stderr Default: True


Example
```
>>> torch.hub.download_url_to_file(
...   "https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth",
...   "/tmp/temporary_file",
... )

```
Copy to clipboard 

torch.hub.load_state_dict_from_url(_url_ , _model_dir =None_, _map_location =None_, _progress =True_, _check_hash =False_, _file_name =None_, _weights_only =False_)[source][source]
    
Loads the Torch serialized object at the given URL.
If downloaded file is a zip file, it will be automatically decompressed.
If the object is already present in model_dir, it’s deserialized and returned. The default value of `model_dir` is `<hub_dir>/checkpoints` where `hub_dir` is the directory returned by `get_dir()`. 

Parameters
    
  * **url** (_str_) – URL of the object to download
  * **model_dir** (_str_ _,__optional_) – directory in which to save the object
  * **map_location** (_optional_) – a function or a dict specifying how to remap storage locations (see torch.load)
  * **progress** (_bool_ _,__optional_) – whether or not to display a progress bar to stderr. Default: True
  * **check_hash** (_bool_ _,__optional_) – If True, the filename part of the URL should follow the naming convention `filename-<sha256>.ext` where `<sha256>` is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False
  * **file_name** (_str_ _,__optional_) – name for the downloaded file. Filename from `url` will be used if not set.
  * **weights_only** (_bool_ _,__optional_) – If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See `load()` for more details.



Return type
    
dict[str, _Any_]
Example
```
>>> state_dict = torch.hub.load_state_dict_from_url(
...   "https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth"
... )

```
Copy to clipboard
### Running a loaded model:
Note that `*args` and `**kwargs` in `torch.hub.load()` are used to **instantiate** a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is
  * `dir(model)` to see all available methods of the model.
  * `help(model.foo)` to check what arguments `model.foo` takes to run


To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It’s also helpful to include a minimal working example.
### Where are my downloaded models saved?
The locations are used in the order of
  * Calling `hub.set_dir(<PATH_TO_HUB_DIR>)`
  * `$TORCH_HOME/hub`, if environment variable `TORCH_HOME` is set.
  * `$XDG_CACHE_HOME/torch/hub`, if environment variable `XDG_CACHE_HOME` is set.
  * `~/.cache/torch/hub`



torch.hub.get_dir()[source][source]
    
Get the Torch Hub cache directory used for storing downloaded models & weights.
If `set_dir()` is not called, default path is `$TORCH_HOME/hub` where environment variable `$TORCH_HOME` defaults to `$XDG_CACHE_HOME/torch`. `$XDG_CACHE_HOME` follows the X Design Group specification of the Linux filesystem layout, with a default value `~/.cache` if the environment variable is not set. 

Return type
    
str 

torch.hub.set_dir(_d_)[source][source]
    
Optionally set the Torch Hub directory used to save downloaded models & weights. 

Parameters
    
**d** (_str_) – path to a local folder to save downloaded models & weights.
### Caching logic
By default, we don’t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by `get_dir()`.
Users can force a reload by calling `hub.load(..., force_reload=True)`. This will delete the existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.
### Known limitations:
Torch hub works by importing the package as if it was installed. There are some side effects introduced by importing in Python. For example, you can see new items in Python caches `sys.modules` and `sys.path_importer_cache` which is normal Python behavior. This also means that you may have import errors when importing different models from different repos, if the repos have the same sub-package names (typically, a `model` subpackage). A workaround for these kinds of import errors is to remove the offending sub-package from the `sys.modules` dict; more details can be found in this GitHub issue.
A known limitation that is worth mentioning here: users **CANNOT** load two different branches of the same repo in the **same python process**. It’s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it’s totally fine to load them in separate processes.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.hub
    * Publishing models
      * How to implement an entrypoint?
      * Important Notice
    * Loading models from Hub
      * `list()`
      * `help()`
      * `load()`
      * `download_url_to_file()`
      * `load_state_dict_from_url()`
      * Running a loaded model:
      * Where are my downloaded models saved?
        * `get_dir()`
        * `set_dir()`
      * Caching logic
      * Known limitations:


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * TorchScript
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# TorchScript
  * TorchScript Language Reference


  * Creating TorchScript Code
  * Mixing Tracing and Scripting
  * TorchScript Language
  * Built-in Functions and Modules
    * PyTorch Functions and Modules
    * Python Functions and Modules
    * Python Language Reference Comparison
  * Debugging
    * Disable JIT for Debugging
    * Inspecting Code
    * Interpreting Graphs
    * Tracer
  * Frequently Asked Questions
  * Known Issues
  * Appendix
    * Migrating to PyTorch 1.2 Recursive Scripting API
    * Fusion Backends
    * References


TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.
We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.
For a gentle introduction to TorchScript, see the Introduction to TorchScript tutorial.
For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the Loading a PyTorch Model in C++ tutorial.
## Creating TorchScript Code
`script` | Script the function.  
---|---  
`trace` | Trace a function and return an executable or `ScriptFunction` that will be optimized using just-in-time compilation.  
`script_if_tracing` | Compiles `fn` when it is first called during tracing.  
`trace_module` | Trace a module and return an executable `ScriptModule` that will be optimized using just-in-time compilation.  
`fork` | Create an asynchronous task executing func and a reference to the value of the result of this execution.  
`wait` | Force completion of a torch.jit.Future[T] asynchronous task, returning the result of the task.  
`ScriptModule` | Wrapper for C++ torch::jit::Module with methods, attributes, and parameters.  
`ScriptFunction` | Functionally equivalent to a `ScriptModule`, but represents a single function and does not have any attributes or Parameters.  
`freeze` | Freeze ScriptModule, inline submodules, and attributes as constants.  
`optimize_for_inference` | Perform a set of optimization passes to optimize a model for the purposes of inference.  
`enable_onednn_fusion` | Enable or disables onednn JIT fusion based on the parameter enabled.  
`onednn_fusion_enabled` | Return whether onednn JIT fusion is enabled.  
`set_fusion_strategy` | Set the type and number of specializations that can occur during fusion.  
`strict_fusion` | Give errors if not all nodes have been fused in inference, or symbolically differentiated in training.  
`save` | Save an offline version of this module for use in a separate process.  
`load` | Load a `ScriptModule` or `ScriptFunction` previously saved with `torch.jit.save`.  
`ignore` | This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.  
`unused` | This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.  
`interface` | Decorate to annotate classes or modules of different types.  
`isinstance` | Provide container type refinement in TorchScript.  
`Attribute` | This method is a pass-through function that returns value, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type of type.  
`annotate` | Use to give type of the_value in TorchScript compiler.  
## Mixing Tracing and Scripting
In many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model.
Scripted functions can call traced functions. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.
Example (calling a traced function in script):
```
import torch
def foo(x, y):
  return 2 * x + y
traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))
@torch.jit.script
def bar(x):
  return traced_foo(x, x)

```
Copy to clipboard
Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.
Example (calling a script function in a traced function):
```
import torch
@torch.jit.script
def foo(x, y):
  if x.max() > y.max():
    r = x
  else:
    r = y
  return r

def bar(x, y, z):
  return foo(x, y) + z
traced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)))

```
Copy to clipboard
This composition also works for `nn.Module`s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.
Example (using a traced module):
```
import torch
import torchvision
class MyScriptModule(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])
                    .resize_(1, 3, 1, 1))
    self.resnet = torch.jit.trace(torchvision.models.resnet18(),
                   torch.rand(1, 3, 224, 224))
  def forward(self, input):
    return self.resnet(input - self.means)
my_script_module = torch.jit.script(MyScriptModule())

```
Copy to clipboard
## TorchScript Language
TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full TorchScript Language Reference for details.
## Built-in Functions and Modules
TorchScript supports the use of most PyTorch functions and many Python built-ins. See TorchScript Builtins for a full reference of supported functions.
### PyTorch Functions and Modules
TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the `torch` namespace, all functions in `torch.nn.functional` and most modules from `torch.nn` are supported in TorchScript.
See TorchScript Unsupported PyTorch Constructs for a list of unsupported PyTorch functions and modules.
### Python Functions and Modules
Many of Python’s built-in functions are supported in TorchScript. The `math` module is also supported (see math Module for details), but no other Python modules (built-in or third party) are supported.
### Python Language Reference Comparison
For a full listing of supported Python features, see Python Language Reference Coverage.
## Debugging
### Disable JIT for Debugging 

PYTORCH_JIT

Setting the environment variable `PYTORCH_JIT=0` will disable all script and tracing annotations. If there is hard-to-debug error in one of your TorchScript models, you can use this flag to force everything to run using native Python. Since TorchScript (scripting and tracing) is disabled with this flag, you can use tools like `pdb` to debug the model code. For example:
```
@torch.jit.script
def scripted_fn(x : torch.Tensor):
  for i in range(12):
    x = x + x
  return x
def fn(x):
  x = torch.neg(x)
  import pdb; pdb.set_trace()
  return scripted_fn(x)
traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))
traced_fn(torch.rand(3, 4))

```
Copy to clipboard
Debugging this script with `pdb` works except for when we invoke the `@torch.jit.script` function. We can globally disable JIT, so that we can call the `@torch.jit.script` function as a normal Python function and not compile it. If the above script is called `disable_jit_example.py`, we can invoke it like so:
```
$ PYTORCH_JIT=0 python disable_jit_example.py

```
Copy to clipboard
and we will be able to step into the `@torch.jit.script` function as a normal Python function. To disable the TorchScript compiler for a specific function, see `@torch.jit.ignore`.
### Inspecting Code
TorchScript provides a code pretty-printer for all `ScriptModule` instances. This pretty-printer gives an interpretation of the script method’s code as valid Python syntax. For example:
```
@torch.jit.script
def foo(len):
  # type: (int) -> torch.Tensor
  rv = torch.zeros(3, 4)
  for i in range(len):
    if i < 10:
      rv = rv - 1.0
    else:
      rv = rv + 1.0
  return rv
print(foo.code)

```
Copy to clipboard
A `ScriptModule` with a single `forward` method will have an attribute `code`, which you can use to inspect the `ScriptModule`’s code. If the `ScriptModule` has more than one method, you will need to access `.code` on the method itself and not the module. We can inspect the code of a method named `foo` on a `ScriptModule` by accessing `.foo.code`. The example above produces this output:
```
def foo(len: int) -> Tensor:
  rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)
  rv0 = rv
  for i in range(len):
    if torch.lt(i, 10):
      rv1 = torch.sub(rv0, 1., 1)
    else:
      rv1 = torch.add(rv0, 1., 1)
    rv0 = rv1
  return rv0

```
Copy to clipboard
This is TorchScript’s compilation of the code for the `forward` method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.
### Interpreting Graphs
TorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.
TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:
```
@torch.jit.script
def foo(len):
  # type: (int) -> torch.Tensor
  rv = torch.zeros(3, 4)
  for i in range(len):
    if i < 10:
      rv = rv - 1.0
    else:
      rv = rv + 1.0
  return rv
print(foo.graph)

```
Copy to clipboard
`graph` follows the same rules described in the Inspecting Code section with regard to `forward` method lookup.
The example script above produces the graph:
```
graph(%len.1 : int):
 %24 : int = prim::Constant[value=1]()
 %17 : bool = prim::Constant[value=1]() # test.py:10:5
 %12 : bool? = prim::Constant()
 %10 : Device? = prim::Constant()
 %6 : int? = prim::Constant()
 %1 : int = prim::Constant[value=3]() # test.py:9:22
 %2 : int = prim::Constant[value=4]() # test.py:9:25
 %20 : int = prim::Constant[value=10]() # test.py:11:16
 %23 : float = prim::Constant[value=1]() # test.py:12:23
 %4 : int[] = prim::ListConstruct(%1, %2)
 %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10
 %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5
  block0(%i.1 : int, %rv.14 : Tensor):
   %21 : bool = aten::lt(%i.1, %20) # test.py:11:12
   %rv.13 : Tensor = prim::If(%21) # test.py:11:9
    block0():
     %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18
     -> (%rv.3)
    block1():
     %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18
     -> (%rv.6)
   -> (%17, %rv.13)
 return (%rv)

```
Copy to clipboard
Take the instruction `%rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10` for example.
  * `%rv.1 : Tensor` means we assign the output to a (unique) value named `rv.1`, that value is of `Tensor` type and that we do not know its concrete shape.
  * `aten::zeros` is the operator (equivalent to `torch.zeros`) and the input list `(%4, %6, %6, %10, %12)` specifies which values in scope should be passed as inputs. The schema for built-in functions like `aten::zeros` can be found at Builtin Functions.
  * `# test.py:9:10` is the location in the original source file that generated this instruction. In this case, it is a file named test.py, on line 9, and at character 10.


Notice that operators can also have associated `blocks`, namely the `prim::Loop` and `prim::If` operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging.
Graphs can be inspected as shown to confirm that the computation described by a `ScriptModule` is correct, in both automated and manual fashion, as described below.
### Tracer
#### Tracing Edge Cases
There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:
  * Tracing of control flow that is dependent on inputs (e.g. tensor shapes)
  * Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)


Note that these cases may in fact be traceable in the future.
#### Automatic Trace Checking
One way to automatically catch many errors in traces is by using `check_inputs` on the `torch.jit.trace()` API. `check_inputs` takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example:
```
def loop_in_traced_fn(x):
  result = x[0]
  for i in range(x.size(0)):
    result = result * x[i]
  return result
inputs = (torch.rand(3, 4, 5),)
check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]
traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)

```
Copy to clipboard
Gives us the following diagnostic information:
```
ERROR: Graphs differed across invocations!
Graph diff:
      graph(%x : Tensor) {
      %1 : int = prim::Constant[value=0]()
      %2 : int = prim::Constant[value=0]()
      %result.1 : Tensor = aten::select(%x, %1, %2)
      %4 : int = prim::Constant[value=0]()
      %5 : int = prim::Constant[value=0]()
      %6 : Tensor = aten::select(%x, %4, %5)
      %result.2 : Tensor = aten::mul(%result.1, %6)
      %8 : int = prim::Constant[value=0]()
      %9 : int = prim::Constant[value=1]()
      %10 : Tensor = aten::select(%x, %8, %9)
    -  %result : Tensor = aten::mul(%result.2, %10)
    +  %result.3 : Tensor = aten::mul(%result.2, %10)
    ?     ++
      %12 : int = prim::Constant[value=0]()
      %13 : int = prim::Constant[value=2]()
      %14 : Tensor = aten::select(%x, %12, %13)
    +  %result : Tensor = aten::mul(%result.3, %14)
    +  %16 : int = prim::Constant[value=0]()
    +  %17 : int = prim::Constant[value=3]()
    +  %18 : Tensor = aten::select(%x, %16, %17)
    -  %15 : Tensor = aten::mul(%result, %14)
    ?   ^                 ^
    +  %19 : Tensor = aten::mul(%result, %18)
    ?   ^                 ^
    -  return (%15);
    ?       ^
    +  return (%19);
    ?       ^
      }

```
Copy to clipboard
This message indicates to us that the computation differed between when we first traced it and when we traced it with the `check_inputs`. Indeed, the loop within the body of `loop_in_traced_fn` depends on the shape of the input `x`, and thus when we try another `x` with a different shape, the trace differs.
In this case, data-dependent control flow like this can be captured using `torch.jit.script()` instead:
```
def fn(x):
  result = x[0]
  for i in range(x.size(0)):
    result = result * x[i]
  return result
inputs = (torch.rand(3, 4, 5),)
check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]
scripted_fn = torch.jit.script(fn)
print(scripted_fn.graph)
#print(str(scripted_fn.graph).strip())
for input_tuple in [inputs] + check_inputs:
  torch.testing.assert_close(fn(*input_tuple), scripted_fn(*input_tuple))

```
Copy to clipboard
Which produces:
```
graph(%x : Tensor) {
  %5 : bool = prim::Constant[value=1]()
  %1 : int = prim::Constant[value=0]()
  %result.1 : Tensor = aten::select(%x, %1, %1)
  %4 : int = aten::size(%x, %1)
  %result : Tensor = prim::Loop(%4, %5, %result.1)
  block0(%i : int, %7 : Tensor) {
    %10 : Tensor = aten::select(%x, %1, %i)
    %result.2 : Tensor = aten::mul(%7, %10)
    -> (%5, %result.2)
  }
  return (%result);
}

```
Copy to clipboard
#### Tracer Warnings
The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:
```
def fill_row_zero(x):
  x[0] = torch.rand(*x.shape[1:2])
  return x
traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))
print(traced.graph)

```
Copy to clipboard
Produces several warnings and a graph which simply returns the input:
```
fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.
  x[0] = torch.rand(*x.shape[1:2])
fill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:
Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)
  traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))
graph(%0 : Float(3, 4)) {
  return (%0);
}

```
Copy to clipboard
We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with `torch.cat`:
```
def fill_row_zero(x):
  x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)
  return x
traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))
print(traced.graph)

```
Copy to clipboard
## Frequently Asked Questions
Q: I would like to train a model on GPU and do inference on CPU. What are the best practices?
> First convert your model from GPU to CPU and then save it, like so:
> ```
cpu_model = gpu_model.cpu()
sample_input_cpu = sample_input_gpu.cpu()
traced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)
torch.jit.save(traced_cpu, "cpu.pt")
traced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)
torch.jit.save(traced_gpu, "gpu.pt")
# ... later, when using the model:
if use_gpu:
 model = torch.jit.load("gpu.pt")
else:
 model = torch.jit.load("cpu.pt")
model(input)

```
Copy to clipboard
> This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model _before_ saving it ensures that the tracer has the correct device information.
Q: How do I store attributes on a `ScriptModule`?
> Say we have a model like:
> ```
import torch
class Model(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.x = 2
  def forward(self):
    return self.x
m = torch.jit.script(Model())

```
Copy to clipboard
> If `Model` is instantiated it will result in a compilation error since the compiler doesn’t know about `x`. There are 4 ways to inform the compiler of attributes on `ScriptModule`:
> 1. `nn.Parameter` - Values wrapped in `nn.Parameter` will work as they do on `nn.Module`s
> 2. `register_buffer` - Values wrapped in `register_buffer` will work as they do on `nn.Module`s. This is equivalent to an attribute (see 4) of type `Tensor`.
> 3. Constants - Annotating a class member as `Final` (or adding it to a list called `__constants__` at the class definition level) will mark the contained names as constants. Constants are saved directly in the code of the model. See builtin-constants for details.
> 4. Attributes - Values that are a supported type can be added as mutable attributes. Most types can be inferred but some may need to be specified, see module attributes for details.
Q: I would like to trace module’s method but I keep getting this error:
`RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient`
> This error usually means that the method you are tracing uses a module’s parameters and you are passing the module’s method instead of the module instance (e.g. `my_module_instance.forward` vs `my_module_instance`).
>>   * Invoking `trace` with a module’s method captures module parameters (which may require gradients) as **constants**.
>>   * On the other hand, invoking `trace` with module’s instance (e.g. `my_module`) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.
>> 

> To trace a specific method on a module, see `torch.jit.trace_module`
## Known Issues
If you’re using `Sequential` with TorchScript, the inputs of some of the `Sequential` submodules may be falsely inferred to be `Tensor`, even if they’re annotated otherwise. The canonical solution is to subclass `nn.Sequential` and redeclare `forward` with the input typed correctly.
## Appendix
### Migrating to PyTorch 1.2 Recursive Scripting API
This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.
1. `torch.jit.script` will now attempt to recursively compile functions, methods, and classes that it encounters. Once you call `torch.jit.script`, compilation is “opt-out”, rather than “opt-in”.
2. `torch.jit.script(nn_module_instance)` is now the preferred way to create `ScriptModule`s, instead of inheriting from `torch.jit.ScriptModule`. These changes combine to provide a simpler, easier-to-use API for converting your `nn.Module`s into `ScriptModule`s, ready to be optimized and executed in a non-Python environment.
The new usage looks like this:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
class Model(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1, 20, 5)
    self.conv2 = nn.Conv2d(20, 20, 5)
  def forward(self, x):
    x = F.relu(self.conv1(x))
    return F.relu(self.conv2(x))
my_model = Model()
my_scripted_model = torch.jit.script(my_model)

```
Copy to clipboard
  * The module’s `forward` is compiled by default. Methods called from `forward` are lazily compiled in the order they are used in `forward`.
  * To compile a method other than `forward` that is not called from `forward`, add `@torch.jit.export`.
  * To stop the compiler from compiling a method, add `@torch.jit.ignore` or `@torch.jit.unused`. `@ignore` leaves the
  * method as a call to python, and `@unused` replaces it with an exception. `@ignored` cannot be exported; `@unused` can.
  * Most attribute types can be inferred, so `torch.jit.Attribute` is not necessary. For empty container types, annotate their types using PEP 526-style class annotations.
  * Constants can be marked with a `Final` class annotation instead of adding the name of the member to `__constants__`.
  * Python 3 type hints can be used in place of `torch.jit.annotate`



As a result of these changes, the following items are considered deprecated and should not appear in new code:
    
  * The `@torch.jit.script_method` decorator
  * Classes that inherit from `torch.jit.ScriptModule`
  * The `torch.jit.Attribute` wrapper class
  * The `__constants__` array
  * The `torch.jit.annotate` function


#### Modules
Warning
The `@torch.jit.ignore` annotation’s behavior changes in PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function or method callable from code that is exported. To get this functionality back, use `@torch.jit.unused()`. `@torch.jit.ignore` is now equivalent to `@torch.jit.ignore(drop=False)`. See `@torch.jit.ignore` and `@torch.jit.unused` for details.
When passed to the `torch.jit.script` function, a `torch.nn.Module`'s data is copied to a `ScriptModule` and the TorchScript compiler compiles the module. The module’s `forward` is compiled by default. Methods called from `forward` are lazily compiled in the order they are used in `forward`, as well as any `@torch.jit.export` methods. 

torch.jit.export(_fn_)[source][source]
    
This decorator indicates that a method on an `nn.Module` is used as an entry point into a `ScriptModule` and should be compiled.
`forward` implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from `forward` are compiled as they are seen by the compiler, so they do not need this decorator either.
Example (using `@torch.jit.export` on a method):
```
import torch
import torch.nn as nn
class MyModule(nn.Module):
  def implicitly_compiled_method(self, x):
    return x + 99
  # `forward` is implicitly decorated with `@torch.jit.export`,
  # so adding it here would have no effect
  def forward(self, x):
    return x + 10
  @torch.jit.export
  def another_forward(self, x):
    # When the compiler sees this call, it will compile
    # `implicitly_compiled_method`
    return self.implicitly_compiled_method(x)
  def unused_method(self, x):
    return x - 20
# `m` will contain compiled methods:
#   `forward`
#   `another_forward`
#   `implicitly_compiled_method`
# `unused_method` will not be compiled since it was not called from
# any compiled methods and wasn't decorated with `@torch.jit.export`
m = torch.jit.script(MyModule())

```
Copy to clipboard
#### Functions
Functions don’t change much, they can be decorated with `@torch.jit.ignore` or `torch.jit.unused` if needed.
```
# Same behavior as pre-PyTorch 1.2
@torch.jit.script
def some_fn():
  return 2
# Marks a function as ignored, if nothing
# ever calls it then this has no effect
@torch.jit.ignore
def some_fn2():
  return 2
# As with ignore, if nothing calls it then it has no effect.
# If it is called in script it is replaced with an exception.
@torch.jit.unused
def some_fn3():
 import pdb; pdb.set_trace()
 return 4
# Doesn't do anything, this function is already
# the main entry point
@torch.jit.export
def some_fn4():
  return 2

```
Copy to clipboard
#### TorchScript Classes
Warning
TorchScript class support is experimental. Currently it is best suited for simple record-like types (think a `NamedTuple` with methods attached).
Everything in a user defined TorchScript Class is exported by default, functions can be decorated with `@torch.jit.ignore` if needed.
#### Attributes
The TorchScript compiler needs to know the types of module attributes. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with PEP 526-style class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting `ScriptModule`
Old API:
```
from typing import Dict
import torch
class MyModule(torch.jit.ScriptModule):
  def __init__(self):
    super().__init__()
    self.my_dict = torch.jit.Attribute({}, Dict[str, int])
    self.my_int = torch.jit.Attribute(20, int)
m = MyModule()

```
Copy to clipboard
New API:
```
from typing import Dict
class MyModule(torch.nn.Module):
  my_dict: Dict[str, int]
  def __init__(self):
    super().__init__()
    # This type cannot be inferred and must be specified
    self.my_dict = {}
    # The attribute type here is inferred to be `int`
    self.my_int = 20
  def forward(self):
    pass
m = torch.jit.script(MyModule())

```
Copy to clipboard
#### Constants
The `Final` type constructor can be used to mark members as constant. If members are not marked constant, they will be copied to the resulting `ScriptModule` as an attribute. Using `Final` opens opportunities for optimization if the value is known to be fixed and gives additional type safety.
Old API:
```
class MyModule(torch.jit.ScriptModule):
  __constants__ = ['my_constant']
  def __init__(self):
    super().__init__()
    self.my_constant = 2
  def forward(self):
    pass
m = MyModule()

```
Copy to clipboard
New API:
```
from typing import Final
class MyModule(torch.nn.Module):
  my_constant: Final[int]
  def __init__(self):
    super().__init__()
    self.my_constant = 2
  def forward(self):
    pass
m = torch.jit.script(MyModule())

```
Copy to clipboard
#### Variables
Containers are assumed to have type `Tensor` and be non-optional (see Default Types for more information). Previously, `torch.jit.annotate` was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.
```
import torch
from typing import Dict, Optional
@torch.jit.script
def make_dict(flag: bool):
  x: Dict[str, int] = {}
  x['hi'] = 2
  b: Optional[int] = None
  if flag:
    b = 2
  return x, b

```
Copy to clipboard
### Fusion Backends
There are a couple of fusion backends available to optimize TorchScript execution. The default fuser on CPUs is NNC, which can perform fusions for both CPUs and GPUs. The default fuser on GPUs is NVFuser, which supports a wider range of operators and has demonstrated generated kernels with improved throughput. See the NVFuser documentation for more details on usage and debugging.
### References
  * Python Language Reference Coverage
  * TorchScript Unsupported PyTorch Constructs


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * TorchScript
    * Creating TorchScript Code
    * Mixing Tracing and Scripting
    * TorchScript Language
    * Built-in Functions and Modules
      * PyTorch Functions and Modules
      * Python Functions and Modules
      * Python Language Reference Comparison
    * Debugging
      * Disable JIT for Debugging
      * Inspecting Code
      * Interpreting Graphs
      * Tracer
        * Tracing Edge Cases
        * Automatic Trace Checking
        * Tracer Warnings
    * Frequently Asked Questions
    * Known Issues
    * Appendix
      * Migrating to PyTorch 1.2 Recursive Scripting API
        * Modules
          * `export()`
        * Functions
        * TorchScript Classes
        * Attributes
        * Constants
        * Variables
      * Fusion Backends
      * References


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Index


Shortcuts 
# Index
**_** | **A** | **B** | **C** | **D** | **E** | **F** | **G** | **H** | **I** | **J** | **K** | **L** | **M** | **N** | **O** | **P** | **Q** | **R** | **S** | **T** | **U** | **V** | **W** | **X** | **Z**
## _
  * __create_chunk_list__() (torch.distributed.tensor.DTensor method)
  * __getstate__() (torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState method)
  * __init__() (torch.distributed.FileStore method)
    * (torch.distributed.HashStore method)
    * (torch.distributed.PrefixStore method)
    * (torch.distributed.Store method)
    * (torch.distributed.TCPStore method)
    * (torch.fx.Graph method)
    * (torch.fx.GraphModule method)
    * (torch.monitor.Event method)
    * (torch.monitor.Stat method)
    * (torch.monitor.TensorboardEventHandler method)
    * (torch.package.PackageExporter method)
    * (torch.package.PackageImporter method)
    * (torch.Tensor method)
    * (torch.utils.tensorboard.writer.SummaryWriter method)
  * __setstate__() (torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState method)
  * _assert() (in module torch)
  * _assign_worker_ranks() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _create_jit_fn() (in module torch.cuda.jiterator)
  * _create_multi_output_jit_fn() (in module torch.cuda.jiterator)
  * _dump_snapshot() (in module torch.cuda.memory)
  * _exit_barrier() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _foreach_abs() (in module torch)
  * _foreach_abs_() (in module torch)
  * _foreach_acos() (in module torch)
  * _foreach_acos_() (in module torch)
  * _foreach_asin() (in module torch)
  * _foreach_asin_() (in module torch)
  * _foreach_atan() (in module torch)
  * _foreach_atan_() (in module torch)
  * _foreach_ceil() (in module torch)
  * _foreach_ceil_() (in module torch)
  * _foreach_cos() (in module torch)
  * _foreach_cos_() (in module torch)
  * _foreach_cosh() (in module torch)
  * _foreach_cosh_() (in module torch)
  * _foreach_erf() (in module torch)
  * _foreach_erf_() (in module torch)
  * _foreach_erfc() (in module torch)
  * _foreach_erfc_() (in module torch)
  * _foreach_exp() (in module torch)
  * _foreach_exp_() (in module torch)
  * _foreach_expm1() (in module torch)

| 
  * _foreach_expm1_() (in module torch)
  * _foreach_floor() (in module torch)
  * _foreach_floor_() (in module torch)
  * _foreach_frac() (in module torch)
  * _foreach_frac_() (in module torch)
  * _foreach_lgamma() (in module torch)
  * _foreach_lgamma_() (in module torch)
  * _foreach_log() (in module torch)
  * _foreach_log10() (in module torch)
  * _foreach_log10_() (in module torch)
  * _foreach_log1p() (in module torch)
  * _foreach_log1p_() (in module torch)
  * _foreach_log2() (in module torch)
  * _foreach_log2_() (in module torch)
  * _foreach_log_() (in module torch)
  * _foreach_neg() (in module torch)
  * _foreach_neg_() (in module torch)
  * _foreach_reciprocal() (in module torch)
  * _foreach_reciprocal_() (in module torch)
  * _foreach_round() (in module torch)
  * _foreach_round_() (in module torch)
  * _foreach_sigmoid() (in module torch)
  * _foreach_sigmoid_() (in module torch)
  * _foreach_sin() (in module torch)
  * _foreach_sin_() (in module torch)
  * _foreach_sinh() (in module torch)
  * _foreach_sinh_() (in module torch)
  * _foreach_sqrt() (in module torch)
  * _foreach_sqrt_() (in module torch)
  * _foreach_tan() (in module torch)
  * _foreach_tan_() (in module torch)
  * _foreach_trunc() (in module torch)
  * _foreach_trunc_() (in module torch)
  * _foreach_zero_() (in module torch)
  * _initialize_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _KinetoProfile (class in torch.profiler)
  * _monitor_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _record_memory_history() (in module torch.cuda.memory)
  * _rendezvous() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _restart_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _shutdown() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _snapshot() (in module torch.cuda.memory)
  * _start_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)
  * _stop_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)

  
---|---  
## A
  * abs() (in module torch)
    * (torch.Tensor method)
  * abs_() (torch.Tensor method)
  * absolute() (in module torch)
    * (torch.Tensor method)
  * absolute_() (torch.Tensor method)
  * AbsTransform (class in torch.distributions.transforms)
  * acos() (in module torch)
    * (torch.Tensor method)
  * acos_() (torch.Tensor method)
  * acosh() (in module torch)
    * (torch.Tensor method)
  * acosh_() (torch.Tensor method)
  * acquire() (torch.distributed.elastic.timer.TimerClient method)
  * active_pool() (torch.cuda.MemPoolContext static method)
  * Adadelta (class in torch.optim)
  * Adafactor (class in torch.optim)
  * Adagrad (class in torch.optim)
  * Adam (class in torch.optim)
  * Adamax (class in torch.optim)
  * AdamW (class in torch.optim)
  * adapt() (torch.export.unflatten.FlatArgsAdapter method)
  * adaptive_autorange() (torch.utils.benchmark.Timer method)
  * adaptive_avg_pool1d() (in module torch.nn.functional)
  * adaptive_avg_pool2d (class in torch.ao.nn.quantized.functional)
  * adaptive_avg_pool2d() (in module torch.nn.functional)
  * adaptive_avg_pool3d (class in torch.ao.nn.quantized.functional)
  * adaptive_avg_pool3d() (in module torch.nn.functional)
  * adaptive_max_pool1d() (in module torch.nn.functional)
  * adaptive_max_pool2d() (in module torch.nn.functional)
  * adaptive_max_pool3d() (in module torch.nn.functional)
  * AdaptiveAvgPool1d (class in torch.nn)
  * AdaptiveAvgPool2d (class in torch.nn)
  * AdaptiveAvgPool3d (class in torch.nn)
  * AdaptiveLogSoftmaxWithLoss (class in torch.nn)
  * AdaptiveMaxPool1d (class in torch.nn)
  * AdaptiveMaxPool2d (class in torch.nn)
  * AdaptiveMaxPool3d (class in torch.nn)
  * add() (in module torch)
    * (torch.ao.ns._numeric_suite.Shadow method)
    * (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method)
    * (torch.distributed.Store method)
    * (torch.fx.experimental.symbolic_shapes.DimConstraints method)
    * (torch.monitor.Stat method)
    * (torch.Tensor method)
  * add_() (torch.Tensor method)
  * add_audio() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_custom_scalars() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_dependency() (torch.package.PackageExporter method)
  * add_done_callback() (torch.futures.Future method)
  * add_dtype_config() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * add_embedding() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_equality() (torch.fx.experimental.symbolic_shapes.DimConstraints method)
  * add_figure() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_graph() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_histogram() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_hparams() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_image() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_images() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_loggers() (in module torch.ao.ns._numeric_suite_fx)
  * add_mesh() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_metadata() (torch.profiler._KinetoProfile method)
  * add_metadata_json() (torch.profiler._KinetoProfile method)
  * add_module() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * add_param_group() (torch.distributed.optim.ZeroRedundancyOptimizer method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
  * add_pr_curve() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_pruning_method() (torch.nn.utils.prune.PruningContainer method)
  * add_quant_dequant (class in torch.ao.quantization)
  * add_relu() (torch.ao.ns._numeric_suite.Shadow method)
  * add_safe_globals() (in module torch.serialization)
  * add_scalar() (torch.ao.ns._numeric_suite.Shadow method)
    * (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_scalars() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_shadow_loggers() (in module torch.ao.ns._numeric_suite_fx)
  * add_submodule() (torch.fx.GraphModule method)
  * add_text() (torch.utils.tensorboard.writer.SummaryWriter method)
  * add_var_to_val() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * add_video() (torch.utils.tensorboard.writer.SummaryWriter method)
  * addbmm() (in module torch)
    * (torch.Tensor method)
  * addbmm_() (torch.Tensor method)
  * addcdiv() (in module torch)
    * (torch.Tensor method)
  * addcdiv_() (torch.Tensor method)
  * addcmul() (in module torch)
    * (torch.Tensor method)
  * addcmul_() (torch.Tensor method)
  * addmm() (in module torch)
    * (in module torch.sparse)
    * (torch.Tensor method)
  * addmm_() (torch.Tensor method)
  * addmv() (in module torch)
    * (torch.Tensor method)
  * addmv_() (torch.Tensor method)
  * addr() (in module torch)
    * (torch.Tensor method)
  * addr_() (torch.Tensor method)
  * adjoint() (in module torch)
    * (torch.Tensor method)
  * affine_grid() (in module torch.nn.functional)
  * AffineQuantizedObserverBase (class in torch.ao.quantization.observer)
  * AffineTransform (class in torch.distributions.transforms)
  * Aggregation (class in torch.monitor)
  * airy_ai() (in module torch.special)
  * align_as() (torch.Tensor method)
  * align_to() (torch.Tensor method)
  * all() (in module torch)
    * (torch.Tensor method)
  * all_gather() (in module torch.distributed)
  * all_gather_into_tensor() (in module torch.distributed)
  * all_gather_object() (in module torch.distributed)
  * all_input_nodes (torch.fx.Node property)
  * all_paths() (torch.package.PackageExporter method)
  * all_reduce() (in module torch.distributed)
  * all_to_all() (in module torch.distributed)
  * all_to_all_single() (in module torch.distributed)
  * allclose() (in module torch)
    * (torch.Tensor method)
  * allocator (torch.cuda.MemPool property)
  * allow_bf16_reduced_precision_reduction (in module torch.backends.cuda.matmul)
  * allow_fp16_bf16_reduction_math_sdp() (in module torch.backends.cuda)
  * allow_fp16_reduced_precision_reduction (in module torch.backends.cuda.matmul)
  * allow_in_graph() (in module torch.compiler)
  * allow_mutation_on_saved_tensors (class in torch.autograd.graph)
  * allow_tf32 (in module torch.backends.cuda.matmul)
    * (in module torch.backends.cudnn)
  * allreduce_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks)
  * alpha_dropout() (in module torch.nn.functional)
  * AlphaDropout (class in torch.nn)
  * amax() (in module torch)
    * (torch.Tensor method)
  * amin() (in module torch)
    * (torch.Tensor method)
  * aminmax() (in module torch)
    * (torch.Tensor method)
  * and_masks() (in module torch.nn.attention.flex_attention)
  * angle() (in module torch)
    * (torch.Tensor method)

| 
  * annotate() (in module torch.jit)
  * any() (in module torch)
    * (torch.Tensor method)
  * aoti_compile_and_package() (in module torch._inductor)
  * aoti_load_package() (in module torch._inductor)
  * append() (torch.distributed.Store method)
    * (torch.fx.Node method)
    * (torch.nn.ModuleList method)
    * (torch.nn.ParameterList method)
    * (torch.nn.Sequential method)
  * apply() (torch.autograd.function.BackwardCFunction method)
    * (torch.distributed.fsdp.FullyShardedDataParallel method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.nn.utils.prune.BasePruningMethod class method)
    * (torch.nn.utils.prune.CustomFromMask class method)
    * (torch.nn.utils.prune.Identity class method)
    * (torch.nn.utils.prune.L1Unstructured class method)
    * (torch.nn.utils.prune.LnStructured class method)
    * (torch.nn.utils.prune.PruningContainer class method)
    * (torch.nn.utils.prune.RandomStructured class method)
    * (torch.nn.utils.prune.RandomUnstructured class method)
    * (torch.optim.swa_utils.AveragedModel method)
  * apply_() (torch.Tensor method)
  * apply_jvp() (torch.autograd.function.BackwardCFunction method)
  * apply_mask() (torch.nn.utils.prune.BasePruningMethod method)
    * (torch.nn.utils.prune.CustomFromMask method)
    * (torch.nn.utils.prune.Identity method)
    * (torch.nn.utils.prune.L1Unstructured method)
    * (torch.nn.utils.prune.LnStructured method)
    * (torch.nn.utils.prune.PruningContainer method)
    * (torch.nn.utils.prune.RandomStructured method)
    * (torch.nn.utils.prune.RandomUnstructured method)
  * apply_weights() (torch.onnx.ONNXProgram method)
  * arange() (in module torch)
  * arccos() (in module torch)
    * (torch.Tensor method)
  * arccos_() (torch.Tensor method)
  * arccosh() (in module torch)
    * (torch.Tensor method)
  * arccosh_() (torch.Tensor method)
  * arcsin() (in module torch)
    * (torch.Tensor method)
  * arcsin_() (torch.Tensor method)
  * arcsinh() (in module torch)
    * (torch.Tensor method)
  * arcsinh_() (torch.Tensor method)
  * arctan() (in module torch)
    * (torch.Tensor method)
  * arctan2() (in module torch)
    * (torch.Tensor method)
  * arctan2_() (torch.Tensor method)
  * arctan_() (torch.Tensor method)
  * arctanh() (in module torch)
    * (torch.Tensor method)
  * arctanh_() (torch.Tensor method)
  * are_deterministic_algorithms_enabled() (in module torch)
  * arg_constraints (torch.distributions.bernoulli.Bernoulli attribute)
    * (torch.distributions.beta.Beta attribute)
    * (torch.distributions.binomial.Binomial attribute)
    * (torch.distributions.categorical.Categorical attribute)
    * (torch.distributions.cauchy.Cauchy attribute)
    * (torch.distributions.chi2.Chi2 attribute)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli attribute)
    * (torch.distributions.dirichlet.Dirichlet attribute)
    * (torch.distributions.distribution.Distribution property)
    * (torch.distributions.exponential.Exponential attribute)
    * (torch.distributions.fishersnedecor.FisherSnedecor attribute)
    * (torch.distributions.gamma.Gamma attribute)
    * (torch.distributions.geometric.Geometric attribute)
    * (torch.distributions.gumbel.Gumbel attribute)
    * (torch.distributions.half_cauchy.HalfCauchy attribute)
    * (torch.distributions.half_normal.HalfNormal attribute)
    * (torch.distributions.independent.Independent attribute)
    * (torch.distributions.inverse_gamma.InverseGamma attribute)
    * (torch.distributions.kumaraswamy.Kumaraswamy attribute)
    * (torch.distributions.laplace.Laplace attribute)
    * (torch.distributions.lkj_cholesky.LKJCholesky attribute)
    * (torch.distributions.log_normal.LogNormal attribute)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal attribute)
    * (torch.distributions.mixture_same_family.MixtureSameFamily attribute)
    * (torch.distributions.multinomial.Multinomial attribute)
    * (torch.distributions.multivariate_normal.MultivariateNormal attribute)
    * (torch.distributions.negative_binomial.NegativeBinomial attribute)
    * (torch.distributions.normal.Normal attribute)
    * (torch.distributions.one_hot_categorical.OneHotCategorical attribute)
    * (torch.distributions.pareto.Pareto attribute)
    * (torch.distributions.poisson.Poisson attribute)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli attribute)
    * (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)
    * (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)
    * (torch.distributions.studentT.StudentT attribute)
    * (torch.distributions.transformed_distribution.TransformedDistribution attribute)
    * (torch.distributions.uniform.Uniform attribute)
    * (torch.distributions.von_mises.VonMises attribute)
    * (torch.distributions.weibull.Weibull attribute)
    * (torch.distributions.wishart.Wishart attribute)
  * argmax() (in module torch)
    * (torch.Tensor method)
  * argmin() (in module torch)
    * (torch.Tensor method)
  * args (torch.fx.Node property)
  * argsort() (in module torch)
    * (torch.Tensor method)
  * argwhere() (in module torch)
    * (torch.Tensor method)
  * as_integer_ratio() (torch.SymFloat method)
    * (torch.SymInt method)
  * as_nested_tensor() (in module torch.nested)
  * as_sparse_gradcheck() (in module torch.sparse)
  * as_standardized() (torch.utils.benchmark.CallgrindStats method)
  * as_strided() (in module torch)
    * (torch.Tensor method)
  * as_subclass() (torch.Tensor method)
  * as_tensor() (in module torch)
  * as_tuple() (torch.nn.attention.flex_attention.BlockMask method)
  * asarray() (in module torch)
  * ASGD (class in torch.optim)
  * asin() (in module torch)
    * (torch.Tensor method)
  * asin_() (torch.Tensor method)
  * asinh() (in module torch)
    * (torch.Tensor method)
  * asinh_() (torch.Tensor method)
  * assert_allclose() (in module torch.testing)
  * assert_close() (in module torch.testing)
  * assume_constant_result() (in module torch.compiler)
  * async_execution() (in module torch.distributed.rpc.functions)
  * async_save() (in module torch.distributed.checkpoint.state_dict_saver)
  * AsyncCheckpointerType (class in torch.distributed.checkpoint.state_dict_saver)
  * AsyncStager (class in torch.distributed.checkpoint.staging)
  * atan() (in module torch)
    * (torch.Tensor method)
  * atan2() (in module torch)
    * (torch.Tensor method)
  * atan2_() (torch.Tensor method)
  * atan_() (torch.Tensor method)
  * atanh() (in module torch)
    * (torch.Tensor method)
  * atanh_() (torch.Tensor method)
  * atleast_1d() (in module torch)
  * atleast_2d() (in module torch)
  * atleast_3d() (in module torch)
  * Attribute (class in torch.jit)
  * autocast (class in torch)
    * (class in torch.cpu.amp)
    * (class in torch.cuda.amp)
  * AveragedModel (class in torch.optim.swa_utils)
  * avg_pool1d() (in module torch.nn.functional)
  * avg_pool2d (class in torch.ao.nn.quantized.functional)
  * avg_pool2d() (in module torch.nn.functional)
  * avg_pool3d (class in torch.ao.nn.quantized.functional)
  * avg_pool3d() (in module torch.nn.functional)
  * AvgPool1d (class in torch.nn)
  * AvgPool2d (class in torch.nn)
  * AvgPool3d (class in torch.nn)

  
---|---  
## B
  * Backend (class in torch.distributed)
  * BackendConfig (class in torch.ao.quantization.backend_config)
  * BackendPatternConfig (class in torch.ao.quantization.backend_config)
  * BackendType (class in torch.distributed.rpc)
  * backward() (in module torch.autograd)
    * (in module torch.distributed.autograd)
    * (torch.autograd.Function static method)
    * (torch.autograd.function.InplaceFunction static method)
    * (torch.autograd.function.NestedIOFunction method)
    * (torch.distributed.rpc.PyRRef method)
    * (torch.Tensor method)
  * backward_extended() (torch.autograd.function.NestedIOFunction method)
  * BackwardCFunction (class in torch.autograd.function)
  * BackwardPrefetch (class in torch.distributed.fsdp)
  * baddbmm() (in module torch)
    * (torch.Tensor method)
  * baddbmm_() (torch.Tensor method)
  * barrier() (in module torch.distributed)
  * bartlett() (in module torch.signal.windows)
  * bartlett_window() (in module torch)
  * BasePruningMethod (class in torch.nn.utils.prune)
  * batch_isend_irecv() (in module torch.distributed)
  * batch_norm() (in module torch.nn.functional)
  * batch_shape (torch.distributions.distribution.Distribution property)
  * batch_sizes (torch.nn.utils.rnn.PackedSequence attribute)
  * batched_powerSGD_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook)
  * BatchNorm1d (class in torch.nn)
  * BatchNorm2d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * BatchNorm3d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * BatchSampler (class in torch.utils.data)
  * BCELoss (class in torch.nn)
  * BCEWithLogitsLoss (class in torch.nn)
  * benchmark (in module torch.backends.cudnn)
  * benchmark_limit (in module torch.backends.cudnn)
  * Bernoulli (class in torch.distributions.bernoulli)
  * bernoulli() (in module torch)
    * (torch.Tensor method)
  * bernoulli_() (torch.Tensor method)
  * bessel_j0() (in module torch.special)
  * bessel_j1() (in module torch.special)
  * Beta (class in torch.distributions.beta)
  * bf16_compress_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks)
  * bf16_compress_wrapper() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks)
  * bfloat16() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * BFloat16Storage (class in torch)
  * Bilinear (class in torch.nn)
  * bilinear() (in module torch.nn.functional)
  * binary_cross_entropy() (in module torch.nn.functional)
  * binary_cross_entropy_with_logits() (in module torch.nn.functional)
  * bincount() (in module torch)
    * (torch.Tensor method)
  * bind_symbols() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * Binomial (class in torch.distributions.binomial)
  * bitwise_and() (in module torch)
    * (torch.Tensor method)

| 
  * bitwise_and_() (torch.Tensor method)
  * bitwise_left_shift() (in module torch)
    * (torch.Tensor method)
  * bitwise_left_shift_() (torch.Tensor method)
  * bitwise_not() (in module torch)
    * (torch.Tensor method)
  * bitwise_not_() (torch.Tensor method)
  * bitwise_or() (in module torch)
    * (torch.Tensor method)
  * bitwise_or_() (torch.Tensor method)
  * bitwise_right_shift() (in module torch)
    * (torch.Tensor method)
  * bitwise_right_shift_() (torch.Tensor method)
  * bitwise_xor() (in module torch)
    * (torch.Tensor method)
  * bitwise_xor_() (torch.Tensor method)
  * blackman() (in module torch.signal.windows)
  * blackman_window() (in module torch)
  * block_diag() (in module torch)
  * BLOCK_SIZE (torch.nn.attention.flex_attention.BlockMask attribute)
  * blocked_autorange() (torch.utils.benchmark.Timer method)
  * BlockingAsyncStager (class in torch.distributed.checkpoint.staging)
  * BlockMask (class in torch.nn.attention.flex_attention)
  * bmm() (in module torch)
    * (torch.Tensor method)
  * BNReLU2d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.quantized)
  * BNReLU3d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.quantized)
  * bool() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * BoolStorage (class in torch)
  * bound_sympy() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * boxed() (torch.distributed.Work method)
  * boxed_run() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Interpreter method)
  * breakpoint() (in module torch.distributed)
  * broadcast() (in module torch.cuda.comm)
    * (in module torch.distributed)
  * broadcast_coalesced() (in module torch.cuda.comm)
  * broadcast_object_list() (in module torch.distributed)
  * broadcast_shapes() (in module torch)
  * broadcast_tensors() (in module torch)
  * broadcast_to() (in module torch)
    * (torch.Tensor method)
  * BroadcastingTorchSaveReader (class in torch.distributed.checkpoint.format_utils)
  * bucketize() (in module torch)
  * Buffer (class in torch.nn.parameter)
  * buffer() (in module torch.distributed.GradBucket)
  * buffers() (torch.export.ExportedProgram method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * build() (torch.distributed.elastic.rendezvous.api.RendezvousStoreInfo static method)
  * build_stage() (in module torch.distributed.pipelining.stage)
  * BuildExtension() (in module torch.utils.cpp_extension)
  * byte() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * ByteStorage (class in torch)
  * byteswap() (torch.UntypedStorage method)

  
---|---  
## C
  * C10dRendezvousBackend (class in torch.distributed.elastic.rendezvous.c10d_rendezvous_backend)
  * cached() (in module torch.nn.utils.parametrize)
  * caching_allocator_alloc() (in module torch.cuda)
  * caching_allocator_delete() (in module torch.cuda)
  * caching_allocator_enable() (in module torch.cuda.memory)
  * calculate_gain() (in module torch.nn.init)
  * calculate_qparams() (torch.ao.quantization.observer.AffineQuantizedObserverBase method)
    * (torch.ao.quantization.observer.MinMaxObserver method)
  * call_function() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Graph method)
    * (torch.fx.Interpreter method)
    * (torch.fx.Transformer method)
  * call_method() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Graph method)
    * (torch.fx.Interpreter method)
  * call_module() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Graph method)
    * (torch.fx.Interpreter method)
    * (torch.fx.Tracer method)
    * (torch.fx.Transformer method)
  * CallgrindStats (class in torch.utils.benchmark)
  * CallMethodKey (class in torch.fx.experimental.symbolic_shapes)
  * can_cast() (in module torch)
  * can_device_access_peer() (in module torch.cuda)
  * can_use_cudnn_attention() (in module torch.backends.cuda)
  * can_use_efficient_attention() (in module torch.backends.cuda)
  * can_use_flash_attention() (in module torch.backends.cuda)
  * canonicalize_bool_expr() (in module torch.fx.experimental.symbolic_shapes)
  * capture_begin() (torch.cuda.CUDAGraph method)
  * capture_end() (torch.cuda.CUDAGraph method)
  * cartesian_prod() (in module torch)
  * cat (in module torch.distributions.constraints)
  * cat() (in module torch)
    * (torch.ao.ns._numeric_suite.Shadow method)
  * Categorical (class in torch.distributions.categorical)
  * CatTransform (class in torch.distributions.transforms)
  * Cauchy (class in torch.distributions.cauchy)
  * cauchy_() (torch.Tensor method)
  * causal_lower_right() (in module torch.nn.attention.bias)
  * causal_upper_left() (in module torch.nn.attention.bias)
  * CausalBias (class in torch.nn.attention.bias)
  * CausalVariant (class in torch.nn.attention.bias)
  * ccol_indices() (torch.Tensor method)
  * cdf() (torch.distributions.cauchy.Cauchy method)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.exponential.Exponential method)
    * (torch.distributions.gamma.Gamma method)
    * (torch.distributions.half_cauchy.HalfCauchy method)
    * (torch.distributions.half_normal.HalfNormal method)
    * (torch.distributions.laplace.Laplace method)
    * (torch.distributions.mixture_same_family.MixtureSameFamily method)
    * (torch.distributions.normal.Normal method)
    * (torch.distributions.transformed_distribution.TransformedDistribution method)
    * (torch.distributions.uniform.Uniform method)
  * cdist() (in module torch)
  * cdouble() (torch.Tensor method)
  * ceil() (in module torch)
    * (torch.Tensor method)
  * ceil_() (torch.Tensor method)
  * celu (class in torch.ao.nn.quantized.functional)
  * CELU (class in torch.nn)
  * celu() (in module torch.nn.functional)
  * cfloat() (torch.Tensor method)
  * chain_matmul() (in module torch)
  * ChainDataset (class in torch.utils.data)
  * ChainedScheduler (class in torch.optim.lr_scheduler)
  * chalf() (torch.Tensor method)
  * change_current_allocator() (in module torch.cuda)
  * ChannelShuffle (class in torch.nn)
  * char() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * CharStorage (class in torch)
  * check() (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method)
    * (torch.distributed.Store method)
    * (torch.distributions.constraints.Constraint method)
  * check_consistent() (in module torch.fx.experimental.symbolic_shapes)
  * check_equal() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * check_export_model_diff (class in torch.onnx.verification)
  * check_is_root() (torch.distributed.fsdp.FullyShardedDataParallel method)
  * check_sparse_tensor_invariants (class in torch.sparse)
  * checkpoint() (in module torch.utils.checkpoint)
  * checkpoint_id (torch.distributed.checkpoint.FileSystemReader property)
  * checkpoint_sequential() (in module torch.utils.checkpoint)
  * CheckpointPolicy (class in torch.utils.checkpoint)
  * Chi2 (class in torch.distributions.chi2)
  * ChildFailedError (class in torch.distributed.elastic.multiprocessing.errors)
  * children() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * cholesky() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * cholesky_ex() (in module torch.linalg)
  * cholesky_inverse() (in module torch)
    * (torch.Tensor method)
  * cholesky_solve() (in module torch)
    * (torch.Tensor method)
  * chunk() (in module torch)
    * (torch.Tensor method)
  * CircularPad1d (class in torch.nn)
  * CircularPad2d (class in torch.nn)
  * CircularPad3d (class in torch.nn)
  * clamp (class in torch.ao.nn.quantized.functional)
  * clamp() (in module torch)
    * (torch.Tensor method)
  * clamp_() (torch.Tensor method)
  * cleanup() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * clear() (in module torch.backends.cuda.cufft_plan_cache)
    * (torch.autograd.profiler_util.StringTable method)
    * (torch.nn.ModuleDict method)
    * (torch.nn.ParameterDict method)
  * clear_safe_globals() (in module torch.serialization)
  * clear_timers() (torch.distributed.elastic.timer.TimerServer method)
  * clip() (in module torch)
    * (torch.Tensor method)
  * clip_() (torch.Tensor method)
  * clip_grad_norm() (in module torch.nn.utils)
  * clip_grad_norm_() (in module torch.nn.utils)
    * (torch.distributed.fsdp.FullyShardedDataParallel method)
  * clip_grad_value_() (in module torch.nn.utils)
  * clip_grads_with_norm_() (in module torch.nn.utils)
  * clock_rate() (in module torch.cuda)
  * clone() (in module torch)
    * (torch.autograd.grad_mode.inference_mode method)
    * (torch.autograd.grad_mode.set_grad_enabled method)
    * (torch.autograd.grad_mode.set_multithreading_enabled method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * clone_state() (torch.Generator method)
  * close (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property)
  * close() (torch.package.PackageExporter method)
    * (torch.utils.tensorboard.writer.SummaryWriter method)
  * cls_to_become (torch.nn.LazyBatchNorm1d attribute)
    * (torch.nn.LazyBatchNorm2d attribute)
    * (torch.nn.LazyBatchNorm3d attribute)
    * (torch.nn.LazyConv1d attribute)
    * (torch.nn.LazyConv2d attribute)
    * (torch.nn.LazyConv3d attribute)
    * (torch.nn.LazyConvTranspose1d attribute)
    * (torch.nn.LazyConvTranspose2d attribute)
    * (torch.nn.LazyConvTranspose3d attribute)
    * (torch.nn.LazyInstanceNorm1d attribute)
    * (torch.nn.LazyInstanceNorm2d attribute)
    * (torch.nn.LazyInstanceNorm3d attribute)
    * (torch.nn.LazyLinear attribute)
    * (torch.nn.parameter.UninitializedParameter attribute)
  * coalesce() (torch.Tensor method)
  * code (torch.fx.GraphModule property)
    * (torch.jit.ScriptModule property)
  * code_with_constants (torch.jit.ScriptModule property)
  * col_indices() (torch.Tensor method)
  * collate() (in module torch.utils.data._utils.collate)
  * collect_all() (in module torch.futures)
  * collect_callgrind() (torch.utils.benchmark.Timer method)
  * colorize() (torch.utils.benchmark.Compare method)
  * column_stack() (in module torch)
  * ColwiseParallel (class in torch.distributed.tensor.parallel)
  * combinations() (in module torch)
  * CommDebugMode (class in torch.distributed.tensor.debug)
  * commit_tensor() (torch.distributed.checkpoint.LoadPlanner method)
  * Compare (class in torch.utils.benchmark)
  * compare_model_outputs() (in module torch.ao.ns._numeric_suite)
  * compare_model_stub() (in module torch.ao.ns._numeric_suite)
  * compare_results (class in torch.ao.quantization)
  * compare_set() (torch.distributed.Store method)
  * compare_weights() (in module torch.ao.ns._numeric_suite)
  * compile() (in module torch)
    * (in module torch.compiler)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * compile_shader() (in module torch.mps)
  * compiled_with_cxx11_abi() (in module torch)
  * complex() (in module torch)
  * complex_double() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * complex_float() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * ComplexDoubleStorage (class in torch)
  * ComplexFloatStorage (class in torch)
  * component_distribution (torch.distributions.mixture_same_family.MixtureSameFamily property)
  * ComposeTransform (class in torch.distributions.transforms)
  * compute_cosine_similarity() (in module torch.ao.ns.fx.utils)
  * compute_mask() (torch.nn.utils.prune.BasePruningMethod method)
    * (torch.nn.utils.prune.LnStructured method)
    * (torch.nn.utils.prune.PruningContainer method)
    * (torch.nn.utils.prune.RandomStructured method)
  * compute_normalized_l2_error() (in module torch.ao.ns.fx.utils)
  * compute_sqnr() (in module torch.ao.ns.fx.utils)
  * compute_unbacked_bindings() (in module torch.fx.experimental.symbolic_shapes)
  * compute_values() (torch.onnx.ONNXProgram method)
  * concat() (in module torch)
  * ConcatDataset (class in torch.utils.data)
  * concatenate() (in module torch)
  * concentration (torch.distributions.inverse_gamma.InverseGamma property)
  * concentration0 (torch.distributions.beta.Beta property)
  * concentration1 (torch.distributions.beta.Beta property)
  * cond() (in module torch)
    * (in module torch._higher_order_ops.cond)
    * (in module torch.linalg)
  * configs (torch.ao.quantization.backend_config.BackendConfig property)
  * configure() (in module torch.distributed.elastic.metrics)
    * (in module torch.distributed.elastic.timer)
  * confirmed_by_owner() (torch.distributed.rpc.PyRRef method)
  * conj() (in module torch)
    * (torch.Tensor method)

| 
  * conj_physical() (in module torch)
    * (torch.Tensor method)
  * conj_physical_() (torch.Tensor method)
  * conjugate() (torch.SymFloat method)
  * ConsoleMetricHandler (class in torch.distributed.elastic.metrics.api)
  * consolidate_state_dict() (torch.distributed.optim.ZeroRedundancyOptimizer method)
  * constant_() (in module torch.nn.init)
  * ConstantLR (class in torch.optim.lr_scheduler)
  * ConstantPad1d (class in torch.nn)
  * ConstantPad2d (class in torch.nn)
  * ConstantPad3d (class in torch.nn)
  * constrain_range() (in module torch.fx.experimental.symbolic_shapes)
  * constrain_unify() (in module torch.fx.experimental.symbolic_shapes)
  * Constraint (class in torch.distributions.constraints)
    * (in module torch.export)
  * ConstraintRegistry (class in torch.distributions.constraint_registry)
  * construct_and_record_rdzv_event() (in module torch.distributed.elastic.events)
  * context (class in torch.distributed.autograd)
  * context_parallel() (in module torch.distributed.tensor.experimental)
  * contiguous() (torch.Tensor method)
  * ContinuousBernoulli (class in torch.distributions.continuous_bernoulli)
  * Conv1d (class in torch.ao.nn.quantized)
  * conv1d (class in torch.ao.nn.quantized.functional)
  * Conv1d (class in torch.nn)
  * conv1d() (in module torch.nn.functional)
  * Conv2d (class in torch.ao.nn.qat)
    * (class in torch.ao.nn.quantized)
  * conv2d (class in torch.ao.nn.quantized.functional)
  * Conv2d (class in torch.nn)
  * conv2d() (in module torch.nn.functional)
  * Conv3d (class in torch.ao.nn.qat)
    * (class in torch.ao.nn.quantized)
  * conv3d (class in torch.ao.nn.quantized.functional)
  * Conv3d (class in torch.nn)
  * conv3d() (in module torch.nn.functional)
  * conv_transpose1d() (in module torch.nn.functional)
  * conv_transpose2d() (in module torch.nn.functional)
  * conv_transpose3d() (in module torch.nn.functional)
  * ConvBn1d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
  * ConvBn2d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
  * ConvBn3d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
  * ConvBnReLU1d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
  * ConvBnReLU2d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
  * ConvBnReLU3d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
  * convert (class in torch.ao.quantization)
  * convert_conv2d_weight_memory_format() (in module torch.nn.utils)
  * convert_conv3d_weight_memory_format() (in module torch.nn.utils)
  * convert_fx (class in torch.ao.quantization.quantize_fx)
  * convert_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx)
  * convert_sync_batchnorm() (torch.nn.SyncBatchNorm class method)
  * ConvertCustomConfig (class in torch.ao.quantization.fx.custom_config)
  * ConvertIntKey (class in torch.fx.experimental.symbolic_shapes)
  * ConvReLU1d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.quantized)
  * ConvReLU2d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
    * (class in torch.ao.nn.intrinsic.quantized)
  * ConvReLU3d (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
    * (class in torch.ao.nn.intrinsic.quantized)
  * ConvTranspose1d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * ConvTranspose2d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * ConvTranspose3d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * copy() (torch.autograd.profiler_util.StringTable method)
    * (torch.export.decomp_utils.CustomDecompTable method)
    * (torch.nn.ParameterDict method)
  * copy_() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * copysign() (in module torch)
    * (torch.Tensor method)
  * copysign_() (torch.Tensor method)
  * CorrCholeskyTransform (class in torch.distributions.transforms)
  * corrcoef() (in module torch)
    * (torch.Tensor method)
  * cos() (in module torch)
    * (torch.Tensor method)
  * cos_() (torch.Tensor method)
  * cosh() (in module torch)
    * (torch.Tensor method)
  * cosh_() (torch.Tensor method)
  * cosine() (in module torch.signal.windows)
  * cosine_embedding_loss() (in module torch.nn.functional)
  * cosine_similarity() (in module torch.nn.functional)
  * CosineAnnealingLR (class in torch.optim.lr_scheduler)
  * CosineAnnealingWarmRestarts (class in torch.optim.lr_scheduler)
  * CosineEmbeddingLoss (class in torch.nn)
  * CosineSimilarity (class in torch.nn)
  * count (torch.monitor.Stat property)
  * count() (torch.autograd.forward_ad.UnpackedDualTensor method)
    * (torch.autograd.profiler_util.Kernel method)
    * (torch.jit.Attribute method)
    * (torch.nn.utils.rnn.PackedSequence method)
    * (torch.Size method)
  * count_nonzero() (in module torch)
    * (torch.Tensor method)
  * counts() (torch.utils.benchmark.CallgrindStats method)
  * cov() (in module torch)
    * (torch.Tensor method)
  * covariance_matrix (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property)
    * (torch.distributions.multivariate_normal.MultivariateNormal property)
    * (torch.distributions.wishart.Wishart property)
  * CppExtension() (in module torch.utils.cpp_extension)
  * cpu() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * CPUOffload (class in torch.distributed.fsdp)
  * CPUOffloadPolicy (class in torch.distributed.fsdp)
  * create_arg() (torch.fx.Tracer method)
  * create_args_for_root() (torch.fx.Tracer method)
  * create_backend() (in module torch.distributed.elastic.rendezvous.c10d_rendezvous_backend)
    * (in module torch.distributed.elastic.rendezvous.etcd_rendezvous_backend)
  * create_block_mask() (in module torch.nn.attention.flex_attention)
  * create_global_plan() (torch.distributed.checkpoint.LoadPlanner method)
    * (torch.distributed.checkpoint.SavePlanner method)
  * create_handler() (in module torch.distributed.elastic.rendezvous.dynamic_rendezvous)
  * create_healthcheck_server() (in module torch.distributed.elastic.agent.server.health_check_server)
  * create_local_plan() (torch.distributed.checkpoint.LoadPlanner method)
    * (torch.distributed.checkpoint.SavePlanner method)
  * create_mask() (in module torch.nn.attention.flex_attention)
  * create_nested_block_mask() (in module torch.nn.attention.flex_attention)
  * create_node() (torch.fx.Graph method)
    * (torch.fx.Tracer method)
  * create_proxy() (torch.fx.Tracer method)
  * create_selective_checkpoint_contexts() (in module torch.utils.checkpoint)
  * create_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_symbolic_sizes_strides_storage_offset() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_symboolnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_symfloatnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_symintnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_unbacked_symbool() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_unbacked_symfloat() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_unbacked_symint() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_unspecified_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * create_unspecified_symint_and_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * cross() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * cross_entropy() (in module torch.nn.functional)
  * CrossEntropyLoss (class in torch.nn)
  * crow_indices() (torch.Tensor method)
  * ctc_loss() (in module torch.nn.functional)
  * CTCLoss (class in torch.nn)
  * cuda() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * CUDAExtension() (in module torch.utils.cpp_extension)
  * CUDAGraph (class in torch.cuda)
  * cudagraph_mark_step_begin() (in module torch.compiler)
  * CUDAPluggableAllocator (class in torch.cuda)
  * cudart() (in module torch.cuda)
  * cudnn_sdp_enabled() (in module torch.backends.cuda)
  * cufft_plan_cache (in module torch.backends.cuda)
  * cummax() (in module torch)
    * (torch.Tensor method)
  * cummin() (in module torch)
    * (torch.Tensor method)
  * cumprod() (in module torch)
    * (torch.Tensor method)
  * cumprod_() (torch.Tensor method)
  * cumsum() (in module torch)
    * (torch.Tensor method)
  * cumsum_() (torch.Tensor method)
  * cumulative_trapezoid() (in module torch)
  * CumulativeDistributionTransform (class in torch.distributions.transforms)
  * current_accelerator() (in module torch.accelerator)
  * current_allocated_memory() (in module torch.mps)
  * current_blas_handle() (in module torch.cuda)
  * current_device() (in module torch.cpu)
    * (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * current_device_idx() (in module torch.accelerator)
  * current_device_index() (in module torch.accelerator)
  * current_step() (torch.autograd.profiler.KinetoStepTracker class method)
  * current_stream() (in module torch.accelerator)
    * (in module torch.cpu)
    * (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * custom_bwd() (in module torch.amp)
    * (in module torch.cuda.amp)
  * custom_from_mask() (in module torch.nn.utils.prune)
  * custom_fwd() (in module torch.amp)
    * (in module torch.cuda.amp)
  * CUSTOM_KEY (in module torch.ao.quantization)
  * custom_op() (in module torch.library)
  * CustomDecompTable (class in torch.export.decomp_utils)
  * CustomFromMask (class in torch.nn.utils.prune)
  * CustomObjArgument (class in torch.export.graph_signature)
  * CustomOpDef (class in torch._library.custom_ops)
  * CyclicLR (class in torch.optim.lr_scheduler)

  
---|---  
## D
  * data (torch.monitor.Event property)
    * (torch.nn.utils.rnn.PackedSequence attribute)
  * data_parallel() (in module torch.nn.parallel)
  * data_ptr() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * data_value_t (class in torch.monitor)
  * DataLoader (class in torch.utils.data)
  * DataParallel (class in torch.nn)
  * Dataset (class in torch.utils.data)
  * dcp_to_torch_save() (in module torch.distributed.checkpoint.format_utils)
  * debug_dump() (torch.cuda.CUDAGraph method)
  * debug_unwrap() (in module torch.func)
  * default_activation_only_qconfig (in module torch.ao.quantization.qconfig)
  * default_collate() (in module torch.utils.data)
  * default_convert() (in module torch.utils.data)
  * default_debug_observer (in module torch.ao.quantization.observer)
  * default_debug_qconfig (in module torch.ao.quantization.qconfig)
  * default_decompositions() (in module torch.export.exported_program)
  * default_dynamic_qconfig (in module torch.ao.quantization.qconfig)
  * default_dynamic_quant_observer (in module torch.ao.quantization.observer)
  * default_eval_fn (class in torch.ao.quantization)
  * default_factory (torch.autograd.profiler_util.StringTable attribute)
  * default_fake_quant (in module torch.ao.quantization.fake_quantize)
  * default_float_qparams_observer (in module torch.ao.quantization.observer)
  * default_fused_act_fake_quant (in module torch.ao.quantization.fake_quantize)
  * default_fused_per_channel_wt_fake_quant (in module torch.ao.quantization.fake_quantize)
  * default_fused_wt_fake_quant (in module torch.ao.quantization.fake_quantize)
  * default_generator (torch.torch attribute)
  * default_histogram_fake_quant (in module torch.ao.quantization.fake_quantize)
  * default_histogram_observer (in module torch.ao.quantization.observer)
  * default_observer (in module torch.ao.quantization.observer)
  * default_per_channel_qconfig (in module torch.ao.quantization.qconfig)
  * default_per_channel_weight_fake_quant (in module torch.ao.quantization.fake_quantize)
  * default_per_channel_weight_observer (in module torch.ao.quantization.observer)
  * default_placeholder_observer (in module torch.ao.quantization.observer)
  * default_qat_qconfig (in module torch.ao.quantization.qconfig)
  * default_qat_qconfig_v2 (in module torch.ao.quantization.qconfig)
  * default_qconfig (in module torch.ao.quantization.qconfig)
  * default_stream() (in module torch.cuda)
    * (in module torch.mtia)
  * default_weight_fake_quant (in module torch.ao.quantization.fake_quantize)
  * default_weight_observer (in module torch.ao.quantization.observer)
  * default_weight_only_qconfig (in module torch.ao.quantization.qconfig)
  * DefaultLoadPlanner (class in torch.distributed.checkpoint)
  * DefaultLogsSpecs (class in torch.distributed.elastic.multiprocessing.api)
  * DefaultSavePlanner (class in torch.distributed.checkpoint)
  * defer_runtime_assert() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * DeferredMtiaCallError
  * define() (in module torch.library)
    * (torch.library.Library method)
  * definitely_false() (in module torch.fx.experimental.symbolic_shapes)
  * definitely_true() (in module torch.fx.experimental.symbolic_shapes)
  * deg2rad() (in module torch)
    * (torch.Tensor method)
  * delete_all_unused_submodules() (torch.fx.GraphModule method)
  * delete_key() (torch.distributed.Store method)
  * delete_submodule() (torch.fx.GraphModule method)
  * delta() (torch.utils.benchmark.CallgrindStats method)
  * denied_modules() (torch.package.PackageExporter method)
  * denoise() (torch.utils.benchmark.FunctionCounts method)
  * dense_dim() (torch.Tensor method)
  * deny() (torch.package.PackageExporter method)
  * dependency_graph_string() (torch.package.PackageExporter method)
  * dependent_property (in module torch.distributions.constraints)
  * dequantize() (in module torch)
    * (torch.ao.nn.quantizable.MultiheadAttention method)
    * (torch.Tensor method)
  * DeQuantStub (class in torch.ao.quantization)
  * deregister_handle() (torch.cuda.gds.GdsFile method)
  * deserialize_symexpr() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * det() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * detach() (torch.Tensor method)
  * detach_() (torch.Tensor method)
  * detect_anomaly (class in torch.autograd)
  * deterministic (in module torch.backends.cudnn)
  * device (class in torch)
    * (class in torch.cuda)
    * (class in torch.mtia)
    * (class in torch.xpu)
    * (torch.autograd.profiler_util.Kernel attribute)
    * (torch.Generator attribute)
    * (torch.Tensor attribute)
    * (torch.TypedStorage property)
    * (torch.UntypedStorage attribute)
  * device_count() (in module torch.accelerator)
    * (in module torch.cpu)
    * (in module torch.cuda)
    * (in module torch.mps)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * device_maps (torch.distributed.rpc.TensorPipeRpcBackendOptions property)
  * device_memory_used() (in module torch.cuda)
  * device_mesh (torch.distributed.tensor.DTensor property)
  * device_of (class in torch.cuda)
    * (class in torch.xpu)
  * DeviceMesh (class in torch.distributed.device_mesh)
  * devices (torch.distributed.rpc.TensorPipeRpcBackendOptions property)
  * df (torch.distributions.chi2.Chi2 property)
  * diag() (in module torch)
    * (torch.Tensor method)

| 
  * diag_embed() (in module torch)
    * (torch.Tensor method)
  * diagflat() (in module torch)
    * (torch.Tensor method)
  * DiagnosticOptions (class in torch.onnx)
  * diagonal() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * diagonal_scatter() (in module torch)
    * (torch.Tensor method)
  * diff() (in module torch)
    * (torch.Tensor method)
  * digamma() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * digamma_() (torch.Tensor method)
  * dim (torch.distributed.tensor.placement_types.Shard attribute)
  * Dim() (in module torch.export.dynamic_shapes)
  * dim() (torch.Tensor method)
  * dim_order() (torch.Tensor method)
  * DimConstraints (class in torch.fx.experimental.symbolic_shapes)
  * DimDynamic (class in torch.fx.experimental.symbolic_shapes)
  * dims() (in module torch.export)
  * dirac_() (in module torch.nn.init)
  * Directory (class in torch.package)
  * Dirichlet (class in torch.distributions.dirichlet)
  * disable() (in module torch.compiler)
    * (torch.sparse.check_sparse_tensor_invariants static method)
  * disable_fake_quant (class in torch.ao.quantization.fake_quantize)
  * disable_observer (class in torch.ao.quantization.fake_quantize)
  * disable_saved_tensors_hooks (class in torch.autograd.graph)
  * dist() (in module torch)
    * (torch.Tensor method)
  * DistBackendError (class in torch.distributed)
  * DistError (class in torch.distributed)
  * DistNetworkError (class in torch.distributed)
  * distribute_module() (in module torch.distributed.tensor)
  * distribute_tensor() (in module torch.distributed.tensor)
  * DistributedDataParallel (class in torch.nn.parallel)
  * DistributedOptimizer (class in torch.distributed.optim)
  * DistributedSampler (class in torch.utils.data.distributed)
  * Distribution (class in torch.distributions.distribution)
  * DistStoreError (class in torch.distributed)
  * div() (in module torch)
    * (torch.Tensor method)
  * div_() (torch.Tensor method)
  * divide() (in module torch)
    * (torch.Tensor method)
  * divide_() (torch.Tensor method)
  * DivideByKey (class in torch.fx.experimental.symbolic_shapes)
  * done() (torch.futures.Future method)
  * dot() (in module torch)
    * (torch.Tensor method)
  * double() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * DoubleStorage (class in torch)
  * download_url_to_file() (in module torch.hub)
  * draw() (torch.quasirandom.SobolEngine method)
  * draw_base2() (torch.quasirandom.SobolEngine method)
  * driver_allocated_memory() (in module torch.mps)
  * Dropout (class in torch.nn)
  * dropout() (in module torch.nn.functional)
  * Dropout1d (class in torch.nn)
  * dropout1d() (in module torch.nn.functional)
  * Dropout2d (class in torch.nn)
  * dropout2d() (in module torch.nn.functional)
  * Dropout3d (class in torch.nn)
  * dropout3d() (in module torch.nn.functional)
  * dsplit() (in module torch)
    * (torch.Tensor method)
  * dstack() (in module torch)
  * DTensor (class in torch.distributed.tensor)
  * dtype (class in torch)
    * (torch.BFloat16Storage attribute)
    * (torch.BoolStorage attribute)
    * (torch.ByteStorage attribute)
    * (torch.CharStorage attribute)
    * (torch.ComplexDoubleStorage attribute)
    * (torch.ComplexFloatStorage attribute)
    * (torch.DoubleStorage attribute)
    * (torch.FloatStorage attribute)
    * (torch.HalfStorage attribute)
    * (torch.IntStorage attribute)
    * (torch.LongStorage attribute)
    * (torch.QInt32Storage attribute)
    * (torch.QInt8Storage attribute)
    * (torch.QUInt2x4Storage attribute)
    * (torch.QUInt4x2Storage attribute)
    * (torch.QUInt8Storage attribute)
    * (torch.ShortStorage attribute)
    * (torch.TypedStorage attribute)
  * dtype() (torch.onnx.JitScalarType method)
  * DTypeConfig (class in torch.ao.quantization.backend_config)
  * DTypeWithConstraints (class in torch.ao.quantization.backend_config)
  * dual_level (class in torch.autograd.forward_ad)
  * duration (torch.autograd.profiler_util.Kernel attribute)
  * dynamic_shapes() (torch.export.dynamic_shapes.ShapesCollection method)
  * DynamicMetaLoadPlanner (class in torch.distributed.checkpoint.format_utils)
  * DynamicRendezvousHandler (class in torch.distributed.elastic.rendezvous.dynamic_rendezvous)
  * dynamo_export() (in module torch.onnx)

  
---|---  
## E
  * eig() (in module torch.linalg)
  * eigh() (in module torch.linalg)
  * eigvals() (in module torch.linalg)
  * eigvalsh() (in module torch.linalg)
  * einsum() (in module torch)
  * elapsed_time() (torch.cuda.Event method)
    * (torch.Event method)
    * (torch.mps.event.Event method)
    * (torch.mtia.Event method)
    * (torch.xpu.Event method)
  * elapsed_us() (torch.autograd.profiler_util.Interval method)
  * ElasticAgent (class in torch.distributed.elastic.agent.server)
  * element_size() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * eliminate_dead_code() (torch.fx.Graph method)
  * ELU (class in torch.ao.nn.quantized)
  * elu (class in torch.ao.nn.quantized.functional)
  * ELU (class in torch.nn)
  * elu() (in module torch.nn.functional)
  * elu_() (in module torch.nn.functional)
  * Embedding (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * embedding() (in module torch.nn.functional)
  * embedding_bag() (in module torch.nn.functional)
  * EmbeddingBag (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * emit_itt (class in torch.autograd.profiler)
  * emit_nvtx (class in torch.autograd.profiler)
  * empty() (in module torch)
    * (in module torch.distributed.tensor)
  * empty_cache() (in module torch.cuda)
    * (in module torch.mps)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * empty_like() (in module torch)
  * empty_strided() (in module torch)
  * EmptyMatchError (class in torch.package)
  * enable() (in module torch.cuda.tunable)
    * (torch.sparse.check_sparse_tensor_invariants static method)
  * enable_cuda_sanitizer() (in module torch.cuda._sanitizer)
  * enable_cudnn_sdp() (in module torch.backends.cuda)
  * enable_debug_mode() (torch.cuda.CUDAGraph method)
  * enable_fake_mode() (in module torch.onnx)
  * enable_fake_quant (class in torch.ao.quantization.fake_quantize)
  * enable_flash_sdp() (in module torch.backends.cuda)
  * enable_grad (class in torch)
  * enable_math_sdp() (in module torch.backends.cuda)
  * enable_mem_efficient_sdp() (in module torch.backends.cuda)
  * enable_observer (class in torch.ao.quantization.fake_quantize)
  * enable_onednn_fusion() (in module torch.jit)
  * enabled (in module torch.backends.cudnn)
    * (in module torch.backends.opt_einsum)
  * EnforceUnique (class in torch.autograd.profiler)
  * enter_dual_level() (in module torch.autograd.forward_ad)
  * entr() (in module torch.special)
  * entropy() (torch.distributions.bernoulli.Bernoulli method)
    * (torch.distributions.beta.Beta method)
    * (torch.distributions.binomial.Binomial method)
    * (torch.distributions.categorical.Categorical method)
    * (torch.distributions.cauchy.Cauchy method)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli method)
    * (torch.distributions.dirichlet.Dirichlet method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.exp_family.ExponentialFamily method)
    * (torch.distributions.exponential.Exponential method)
    * (torch.distributions.gamma.Gamma method)
    * (torch.distributions.geometric.Geometric method)
    * (torch.distributions.gumbel.Gumbel method)
    * (torch.distributions.half_cauchy.HalfCauchy method)
    * (torch.distributions.half_normal.HalfNormal method)
    * (torch.distributions.independent.Independent method)
    * (torch.distributions.inverse_gamma.InverseGamma method)
    * (torch.distributions.kumaraswamy.Kumaraswamy method)
    * (torch.distributions.laplace.Laplace method)
    * (torch.distributions.log_normal.LogNormal method)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method)
    * (torch.distributions.multinomial.Multinomial method)
    * (torch.distributions.multivariate_normal.MultivariateNormal method)
    * (torch.distributions.normal.Normal method)
    * (torch.distributions.one_hot_categorical.OneHotCategorical method)
    * (torch.distributions.pareto.Pareto method)
    * (torch.distributions.studentT.StudentT method)
    * (torch.distributions.uniform.Uniform method)
    * (torch.distributions.weibull.Weibull method)
    * (torch.distributions.wishart.Wishart method)
  * enumerate_support() (torch.distributions.bernoulli.Bernoulli method)
    * (torch.distributions.binomial.Binomial method)
    * (torch.distributions.categorical.Categorical method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.independent.Independent method)
    * (torch.distributions.one_hot_categorical.OneHotCategorical method)
  * environment variable 
    * PYTORCH_JIT
    * TORCH_COMPILE_JOB_ID
  * eq() (in module torch)
    * (torch.Tensor method)
  * eq_() (torch.Tensor method)
  * equal() (in module torch)
    * (torch.Tensor method)
  * EqualityConstraint (class in torch.fx.experimental.symbolic_shapes)
  * erase_node() (torch.fx.Graph method)
  * erase_step_count() (torch.autograd.profiler.KinetoStepTracker class method)
  * erf() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * erf_() (torch.Tensor method)
  * erfc() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * erfc_() (torch.Tensor method)
  * erfcx() (in module torch.special)
  * erfinv() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * erfinv_() (torch.Tensor method)

| 
  * ErrorHandler (class in torch.distributed.elastic.multiprocessing.errors)
  * EtcdRendezvousBackend (class in torch.distributed.elastic.rendezvous.etcd_rendezvous_backend)
  * EtcdRendezvousHandler (class in torch.distributed.elastic.rendezvous.etcd_rendezvous)
  * EtcdServer (class in torch.distributed.elastic.rendezvous.etcd_server)
  * EtcdStore (class in torch.distributed.elastic.rendezvous.etcd_store)
  * eval() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * evaluate_guards_expression() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * evaluate_guards_for_args() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * evaluate_sym_node() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * evaluate_symexpr() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * Event (class in torch)
    * (class in torch.cuda)
    * (class in torch.distributed.elastic.events.api)
    * (class in torch.monitor)
    * (class in torch.mps.event)
    * (class in torch.mtia)
    * (class in torch.xpu)
  * event_shape (torch.distributions.distribution.Distribution property)
  * EventHandlerHandle (class in torch.monitor)
  * EventMetadataValue (in module torch.distributed.elastic.events.api)
  * events() (torch.profiler._KinetoProfile method)
  * EventSource (class in torch.distributed.elastic.events.api)
  * exception() (torch.distributed.Work method)
  * exit_dual_level() (in module torch.autograd.forward_ad)
  * exp() (in module torch)
    * (torch.Tensor method)
  * exp2() (in module torch)
    * (in module torch.special)
  * exp_() (torch.Tensor method)
  * expand() (torch.distributions.bernoulli.Bernoulli method)
    * (torch.distributions.beta.Beta method)
    * (torch.distributions.binomial.Binomial method)
    * (torch.distributions.categorical.Categorical method)
    * (torch.distributions.cauchy.Cauchy method)
    * (torch.distributions.chi2.Chi2 method)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli method)
    * (torch.distributions.dirichlet.Dirichlet method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.exponential.Exponential method)
    * (torch.distributions.fishersnedecor.FisherSnedecor method)
    * (torch.distributions.gamma.Gamma method)
    * (torch.distributions.geometric.Geometric method)
    * (torch.distributions.gumbel.Gumbel method)
    * (torch.distributions.half_cauchy.HalfCauchy method)
    * (torch.distributions.half_normal.HalfNormal method)
    * (torch.distributions.independent.Independent method)
    * (torch.distributions.inverse_gamma.InverseGamma method)
    * (torch.distributions.kumaraswamy.Kumaraswamy method)
    * (torch.distributions.laplace.Laplace method)
    * (torch.distributions.lkj_cholesky.LKJCholesky method)
    * (torch.distributions.log_normal.LogNormal method)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method)
    * (torch.distributions.mixture_same_family.MixtureSameFamily method)
    * (torch.distributions.multinomial.Multinomial method)
    * (torch.distributions.multivariate_normal.MultivariateNormal method)
    * (torch.distributions.negative_binomial.NegativeBinomial method)
    * (torch.distributions.normal.Normal method)
    * (torch.distributions.one_hot_categorical.OneHotCategorical method)
    * (torch.distributions.pareto.Pareto method)
    * (torch.distributions.poisson.Poisson method)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli method)
    * (torch.distributions.relaxed_bernoulli.RelaxedBernoulli method)
    * (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical method)
    * (torch.distributions.studentT.StudentT method)
    * (torch.distributions.transformed_distribution.TransformedDistribution method)
    * (torch.distributions.uniform.Uniform method)
    * (torch.distributions.von_mises.VonMises method)
    * (torch.distributions.weibull.Weibull method)
    * (torch.distributions.wishart.Wishart method)
    * (torch.Tensor method)
  * expand_as() (torch.Tensor method)
  * expires() (in module torch.distributed.elastic.timer)
  * expit() (in module torch.special)
  * expm1() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * expm1_() (torch.Tensor method)
  * Exponential (class in torch.distributions.exponential)
  * exponential() (in module torch.signal.windows)
  * exponential_() (torch.Tensor method)
  * ExponentialFamily (class in torch.distributions.exp_family)
  * ExponentialLR (class in torch.optim.lr_scheduler)
  * export() (in module torch.export)
    * (in module torch.jit)
    * (in module torch.onnx)
  * export_chrome_trace() (torch.autograd.profiler.profile method)
    * (torch.profiler._KinetoProfile method)
  * export_memory_timeline() (torch.profiler._KinetoProfile method)
  * export_stacks() (torch.profiler._KinetoProfile method)
  * ExportBackwardSignature (class in torch.export)
  * ExportedProgram (class in torch.export)
  * ExportGraphSignature (class in torch.export)
    * (class in torch.export.graph_signature)
  * ExportOptions (class in torch.onnx)
  * ExpTransform (class in torch.distributions.transforms)
  * extend() (torch.nn.ModuleList method)
    * (torch.nn.ParameterList method)
  * extend_logger_results_with_comparison() (in module torch.ao.ns._numeric_suite_fx)
  * extend_results() (torch.utils.benchmark.Compare method)
  * extern() (torch.package.PackageExporter method)
  * ExternalStream (class in torch.cuda)
  * externed_modules() (torch.package.PackageExporter method)
  * extra_repr() (torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.nn.modules.normalization.RMSNorm method)
    * (torch.nn.RMSNorm method)
    * (torch.optim.swa_utils.AveragedModel method)
  * extract_logger_info() (in module torch.ao.ns._numeric_suite_fx)
  * extract_results_from_loggers (class in torch.ao.quantization)
  * extract_results_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx)
  * extract_shadow_logger_info() (in module torch.ao.ns._numeric_suite_fx)
  * extract_weights() (in module torch.ao.ns._numeric_suite_fx)
  * eye() (in module torch)
  * eye_() (in module torch.nn.init)

  
---|---  
## F
  * fake_quantize_per_channel_affine() (in module torch)
  * fake_quantize_per_tensor_affine() (in module torch)
  * FakeQuantize (class in torch.ao.quantization.fake_quantize)
  * FakeQuantizeBase (class in torch.ao.quantization.fake_quantize)
  * fallback() (torch.library.Library method)
  * fallthrough_kernel() (in module torch.library)
  * fast_forward() (torch.quasirandom.SobolEngine method)
  * feature_alpha_dropout() (in module torch.nn.functional)
  * FeatureAlphaDropout (class in torch.nn)
  * fetch_args_kwargs_from_env() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Interpreter method)
  * fetch_attr() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Interpreter method)
  * fft() (in module torch.fft)
  * fft2() (in module torch.fft)
  * fftfreq() (in module torch.fft)
  * fftn() (in module torch.fft)
  * fftshift() (in module torch.fft)
  * file_structure() (torch.package.PackageImporter method)
  * filename (torch.TypedStorage property)
    * (torch.UntypedStorage property)
  * FileStore (class in torch.distributed)
  * FileSystemReader (class in torch.distributed.checkpoint)
  * FileSystemWriter (class in torch.distributed.checkpoint)
  * FileTimerClient (class in torch.distributed.elastic.timer)
  * FileTimerServer (class in torch.distributed.elastic.timer)
  * fill_() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * fill_diagonal_() (torch.Tensor method)
  * fill_uninitialized_memory (in module torch.utils.deterministic)
  * filter() (torch.utils.benchmark.FunctionCounts method)
  * find_mismatch() (in module torch.onnx.verification)
  * find_nodes() (torch.fx.Graph method)
  * finish() (torch.distributed.checkpoint.StorageWriter method)
  * finish_plan() (torch.distributed.checkpoint.LoadPlanner method)
    * (torch.distributed.checkpoint.SavePlanner method)
  * FisherSnedecor (class in torch.distributions.fishersnedecor)
  * fix() (in module torch)
    * (torch.Tensor method)
  * fix_() (torch.Tensor method)
  * FixedQParamsFakeQuantize (class in torch.ao.quantization.fake_quantize)
  * flags() (in module torch.backends.nnpack)
  * flash_sdp_enabled() (in module torch.backends.cuda)
  * FlatArgsAdapter (class in torch.export.unflatten)
  * Flatten (class in torch.nn)
  * flatten() (in module torch)
    * (torch.Tensor method)
  * flatten_parameters() (torch.nn.RNNBase method)
  * flatten_sharded_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * flex_attention() (in module torch.nn.attention.flex_attention)
  * flip() (in module torch)
    * (torch.Tensor method)
  * fliplr() (in module torch)
    * (torch.Tensor method)
  * flipud() (in module torch)
    * (torch.Tensor method)
  * float() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * float16_dynamic_qconfig (in module torch.ao.quantization.qconfig)
  * float16_static_qconfig (in module torch.ao.quantization.qconfig)
  * float8_e4m3fn() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * float8_e4m3fnuz() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * float8_e5m2() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * float8_e5m2fnuz() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * float_power() (in module torch)
    * (torch.Tensor method)
  * float_power_() (torch.Tensor method)
  * float_qparams_weight_only_qconfig (in module torch.ao.quantization.qconfig)
  * FloatFunctional (class in torch.ao.nn.quantized)
  * FloatStorage (class in torch)
  * floor() (in module torch)
    * (torch.Tensor method)
  * floor_() (torch.Tensor method)
  * floor_divide() (in module torch)
    * (torch.Tensor method)
  * floor_divide_() (torch.Tensor method)
  * flush() (torch.utils.tensorboard.writer.SummaryWriter method)
  * fmax() (in module torch)
    * (torch.Tensor method)
  * fmin() (in module torch)
    * (torch.Tensor method)
  * fmod() (in module torch)
    * (torch.Tensor method)
  * fmod_() (torch.Tensor method)
  * Fold (class in torch.nn)
  * fold() (in module torch.nn.functional)
  * forced_specializations() (torch.fx.experimental.symbolic_shapes.DimConstraints method)
  * fork() (in module torch.jit)
  * fork_rng() (in module torch.random)
  * format_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * format_node() (torch.fx.Node method)
  * forward() (torch.ao.nn.quantizable.MultiheadAttention method)
    * (torch.ao.ns._numeric_suite.Logger method)
    * (torch.ao.ns._numeric_suite.OutputLogger method)
    * (torch.ao.ns._numeric_suite.Shadow method)
    * (torch.ao.ns._numeric_suite.ShadowLogger method)
    * (torch.ao.ns._numeric_suite_fx.OutputComparisonLogger method)
    * (torch.ao.ns._numeric_suite_fx.OutputLogger method)
    * (torch.ao.quantization.observer.AffineQuantizedObserverBase method)
    * (torch.ao.quantization.observer.MinMaxObserver method)
    * (torch.autograd.Function static method)
    * (torch.autograd.function.InplaceFunction static method)
    * (torch.autograd.function.NestedIOFunction method)
    * (torch.distributed.fsdp.FullyShardedDataParallel method)
    * (torch.nn.EmbeddingBag method)
    * (torch.nn.Module method)
    * (torch.nn.modules.normalization.RMSNorm method)
    * (torch.nn.MultiheadAttention method)
    * (torch.nn.RMSNorm method)
    * (torch.nn.Transformer method)
    * (torch.nn.TransformerDecoder method)
    * (torch.nn.TransformerDecoderLayer method)
    * (torch.nn.TransformerEncoder method)
    * (torch.nn.TransformerEncoderLayer method)
    * (torch.optim.swa_utils.AveragedModel method)

| 
  * forward_extended() (torch.autograd.function.NestedIOFunction method)
  * forward_shape() (torch.distributions.transforms.Transform method)
  * fp16_bf16_reduction_math_sdp_allowed() (in module torch.backends.cuda)
  * fp16_compress_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks)
  * fp16_compress_wrapper() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks)
  * frac() (in module torch)
    * (torch.Tensor method)
  * frac_() (torch.Tensor method)
  * fractional_max_pool2d() (in module torch.nn.functional)
  * fractional_max_pool3d() (in module torch.nn.functional)
  * FractionalMaxPool2d (class in torch.nn)
  * FractionalMaxPool3d (class in torch.nn)
  * freeze() (in module torch.jit)
    * (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * freeze_bn_stats (class in torch.ao.nn.intrinsic.qat)
  * freeze_runtime_asserts() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * frexp() (in module torch)
    * (torch.Tensor method)
  * from_backend() (torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler class method)
  * from_buffer() (torch.TypedStorage class method)
    * (torch.UntypedStorage static method)
  * from_dict() (torch.ao.quantization.backend_config.BackendConfig class method)
    * (torch.ao.quantization.backend_config.BackendPatternConfig class method)
    * (torch.ao.quantization.backend_config.DTypeConfig class method)
    * (torch.ao.quantization.fx.custom_config.ConvertCustomConfig class method)
    * (torch.ao.quantization.fx.custom_config.FuseCustomConfig class method)
    * (torch.ao.quantization.fx.custom_config.PrepareCustomConfig class method)
    * (torch.ao.quantization.qconfig_mapping.QConfigMapping class method)
  * from_dlpack() (in module torch)
    * (in module torch.utils.dlpack)
  * from_dtype() (torch.onnx.JitScalarType class method)
  * from_file() (in module torch)
    * (torch.TypedStorage class method)
    * (torch.UntypedStorage static method)
  * from_float() (torch.ao.nn.qat.Linear class method)
    * (torch.ao.nn.quantized.Conv1d class method)
    * (torch.ao.nn.quantized.Conv2d class method)
    * (torch.ao.nn.quantized.Conv3d class method)
    * (torch.ao.nn.quantized.dynamic.Linear class method)
    * (torch.ao.nn.quantized.Embedding class method)
    * (torch.ao.nn.quantized.EmbeddingBag class method)
    * (torch.ao.nn.quantized.Linear class method)
  * from_group() (torch.distributed.device_mesh.DeviceMesh static method)
  * from_ipc_handle() (torch.cuda.Event class method)
  * from_kv_blocks() (torch.nn.attention.flex_attention.BlockMask class method)
  * from_local() (torch.distributed.tensor.DTensor static method)
  * from_numpy() (in module torch)
  * from_onnx_type() (torch.onnx.JitScalarType class method)
  * from_pretrained() (torch.nn.Embedding class method)
    * (torch.nn.EmbeddingBag class method)
  * from_reference() (torch.ao.nn.quantized.dynamic.Linear class method)
    * (torch.ao.nn.quantized.Linear class method)
  * from_tensors() (torch.onnx.verification.VerificationInfo class method)
  * from_value() (torch.onnx.JitScalarType class method)
  * frombuffer() (in module torch)
  * fromkeys() (torch.autograd.profiler_util.StringTable method)
    * (torch.nn.ParameterDict method)
  * fsdp_modules() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * FSDPModule (class in torch.distributed.fsdp)
  * full() (in module torch)
    * (in module torch.distributed.tensor)
  * full_kv_indices (torch.nn.attention.flex_attention.BlockMask attribute)
  * full_kv_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute)
  * full_like() (in module torch)
  * full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * full_q_indices (torch.nn.attention.flex_attention.BlockMask attribute)
  * full_q_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute)
  * full_tensor() (torch.distributed.tensor.DTensor method)
  * FullOptimStateDictConfig (class in torch.distributed.fsdp)
  * FullStateDictConfig (class in torch.distributed.fsdp)
  * fully_shard() (in module torch.distributed.fsdp)
  * FullyShardedDataParallel (class in torch.distributed.fsdp)
  * Function (class in torch.autograd)
  * functional_call() (in module torch.func)
    * (in module torch.nn.utils.stateless)
  * functionalize() (in module torch.func)
  * FunctionCounts (class in torch.utils.benchmark)
  * fuse_conv_bn_eval() (in module torch.nn.utils)
  * fuse_conv_bn_weights() (in module torch.nn.utils)
  * fuse_fx (class in torch.ao.quantization.quantize_fx)
  * fuse_linear_bn_eval() (in module torch.nn.utils)
  * fuse_linear_bn_weights() (in module torch.nn.utils)
  * fuse_modules (class in torch.ao.quantization.fuse_modules)
  * FuseCustomConfig (class in torch.ao.quantization.fx.custom_config)
  * FusedMovingAvgObsFakeQuantize (class in torch.ao.quantization.fake_quantize)
  * Future (class in torch.futures)
  * FXFloatFunctional (class in torch.ao.nn.quantized)

  
---|---  
## G
  * Gamma (class in torch.distributions.gamma)
  * gammainc() (in module torch.special)
  * gammaincc() (in module torch.special)
  * gammaln() (in module torch.special)
  * gather() (in module torch)
    * (in module torch.cuda.comm)
    * (in module torch.distributed)
    * (torch.Tensor method)
  * gather_object() (in module torch.distributed)
  * gaussian() (in module torch.signal.windows)
  * gaussian_nll_loss() (in module torch.nn.functional)
  * GaussianNLLLoss (class in torch.nn)
  * gcd() (in module torch)
    * (torch.Tensor method)
  * gcd_() (torch.Tensor method)
  * gds_deregister_buffer() (in module torch.cuda.gds)
  * gds_register_buffer() (in module torch.cuda.gds)
  * GdsFile (class in torch.cuda.gds)
  * ge() (in module torch)
    * (torch.Tensor method)
  * ge_() (torch.Tensor method)
  * GELU (class in torch.nn)
  * gelu() (in module torch.nn.functional)
  * general_cosine() (in module torch.signal.windows)
  * general_hamming() (in module torch.signal.windows)
  * generate_comm_debug_tracing_table() (torch.distributed.tensor.debug.CommDebugMode method)
  * generate_json_dump() (torch.distributed.tensor.debug.CommDebugMode method)
  * generate_methods_for_privateuse1_backend() (in module torch.utils)
  * generate_numeric_debug_handle (class in torch.ao.quantization)
  * generate_square_subsequent_mask() (torch.nn.Transformer static method)
  * Generator (class in torch)
  * Geometric (class in torch.distributions.geometric)
  * geometric_() (torch.Tensor method)
  * geqrf() (in module torch)
    * (torch.Tensor method)
  * ger() (in module torch)
    * (torch.Tensor method)
  * get() (torch.autograd.profiler_util.StringTable method)
    * (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method)
    * (torch.distributed.elastic.rendezvous.RendezvousParameters method)
    * (torch.distributed.Store method)
    * (torch.fx.experimental.symbolic_shapes.CallMethodKey method)
    * (torch.fx.experimental.symbolic_shapes.ConvertIntKey method)
    * (torch.fx.experimental.symbolic_shapes.DivideByKey method)
    * (torch.fx.experimental.symbolic_shapes.InnerTensorKey method)
    * (torch.monitor.Stat method)
    * (torch.nn.ParameterDict method)
  * get_all_groups() (torch.distributed.device_mesh.DeviceMesh method)
  * get_all_sharing_strategies() (in module torch.multiprocessing)
  * get_allocator_backend() (in module torch.cuda)
  * get_arch_list() (in module torch.cuda)
    * (in module torch.xpu)
  * get_as_bool() (torch.distributed.elastic.rendezvous.RendezvousParameters method)
  * get_as_int() (torch.distributed.elastic.rendezvous.RendezvousParameters method)
  * get_attr() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Graph method)
    * (torch.fx.Interpreter method)
    * (torch.fx.Transformer method)
  * get_axioms() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * get_backend() (in module torch.distributed)
    * (torch.distributed.elastic.rendezvous.RendezvousHandler method)
  * get_block_size (class in torch.ao.quantization.observer)
  * get_buffer() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * get_comm_counts() (torch.distributed.tensor.debug.CommDebugMode method)
  * get_compiler_abi_compatibility_and_version() (in module torch.utils.cpp_extension)
  * get_coordinate() (torch.distributed.device_mesh.DeviceMesh method)
  * get_cpp_backtrace() (in module torch.utils)
  * get_cpu_capability() (in module torch.backends.cpu)
  * get_crc32_options() (in module torch.serialization)
  * get_ctx() (in module torch.library)
  * get_debug_state() (torch.jit.ScriptFunction method)
  * get_default_device() (in module torch)
  * get_default_dtype() (in module torch)
  * get_default_load_endianness() (in module torch.serialization)
  * get_default_mmap_options() (in module torch.serialization)
  * get_default_qat_qconfig_mapping (class in torch.ao.quantization.qconfig_mapping)
  * get_default_qconfig_mapping (class in torch.ao.quantization.qconfig_mapping)
  * get_deterministic_debug_mode() (in module torch)
  * get_device() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * get_device_capability() (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * get_device_module() (in module torch)
  * get_device_name() (in module torch.cuda)
    * (in module torch.xpu)
  * get_device_properties() (in module torch.cuda)
    * (in module torch.xpu)
  * get_dir() (in module torch.hub)
  * get_ema_multi_avg_fn() (in module torch.optim.swa_utils)
  * get_entrypoint_name() (torch.distributed.elastic.agent.server.WorkerSpec method)
  * get_expired_timers() (torch.distributed.elastic.timer.TimerServer method)
  * get_extra_state() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * get_fastpath_enabled() (in module torch.backends.mha)
  * get_filename() (in module torch.cuda.tunable)
  * get_float32_matmul_precision() (in module torch)
  * get_fresh_qualname() (torch.fx.Tracer method)
  * get_future() (torch.distributed.Work method)
  * get_future_result() (torch.distributed.Work method)
  * get_gencode_flags() (in module torch.cuda)
    * (in module torch.xpu)
  * get_global_rank() (in module torch.distributed)
  * get_gradient_edge() (in module torch.autograd.graph)
  * get_gradients() (in module torch.distributed.autograd)
  * get_group() (torch.distributed.device_mesh.DeviceMesh method)
  * get_group_rank() (in module torch.distributed)
  * get_ignored_functions() (in module torch.overrides)
  * get_implications() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * get_last_lr() (torch.optim.lr_scheduler.ChainedScheduler method)
    * (torch.optim.lr_scheduler.ConstantLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method)
    * (torch.optim.lr_scheduler.CyclicLR method)
    * (torch.optim.lr_scheduler.ExponentialLR method)
    * (torch.optim.lr_scheduler.LambdaLR method)
    * (torch.optim.lr_scheduler.LinearLR method)
    * (torch.optim.lr_scheduler.LRScheduler method)
    * (torch.optim.lr_scheduler.MultiplicativeLR method)
    * (torch.optim.lr_scheduler.MultiStepLR method)
    * (torch.optim.lr_scheduler.OneCycleLR method)
    * (torch.optim.lr_scheduler.PolynomialLR method)
    * (torch.optim.lr_scheduler.ReduceLROnPlateau method)
    * (torch.optim.lr_scheduler.SequentialLR method)
    * (torch.optim.lr_scheduler.StepLR method)
    * (torch.optim.swa_utils.SWALR method)
  * get_local_rank() (torch.distributed.device_mesh.DeviceMesh method)
  * get_logger_dict() (in module torch.ao.ns._numeric_suite)
  * get_logging_handler() (in module torch.distributed.elastic.events)

| 
  * get_lr() (torch.optim.lr_scheduler.ChainedScheduler method)
    * (torch.optim.lr_scheduler.ConstantLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method)
    * (torch.optim.lr_scheduler.CyclicLR method)
    * (torch.optim.lr_scheduler.ExponentialLR method)
    * (torch.optim.lr_scheduler.LambdaLR method)
    * (torch.optim.lr_scheduler.LinearLR method)
    * (torch.optim.lr_scheduler.LRScheduler method)
    * (torch.optim.lr_scheduler.MultiplicativeLR method)
    * (torch.optim.lr_scheduler.MultiStepLR method)
    * (torch.optim.lr_scheduler.OneCycleLR method)
    * (torch.optim.lr_scheduler.PolynomialLR method)
    * (torch.optim.lr_scheduler.ReduceLROnPlateau method)
    * (torch.optim.lr_scheduler.SequentialLR method)
    * (torch.optim.lr_scheduler.StepLR method)
    * (torch.optim.swa_utils.SWALR method)
  * get_matching_activations() (in module torch.ao.ns._numeric_suite)
  * get_max_tuning_duration() (in module torch.cuda.tunable)
  * get_max_tuning_iterations() (in module torch.cuda.tunable)
  * get_model_state_dict() (in module torch.distributed.checkpoint.state_dict)
  * get_module_rref() (torch.distributed.nn.api.remote_module.RemoteModule method)
  * get_nontrivial_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * get_num_interop_threads() (in module torch)
  * get_num_threads() (in module torch)
  * get_observer_state_dict (class in torch.ao.quantization.observer)
  * get_op_functions() (torch.onnx.OnnxRegistry method)
  * get_opt_einsum() (in module torch.backends.opt_einsum)
  * get_optimizer_state_dict() (in module torch.distributed.checkpoint.state_dict)
  * get_overridable_functions() (in module torch.overrides)
  * get_overwrite_module_params_on_conversion() (in module torch.__future__)
  * get_parameter() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * get_parameter_info() (torch.distributed.tensor.debug.CommDebugMode method)
  * get_per_process_memory_fraction() (in module torch.cuda)
  * get_process_group_ranks() (in module torch.distributed)
  * get_proxy_mode() (in module torch.fx.experimental.proxy_tensor)
  * get_pruned_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * get_rank() (in module torch.distributed)
    * (torch.distributed.device_mesh.DeviceMesh method)
  * get_rdeps() (torch.package.PackageExporter method)
  * get_replace_hook() (torch.export.graph_signature.ExportGraphSignature method)
  * get_results() (in module torch.cuda.tunable)
  * get_rng_state() (in module torch)
    * (in module torch.cuda)
    * (in module torch.mps)
    * (in module torch.mtia)
    * (in module torch.random)
    * (in module torch.xpu)
  * get_rng_state_all() (in module torch.cuda)
    * (in module torch.xpu)
  * get_rotating_buffer_size() (in module torch.cuda.tunable)
  * get_run_id() (torch.distributed.elastic.rendezvous.RendezvousHandler method)
  * get_safe_globals() (in module torch.serialization)
  * get_sharding_info() (torch.distributed.tensor.debug.CommDebugMode method)
  * get_sharing_strategy() (in module torch.multiprocessing)
  * get_state() (torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend method)
    * (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend method)
    * (torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend method)
    * (torch.Generator method)
  * get_state_dict() (in module torch.distributed.checkpoint.state_dict)
  * get_state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * get_stream_from_external() (in module torch.cuda)
    * (in module torch.xpu)
  * get_submodule() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * get_subprocess_handler() (in module torch.distributed.elastic.multiprocessing.subprocess_handler.handlers)
  * get_swap_module_params_on_conversion() (in module torch.__future__)
  * get_sync_debug_mode() (in module torch.cuda)
  * get_testing_overrides() (in module torch.overrides)
  * get_total_counts() (torch.distributed.tensor.debug.CommDebugMode method)
  * get_total_norm() (in module torch.nn.utils)
  * get_trace_id() (torch.profiler.profile method)
  * get_unique_id() (torch.package.PackageExporter method)
  * get_unsafe_globals_in_checkpoint() (in module torch.serialization)
  * get_validators() (in module torch.cuda.tunable)
  * get_worker_group() (torch.distributed.elastic.agent.server.ElasticAgent method)
  * get_worker_info() (in module torch.distributed.rpc)
    * (in module torch.utils.data)
  * get_world_size() (in module torch.distributed)
  * getattr() (torch.fx.Tracer method)
  * global_unstructured() (in module torch.nn.utils.prune)
  * GLU (class in torch.nn)
  * glu() (in module torch.nn.functional)
  * grad (torch.Tensor attribute)
  * grad() (in module torch.autograd)
    * (in module torch.func)
  * grad_and_value() (in module torch.func)
  * GradBucket (class in torch.distributed)
  * gradcheck() (in module torch.autograd.gradcheck)
  * GradcheckError
  * gradgradcheck() (in module torch.autograd.gradcheck)
  * gradient() (in module torch)
  * GradientEdge (class in torch.autograd.graph)
  * gradients() (in module torch.distributed.GradBucket)
  * GradScaler (class in torch.cpu.amp)
    * (class in torch.cuda.amp)
  * Granularity (class in torch.ao.quantization.observer)
  * graph (class in torch.cuda)
  * Graph (class in torch.fx)
  * graph (torch.fx.GraphModule property)
    * (torch.jit.ScriptModule property)
  * graph_copy() (torch.fx.Graph method)
  * graph_pool_handle() (in module torch.cuda)
  * GraphInfo (class in torch.onnx.verification)
  * GraphInfoPrettyPrinter (class in torch.onnx.verification)
  * GraphModule (class in torch.fx)
  * graphsafe_get_state() (torch.Generator method)
  * graphsafe_set_state() (torch.Generator method)
  * greater() (in module torch)
    * (torch.Tensor method)
  * greater_() (torch.Tensor method)
  * greater_equal() (in module torch)
    * (torch.Tensor method)
  * greater_equal_() (torch.Tensor method)
  * greater_than (in module torch.distributions.constraints)
  * greater_than_eq (in module torch.distributions.constraints)
  * grid_sample() (in module torch.nn.functional)
  * group_norm() (in module torch.nn.functional)
  * GroupNorm (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * GRU (class in torch.ao.nn.quantized.dynamic)
    * (class in torch.nn)
  * GRUCell (class in torch.ao.nn.quantized.dynamic)
    * (class in torch.nn)
  * gt() (in module torch)
    * (torch.Tensor method)
  * gt_() (torch.Tensor method)
  * guard_size_oblivious() (in module torch.fx.experimental.symbolic_shapes)
  * Gumbel (class in torch.distributions.gumbel)
  * gumbel_softmax() (in module torch.nn.functional)

  
---|---  
## H
  * H (torch.Tensor attribute)
  * half() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * half_open_interval (in module torch.distributions.constraints)
  * HalfCauchy (class in torch.distributions.half_cauchy)
  * HalfNormal (class in torch.distributions.half_normal)
  * HalfStorage (class in torch)
  * hamming() (in module torch.signal.windows)
  * hamming_window() (in module torch)
  * handle_sym_dispatch() (in module torch.fx.experimental.proxy_tensor)
  * handle_torch_function() (in module torch.overrides)
  * hann() (in module torch.signal.windows)
  * hann_window() (in module torch)
  * Hardshrink (class in torch.nn)
  * hardshrink() (in module torch.nn.functional)
    * (torch.Tensor method)
  * hardsigmoid (class in torch.ao.nn.quantized.functional)
  * Hardsigmoid (class in torch.nn)
  * hardsigmoid() (in module torch.nn.functional)
  * Hardswish (class in torch.ao.nn.quantized)
  * hardswish (class in torch.ao.nn.quantized.functional)
  * Hardswish (class in torch.nn)
  * hardswish() (in module torch.nn.functional)
  * hardtanh (class in torch.ao.nn.quantized.functional)
  * Hardtanh (class in torch.nn)
  * hardtanh() (in module torch.nn.functional)
  * hardtanh_() (in module torch.nn.functional)
  * has_enumerate_support (torch.distributions.bernoulli.Bernoulli attribute)
    * (torch.distributions.binomial.Binomial attribute)
    * (torch.distributions.categorical.Categorical attribute)
    * (torch.distributions.independent.Independent property)
    * (torch.distributions.one_hot_categorical.OneHotCategorical attribute)
  * has_extended_api() (torch.distributed.Store method)
  * has_file() (torch.package.Directory method)
  * has_free_symbols() (in module torch.fx.experimental.symbolic_shapes)
  * has_free_unbacked_symbols() (in module torch.fx.experimental.symbolic_shapes)
  * has_rsample (torch.distributions.beta.Beta attribute)
    * (torch.distributions.cauchy.Cauchy attribute)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli attribute)
    * (torch.distributions.dirichlet.Dirichlet attribute)
    * (torch.distributions.exponential.Exponential attribute)
    * (torch.distributions.fishersnedecor.FisherSnedecor attribute)
    * (torch.distributions.gamma.Gamma attribute)
    * (torch.distributions.half_cauchy.HalfCauchy attribute)
    * (torch.distributions.half_normal.HalfNormal attribute)
    * (torch.distributions.independent.Independent property)
    * (torch.distributions.inverse_gamma.InverseGamma attribute)
    * (torch.distributions.kumaraswamy.Kumaraswamy attribute)
    * (torch.distributions.laplace.Laplace attribute)
    * (torch.distributions.log_normal.LogNormal attribute)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal attribute)
    * (torch.distributions.mixture_same_family.MixtureSameFamily attribute)
    * (torch.distributions.multivariate_normal.MultivariateNormal attribute)
    * (torch.distributions.normal.Normal attribute)
    * (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)
    * (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)
    * (torch.distributions.studentT.StudentT attribute)
    * (torch.distributions.transformed_distribution.TransformedDistribution property)
    * (torch.distributions.uniform.Uniform attribute)
    * (torch.distributions.von_mises.VonMises attribute)
    * (torch.distributions.wishart.Wishart attribute)

| 
  * has_torch_function() (in module torch.overrides)
  * has_uninitialized_params() (torch.nn.modules.lazy.LazyModuleMixin method)
  * HashStore (class in torch.distributed)
  * HealthCheckServer (class in torch.distributed.elastic.agent.server.health_check_server)
  * heartbeat (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property)
  * heaviside() (in module torch)
    * (torch.Tensor method)
  * help() (in module torch.hub)
  * hessian() (in module torch.autograd.functional)
    * (in module torch.func)
  * hex() (torch.SymFloat method)
  * hfft() (in module torch.fft)
  * hfft2() (in module torch.fft)
  * hfftn() (in module torch.fft)
  * highlight_warnings() (torch.utils.benchmark.Compare method)
  * hinge_embedding_loss() (in module torch.nn.functional)
  * HingeEmbeddingLoss (class in torch.nn)
  * hint_int() (in module torch.fx.experimental.symbolic_shapes)
  * histc() (in module torch)
    * (torch.Tensor method)
  * histogram() (in module torch)
    * (torch.Tensor method)
  * histogramdd() (in module torch)
  * HistogramObserver (class in torch.ao.quantization.observer)
  * host (torch.distributed.TCPStore property)
  * host_memory_stats() (in module torch.cuda)
  * householder_product() (in module torch.linalg)
  * hpu() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * hsplit() (in module torch)
    * (torch.Tensor method)
  * hspmm() (in module torch)
  * hstack() (in module torch)
  * huber_loss() (in module torch.nn.functional)
  * HuberLoss (class in torch.nn)
  * hvp() (in module torch.autograd.functional)
  * hypot() (in module torch)
    * (torch.Tensor method)
  * hypot_() (torch.Tensor method)

  
---|---  
## I
  * i0() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * i0_() (torch.Tensor method)
  * i0e() (in module torch.special)
  * i1() (in module torch.special)
  * i1e() (in module torch.special)
  * icdf() (torch.distributions.cauchy.Cauchy method)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.exponential.Exponential method)
    * (torch.distributions.half_cauchy.HalfCauchy method)
    * (torch.distributions.half_normal.HalfNormal method)
    * (torch.distributions.laplace.Laplace method)
    * (torch.distributions.normal.Normal method)
    * (torch.distributions.transformed_distribution.TransformedDistribution method)
    * (torch.distributions.uniform.Uniform method)
  * id (torch.cuda.MemPool property)
    * (torch.distributed.rpc.WorkerInfo property)
  * id() (torch.package.PackageImporter method)
  * Identity (class in torch.nn)
    * (class in torch.nn.utils.prune)
  * identity() (in module torch.nn.utils.prune)
  * ifft() (in module torch.fft)
  * ifft2() (in module torch.fft)
  * ifftn() (in module torch.fft)
  * ifftshift() (in module torch.fft)
  * igamma() (in module torch)
    * (torch.Tensor method)
  * igamma_() (torch.Tensor method)
  * igammac() (in module torch)
    * (torch.Tensor method)
  * igammac_() (torch.Tensor method)
  * ignore() (in module torch.jit)
  * ignore_fresh_unbacked_symbols() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * ihfft() (in module torch.fft)
  * ihfft2() (in module torch.fft)
  * ihfftn() (in module torch.fft)
  * imag (torch.Tensor attribute)
  * imag() (in module torch)
  * impl() (in module torch.library)
    * (torch.library.Library method)
  * impl_abstract() (in module torch.library)
  * import_module() (torch.package.PackageImporter method)
  * in_interval() (torch.autograd.profiler_util.MemRecordsAcc method)
  * include_paths() (in module torch.utils.cpp_extension)
  * increment_step() (torch.autograd.profiler.KinetoStepTracker class method)
  * increment_version() (in module torch.autograd.graph)
  * Independent (class in torch.distributions.independent)
  * independent (in module torch.distributions.constraints)
  * IndependentTransform (class in torch.distributions.transforms)
  * index() (in module torch.distributed.GradBucket)
    * (torch.autograd.forward_ad.UnpackedDualTensor method)
    * (torch.autograd.profiler_util.Kernel method)
    * (torch.jit.Attribute method)
    * (torch.nn.utils.rnn.PackedSequence method)
    * (torch.Size method)
  * index_add() (in module torch)
    * (torch.Tensor method)
  * index_add_() (torch.Tensor method)
  * index_copy() (in module torch)
    * (torch.Tensor method)
  * index_copy_() (torch.Tensor method)
  * index_fill() (torch.Tensor method)
  * index_fill_() (torch.Tensor method)
  * index_put() (torch.Tensor method)
  * index_put_() (torch.Tensor method)
  * index_reduce() (in module torch)
    * (torch.Tensor method)
  * index_reduce_() (torch.Tensor method)
  * index_select() (in module torch)
    * (torch.Tensor method)
  * indices() (torch.Tensor method)
  * infer_schema() (in module torch.library)
  * inference_mode (class in torch.autograd.grad_mode)
  * init() (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * init_device_mesh() (in module torch.distributed.device_mesh)
  * init_method (torch.distributed.rpc.RpcBackendOptions property)
    * (torch.distributed.rpc.TensorPipeRpcBackendOptions property)
  * init_process_group() (in module torch.distributed)
  * init_rpc() (in module torch.distributed.rpc)
  * init_step_count() (torch.autograd.profiler.KinetoStepTracker class method)
  * initial_seed() (in module torch)
    * (in module torch.cuda)
    * (in module torch.random)
    * (in module torch.xpu)
    * (torch.Generator method)
  * initialize_inference_session() (torch.onnx.ONNXProgram method)
  * initialize_parameters() (torch.nn.modules.lazy.LazyModuleMixin method)
  * inlined_graph (torch.jit.ScriptModule property)
  * inner() (in module torch)
    * (torch.Tensor method)
  * InnerTensorKey (class in torch.fx.experimental.symbolic_shapes)
  * InplaceFunction (class in torch.autograd.function)
  * INPUT_OUTPUT_NOT_OBSERVED (torch.ao.quantization.backend_config.ObservationType attribute)
  * InputKind (class in torch.export.graph_signature)
  * InputSpec (class in torch.export.graph_signature)
  * insert() (torch.nn.ModuleList method)
  * insert_arg() (torch.fx.Node method)
  * inserting_after() (torch.fx.Graph method)
  * inserting_before() (torch.fx.Graph method)
  * instance_norm() (in module torch.nn.functional)
  * InstanceNorm1d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * InstanceNorm2d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * InstanceNorm3d (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * int() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * int_repr() (torch.Tensor method)
  * integer_interval (in module torch.distributions.constraints)
  * interface() (in module torch.jit)
  * intern() (torch.package.PackageExporter method)
  * interned_modules() (torch.package.PackageExporter method)
  * interpolate (class in torch.ao.nn.quantized.functional)
  * interpolate() (in module torch.nn.functional)
  * Interpreter (class in torch.fx)
  * InterpreterModule (class in torch.export.unflatten)
  * InterpreterModuleDispatcher (class in torch.export.unflatten)
  * Interval (class in torch.autograd.profiler_util)
  * interval (in module torch.distributions.constraints)
  * IntStorage (class in torch)
  * inv (torch.distributions.transforms.Transform property)
  * inv() (in module torch.linalg)
  * inv_ex() (in module torch.linalg)
  * inverse() (in module torch)
    * (torch.Tensor method)
  * inverse_shape() (torch.distributions.transforms.Transform method)
  * InverseGamma (class in torch.distributions.inverse_gamma)
  * ipc_collect() (in module torch.cuda)
  * ipc_handle() (torch.cuda.Event method)
  * ipu() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)

| 
  * irecv() (in module torch.distributed)
  * irfft() (in module torch.fft)
  * irfft2() (in module torch.fft)
  * irfftn() (in module torch.fft)
  * is_accessor_node() (in module torch.fx.experimental.symbolic_shapes)
  * is_autocast_available() (in module torch.amp.autocast_mode)
  * is_available() (in module torch.accelerator)
    * (in module torch.backends.cudnn)
    * (in module torch.backends.cusparselt)
    * (in module torch.backends.mkl)
    * (in module torch.backends.mkldnn)
    * (in module torch.backends.mps)
    * (in module torch.backends.nnpack)
    * (in module torch.backends.openmp)
    * (in module torch.backends.opt_einsum)
    * (in module torch.cpu)
    * (in module torch.cuda)
    * (in module torch.distributed)
    * (in module torch.mtia)
    * (in module torch.profiler.itt)
    * (in module torch.xpu)
  * is_built() (in module torch.backends.cuda)
    * (in module torch.backends.mps)
  * is_capturing_metal() (in module torch.mps.profiler)
  * is_closed() (torch.distributed.elastic.rendezvous.RendezvousHandler method)
  * is_coalesced() (torch.Tensor method)
  * is_compiling() (in module torch.compiler)
  * is_completed() (torch.distributed.Work method)
  * is_complex() (in module torch)
    * (torch.Tensor method)
  * is_concrete_bool() (in module torch.fx.experimental.symbolic_shapes)
  * is_concrete_float() (in module torch.fx.experimental.symbolic_shapes)
  * is_concrete_int() (in module torch.fx.experimental.symbolic_shapes)
  * is_conj() (in module torch)
    * (torch.Tensor method)
  * is_contiguous() (torch.Tensor method)
  * is_cuda (torch.nn.utils.rnn.PackedSequence property)
    * (torch.Tensor attribute)
    * (torch.TypedStorage property)
    * (torch.UntypedStorage property)
  * is_current_stream_capturing() (in module torch.cuda)
  * is_dependent() (in module torch.distributions.constraints)
  * is_deterministic_algorithms_warn_only_enabled() (in module torch)
  * is_dynamo_compiling() (in module torch.compiler)
  * is_enabled() (in module torch.cuda.tunable)
    * (torch.sparse.check_sparse_tensor_invariants static method)
  * is_exporting() (in module torch.compiler)
  * is_flash_attention_available() (in module torch.backends.cuda)
  * is_floating_point() (in module torch)
    * (torch.Tensor method)
  * is_gloo_available() (in module torch.distributed)
  * is_grad_enabled() (in module torch)
  * is_hpu (torch.TypedStorage property)
    * (torch.UntypedStorage property)
  * is_impure() (torch.fx.Node method)
  * is_in_onnx_export() (in module torch.onnx)
  * is_inference() (torch.Tensor method)
  * is_inference_mode_enabled() (in module torch)
  * is_initialized() (in module torch.cuda)
    * (in module torch.distributed)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * is_integer() (torch.SymFloat method)
  * is_last() (in module torch.distributed.GradBucket)
  * is_leaf (torch.Tensor attribute)
  * is_leaf_module() (torch.ao.ns._numeric_suite_fx.NSTracer method)
    * (torch.fx.Tracer method)
  * is_meta (torch.Tensor attribute)
  * is_metal_capture_enabled() (in module torch.mps.profiler)
  * is_mpi_available() (in module torch.distributed)
  * is_nccl_available() (in module torch.distributed)
  * is_ninja_available() (in module torch.utils.cpp_extension)
  * is_nonzero() (in module torch)
  * is_onnxrt_backend_supported() (in module torch.onnx)
  * is_owner() (torch.distributed.rpc.PyRRef method)
  * is_parametrized() (in module torch.nn.utils.parametrize)
  * is_partial() (torch.distributed.tensor.placement_types.Placement method)
  * is_pinned() (torch.nn.utils.rnn.PackedSequence method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * is_pruned() (in module torch.nn.utils.prune)
  * is_quantized (torch.Tensor attribute)
  * is_registered_op() (torch.onnx.OnnxRegistry method)
  * is_replicate() (torch.distributed.tensor.placement_types.Placement method)
  * is_running() (torch.distributed.elastic.agent.server.WorkerState static method)
  * is_scripting() (in module torch.jit)
  * is_set_to() (torch.Tensor method)
  * is_shard() (torch.distributed.tensor.placement_types.Placement method)
  * is_shared() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * is_signed() (torch.Tensor method)
  * is_sparse (torch.Tensor attribute)
    * (torch.TypedStorage attribute)
    * (torch.UntypedStorage attribute)
  * is_sparse_csr (torch.Tensor attribute)
    * (torch.UntypedStorage attribute)
  * is_storage() (in module torch)
  * is_success() (torch.distributed.Work method)
  * is_tensor() (in module torch)
  * is_tensor_like() (in module torch.overrides)
  * is_tensor_method_or_property() (in module torch.overrides)
  * is_tf32_supported() (in module torch.cuda)
  * is_torchelastic_launched() (in module torch.distributed)
  * is_tracing() (in module torch.jit)
  * is_unbacked_symint() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * is_warn_always_enabled() (in module torch)
  * is_xccl_available() (in module torch.distributed.distributed_c10d)
  * isclose() (in module torch)
    * (torch.Tensor method)
  * isend() (in module torch.distributed)
  * isfinite() (in module torch)
    * (torch.Tensor method)
  * isin() (in module torch)
  * isinf() (in module torch)
    * (torch.Tensor method)
  * isinstance() (in module torch.jit)
  * isnan() (in module torch)
    * (torch.Tensor method)
  * isneginf() (in module torch)
    * (torch.Tensor method)
  * isposinf() (in module torch)
    * (torch.Tensor method)
  * isreal() (in module torch)
    * (torch.Tensor method)
  * istft() (in module torch)
    * (torch.Tensor method)
  * item() (torch.Tensor method)
  * items() (torch.autograd.profiler_util.StringTable method)
    * (torch.export.decomp_utils.CustomDecompTable method)
    * (torch.nn.ModuleDict method)
    * (torch.nn.ParameterDict method)
  * itemsize (torch.Tensor attribute)
  * iter() (torch.fx.Tracer method)
  * IterableDataset (class in torch.utils.data)

  
---|---  
## J
  * jacfwd() (in module torch.func)
  * jacobian() (in module torch.autograd.functional)
  * jacrev() (in module torch.func)
  * JitScalarType (class in torch.onnx)
  * job_id (in module torch.compiler.config)
  * Join (class in torch.distributed.algorithms)
  * join (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property)
  * join() (torch.multiprocessing.SpawnContext method)
    * (torch.nn.parallel.DistributedDataParallel method)
  * join_device (torch.distributed.algorithms.Joinable property)
    * (torch.distributed.optim.ZeroRedundancyOptimizer property)

| 
  * join_hook() (torch.distributed.algorithms.Joinable method)
    * (torch.distributed.optim.ZeroRedundancyOptimizer method)
    * (torch.nn.parallel.DistributedDataParallel method)
  * join_process_group (torch.distributed.algorithms.Joinable property)
    * (torch.distributed.optim.ZeroRedundancyOptimizer property)
  * Joinable (class in torch.distributed.algorithms)
  * JoinHook (class in torch.distributed.algorithms)
  * jvp() (in module torch.autograd.functional)
    * (in module torch.func)
    * (torch.autograd.Function static method)
    * (torch.autograd.function.InplaceFunction static method)
    * (torch.autograd.function.NestedIOFunction static method)

  
---|---  
## K
  * kaiming_normal_() (in module torch.nn.init)
  * kaiming_uniform_() (in module torch.nn.init)
  * kaiser() (in module torch.signal.windows)
  * kaiser_window() (in module torch)
  * Kernel (class in torch.autograd.profiler_util)
  * key_averages() (torch.autograd.profiler.profile method)
    * (torch.profiler._KinetoProfile method)
  * keys() (torch.autograd.profiler_util.StringTable method)
    * (torch.export.decomp_utils.CustomDecompTable method)
    * (torch.fx.Tracer method)
    * (torch.nn.ModuleDict method)
    * (torch.nn.ParameterDict method)

| 
  * KinetoStepTracker (class in torch.autograd.profiler)
  * kl_div() (in module torch.nn.functional)
  * kl_divergence() (in module torch.distributions.kl)
  * KLDivLoss (class in torch.nn)
  * kron() (in module torch)
  * kthvalue() (in module torch)
    * (torch.Tensor method)
  * Kumaraswamy (class in torch.distributions.kumaraswamy)
  * kv_indices (torch.nn.attention.flex_attention.BlockMask attribute)
  * kv_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute)
  * kwargs (torch.fx.Node property)

  
---|---  
## L
  * l1_loss() (in module torch.nn.functional)
  * l1_unstructured() (in module torch.nn.utils.prune)
  * L1Loss (class in torch.nn)
  * L1Unstructured (class in torch.nn.utils.prune)
  * LambdaLR (class in torch.optim.lr_scheduler)
  * Laplace (class in torch.distributions.laplace)
  * last_call (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property)
  * layer_norm() (in module torch.nn.functional)
  * LayerNorm (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * layout (class in torch)
  * LazyBatchNorm1d (class in torch.nn)
  * LazyBatchNorm2d (class in torch.nn)
  * LazyBatchNorm3d (class in torch.nn)
  * LazyConv1d (class in torch.nn)
  * LazyConv2d (class in torch.nn)
  * LazyConv3d (class in torch.nn)
  * LazyConvTranspose1d (class in torch.nn)
  * LazyConvTranspose2d (class in torch.nn)
  * LazyConvTranspose3d (class in torch.nn)
  * LazyInstanceNorm1d (class in torch.nn)
  * LazyInstanceNorm2d (class in torch.nn)
  * LazyInstanceNorm3d (class in torch.nn)
  * LazyLinear (class in torch.nn)
  * LazyModuleMixin (class in torch.nn.modules.lazy)
  * LBFGS (class in torch.optim)
  * lcm() (in module torch)
    * (torch.Tensor method)
  * lcm_() (torch.Tensor method)
  * ldexp() (in module torch)
    * (torch.Tensor method)
  * ldexp_() (torch.Tensor method)
  * ldl_factor() (in module torch.linalg)
  * ldl_factor_ex() (in module torch.linalg)
  * ldl_solve() (in module torch.linalg)
  * le() (in module torch)
    * (torch.Tensor method)
  * le_() (torch.Tensor method)
  * leaky_relu (class in torch.ao.nn.quantized.functional)
  * leaky_relu() (in module torch.nn.functional)
  * leaky_relu_() (in module torch.nn.functional)
  * LeakyReLU (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * lerp() (in module torch)
    * (torch.Tensor method)
  * lerp_() (torch.Tensor method)
  * less() (in module torch)
    * (torch.Tensor method)
  * less_() (torch.Tensor method)
  * less_equal() (in module torch)
    * (torch.Tensor method)
  * less_equal_() (torch.Tensor method)
  * less_than (in module torch.distributions.constraints)
  * lgamma() (in module torch)
    * (torch.Tensor method)
  * lgamma_() (torch.Tensor method)
  * Library (class in torch.library)
  * libuvBackend (torch.distributed.TCPStore property)
  * Linear (class in torch.ao.nn.qat)
    * (class in torch.ao.nn.qat.dynamic)
    * (class in torch.ao.nn.quantized)
    * (class in torch.ao.nn.quantized.dynamic)
  * linear (class in torch.ao.nn.quantized.functional)
  * Linear (class in torch.nn)
  * linear() (in module torch.nn.functional)
  * linearize() (in module torch.func)
  * LinearLR (class in torch.optim.lr_scheduler)
  * LinearReLU (class in torch.ao.nn.intrinsic)
    * (class in torch.ao.nn.intrinsic.qat)
    * (class in torch.ao.nn.intrinsic.quantized)
    * (class in torch.ao.nn.intrinsic.quantized.dynamic)
  * linspace() (in module torch)
  * lint() (torch.fx.Graph method)
  * list() (in module torch.hub)
  * list_backends() (in module torch.compiler)
  * list_gpu_processes() (in module torch.cuda)
  * LKJCholesky (class in torch.distributions.lkj_cholesky)
  * ln_structured() (in module torch.nn.utils.prune)
  * LnStructured (class in torch.nn.utils.prune)
  * load() (in module torch)
    * (in module torch.distributed.checkpoint.state_dict_loader)
    * (in module torch.export)
    * (in module torch.hub)
    * (in module torch.jit)
    * (in module torch.utils.cpp_extension)
  * load_binary() (torch.package.PackageImporter method)
  * load_bytes() (torch.distributed.checkpoint.LoadPlanner method)
  * load_inline() (in module torch.utils.cpp_extension)
  * load_nvprof() (in module torch.autograd.profiler)
  * load_observer_state_dict (class in torch.ao.quantization.observer)
  * load_pickle() (torch.package.PackageImporter method)
  * load_state_dict() (in module torch.distributed.checkpoint.state_dict_loader)
    * (torch.distributed.checkpoint.stateful.Stateful method)
    * (torch.distributed.optim.PostLocalSGDOptimizer method)
    * (torch.distributed.optim.ZeroRedundancyOptimizer method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.lr_scheduler.ChainedScheduler method)
    * (torch.optim.lr_scheduler.ConstantLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method)
    * (torch.optim.lr_scheduler.CyclicLR method)
    * (torch.optim.lr_scheduler.ExponentialLR method)
    * (torch.optim.lr_scheduler.LambdaLR method)
    * (torch.optim.lr_scheduler.LinearLR method)
    * (torch.optim.lr_scheduler.LRScheduler method)
    * (torch.optim.lr_scheduler.MultiplicativeLR method)
    * (torch.optim.lr_scheduler.MultiStepLR method)
    * (torch.optim.lr_scheduler.OneCycleLR method)
    * (torch.optim.lr_scheduler.PolynomialLR method)
    * (torch.optim.lr_scheduler.ReduceLROnPlateau method)
    * (torch.optim.lr_scheduler.SequentialLR method)
    * (torch.optim.lr_scheduler.StepLR method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.optim.swa_utils.SWALR method)
  * load_state_dict_from_url() (in module torch.hub)
  * load_storage() (torch.cuda.gds.GdsFile method)
  * load_text() (torch.package.PackageImporter method)
  * load_url() (in module torch.utils.model_zoo)
  * LoadPlan (class in torch.distributed.checkpoint)
  * LoadPlanner (class in torch.distributed.checkpoint)
  * lobpcg() (in module torch)
  * loc (torch.distributions.log_normal.LogNormal property)
  * local_map() (in module torch.distributed.tensor.experimental)
  * local_response_norm() (in module torch.nn.functional)
  * local_value() (torch.distributed.rpc.PyRRef method)
  * LocalElasticAgent (class in torch.distributed.elastic.agent.server.local_elastic_agent)
  * LocalOptimStateDictConfig (class in torch.distributed.fsdp)

| 
  * LocalResponseNorm (class in torch.nn)
  * LocalStateDictConfig (class in torch.distributed.fsdp)
  * LocalTimerClient (class in torch.distributed.elastic.timer)
  * LocalTimerServer (class in torch.distributed.elastic.timer)
  * log() (in module torch)
    * (torch.Tensor method)
  * log10() (in module torch)
    * (torch.Tensor method)
  * log10_() (torch.Tensor method)
  * log1p() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * log1p_() (torch.Tensor method)
  * log2() (in module torch)
    * (torch.Tensor method)
  * log2_() (torch.Tensor method)
  * log_() (torch.Tensor method)
  * log_abs_det_jacobian() (torch.distributions.transforms.Transform method)
  * log_comm_debug_tracing_table_to_file() (torch.distributed.tensor.debug.CommDebugMode method)
  * log_debug_info_for_expired_timers() (in module torch.distributed.elastic.timer.debug_info_logging)
  * log_event() (in module torch.monitor)
  * log_ndtr() (in module torch.special)
  * log_normal_() (torch.Tensor method)
  * log_prob() (torch.distributions.bernoulli.Bernoulli method)
    * (torch.distributions.beta.Beta method)
    * (torch.distributions.binomial.Binomial method)
    * (torch.distributions.categorical.Categorical method)
    * (torch.distributions.cauchy.Cauchy method)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli method)
    * (torch.distributions.dirichlet.Dirichlet method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.exponential.Exponential method)
    * (torch.distributions.fishersnedecor.FisherSnedecor method)
    * (torch.distributions.gamma.Gamma method)
    * (torch.distributions.geometric.Geometric method)
    * (torch.distributions.gumbel.Gumbel method)
    * (torch.distributions.half_cauchy.HalfCauchy method)
    * (torch.distributions.half_normal.HalfNormal method)
    * (torch.distributions.independent.Independent method)
    * (torch.distributions.laplace.Laplace method)
    * (torch.distributions.lkj_cholesky.LKJCholesky method)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method)
    * (torch.distributions.mixture_same_family.MixtureSameFamily method)
    * (torch.distributions.multinomial.Multinomial method)
    * (torch.distributions.multivariate_normal.MultivariateNormal method)
    * (torch.distributions.negative_binomial.NegativeBinomial method)
    * (torch.distributions.normal.Normal method)
    * (torch.distributions.one_hot_categorical.OneHotCategorical method)
    * (torch.distributions.poisson.Poisson method)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli method)
    * (torch.distributions.studentT.StudentT method)
    * (torch.distributions.transformed_distribution.TransformedDistribution method)
    * (torch.distributions.uniform.Uniform method)
    * (torch.distributions.von_mises.VonMises method)
    * (torch.distributions.wishart.Wishart method)
    * (torch.nn.AdaptiveLogSoftmaxWithLoss method)
  * log_softmax() (in module torch.nn.functional)
    * (in module torch.sparse)
    * (in module torch.special)
  * logaddexp() (in module torch)
    * (torch.Tensor method)
  * logaddexp2() (in module torch)
    * (torch.Tensor method)
  * logcumsumexp() (in module torch)
    * (torch.Tensor method)
  * logdet() (in module torch)
    * (torch.Tensor method)
  * Logger (class in torch.ao.ns._numeric_suite)
  * loggers_set_enabled() (in module torch.ao.ns._numeric_suite_fx)
  * loggers_set_save_activations() (in module torch.ao.ns._numeric_suite_fx)
  * logical_and() (in module torch)
    * (torch.Tensor method)
  * logical_and_() (torch.Tensor method)
  * logical_not() (in module torch)
    * (torch.Tensor method)
  * logical_not_() (torch.Tensor method)
  * logical_or() (in module torch)
    * (torch.Tensor method)
  * logical_or_() (torch.Tensor method)
  * logical_xor() (in module torch)
    * (torch.Tensor method)
  * logical_xor_() (torch.Tensor method)
  * logit() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * logit_() (torch.Tensor method)
  * LogitRelaxedBernoulli (class in torch.distributions.relaxed_bernoulli)
  * logits (torch.distributions.bernoulli.Bernoulli property)
    * (torch.distributions.binomial.Binomial property)
    * (torch.distributions.categorical.Categorical property)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli property)
    * (torch.distributions.geometric.Geometric property)
    * (torch.distributions.multinomial.Multinomial property)
    * (torch.distributions.negative_binomial.NegativeBinomial property)
    * (torch.distributions.one_hot_categorical.OneHotCategorical property)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli property)
    * (torch.distributions.relaxed_bernoulli.RelaxedBernoulli property)
    * (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical property)
  * LogNormal (class in torch.distributions.log_normal)
  * LogsDest (class in torch.distributed.elastic.multiprocessing.api)
  * LogSigmoid (class in torch.nn)
  * logsigmoid() (in module torch.nn.functional)
  * LogSoftmax (class in torch.nn)
  * logspace() (in module torch)
  * LogsSpecs (class in torch.distributed.elastic.multiprocessing.api)
  * logsumexp() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * long() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * LongStorage (class in torch)
  * lookup_object() (torch.distributed.checkpoint.DefaultSavePlanner method)
  * lookup_tensor() (torch.distributed.checkpoint.DefaultLoadPlanner method)
  * loss_parallel() (in module torch.distributed.tensor.parallel)
  * LowerCholeskyTransform (class in torch.distributions.transforms)
  * LowRankMultivariateNormal (class in torch.distributions.lowrank_multivariate_normal)
  * lp_pool1d() (in module torch.nn.functional)
  * lp_pool2d() (in module torch.nn.functional)
  * lp_pool3d() (in module torch.nn.functional)
  * LPPool1d (class in torch.nn)
  * LPPool2d (class in torch.nn)
  * LPPool3d (class in torch.nn)
  * LRScheduler (class in torch.optim.lr_scheduler)
  * lru_cache() (in module torch.fx.experimental.symbolic_shapes)
  * LSTM (class in torch.ao.nn.quantizable)
    * (class in torch.ao.nn.quantized.dynamic)
    * (class in torch.nn)
  * LSTMCell (class in torch.ao.nn.quantized.dynamic)
    * (class in torch.nn)
  * lstsq() (in module torch.linalg)
  * lt() (in module torch)
    * (torch.Tensor method)
  * lt_() (torch.Tensor method)
  * lu() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * lu_factor() (in module torch.linalg)
  * lu_factor_ex() (in module torch.linalg)
  * lu_solve() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * lu_unpack() (in module torch)

  
---|---  
## M
  * main_hook() (torch.distributed.algorithms.JoinHook method)
  * make_dual() (in module torch.autograd.forward_ad)
  * make_fx() (in module torch.fx.experimental.proxy_tensor)
  * make_graphed_callables() (in module torch.cuda)
  * make_tensor() (in module torch.testing)
  * manual_seed() (in module torch)
    * (in module torch.cuda)
    * (in module torch.mps)
    * (in module torch.random)
    * (in module torch.xpu)
    * (torch.Generator method)
  * manual_seed_all() (in module torch.cuda)
    * (in module torch.xpu)
  * map_() (torch.Tensor method)
  * map_nodes_to_values() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Interpreter method)
  * MappingType (class in torch.ao.quantization.observer)
  * margin_ranking_loss() (in module torch.nn.functional)
  * MarginRankingLoss (class in torch.nn)
  * mark() (in module torch.cuda.nvtx)
    * (in module torch.profiler.itt)
  * mark_dirty() (torch.autograd.function.BackwardCFunction method)
    * (torch.autograd.function.FunctionCtx method)
    * (torch.autograd.function.InplaceFunction method)
    * (torch.autograd.function.NestedIOFunction method)
  * mark_non_differentiable() (torch.autograd.function.BackwardCFunction method)
    * (torch.autograd.function.FunctionCtx method)
    * (torch.autograd.function.InplaceFunction method)
    * (torch.autograd.function.NestedIOFunction method)
  * mask_mod (torch.nn.attention.flex_attention.BlockMask attribute)
  * masked_fill() (torch.Tensor method)
  * masked_fill_() (torch.Tensor method)
  * masked_scatter() (torch.Tensor method)
  * masked_scatter_() (torch.Tensor method)
  * masked_select() (in module torch)
    * (in module torch.nested)
    * (torch.Tensor method)
  * materialize() (torch.export.decomp_utils.CustomDecompTable method)
  * math_sdp_enabled() (in module torch.backends.cuda)
  * matmul() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * matrix_exp() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * matrix_norm() (in module torch.linalg)
  * matrix_power() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * matrix_rank() (in module torch.linalg)
  * max() (in module torch)
    * (torch.Tensor method)
  * max_memory_allocated() (in module torch.cuda)
    * (in module torch.xpu)
  * max_memory_cached() (in module torch.cuda)
  * max_memory_reserved() (in module torch.cuda)
    * (in module torch.xpu)
  * max_pool1d (class in torch.ao.nn.quantized.functional)
  * max_pool1d() (in module torch.nn.functional)
  * max_pool2d (class in torch.ao.nn.quantized.functional)
  * max_pool2d() (in module torch.nn.functional)
  * max_pool3d() (in module torch.nn.functional)
  * max_size (in module torch.backends.cuda.cufft_plan_cache)
  * max_unpool1d() (in module torch.nn.functional)
  * max_unpool2d() (in module torch.nn.functional)
  * max_unpool3d() (in module torch.nn.functional)
  * maximum() (in module torch)
    * (torch.Tensor method)
  * MaxPool1d (class in torch.nn)
  * MaxPool2d (class in torch.nn)
  * MaxPool3d (class in torch.nn)
  * MaxUnpool1d (class in torch.nn)
  * MaxUnpool2d (class in torch.nn)
  * MaxUnpool3d (class in torch.nn)
  * maybe_disable_thunkify() (in module torch.fx.experimental.proxy_tensor)
  * maybe_enable_thunkify() (in module torch.fx.experimental.proxy_tensor)
  * mean (torch.distributions.bernoulli.Bernoulli property)
    * (torch.distributions.beta.Beta property)
    * (torch.distributions.binomial.Binomial property)
    * (torch.distributions.categorical.Categorical property)
    * (torch.distributions.cauchy.Cauchy property)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli property)
    * (torch.distributions.dirichlet.Dirichlet property)
    * (torch.distributions.distribution.Distribution property)
    * (torch.distributions.exponential.Exponential property)
    * (torch.distributions.fishersnedecor.FisherSnedecor property)
    * (torch.distributions.gamma.Gamma property)
    * (torch.distributions.geometric.Geometric property)
    * (torch.distributions.gumbel.Gumbel property)
    * (torch.distributions.half_cauchy.HalfCauchy property)
    * (torch.distributions.half_normal.HalfNormal property)
    * (torch.distributions.independent.Independent property)
    * (torch.distributions.inverse_gamma.InverseGamma property)
    * (torch.distributions.kumaraswamy.Kumaraswamy property)
    * (torch.distributions.laplace.Laplace property)
    * (torch.distributions.log_normal.LogNormal property)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property)
    * (torch.distributions.mixture_same_family.MixtureSameFamily property)
    * (torch.distributions.multinomial.Multinomial property)
    * (torch.distributions.multivariate_normal.MultivariateNormal property)
    * (torch.distributions.negative_binomial.NegativeBinomial property)
    * (torch.distributions.normal.Normal property)
    * (torch.distributions.one_hot_categorical.OneHotCategorical property)
    * (torch.distributions.pareto.Pareto property)
    * (torch.distributions.poisson.Poisson property)
    * (torch.distributions.studentT.StudentT property)
    * (torch.distributions.uniform.Uniform property)
    * (torch.distributions.von_mises.VonMises property)
    * (torch.distributions.weibull.Weibull property)
    * (torch.distributions.wishart.Wishart property)
  * mean() (in module torch)
    * (torch.Tensor method)
  * Measurement (class in torch.utils.benchmark)
  * median() (in module torch)
    * (torch.Tensor method)
  * mem_efficient_sdp_enabled() (in module torch.backends.cuda)
  * mem_get_info() (in module torch.cuda)
    * (in module torch.xpu)
  * memory_allocated() (in module torch.cuda)
    * (in module torch.xpu)
  * memory_cached() (in module torch.cuda)
  * memory_format (class in torch)
  * memory_reserved() (in module torch.cuda)
    * (in module torch.xpu)
  * memory_snapshot() (in module torch.cuda)
  * memory_stats() (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.mtia.memory)
    * (in module torch.xpu)
  * memory_stats_as_nested_dict() (in module torch.xpu)
  * memory_summary() (in module torch.cuda)
  * memory_usage() (in module torch.cuda)
  * MemPool (class in torch.cuda)
  * MemPoolContext (class in torch.cuda)
  * MemRecordsAcc (class in torch.autograd.profiler_util)
  * merge() (torch.utils.benchmark.Measurement static method)
  * merge_chunks() (in module torch.distributed.pipelining.microbatch)
  * merge_masks() (torch.nn.MultiheadAttention method)
  * meshgrid() (in module torch)
  * metadata() (torch.autograd.graph.Node method)
  * metal_capture() (in module torch.mps.profiler)
  * MetricHandler (class in torch.distributed.elastic.metrics.api)
  * mgpu_tune_gemm_in_file() (in module torch.cuda.tunable)
  * mH (torch.Tensor attribute)
  * min() (in module torch)
    * (torch.Tensor method)
  * minimum() (in module torch)
    * (torch.Tensor method)
  * MinMaxObserver (class in torch.ao.quantization.observer)
  * Mish (class in torch.nn)
  * mish() (in module torch.nn.functional)
  * MixedPrecision (class in torch.distributed.fsdp)
  * MixedPrecisionPolicy (class in torch.distributed.fsdp)
  * mixture_distribution (torch.distributions.mixture_same_family.MixtureSameFamily property)
  * MixtureSameFamily (class in torch.distributions.mixture_same_family)
  * mm() (in module torch)
    * (in module torch.sparse)
    * (torch.Tensor method)
  * mock() (torch.package.PackageExporter method)
  * mocked_modules() (torch.package.PackageExporter method)
  * mode (torch.distributions.bernoulli.Bernoulli property)
    * (torch.distributions.beta.Beta property)
    * (torch.distributions.binomial.Binomial property)
    * (torch.distributions.categorical.Categorical property)
    * (torch.distributions.cauchy.Cauchy property)
    * (torch.distributions.dirichlet.Dirichlet property)
    * (torch.distributions.distribution.Distribution property)
    * (torch.distributions.exponential.Exponential property)
    * (torch.distributions.fishersnedecor.FisherSnedecor property)
    * (torch.distributions.gamma.Gamma property)
    * (torch.distributions.geometric.Geometric property)
    * (torch.distributions.gumbel.Gumbel property)
    * (torch.distributions.half_cauchy.HalfCauchy property)
    * (torch.distributions.half_normal.HalfNormal property)
    * (torch.distributions.independent.Independent property)
    * (torch.distributions.inverse_gamma.InverseGamma property)
    * (torch.distributions.kumaraswamy.Kumaraswamy property)
    * (torch.distributions.laplace.Laplace property)
    * (torch.distributions.log_normal.LogNormal property)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property)
    * (torch.distributions.multivariate_normal.MultivariateNormal property)
    * (torch.distributions.negative_binomial.NegativeBinomial property)
    * (torch.distributions.normal.Normal property)
    * (torch.distributions.one_hot_categorical.OneHotCategorical property)
    * (torch.distributions.pareto.Pareto property)
    * (torch.distributions.poisson.Poisson property)
    * (torch.distributions.studentT.StudentT property)
    * (torch.distributions.uniform.Uniform property)
    * (torch.distributions.von_mises.VonMises property)
    * (torch.distributions.weibull.Weibull property)
    * (torch.distributions.wishart.Wishart property)
  * mode() (in module torch)
    * (torch.Tensor method)
  * model_is_exported (class in torch.ao.quantization.pt2e.export_utils)
  * model_proto (torch.onnx.ONNXProgram property)
  * module 
    * torch
    * torch.__config__
    * torch.__future__
    * torch._logging
    * torch.accelerator
    * torch.amp
    * torch.amp.autocast_mode
    * torch.amp.grad_scaler
    * torch.ao
    * torch.ao.nn
    * torch.ao.nn.intrinsic
    * torch.ao.nn.intrinsic.modules
    * torch.ao.nn.intrinsic.modules.fused
    * torch.ao.nn.intrinsic.qat
    * torch.ao.nn.intrinsic.qat.modules
    * torch.ao.nn.intrinsic.qat.modules.conv_fused
    * torch.ao.nn.intrinsic.qat.modules.linear_fused
    * torch.ao.nn.intrinsic.qat.modules.linear_relu
    * torch.ao.nn.intrinsic.quantized
    * torch.ao.nn.intrinsic.quantized.dynamic
    * torch.ao.nn.intrinsic.quantized.dynamic.modules
    * torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu
    * torch.ao.nn.intrinsic.quantized.modules
    * torch.ao.nn.intrinsic.quantized.modules.bn_relu
    * torch.ao.nn.intrinsic.quantized.modules.conv_add
    * torch.ao.nn.intrinsic.quantized.modules.conv_relu
    * torch.ao.nn.intrinsic.quantized.modules.linear_relu
    * torch.ao.nn.qat
    * torch.ao.nn.qat.dynamic
    * torch.ao.nn.qat.dynamic.modules
    * torch.ao.nn.qat.dynamic.modules.linear
    * torch.ao.nn.qat.modules
    * torch.ao.nn.qat.modules.conv
    * torch.ao.nn.qat.modules.embedding_ops
    * torch.ao.nn.qat.modules.linear
    * torch.ao.nn.quantizable
    * torch.ao.nn.quantizable.modules
    * torch.ao.nn.quantizable.modules.activation
    * torch.ao.nn.quantizable.modules.rnn
    * torch.ao.nn.quantized
    * torch.ao.nn.quantized.dynamic
    * torch.ao.nn.quantized.dynamic.modules
    * torch.ao.nn.quantized.dynamic.modules.conv
    * torch.ao.nn.quantized.dynamic.modules.linear
    * torch.ao.nn.quantized.dynamic.modules.rnn
    * torch.ao.nn.quantized.functional
    * torch.ao.nn.quantized.modules
    * torch.ao.nn.quantized.modules.activation
    * torch.ao.nn.quantized.modules.batchnorm
    * torch.ao.nn.quantized.modules.conv
    * torch.ao.nn.quantized.modules.dropout
    * torch.ao.nn.quantized.modules.embedding_ops
    * torch.ao.nn.quantized.modules.functional_modules
    * torch.ao.nn.quantized.modules.linear
    * torch.ao.nn.quantized.modules.normalization
    * torch.ao.nn.quantized.modules.rnn
    * torch.ao.nn.quantized.modules.utils
    * torch.ao.nn.quantized.reference
    * torch.ao.nn.quantized.reference.modules
    * torch.ao.nn.quantized.reference.modules.conv
    * torch.ao.nn.quantized.reference.modules.linear
    * torch.ao.nn.quantized.reference.modules.rnn
    * torch.ao.nn.quantized.reference.modules.sparse
    * torch.ao.nn.quantized.reference.modules.utils
    * torch.ao.nn.sparse
    * torch.ao.nn.sparse.quantized
    * torch.ao.nn.sparse.quantized.dynamic
    * torch.ao.nn.sparse.quantized.dynamic.linear
    * torch.ao.nn.sparse.quantized.linear
    * torch.ao.nn.sparse.quantized.utils
    * torch.ao.ns
    * torch.ao.ns._numeric_suite
    * torch.ao.ns._numeric_suite_fx
    * torch.ao.ns.fx
    * torch.ao.ns.fx.graph_matcher
    * torch.ao.ns.fx.graph_passes
    * torch.ao.ns.fx.mappings
    * torch.ao.ns.fx.n_shadows_utils
    * torch.ao.ns.fx.ns_types
    * torch.ao.ns.fx.pattern_utils
    * torch.ao.ns.fx.qconfig_multi_mapping
    * torch.ao.ns.fx.utils
    * torch.ao.ns.fx.weight_utils
    * torch.ao.pruning
    * torch.ao.pruning.scheduler
    * torch.ao.pruning.scheduler.base_scheduler
    * torch.ao.pruning.scheduler.cubic_scheduler
    * torch.ao.pruning.scheduler.lambda_scheduler
    * torch.ao.pruning.sparsifier
    * torch.ao.pruning.sparsifier.base_sparsifier
    * torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier
    * torch.ao.pruning.sparsifier.utils
    * torch.ao.pruning.sparsifier.weight_norm_sparsifier
    * torch.ao.quantization
    * torch.ao.quantization.backend_config
    * torch.ao.quantization.backend_config.backend_config
    * torch.ao.quantization.backend_config.executorch
    * torch.ao.quantization.backend_config.fbgemm
    * torch.ao.quantization.backend_config.native
    * torch.ao.quantization.backend_config.observation_type
    * torch.ao.quantization.backend_config.onednn
    * torch.ao.quantization.backend_config.qnnpack
    * torch.ao.quantization.backend_config.tensorrt
    * torch.ao.quantization.backend_config.utils
    * torch.ao.quantization.backend_config.x86
    * torch.ao.quantization.fake_quantize
    * torch.ao.quantization.fuse_modules
    * torch.ao.quantization.fuser_method_mappings
    * torch.ao.quantization.fx
    * torch.ao.quantization.fx.convert
    * torch.ao.quantization.fx.custom_config
    * torch.ao.quantization.fx.fuse
    * torch.ao.quantization.fx.fuse_handler
    * torch.ao.quantization.fx.graph_module
    * torch.ao.quantization.fx.lower_to_fbgemm
    * torch.ao.quantization.fx.lower_to_qnnpack
    * torch.ao.quantization.fx.lstm_utils
    * torch.ao.quantization.fx.match_utils
    * torch.ao.quantization.fx.pattern_utils
    * torch.ao.quantization.fx.prepare
    * torch.ao.quantization.fx.qconfig_mapping_utils
    * torch.ao.quantization.fx.quantize_handler
    * torch.ao.quantization.fx.tracer
    * torch.ao.quantization.fx.utils
    * torch.ao.quantization.observer
    * torch.ao.quantization.pt2e
    * torch.ao.quantization.pt2e.duplicate_dq_pass
    * torch.ao.quantization.pt2e.export_utils
    * torch.ao.quantization.pt2e.graph_utils
    * torch.ao.quantization.pt2e.port_metadata_pass
    * torch.ao.quantization.pt2e.prepare
    * torch.ao.quantization.pt2e.qat_utils
    * torch.ao.quantization.pt2e.representation
    * torch.ao.quantization.pt2e.representation.rewrite
    * torch.ao.quantization.pt2e.utils
    * torch.ao.quantization.qconfig
    * torch.ao.quantization.qconfig_mapping
    * torch.ao.quantization.quant_type
    * torch.ao.quantization.quantization_mappings
    * torch.ao.quantization.quantize_fx
    * torch.ao.quantization.quantize_jit
    * torch.ao.quantization.quantize_pt2e
    * torch.ao.quantization.quantizer
    * torch.ao.quantization.quantizer.composable_quantizer
    * torch.ao.quantization.quantizer.embedding_quantizer
    * torch.ao.quantization.quantizer.quantizer
    * torch.ao.quantization.quantizer.utils
    * torch.ao.quantization.quantizer.x86_inductor_quantizer
    * torch.ao.quantization.quantizer.xnnpack_quantizer
    * torch.ao.quantization.quantizer.xnnpack_quantizer_utils
    * torch.ao.quantization.quantizer.xpu_inductor_quantizer
    * torch.ao.quantization.stubs
    * torch.ao.quantization.utils
    * torch.autograd
    * torch.autograd.anomaly_mode
    * torch.autograd.forward_ad
    * torch.autograd.function
    * torch.autograd.functional
    * torch.autograd.grad_mode
    * torch.autograd.gradcheck
    * torch.autograd.graph
    * torch.autograd.profiler
    * torch.autograd.profiler_legacy
    * torch.autograd.profiler_util
    * torch.autograd.variable
    * torch.backends
    * torch.backends.cpu
    * torch.backends.cuda
    * torch.backends.cudnn
    * torch.backends.cudnn.rnn
    * torch.backends.cusparselt
    * torch.backends.kleidiai
    * torch.backends.mha
    * torch.backends.mkl
    * torch.backends.mkldnn
    * torch.backends.mps
    * torch.backends.nnpack
    * torch.backends.openmp
    * torch.backends.opt_einsum
    * torch.backends.quantized
    * torch.backends.xeon
    * torch.backends.xeon.run_cpu
    * torch.backends.xnnpack
    * torch.compiler
    * torch.compiler.config
    * torch.contrib
    * torch.cpu
    * torch.cpu.amp
    * torch.cpu.amp.autocast_mode
    * torch.cpu.amp.grad_scaler
    * torch.cuda
    * torch.cuda._sanitizer
    * torch.cuda.amp
    * torch.cuda.amp.autocast_mode
    * torch.cuda.amp.common
    * torch.cuda.amp.grad_scaler
    * torch.cuda.comm
    * torch.cuda.error
    * torch.cuda.gds
    * torch.cuda.graphs
    * torch.cuda.jiterator
    * torch.cuda.memory
    * torch.cuda.nccl
    * torch.cuda.nvtx
    * torch.cuda.profiler
    * torch.cuda.random
    * torch.cuda.sparse
    * torch.cuda.streams
    * torch.cuda.tunable
    * torch.distributed
    * torch.distributed.algorithms
    * torch.distributed.algorithms.ddp_comm_hooks
    * torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook
    * torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks
    * torch.distributed.algorithms.ddp_comm_hooks.default_hooks
    * torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks
    * torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks
    * torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook
    * torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook
    * torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks
    * torch.distributed.algorithms.join
    * torch.distributed.algorithms.model_averaging
    * torch.distributed.algorithms.model_averaging.averagers
    * torch.distributed.algorithms.model_averaging.hierarchical_model_averager
    * torch.distributed.algorithms.model_averaging.utils
    * torch.distributed.argparse_util
    * torch.distributed.autograd
    * torch.distributed.c10d_logger
    * torch.distributed.checkpoint
    * torch.distributed.checkpoint.api
    * torch.distributed.checkpoint.default_planner
    * torch.distributed.checkpoint.filesystem
    * torch.distributed.checkpoint.format_utils
    * torch.distributed.checkpoint.logger
    * torch.distributed.checkpoint.logging_handlers
    * torch.distributed.checkpoint.metadata
    * torch.distributed.checkpoint.optimizer
    * torch.distributed.checkpoint.planner
    * torch.distributed.checkpoint.planner_helpers
    * torch.distributed.checkpoint.resharding
    * torch.distributed.checkpoint.staging
    * torch.distributed.checkpoint.state_dict
    * torch.distributed.checkpoint.state_dict_loader
    * torch.distributed.checkpoint.state_dict_saver
    * torch.distributed.checkpoint.stateful
    * torch.distributed.checkpoint.storage
    * torch.distributed.checkpoint.utils
    * torch.distributed.collective_utils
    * torch.distributed.constants
    * torch.distributed.device_mesh
    * torch.distributed.distributed_c10d
    * torch.distributed.elastic
    * torch.distributed.elastic.agent
    * torch.distributed.elastic.agent.server
    * torch.distributed.elastic.agent.server.api
    * torch.distributed.elastic.agent.server.health_check_server
    * torch.distributed.elastic.agent.server.local_elastic_agent
    * torch.distributed.elastic.control_plane
    * torch.distributed.elastic.events
    * torch.distributed.elastic.events.api
    * torch.distributed.elastic.events.handlers
    * torch.distributed.elastic.metrics
    * torch.distributed.elastic.metrics.api
    * torch.distributed.elastic.multiprocessing
    * torch.distributed.elastic.multiprocessing.api
    * torch.distributed.elastic.multiprocessing.errors
    * torch.distributed.elastic.multiprocessing.errors.error_handler
    * torch.distributed.elastic.multiprocessing.errors.handlers
    * torch.distributed.elastic.multiprocessing.redirects
    * torch.distributed.elastic.multiprocessing.subprocess_handler
    * torch.distributed.elastic.multiprocessing.subprocess_handler.handlers
    * torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler
    * torch.distributed.elastic.multiprocessing.tail_log
    * torch.distributed.elastic.rendezvous
    * torch.distributed.elastic.rendezvous.api
    * torch.distributed.elastic.rendezvous.c10d_rendezvous_backend
    * torch.distributed.elastic.rendezvous.dynamic_rendezvous
    * torch.distributed.elastic.rendezvous.etcd_rendezvous
    * torch.distributed.elastic.rendezvous.etcd_rendezvous_backend
    * torch.distributed.elastic.rendezvous.etcd_server
    * torch.distributed.elastic.rendezvous.etcd_store
    * torch.distributed.elastic.rendezvous.registry
    * torch.distributed.elastic.rendezvous.static_tcp_rendezvous
    * torch.distributed.elastic.rendezvous.utils
    * torch.distributed.elastic.timer
    * torch.distributed.elastic.timer.api
    * torch.distributed.elastic.timer.debug_info_logging
    * torch.distributed.elastic.timer.file_based_local_timer
    * torch.distributed.elastic.timer.local_timer
    * torch.distributed.elastic.utils
    * torch.distributed.elastic.utils.api
    * torch.distributed.elastic.utils.data
    * torch.distributed.elastic.utils.data.cycling_iterator
    * torch.distributed.elastic.utils.data.elastic_distributed_sampler
    * torch.distributed.elastic.utils.distributed
    * torch.distributed.elastic.utils.log_level
    * torch.distributed.elastic.utils.logging
    * torch.distributed.elastic.utils.store
    * torch.distributed.fsdp
    * torch.distributed.fsdp.api
    * torch.distributed.fsdp.fully_sharded_data_parallel
    * torch.distributed.fsdp.sharded_grad_scaler
    * torch.distributed.fsdp.wrap
    * torch.distributed.launch
    * torch.distributed.launcher
    * torch.distributed.launcher.api
    * torch.distributed.logging_handlers
    * torch.distributed.nn
    * torch.distributed.nn.api
    * torch.distributed.nn.api.remote_module
    * torch.distributed.nn.functional
    * torch.distributed.nn.jit
    * torch.distributed.nn.jit.instantiator
    * torch.distributed.nn.jit.templates
    * torch.distributed.nn.jit.templates.remote_module_template
    * torch.distributed.optim
    * torch.distributed.optim.apply_optimizer_in_backward
    * torch.distributed.optim.functional_adadelta
    * torch.distributed.optim.functional_adagrad
    * torch.distributed.optim.functional_adam
    * torch.distributed.optim.functional_adamax
    * torch.distributed.optim.functional_adamw
    * torch.distributed.optim.functional_rmsprop
    * torch.distributed.optim.functional_rprop
    * torch.distributed.optim.functional_sgd
    * torch.distributed.optim.named_optimizer
    * torch.distributed.optim.optimizer
    * torch.distributed.optim.post_localSGD_optimizer
    * torch.distributed.optim.utils
    * torch.distributed.optim.zero_redundancy_optimizer
    * torch.distributed.pipelining
    * torch.distributed.pipelining.microbatch
    * torch.distributed.pipelining.schedules
    * torch.distributed.pipelining.stage
    * torch.distributed.remote_device
    * torch.distributed.rendezvous
    * torch.distributed.rpc
    * torch.distributed.rpc.api
    * torch.distributed.rpc.backend_registry
    * torch.distributed.rpc.constants
    * torch.distributed.rpc.functions
    * torch.distributed.rpc.internal
    * torch.distributed.rpc.options
    * torch.distributed.rpc.rref_proxy
    * torch.distributed.rpc.server_process_global_profiler
    * torch.distributed.run
    * torch.distributed.tensor
    * torch.distributed.tensor.debug
    * torch.distributed.tensor.device_mesh
    * torch.distributed.tensor.experimental
    * torch.distributed.tensor.parallel
    * torch.distributed.tensor.parallel.api
    * torch.distributed.tensor.parallel.ddp
    * torch.distributed.tensor.parallel.fsdp
    * torch.distributed.tensor.parallel.input_reshard
    * torch.distributed.tensor.parallel.loss
    * torch.distributed.tensor.parallel.style
    * torch.distributed.tensor.placement_types
    * torch.distributed.utils
    * torch.distributions
    * torch.distributions.bernoulli
    * torch.distributions.beta
    * torch.distributions.binomial
    * torch.distributions.categorical
    * torch.distributions.cauchy
    * torch.distributions.chi2
    * torch.distributions.constraint_registry
    * torch.distributions.constraints
    * torch.distributions.continuous_bernoulli
    * torch.distributions.dirichlet
    * torch.distributions.distribution
    * torch.distributions.exp_family
    * torch.distributions.exponential
    * torch.distributions.fishersnedecor
    * torch.distributions.gamma
    * torch.distributions.geometric
    * torch.distributions.gumbel
    * torch.distributions.half_cauchy
    * torch.distributions.half_normal
    * torch.distributions.independent
    * torch.distributions.inverse_gamma
    * torch.distributions.kl
    * torch.distributions.kumaraswamy
    * torch.distributions.laplace
    * torch.distributions.lkj_cholesky
    * torch.distributions.log_normal
    * torch.distributions.logistic_normal
    * torch.distributions.lowrank_multivariate_normal
    * torch.distributions.mixture_same_family
    * torch.distributions.multinomial
    * torch.distributions.multivariate_normal
    * torch.distributions.negative_binomial
    * torch.distributions.normal
    * torch.distributions.one_hot_categorical
    * torch.distributions.pareto
    * torch.distributions.poisson
    * torch.distributions.relaxed_bernoulli
    * torch.distributions.relaxed_categorical
    * torch.distributions.studentT
    * torch.distributions.transformed_distribution
    * torch.distributions.transforms
    * torch.distributions.uniform
    * torch.distributions.utils
    * torch.distributions.von_mises
    * torch.distributions.weibull
    * torch.distributions.wishart
    * torch.export
    * torch.export.custom_obj
    * torch.export.custom_ops
    * torch.export.decomp_utils
    * torch.export.dynamic_shapes
    * torch.export.experimental
    * torch.export.exported_program
    * torch.export.graph_signature
    * torch.export.passes
    * torch.export.unflatten
    * torch.fft
    * torch.func
    * torch.functional
    * torch.futures
    * torch.fx
    * torch.fx.annotate
    * torch.fx.config
    * torch.fx.experimental
    * torch.fx.experimental.accelerator_partitioner
    * torch.fx.experimental.const_fold
    * torch.fx.experimental.debug
    * torch.fx.experimental.graph_gradual_typechecker
    * torch.fx.experimental.merge_matmul
    * torch.fx.experimental.meta_tracer
    * torch.fx.experimental.migrate_gradual_types
    * torch.fx.experimental.migrate_gradual_types.constraint
    * torch.fx.experimental.migrate_gradual_types.constraint_generator
    * torch.fx.experimental.migrate_gradual_types.constraint_transformation
    * torch.fx.experimental.migrate_gradual_types.operation
    * torch.fx.experimental.migrate_gradual_types.transform_to_z3
    * torch.fx.experimental.migrate_gradual_types.util
    * torch.fx.experimental.migrate_gradual_types.z3_types
    * torch.fx.experimental.normalize
    * torch.fx.experimental.optimization
    * torch.fx.experimental.partitioner_utils
    * torch.fx.experimental.proxy_tensor
    * torch.fx.experimental.recording
    * torch.fx.experimental.refinement_types
    * torch.fx.experimental.rewriter
    * torch.fx.experimental.schema_type_annotation
    * torch.fx.experimental.sym_node
    * torch.fx.experimental.symbolic_shapes
    * torch.fx.experimental.unification
    * torch.fx.experimental.unification.core
    * torch.fx.experimental.unification.dispatch
    * torch.fx.experimental.unification.match
    * torch.fx.experimental.unification.more
    * torch.fx.experimental.unification.multipledispatch
    * torch.fx.experimental.unification.multipledispatch.conflict
    * torch.fx.experimental.unification.multipledispatch.core
    * torch.fx.experimental.unification.multipledispatch.dispatcher
    * torch.fx.experimental.unification.multipledispatch.utils
    * torch.fx.experimental.unification.multipledispatch.variadic
    * torch.fx.experimental.unification.unification_tools
    * torch.fx.experimental.unification.utils
    * torch.fx.experimental.unification.variable
    * torch.fx.experimental.unify_refinements
    * torch.fx.experimental.validator
    * torch.fx.graph
    * torch.fx.graph_module
    * torch.fx.immutable_collections
    * torch.fx.interpreter
    * torch.fx.node
    * torch.fx.operator_schemas
    * torch.fx.passes
    * torch.fx.passes.annotate_getitem_nodes
    * torch.fx.passes.backends
    * torch.fx.passes.backends.cudagraphs
    * torch.fx.passes.dialect
    * torch.fx.passes.dialect.common
    * torch.fx.passes.dialect.common.cse_pass
    * torch.fx.passes.fake_tensor_prop
    * torch.fx.passes.graph_drawer
    * torch.fx.passes.graph_manipulation
    * torch.fx.passes.graph_transform_observer
    * torch.fx.passes.infra
    * torch.fx.passes.infra.partitioner
    * torch.fx.passes.infra.pass_base
    * torch.fx.passes.infra.pass_manager
    * torch.fx.passes.net_min_base
    * torch.fx.passes.operator_support
    * torch.fx.passes.param_fetch
    * torch.fx.passes.pass_manager
    * torch.fx.passes.reinplace
    * torch.fx.passes.runtime_assert
    * torch.fx.passes.shape_prop
    * torch.fx.passes.split_module
    * torch.fx.passes.split_utils
    * torch.fx.passes.splitter_base
    * torch.fx.passes.tests
    * torch.fx.passes.tests.test_pass_manager
    * torch.fx.passes.tools_common
    * torch.fx.passes.utils
    * torch.fx.passes.utils.common
    * torch.fx.passes.utils.fuser_utils
    * torch.fx.passes.utils.matcher_utils
    * torch.fx.passes.utils.matcher_with_name_node_map_utils
    * torch.fx.passes.utils.source_matcher_utils
    * torch.fx.proxy
    * torch.fx.subgraph_rewriter
    * torch.fx.tensor_type
    * torch.fx.traceback
    * torch.hub
    * torch.jit
    * torch.jit.annotations
    * torch.jit.frontend
    * torch.jit.generate_bytecode
    * torch.jit.mobile
    * torch.jit.quantized
    * torch.jit.supported_ops
    * torch.jit.unsupported_tensor_ops
    * torch.library
    * torch.linalg
    * torch.masked
    * torch.masked.maskedtensor
    * torch.masked.maskedtensor.binary
    * torch.masked.maskedtensor.core
    * torch.masked.maskedtensor.creation
    * torch.masked.maskedtensor.passthrough
    * torch.masked.maskedtensor.reductions
    * torch.masked.maskedtensor.unary
    * torch.monitor
    * torch.mps
    * torch.mps.event
    * torch.mps.profiler
    * torch.mtia
    * torch.mtia.memory
    * torch.multiprocessing
    * torch.multiprocessing.pool
    * torch.multiprocessing.queue
    * torch.multiprocessing.reductions
    * torch.multiprocessing.spawn
    * torch.nested
    * torch.nn
    * torch.nn.attention
    * torch.nn.attention.bias
    * torch.nn.attention.experimental
    * torch.nn.attention.flex_attention
    * torch.nn.backends
    * torch.nn.backends.thnn
    * torch.nn.common_types
    * torch.nn.cpp
    * torch.nn.functional
    * torch.nn.grad
    * torch.nn.init
    * torch.nn.intrinsic
    * torch.nn.intrinsic.modules
    * torch.nn.intrinsic.modules.fused
    * torch.nn.intrinsic.qat
    * torch.nn.intrinsic.qat.modules
    * torch.nn.intrinsic.qat.modules.conv_fused
    * torch.nn.intrinsic.qat.modules.linear_fused
    * torch.nn.intrinsic.qat.modules.linear_relu
    * torch.nn.intrinsic.quantized
    * torch.nn.intrinsic.quantized.dynamic
    * torch.nn.intrinsic.quantized.dynamic.modules
    * torch.nn.intrinsic.quantized.dynamic.modules.linear_relu
    * torch.nn.intrinsic.quantized.modules
    * torch.nn.intrinsic.quantized.modules.bn_relu
    * torch.nn.intrinsic.quantized.modules.conv_relu
    * torch.nn.intrinsic.quantized.modules.linear_relu
    * torch.nn.modules
    * torch.nn.modules.activation
    * torch.nn.modules.adaptive
    * torch.nn.modules.batchnorm
    * torch.nn.modules.channelshuffle
    * torch.nn.modules.container
    * torch.nn.modules.conv
    * torch.nn.modules.distance
    * torch.nn.modules.dropout
    * torch.nn.modules.flatten
    * torch.nn.modules.fold
    * torch.nn.modules.instancenorm
    * torch.nn.modules.lazy
    * torch.nn.modules.linear
    * torch.nn.modules.loss
    * torch.nn.modules.module
    * torch.nn.modules.normalization
    * torch.nn.modules.padding
    * torch.nn.modules.pixelshuffle
    * torch.nn.modules.pooling
    * torch.nn.modules.rnn
    * torch.nn.modules.sparse
    * torch.nn.modules.transformer
    * torch.nn.modules.upsampling
    * torch.nn.modules.utils
    * torch.nn.parallel
    * torch.nn.parallel.comm
    * torch.nn.parallel.distributed
    * torch.nn.parallel.parallel_apply
    * torch.nn.parallel.replicate
    * torch.nn.parallel.scatter_gather
    * torch.nn.parameter
    * torch.nn.qat
    * torch.nn.qat.dynamic
    * torch.nn.qat.dynamic.modules
    * torch.nn.qat.dynamic.modules.linear
    * torch.nn.qat.modules
    * torch.nn.qat.modules.conv
    * torch.nn.qat.modules.embedding_ops
    * torch.nn.qat.modules.linear
    * torch.nn.quantizable
    * torch.nn.quantizable.modules
    * torch.nn.quantizable.modules.activation
    * torch.nn.quantizable.modules.rnn
    * torch.nn.quantized
    * torch.nn.quantized.dynamic
    * torch.nn.quantized.dynamic.modules
    * torch.nn.quantized.dynamic.modules.conv
    * torch.nn.quantized.dynamic.modules.linear
    * torch.nn.quantized.dynamic.modules.rnn
    * torch.nn.quantized.functional
    * torch.nn.quantized.modules
    * torch.nn.quantized.modules.activation
    * torch.nn.quantized.modules.batchnorm
    * torch.nn.quantized.modules.conv
    * torch.nn.quantized.modules.dropout
    * torch.nn.quantized.modules.embedding_ops
    * torch.nn.quantized.modules.functional_modules
    * torch.nn.quantized.modules.linear
    * torch.nn.quantized.modules.normalization
    * torch.nn.quantized.modules.rnn
    * torch.nn.quantized.modules.utils
    * torch.nn.utils
    * torch.nn.utils.clip_grad
    * torch.nn.utils.convert_parameters
    * torch.nn.utils.fusion
    * torch.nn.utils.init
    * torch.nn.utils.memory_format
    * torch.nn.utils.parametrizations
    * torch.nn.utils.parametrize
    * torch.nn.utils.prune
    * torch.nn.utils.rnn
    * torch.nn.utils.stateless
    * torch.onnx
    * torch.onnx.errors
    * torch.onnx.operators
    * torch.onnx.symbolic_caffe2
    * torch.onnx.symbolic_helper
    * torch.onnx.symbolic_opset10
    * torch.onnx.symbolic_opset11
    * torch.onnx.symbolic_opset12
    * torch.onnx.symbolic_opset13
    * torch.onnx.symbolic_opset14
    * torch.onnx.symbolic_opset15
    * torch.onnx.symbolic_opset16
    * torch.onnx.symbolic_opset17
    * torch.onnx.symbolic_opset18
    * torch.onnx.symbolic_opset19
    * torch.onnx.symbolic_opset20
    * torch.onnx.symbolic_opset7
    * torch.onnx.symbolic_opset8
    * torch.onnx.symbolic_opset9
    * torch.onnx.utils
    * torch.onnx.verification
    * torch.optim
    * torch.optim.adadelta
    * torch.optim.adagrad
    * torch.optim.adam
    * torch.optim.adamax
    * torch.optim.adamw
    * torch.optim.asgd
    * torch.optim.lbfgs
    * torch.optim.lr_scheduler
    * torch.optim.nadam
    * torch.optim.optimizer
    * torch.optim.radam
    * torch.optim.rmsprop
    * torch.optim.rprop
    * torch.optim.sgd
    * torch.optim.sparse_adam
    * torch.optim.swa_utils
    * torch.overrides
    * torch.package
    * torch.package.analyze
    * torch.package.analyze.find_first_use_of_broken_modules
    * torch.package.analyze.is_from_package
    * torch.package.analyze.trace_dependencies
    * torch.package.file_structure_representation
    * torch.package.find_file_dependencies
    * torch.package.glob_group
    * torch.package.importer
    * torch.package.package_exporter
    * torch.package.package_importer
    * torch.profiler
    * torch.profiler.itt
    * torch.profiler.profiler
    * torch.profiler.python_tracer
    * torch.quantization
    * torch.quantization.fake_quantize
    * torch.quantization.fuse_modules
    * torch.quantization.fuser_method_mappings
    * torch.quantization.fx
    * torch.quantization.fx.convert
    * torch.quantization.fx.fuse
    * torch.quantization.fx.fusion_patterns
    * torch.quantization.fx.graph_module
    * torch.quantization.fx.match_utils
    * torch.quantization.fx.pattern_utils
    * torch.quantization.fx.prepare
    * torch.quantization.fx.quantization_patterns
    * torch.quantization.fx.quantization_types
    * torch.quantization.fx.utils
    * torch.quantization.observer
    * torch.quantization.qconfig
    * torch.quantization.quant_type
    * torch.quantization.quantization_mappings
    * torch.quantization.quantize
    * torch.quantization.quantize_fx
    * torch.quantization.quantize_jit
    * torch.quantization.stubs
    * torch.quantization.utils
    * torch.quasirandom
    * torch.random
    * torch.return_types
    * torch.serialization
    * torch.signal
    * torch.signal.windows
    * torch.signal.windows.windows
    * torch.sparse
    * torch.sparse.semi_structured
    * torch.special
    * torch.storage
    * torch.testing
    * torch.torch_version
    * torch.types
    * torch.utils
    * torch.utils.backcompat
    * torch.utils.backend_registration
    * torch.utils.benchmark
    * torch.utils.benchmark.examples
    * torch.utils.benchmark.examples.blas_compare_setup
    * torch.utils.benchmark.examples.compare
    * torch.utils.benchmark.examples.fuzzer
    * torch.utils.benchmark.examples.op_benchmark
    * torch.utils.benchmark.examples.simple_timeit
    * torch.utils.benchmark.examples.spectral_ops_fuzz_test
    * torch.utils.benchmark.op_fuzzers
    * torch.utils.benchmark.op_fuzzers.binary
    * torch.utils.benchmark.op_fuzzers.sparse_binary
    * torch.utils.benchmark.op_fuzzers.sparse_unary
    * torch.utils.benchmark.op_fuzzers.spectral
    * torch.utils.benchmark.op_fuzzers.unary
    * torch.utils.benchmark.utils
    * torch.utils.benchmark.utils.common
    * torch.utils.benchmark.utils.compare
    * torch.utils.benchmark.utils.compile
    * torch.utils.benchmark.utils.cpp_jit
    * torch.utils.benchmark.utils.fuzzer
    * torch.utils.benchmark.utils.sparse_fuzzer
    * torch.utils.benchmark.utils.timer
    * torch.utils.benchmark.utils.valgrind_wrapper
    * torch.utils.benchmark.utils.valgrind_wrapper.timer_interface
    * torch.utils.bottleneck
    * torch.utils.bundled_inputs
    * torch.utils.checkpoint
    * torch.utils.collect_env
    * torch.utils.cpp_backtrace
    * torch.utils.cpp_extension
    * torch.utils.data
    * torch.utils.data.backward_compatibility
    * torch.utils.data.dataloader
    * torch.utils.data.datapipes
    * torch.utils.data.datapipes.dataframe
    * torch.utils.data.datapipes.dataframe.dataframe_wrapper
    * torch.utils.data.datapipes.dataframe.dataframes
    * torch.utils.data.datapipes.dataframe.datapipes
    * torch.utils.data.datapipes.dataframe.structures
    * torch.utils.data.datapipes.datapipe
    * torch.utils.data.datapipes.gen_pyi
    * torch.utils.data.datapipes.iter
    * torch.utils.data.datapipes.iter.callable
    * torch.utils.data.datapipes.iter.combinatorics
    * torch.utils.data.datapipes.iter.combining
    * torch.utils.data.datapipes.iter.filelister
    * torch.utils.data.datapipes.iter.fileopener
    * torch.utils.data.datapipes.iter.grouping
    * torch.utils.data.datapipes.iter.routeddecoder
    * torch.utils.data.datapipes.iter.selecting
    * torch.utils.data.datapipes.iter.sharding
    * torch.utils.data.datapipes.iter.streamreader
    * torch.utils.data.datapipes.iter.utils
    * torch.utils.data.datapipes.map
    * torch.utils.data.datapipes.map.callable
    * torch.utils.data.datapipes.map.combinatorics
    * torch.utils.data.datapipes.map.combining
    * torch.utils.data.datapipes.map.grouping
    * torch.utils.data.datapipes.map.utils
    * torch.utils.data.datapipes.utils
    * torch.utils.data.datapipes.utils.common
    * torch.utils.data.datapipes.utils.decoder
    * torch.utils.data.datapipes.utils.snapshot
    * torch.utils.data.dataset
    * torch.utils.data.distributed
    * torch.utils.data.graph
    * torch.utils.data.graph_settings
    * torch.utils.data.sampler
    * torch.utils.deterministic
    * torch.utils.dlpack
    * torch.utils.file_baton
    * torch.utils.flop_counter
    * torch.utils.hipify
    * torch.utils.hipify.constants
    * torch.utils.hipify.cuda_to_hip_mappings
    * torch.utils.hipify.hipify_python
    * torch.utils.hipify.version
    * torch.utils.hooks
    * torch.utils.jit
    * torch.utils.jit.log_extract
    * torch.utils.mkldnn
    * torch.utils.mobile_optimizer
    * torch.utils.model_dump
    * torch.utils.model_zoo
    * torch.utils.module_tracker
    * torch.utils.serialization
    * torch.utils.serialization.config
    * torch.utils.show_pickle
    * torch.utils.tensorboard
    * torch.utils.tensorboard.summary
    * torch.utils.tensorboard.writer
    * torch.utils.throughput_benchmark
    * torch.utils.viz
    * torch.utils.weak
    * torch.version
    * torch.xpu
    * torch.xpu.memory
    * torch.xpu.random
    * torch.xpu.streams

| 
  * Module (class in torch.nn)
  * module (torch.distributed.fsdp.FullyShardedDataParallel property)
  * module() (torch.export.ExportedProgram method)
  * module_load() (torch.Tensor method)
  * ModuleCallEntry (class in torch.export)
  * ModuleCallSignature (class in torch.export)
  * ModuleDict (class in torch.nn)
  * ModuleList (class in torch.nn)
  * modules() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * ModuleTracker (class in torch.utils.module_tracker)
  * monitored_barrier() (in module torch.distributed)
  * move_to_device_pass() (in module torch.export.passes)
  * moveaxis() (in module torch)
    * (torch.Tensor method)
  * movedim() (in module torch)
    * (torch.Tensor method)
  * MovingAverageMinMaxObserver (class in torch.ao.quantization.observer)
  * MovingAveragePerChannelMinMaxObserver (class in torch.ao.quantization.observer)
  * mps() (torch.UntypedStorage method)
  * mse_loss() (in module torch.nn.functional)
  * MSELoss (class in torch.nn)
  * msort() (in module torch)
    * (torch.Tensor method)
  * mT (torch.Tensor attribute)
  * mtia() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * mul() (in module torch)
    * (torch.ao.ns._numeric_suite.Shadow method)
    * (torch.Tensor method)
  * mul_() (torch.Tensor method)
  * mul_scalar() (torch.ao.ns._numeric_suite.Shadow method)
  * multi_dot() (in module torch.linalg)
  * multi_get() (torch.distributed.Store method)
  * multi_margin_loss() (in module torch.nn.functional)
  * multi_set() (torch.distributed.Store method)
  * multigammaln() (in module torch.special)
  * MultiheadAttention (class in torch.ao.nn.quantizable)
    * (class in torch.nn)
  * multilabel_margin_loss() (in module torch.nn.functional)
  * multilabel_soft_margin_loss() (in module torch.nn.functional)
  * MultiLabelMarginLoss (class in torch.nn)
  * MultiLabelSoftMarginLoss (class in torch.nn)
  * MultiMarginLoss (class in torch.nn)
  * Multinomial (class in torch.distributions.multinomial)
  * multinomial (in module torch.distributions.constraints)
  * multinomial() (in module torch)
    * (torch.Tensor method)
  * MultiplicativeLR (class in torch.optim.lr_scheduler)
  * multiply() (in module torch)
    * (torch.Tensor method)
  * multiply_() (torch.Tensor method)
  * MultiprocessContext (class in torch.distributed.elastic.multiprocessing.api)
  * MultiStepLR (class in torch.optim.lr_scheduler)
  * MultivariateNormal (class in torch.distributions.multivariate_normal)
  * mv() (in module torch)
    * (torch.Tensor method)
  * mvlgamma() (in module torch)
    * (torch.Tensor method)
  * mvlgamma_() (torch.Tensor method)

  
---|---  
## N
  * NAdam (class in torch.optim)
  * name (torch.autograd.profiler_util.Kernel attribute)
    * (torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend property)
    * (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend property)
    * (torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend property)
    * (torch.distributed.rpc.WorkerInfo property)
    * (torch.monitor.Aggregation property)
    * (torch.monitor.Event property)
    * (torch.monitor.Stat property)
    * (torch.nn.attention.SDPBackend property)
    * (torch.profiler.ProfilerActivity property)
    * (torch.Tag property)
  * name() (torch.autograd.graph.Node method)
  * named_buffers() (torch.distributed.fsdp.FullyShardedDataParallel method)
    * (torch.export.ExportedProgram method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * named_children() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * named_modules() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * named_parameters() (torch.distributed.fsdp.FullyShardedDataParallel method)
    * (torch.export.ExportedProgram method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * NamedShape (torch.nn.Unflatten attribute)
  * names (torch.Tensor attribute)
  * nan_to_num() (in module torch)
    * (torch.Tensor method)
  * nan_to_num_() (torch.Tensor method)
  * nanmean() (in module torch)
    * (torch.Tensor method)
  * nanmedian() (in module torch)
    * (torch.Tensor method)
  * nanquantile() (in module torch)
    * (torch.Tensor method)
  * nansum() (in module torch)
    * (torch.Tensor method)
  * narrow() (in module torch)
    * (in module torch.nested)
    * (torch.Tensor method)
  * narrow_copy() (in module torch)
    * (torch.Tensor method)
  * nbytes (torch.Tensor attribute)
  * nbytes() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * ndim (torch.Tensor attribute)
  * ndimension() (torch.Tensor method)
  * ndtr() (in module torch.special)
  * ndtri() (in module torch.special)
  * ne() (in module torch)
    * (torch.Tensor method)
  * ne_() (torch.Tensor method)
  * neg() (in module torch)
    * (torch.Tensor method)

| 
  * neg_() (torch.Tensor method)
  * negative() (in module torch)
    * (torch.Tensor method)
  * negative_() (torch.Tensor method)
  * NegativeBinomial (class in torch.distributions.negative_binomial)
  * nelement() (torch.Tensor method)
  * nested_tensor() (in module torch.nested)
  * nested_tensor_from_jagged() (in module torch.nested)
  * NestedIOFunction (class in torch.autograd.function)
  * new() (torch.UntypedStorage method)
  * new_empty() (torch.Tensor method)
  * new_full() (torch.Tensor method)
  * new_group() (in module torch.distributed)
  * new_ones() (torch.Tensor method)
  * new_tensor() (torch.Tensor method)
  * new_zeros() (torch.Tensor method)
  * next (torch.fx.Node property)
  * next_functions (torch.autograd.graph.Node property)
  * next_rendezvous() (torch.distributed.elastic.rendezvous.RendezvousHandler method)
  * nextafter() (in module torch)
    * (torch.Tensor method)
  * nextafter_() (torch.Tensor method)
  * nll_loss() (in module torch.nn.functional)
  * NLLLoss (class in torch.nn)
  * no_grad (class in torch)
  * no_sync() (torch.distributed.fsdp.FullyShardedDataParallel method)
    * (torch.nn.parallel.DistributedDataParallel method)
  * Node (class in torch.fx)
  * node_copy() (torch.fx.Graph method)
  * nodes (torch.fx.Graph property)
  * nonzero() (in module torch)
    * (torch.Tensor method)
  * noop_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks)
  * noop_mask() (in module torch.nn.attention.flex_attention)
  * NoopObserver (class in torch.ao.quantization.observer)
  * norm() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * Normal (class in torch.distributions.normal)
  * normal() (in module torch)
  * normal_() (in module torch.nn.init)
    * (torch.Tensor method)
  * normalize() (in module torch.nn.functional)
  * normalized_arguments() (torch.fx.Node method)
  * not_equal() (in module torch)
    * (torch.Tensor method)
  * not_equal_() (torch.Tensor method)
  * notify_join_context() (torch.distributed.algorithms.Join static method)
  * NSTracer (class in torch.ao.ns._numeric_suite_fx)
  * NullMetricHandler (class in torch.distributed.elastic.metrics.api)
  * num_keys() (torch.distributed.Store method)
  * num_nodes_waiting() (torch.distributed.elastic.rendezvous.RendezvousHandler method)
  * num_worker_threads (torch.distributed.rpc.TensorPipeRpcBackendOptions property)
  * numel() (in module torch)
    * (torch.nn.attention.flex_attention.BlockMask method)
    * (torch.Size method)
    * (torch.Tensor method)
  * NUMERIC_DEBUG_HANDLE_KEY (in module torch.ao.quantization)
  * numpy() (torch.Tensor method)
  * nuttall() (in module torch.signal.windows)

  
---|---  
## O
  * ObservationType (class in torch.ao.quantization.backend_config)
  * ObserverBase (class in torch.ao.quantization.observer)
  * OffloadPolicy (class in torch.distributed.fsdp)
  * on_generate_code() (torch.fx.Graph method)
  * once_differentiable() (in module torch.autograd.function)
  * one_hot() (in module torch.nn.functional)
  * OneCycleLR (class in torch.optim.lr_scheduler)
  * onednn_fusion_enabled() (in module torch.jit)
  * OneHotCategorical (class in torch.distributions.one_hot_categorical)
  * ones() (in module torch)
    * (in module torch.distributed.tensor)
  * ones_() (in module torch.nn.init)
  * ones_like() (in module torch)
  * onnx_compatible() (torch.onnx.JitScalarType method)
  * onnx_type() (torch.onnx.JitScalarType method)
  * OnnxBackend (class in torch.onnx.verification)
  * OnnxExporterError (class in torch.onnx)
  * ONNXProgram (class in torch.onnx)
  * OnnxRegistry (class in torch.onnx)
  * ONNXRuntimeOptions (class in torch.onnx)
  * OnnxTestCaseRepro (class in torch.onnx.verification)
  * opcheck() (in module torch.library)
  * opset_version (torch.onnx.OnnxRegistry property)
  * optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * optim_state_dict_to_load() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * optimize() (torch.onnx.ONNXProgram method)

| 
  * optimize_for_inference() (in module torch.jit)
  * optimize_for_mobile() (in module torch.utils.mobile_optimizer)
  * Optimizer (class in torch.optim)
  * OptimStateDictConfig (class in torch.distributed.fsdp)
  * or_masks() (in module torch.nn.attention.flex_attention)
  * orgqr() (in module torch)
    * (torch.Tensor method)
  * ormqr() (in module torch)
    * (torch.Tensor method)
  * orthogonal() (in module torch.nn.utils.parametrizations)
  * orthogonal_() (in module torch.nn.init)
  * outer() (in module torch)
    * (torch.Tensor method)
  * OutOfMemoryError
  * output() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Graph method)
    * (torch.fx.Interpreter method)
  * output_node() (torch.fx.Graph method)
  * OUTPUT_SHARE_OBSERVER_WITH_INPUT (torch.ao.quantization.backend_config.ObservationType attribute)
  * OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (torch.ao.quantization.backend_config.ObservationType attribute)
  * OutputComparisonLogger (class in torch.ao.ns._numeric_suite_fx)
  * OutputKind (class in torch.export.graph_signature)
  * OutputLogger (class in torch.ao.ns._numeric_suite)
    * (class in torch.ao.ns._numeric_suite_fx)
  * OutputSpec (class in torch.export.graph_signature)
  * owner() (torch.distributed.rpc.PyRRef method)
  * owner_name() (torch.distributed.rpc.PyRRef method)

  
---|---  
## P
  * P2POp (class in torch.distributed)
  * pack_padded_sequence() (in module torch.nn.utils.rnn)
  * pack_sequence() (in module torch.nn.utils.rnn)
  * PackageExporter (class in torch.package)
  * PackageImporter (class in torch.package)
  * PackagingError (class in torch.package)
  * PackedSequence (class in torch.nn.utils.rnn)
  * pad() (in module torch.nn.functional)
  * pad_packed_sequence() (in module torch.nn.utils.rnn)
  * pad_sequence() (in module torch.nn.utils.rnn)
  * pairwise_distance() (in module torch.nn.functional)
  * PairwiseDistance (class in torch.nn)
  * parallel_info() (in module torch.__config__)
  * parallelize_module() (in module torch.distributed.tensor.parallel)
  * param_shape (torch.distributions.bernoulli.Bernoulli property)
    * (torch.distributions.binomial.Binomial property)
    * (torch.distributions.categorical.Categorical property)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli property)
    * (torch.distributions.multinomial.Multinomial property)
    * (torch.distributions.negative_binomial.NegativeBinomial property)
    * (torch.distributions.one_hot_categorical.OneHotCategorical property)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli property)
  * Parameter (class in torch.nn.parameter)
  * ParameterDict (class in torch.nn)
  * ParameterList (class in torch.nn)
  * parameters() (in module torch.distributed.GradBucket)
    * (torch.export.ExportedProgram method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * parameters_to_vector() (in module torch.nn.utils)
  * ParametrizationList (class in torch.nn.utils.parametrize)
  * Pareto (class in torch.distributions.pareto)
  * parse_nvprof_trace() (in module torch.autograd.profiler)
  * Partial (class in torch.distributed.tensor.placement_types)
  * path (torch.distributed.FileStore property)
  * path_of_module() (torch.fx.Tracer method)
  * pca_lowrank() (in module torch)
  * PContext (class in torch.distributed.elastic.multiprocessing.api)
  * pdist() (in module torch.nn.functional)
  * per_channel_dynamic_qconfig (in module torch.ao.quantization.qconfig)
  * PerAxis (class in torch.ao.quantization.observer)
  * PerBlock (class in torch.ao.quantization.observer)
  * PerChannelMinMaxObserver (class in torch.ao.quantization.observer)
  * PerGroup (class in torch.ao.quantization.observer)
  * permute() (in module torch)
    * (torch.Tensor method)
  * perplexity() (torch.distributions.distribution.Distribution method)
  * PerRow (class in torch.ao.quantization.observer)
  * PerTensor (class in torch.ao.quantization.observer)
  * PerToken (class in torch.ao.quantization.observer)
  * pickle_storage_type() (torch.TypedStorage method)
  * pin_memory() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * pinv() (in module torch.linalg)
  * pinverse() (in module torch)
    * (torch.Tensor method)
  * Pipe (class in torch.distributed.pipelining)
  * pipe_split() (in module torch.distributed.pipelining)
  * pipeline() (in module torch.distributed.pipelining)
  * PipelineScheduleMulti (class in torch.distributed.pipelining.schedules)
  * PipelineScheduleSingle (class in torch.distributed.pipelining.schedules)
  * PipelineStage (class in torch.distributed.pipelining.stage)
  * pixel_shuffle() (in module torch.nn.functional)
  * pixel_unshuffle() (in module torch.nn.functional)
  * PixelShuffle (class in torch.nn)
  * PixelUnshuffle (class in torch.nn)
  * placeholder() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Graph method)
    * (torch.fx.Interpreter method)
    * (torch.fx.Transformer method)
  * PlaceholderObserver (class in torch.ao.quantization.observer)
  * Placement (class in torch.distributed.tensor.placement_types)
  * placements (torch.distributed.tensor.DTensor property)
  * Poisson (class in torch.distributions.poisson)
  * poisson() (in module torch)
  * poisson_nll_loss() (in module torch.nn.functional)
  * PoissonNLLLoss (class in torch.nn)
  * polar() (in module torch)
  * polygamma() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * polygamma_() (torch.Tensor method)
  * PolynomialLR (class in torch.optim.lr_scheduler)
  * pool() (torch.cuda.CUDAGraph method)
  * pop() (torch.autograd.profiler_util.StringTable method)
    * (torch.export.decomp_utils.CustomDecompTable method)
    * (torch.nn.ModuleDict method)
    * (torch.nn.ParameterDict method)
  * popitem() (torch.autograd.profiler_util.StringTable method)
    * (torch.nn.ParameterDict method)
  * port (torch.distributed.TCPStore property)

| 
  * positive() (in module torch)
    * (torch.Tensor method)
  * PositiveDefiniteTransform (class in torch.distributions.transforms)
  * post_hook() (torch.distributed.algorithms.JoinHook method)
  * PostLocalSGDOptimizer (class in torch.distributed.optim)
  * pow() (in module torch)
    * (torch.Tensor method)
  * pow_() (torch.Tensor method)
  * power_draw() (in module torch.cuda)
  * powerSGD_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook)
  * PowerSGDState (class in torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook)
  * PowerTransform (class in torch.distributions.transforms)
  * precision_matrix (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property)
    * (torch.distributions.multivariate_normal.MultivariateNormal property)
    * (torch.distributions.wishart.Wishart property)
  * predict() (torch.nn.AdaptiveLogSoftmaxWithLoss method)
  * preferred_blas_library() (in module torch.backends.cuda)
  * preferred_linalg_library() (in module torch.backends.cuda)
  * preferred_rocm_fa_library() (in module torch.backends.cuda)
  * PrefixStore (class in torch.distributed)
  * PReLU (class in torch.nn)
  * prelu() (in module torch.nn.functional)
  * prepare (class in torch.ao.quantization)
  * prepare_for_propagation_comparison (class in torch.ao.quantization)
  * prepare_fx (class in torch.ao.quantization.quantize_fx)
  * prepare_global_plan() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method)
    * (torch.distributed.checkpoint.StorageReader method)
    * (torch.distributed.checkpoint.StorageWriter method)
  * prepare_local_plan() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method)
    * (torch.distributed.checkpoint.StorageReader method)
    * (torch.distributed.checkpoint.StorageWriter method)
  * prepare_model_outputs() (in module torch.ao.ns._numeric_suite)
  * prepare_model_with_stubs() (in module torch.ao.ns._numeric_suite)
  * prepare_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx)
  * prepare_qat (class in torch.ao.quantization)
  * prepare_qat_fx (class in torch.ao.quantization.quantize_fx)
  * PrepareCustomConfig (class in torch.ao.quantization.fx.custom_config)
  * PrepareModuleInput (class in torch.distributed.tensor.parallel)
  * PrepareModuleOutput (class in torch.distributed.tensor.parallel)
  * prepend() (torch.fx.Node method)
  * preset_metadata_json() (torch.profiler._KinetoProfile method)
  * prettify_results() (torch.fx.experimental.symbolic_shapes.DimConstraints method)
  * prev (torch.fx.Node property)
  * primal (torch.autograd.forward_ad.UnpackedDualTensor attribute)
  * print() (torch.utils.benchmark.Compare method)
  * print_comparisons_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx)
  * print_readable() (torch.fx.GraphModule method)
  * print_tabular() (torch.fx.Graph method)
  * probs (torch.distributions.bernoulli.Bernoulli property)
    * (torch.distributions.binomial.Binomial property)
    * (torch.distributions.categorical.Categorical property)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli property)
    * (torch.distributions.geometric.Geometric property)
    * (torch.distributions.multinomial.Multinomial property)
    * (torch.distributions.negative_binomial.NegativeBinomial property)
    * (torch.distributions.one_hot_categorical.OneHotCategorical property)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli property)
    * (torch.distributions.relaxed_bernoulli.RelaxedBernoulli property)
    * (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical property)
  * process_inputs() (torch.fx.Graph method)
  * process_outputs() (torch.fx.Graph method)
  * ProcessFailure (class in torch.distributed.elastic.multiprocessing.errors)
  * prod() (in module torch)
    * (torch.Tensor method)
  * produce_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * produce_guards_expression() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * produce_guards_verbose() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * prof() (in module torch.distributed.elastic.metrics)
  * profile (class in torch.autograd.profiler)
    * (class in torch.profiler)
  * profile() (in module torch.mps.profiler)
  * ProfilerAction (class in torch.profiler)
  * ProfilerActivity (class in torch.profiler)
  * promote_types() (in module torch)
  * propagate_qconfig_ (class in torch.ao.quantization)
  * PropagateUnbackedSymInts (class in torch.fx.experimental.symbolic_shapes)
  * Proxy (class in torch.fx)
  * proxy() (torch.fx.Tracer method)
  * prune() (torch.nn.utils.prune.BasePruningMethod method)
    * (torch.nn.utils.prune.CustomFromMask method)
    * (torch.nn.utils.prune.Identity method)
    * (torch.nn.utils.prune.L1Unstructured method)
    * (torch.nn.utils.prune.LnStructured method)
    * (torch.nn.utils.prune.PruningContainer method)
    * (torch.nn.utils.prune.RandomStructured method)
    * (torch.nn.utils.prune.RandomUnstructured method)
  * PruningContainer (class in torch.nn.utils.prune)
  * psi() (in module torch.special)
  * put_() (torch.Tensor method)
  * put_metric() (in module torch.distributed.elastic.metrics)
  * PyRRef (class in torch.distributed.rpc)
  * python_code() (torch.fx.Graph method)
  * python_version() (torch.package.PackageImporter method)

  
---|---  
## Q
  * q_indices (torch.nn.attention.flex_attention.BlockMask attribute)
  * q_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute)
  * q_per_channel_axis() (torch.Tensor method)
  * q_per_channel_scales() (torch.Tensor method)
  * q_per_channel_zero_points() (torch.Tensor method)
  * q_scale() (torch.Tensor method)
  * q_zero_point() (torch.Tensor method)
  * QConfig (class in torch.ao.quantization.qconfig)
  * QConfigMapping (class in torch.ao.quantization.qconfig_mapping)
  * QFunctional (class in torch.ao.nn.quantized)
  * QInt32Storage (class in torch)
  * QInt8Storage (class in torch)
  * qr() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * qscheme() (torch.Tensor method)
  * quantile() (in module torch)
    * (torch.Tensor method)
  * quantize (class in torch.ao.quantization)
  * quantize_dynamic (class in torch.ao.quantization)

| 
  * quantize_per_channel() (in module torch)
  * quantize_per_tensor() (in module torch)
  * quantize_qat (class in torch.ao.quantization)
  * quantized_batch_norm() (in module torch)
  * quantized_max_pool1d() (in module torch)
  * quantized_max_pool2d() (in module torch)
  * QuantStub (class in torch.ao.quantization)
  * QuantWrapper (class in torch.ao.quantization)
  * query() (torch.cuda.Event method)
    * (torch.cuda.ExternalStream method)
    * (torch.cuda.Stream method)
    * (torch.Event method)
    * (torch.mps.event.Event method)
    * (torch.mtia.Event method)
    * (torch.mtia.Stream method)
    * (torch.Stream method)
    * (torch.xpu.Event method)
    * (torch.xpu.Stream method)
  * QUInt2x4Storage (class in torch)
  * QUInt4x2Storage (class in torch)
  * QUInt8Storage (class in torch)

  
---|---  
## R
  * rad2deg() (in module torch)
    * (torch.Tensor method)
  * RAdam (class in torch.optim)
  * rand() (in module torch)
    * (in module torch.distributed.tensor)
  * rand_like() (in module torch)
  * randint() (in module torch)
  * randint_like() (in module torch)
  * randn() (in module torch)
    * (in module torch.distributed.tensor)
  * randn_like() (in module torch)
  * random_() (torch.Tensor method)
  * random_split() (in module torch.utils.data)
  * random_structured() (in module torch.nn.utils.prune)
  * random_unstructured() (in module torch.nn.utils.prune)
  * RandomSampler (class in torch.utils.data)
  * RandomStructured (class in torch.nn.utils.prune)
  * RandomUnstructured (class in torch.nn.utils.prune)
  * randperm() (in module torch)
  * range() (in module torch)
    * (in module torch.cuda.nvtx)
  * range_pop() (in module torch.cuda.nvtx)
    * (in module torch.profiler.itt)
  * range_push() (in module torch.cuda.nvtx)
    * (in module torch.profiler.itt)
  * rate (torch.distributions.inverse_gamma.InverseGamma property)
  * ravel() (in module torch)
    * (torch.Tensor method)
  * read_data() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method)
    * (torch.distributed.checkpoint.StorageReader method)
  * read_file() (in module torch.cuda.tunable)
  * read_metadata() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method)
    * (torch.distributed.checkpoint.StorageReader method)
  * ReadItem (class in torch.distributed.checkpoint)
  * real (torch.Tensor attribute)
  * real() (in module torch)
  * rebind_unbacked() (in module torch.fx.experimental.symbolic_shapes)
  * reciprocal() (in module torch)
    * (torch.Tensor method)
  * reciprocal_() (torch.Tensor method)
  * recommended_max_memory() (in module torch.mps)
  * recompile() (torch.fx.GraphModule method)
  * record() (in module torch.distributed.elastic.events)
    * (in module torch.distributed.elastic.multiprocessing.errors)
    * (torch.cuda.Event method)
    * (torch.Event method)
    * (torch.mps.event.Event method)
    * (torch.mtia.Event method)
    * (torch.xpu.Event method)
  * record_event() (torch.cuda.ExternalStream method)
    * (torch.cuda.Stream method)
    * (torch.mtia.Stream method)
    * (torch.Stream method)
    * (torch.xpu.Stream method)
  * record_function (class in torch.autograd.profiler)
  * record_memory_history() (in module torch.mtia)
  * record_stream() (torch.Tensor method)
  * record_untuned_enable() (in module torch.cuda.tunable)
  * record_untuned_is_enabled() (in module torch.cuda.tunable)
  * RecordingObserver (class in torch.ao.quantization.observer)
  * recursive_undo() (torch.optim.lr_scheduler.SequentialLR method)
  * recv() (in module torch.distributed)
  * recv_object_list() (in module torch.distributed)
  * redistribute() (torch.distributed.tensor.DTensor method)
  * reduce() (in module torch.distributed)
  * reduce_add() (in module torch.cuda.comm)
  * reduce_op (class in torch.distributed)
    * (torch.distributed.tensor.placement_types.Partial attribute)
  * reduce_scatter() (in module torch.distributed)
  * reduce_scatter_tensor() (in module torch.distributed)
  * ReduceLROnPlateau (class in torch.optim.lr_scheduler)
  * ReduceOp (class in torch.distributed)
  * refine_dynamic_shapes_from_suggested_fixes() (in module torch.export.dynamic_shapes)
  * refine_names() (torch.Tensor method)
  * ReflectionPad1d (class in torch.nn)
  * ReflectionPad2d (class in torch.nn)
  * ReflectionPad3d (class in torch.nn)
  * register() (torch.distributions.constraint_registry.ConstraintRegistry method)
  * register_autocast() (in module torch.library)
  * register_autograd() (in module torch.library)
  * register_backend() (torch.distributed.Backend class method)
  * register_backward_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_buffer() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_comm_hook() (torch.distributed.fsdp.FullyShardedDataParallel method)
    * (torch.nn.parallel.DistributedDataParallel method)
  * register_custom_op_symbolic() (in module torch.onnx)
  * register_dataclass() (in module torch.export)
  * register_event_handler() (in module torch.monitor)
  * register_extern_hook() (torch.package.PackageExporter method)
  * register_fake() (in module torch.library)
  * register_forward_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_forward_pre_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_fsdp_forward_method() (in module torch.distributed.fsdp)
  * register_full_backward_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_full_backward_pre_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_handle() (torch.cuda.gds.GdsFile method)
  * register_hook() (torch.autograd.graph.Node method)
    * (torch.Tensor method)
  * register_intern_hook() (torch.package.PackageExporter method)
  * register_kernel() (in module torch.library)
  * register_kl() (in module torch.distributions.kl)
  * register_load_state_dict_post_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_load_state_dict_pre_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_mock_hook() (torch.package.PackageExporter method)
  * register_module() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_module_backward_hook() (in module torch.nn.modules.module)
  * register_module_buffer_registration_hook() (in module torch.nn.modules.module)
  * register_module_forward_hook() (in module torch.nn.modules.module)
  * register_module_forward_pre_hook() (in module torch.nn.modules.module)
  * register_module_full_backward_hook() (in module torch.nn.modules.module)
  * register_module_full_backward_pre_hook() (in module torch.nn.modules.module)
  * register_module_module_registration_hook() (in module torch.nn.modules.module)
  * register_module_parameter_registration_hook() (in module torch.nn.modules.module)
  * register_multi_grad_hook (class in torch.autograd.graph)
  * register_op() (torch.onnx.OnnxRegistry method)
  * register_package() (in module torch.serialization)
  * register_parameter() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_parametrization() (in module torch.nn.utils.parametrize)
  * register_post_accumulate_grad_hook() (torch.Tensor method)
  * register_prehook() (torch.autograd.graph.Node method)
  * register_sharding() (in module torch.distributed.tensor.experimental)
  * register_state_dict_post_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_state_dict_pre_hook() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.AveragedModel method)
  * register_step_post_hook() (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)

| 
  * register_step_pre_hook() (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
  * register_timers() (torch.distributed.elastic.timer.TimerServer method)
  * register_torch_dispatch() (in module torch.library)
  * register_vmap() (in module torch.library)
  * reify() (torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs method)
    * (torch.distributed.elastic.multiprocessing.api.LogsSpecs method)
  * rekey_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * RelaxedBernoulli (class in torch.distributions.relaxed_bernoulli)
  * RelaxedOneHotCategorical (class in torch.distributions.relaxed_categorical)
  * RelaxedUnspecConstraint (class in torch.fx.experimental.symbolic_shapes)
  * release() (torch.distributed.elastic.timer.TimerClient method)
    * (torch.onnx.ONNXProgram method)
  * ReLU (class in torch.nn)
  * relu() (in module torch.nn.functional)
  * ReLU6 (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * relu6() (in module torch.nn.functional)
  * relu_() (in module torch.nn.functional)
  * remainder() (in module torch)
    * (torch.Tensor method)
  * remainder_() (torch.Tensor method)
  * remote() (in module torch.distributed.rpc)
    * (torch.distributed.rpc.PyRRef method)
  * remote_parameters() (torch.distributed.nn.api.remote_module.RemoteModule method)
  * RemoteModule (class in torch.distributed.nn.api.remote_module)
  * remove() (in module torch.nn.utils.prune)
    * (torch.nn.utils.prune.BasePruningMethod method)
    * (torch.nn.utils.prune.CustomFromMask method)
    * (torch.nn.utils.prune.Identity method)
    * (torch.nn.utils.prune.L1Unstructured method)
    * (torch.nn.utils.prune.LnStructured method)
    * (torch.nn.utils.prune.PruningContainer method)
    * (torch.nn.utils.prune.RandomStructured method)
    * (torch.nn.utils.prune.RandomUnstructured method)
  * remove_parametrizations() (in module torch.nn.utils.parametrize)
  * remove_spectral_norm() (in module torch.nn.utils)
  * remove_weight_norm() (in module torch.nn.utils)
  * rename() (torch.Tensor method)
  * rename_() (torch.Tensor method)
  * rename_privateuse1_backend() (in module torch.utils)
  * render() (torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint method)
  * RendezvousBackend (class in torch.distributed.elastic.rendezvous.dynamic_rendezvous)
  * RendezvousClosedError (class in torch.distributed.elastic.rendezvous.api)
  * RendezvousConnectionError (class in torch.distributed.elastic.rendezvous.api)
  * RendezvousError (class in torch.distributed.elastic.rendezvous.api)
  * RendezvousGracefulExitError (class in torch.distributed.elastic.rendezvous.api)
  * RendezvousHandler (class in torch.distributed.elastic.rendezvous)
  * RendezvousHandlerRegistry (class in torch.distributed.elastic.rendezvous)
  * RendezvousInfo (class in torch.distributed.elastic.rendezvous)
  * RendezvousParameters (class in torch.distributed.elastic.rendezvous)
  * RendezvousStateError (class in torch.distributed.elastic.rendezvous.api)
  * RendezvousStoreInfo (class in torch.distributed.elastic.rendezvous.api)
  * RendezvousTimeout (class in torch.distributed.elastic.rendezvous.dynamic_rendezvous)
  * RendezvousTimeoutError (class in torch.distributed.elastic.rendezvous.api)
  * renorm() (in module torch)
    * (torch.Tensor method)
  * renorm_() (torch.Tensor method)
  * repeat() (torch.Tensor method)
  * repeat_interleave() (in module torch)
    * (torch.Tensor method)
  * replace() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * replace_all_batch_norm_modules_() (in module torch.func)
  * replace_all_uses() (torch.export.graph_signature.ExportGraphSignature method)
  * replace_all_uses_with() (torch.fx.Node method)
  * replace_input_with() (torch.fx.Node method)
  * replace_pattern() (in module torch.fx)
  * replay() (torch.cuda.CUDAGraph method)
  * Replicate (class in torch.distributed.tensor.placement_types)
  * ReplicationPad1d (class in torch.nn)
  * ReplicationPad2d (class in torch.nn)
  * ReplicationPad3d (class in torch.nn)
  * requires_grad (torch.Tensor attribute)
  * requires_grad_() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
  * reset() (in module torch.compiler)
    * (torch.cuda.CUDAGraph method)
    * (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method)
    * (torch.distributed.checkpoint.StorageReader method)
    * (torch.distributed.checkpoint.StorageWriter method)
    * (torch.quasirandom.SobolEngine method)
  * reset_accumulated_memory_stats() (in module torch.xpu)
  * reset_max_memory_allocated() (in module torch.cuda)
  * reset_max_memory_cached() (in module torch.cuda)
  * reset_min_max_vals() (torch.ao.quantization.observer.MinMaxObserver method)
    * (torch.ao.quantization.observer.PerChannelMinMaxObserver method)
  * reset_parameters() (torch.nn.modules.normalization.RMSNorm method)
    * (torch.nn.RMSNorm method)
  * reset_peak_host_memory_stats() (in module torch.cuda)
  * reset_peak_memory_stats() (in module torch.cuda)
    * (in module torch.xpu)
  * reshape() (in module torch)
    * (torch.Tensor method)
  * reshape_as() (torch.Tensor method)
  * ReshapeTransform (class in torch.distributions.transforms)
  * reshard() (torch.distributed.fsdp.FSDPModule method)
  * resizable() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * resize_() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * resize_as_() (torch.Tensor method)
  * resolve_bytes() (torch.distributed.checkpoint.LoadPlanner method)
  * resolve_conj() (in module torch)
    * (torch.Tensor method)
  * resolve_data() (torch.distributed.checkpoint.SavePlanner method)
  * resolve_name() (in module torch.overrides)
  * resolve_neg() (in module torch)
    * (torch.Tensor method)
  * resolve_tensor() (torch.distributed.checkpoint.LoadPlanner method)
  * resolve_unbacked_bindings() (in module torch.fx.experimental.symbolic_shapes)
  * result() (torch.distributed.Work method)
  * result_type() (in module torch)
  * retain_grad() (torch.Tensor method)
  * retains_grad (torch.Tensor attribute)
  * rewrite_with_congruences() (torch.fx.experimental.symbolic_shapes.DimConstraints method)
  * rfft() (in module torch.fft)
  * rfft2() (in module torch.fft)
  * rfftfreq() (in module torch.fft)
  * rfftn() (in module torch.fft)
  * right_inverse() (torch.nn.utils.parametrize.ParametrizationList method)
  * rms_norm() (in module torch.nn.functional)
  * RMSNorm (class in torch.nn)
    * (class in torch.nn.modules.normalization)
  * RMSprop (class in torch.optim)
  * RNN (class in torch.nn)
  * RNNBase (class in torch.nn)
  * RNNCell (class in torch.ao.nn.quantized.dynamic)
    * (class in torch.nn)
  * roll() (in module torch)
    * (torch.Tensor method)
  * rot90() (in module torch)
    * (torch.Tensor method)
  * round() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * round_() (torch.Tensor method)
  * row_indices() (torch.Tensor method)
  * row_stack() (in module torch)
  * RowwiseParallel (class in torch.distributed.tensor.parallel)
  * rpc_async() (in module torch.distributed.rpc)
    * (torch.distributed.rpc.PyRRef method)
  * rpc_sync() (in module torch.distributed.rpc)
    * (torch.distributed.rpc.PyRRef method)
  * rpc_timeout (torch.distributed.rpc.RpcBackendOptions property)
    * (torch.distributed.rpc.TensorPipeRpcBackendOptions property)
  * RpcBackendOptions (class in torch.distributed.rpc)
  * Rprop (class in torch.optim)
  * RReLU (class in torch.nn)
  * rrelu() (in module torch.nn.functional)
  * rrelu_() (in module torch.nn.functional)
  * rsample() (torch.distributions.beta.Beta method)
    * (torch.distributions.cauchy.Cauchy method)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli method)
    * (torch.distributions.dirichlet.Dirichlet method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.exponential.Exponential method)
    * (torch.distributions.fishersnedecor.FisherSnedecor method)
    * (torch.distributions.gamma.Gamma method)
    * (torch.distributions.independent.Independent method)
    * (torch.distributions.laplace.Laplace method)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method)
    * (torch.distributions.multivariate_normal.MultivariateNormal method)
    * (torch.distributions.normal.Normal method)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli method)
    * (torch.distributions.studentT.StudentT method)
    * (torch.distributions.transformed_distribution.TransformedDistribution method)
    * (torch.distributions.uniform.Uniform method)
    * (torch.distributions.wishart.Wishart method)
  * rsqrt() (in module torch)
    * (torch.Tensor method)
  * rsqrt_() (torch.Tensor method)
  * run() (torch.distributed.elastic.agent.server.ElasticAgent method)
    * (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Interpreter method)
  * run_decompositions() (torch.export.ExportedProgram method)
  * run_node() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method)
    * (torch.fx.Interpreter method)
  * RunProcsResult (class in torch.distributed.elastic.multiprocessing.api)
  * RunResult (class in torch.distributed.elastic.agent.server.api)

  
---|---  
## S
  * safe_globals (class in torch.serialization)
  * sample() (torch.distributions.bernoulli.Bernoulli method)
    * (torch.distributions.binomial.Binomial method)
    * (torch.distributions.categorical.Categorical method)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli method)
    * (torch.distributions.distribution.Distribution method)
    * (torch.distributions.geometric.Geometric method)
    * (torch.distributions.independent.Independent method)
    * (torch.distributions.lkj_cholesky.LKJCholesky method)
    * (torch.distributions.mixture_same_family.MixtureSameFamily method)
    * (torch.distributions.multinomial.Multinomial method)
    * (torch.distributions.negative_binomial.NegativeBinomial method)
    * (torch.distributions.normal.Normal method)
    * (torch.distributions.one_hot_categorical.OneHotCategorical method)
    * (torch.distributions.poisson.Poisson method)
    * (torch.distributions.transformed_distribution.TransformedDistribution method)
    * (torch.distributions.von_mises.VonMises method)
  * sample_n() (torch.distributions.distribution.Distribution method)
  * sampled_addmm() (in module torch.sparse)
  * Sampler (class in torch.utils.data)
  * save() (in module torch)
    * (in module torch.distributed.checkpoint.state_dict_saver)
    * (in module torch.export)
    * (in module torch.jit)
    * (torch.jit.ScriptFunction method)
    * (torch.jit.ScriptModule method)
    * (torch.onnx.ONNXProgram method)
  * save_binary() (torch.package.PackageExporter method)
  * save_for_backward() (torch.autograd.function.BackwardCFunction method)
    * (torch.autograd.function.FunctionCtx method)
    * (torch.autograd.function.InplaceFunction method)
    * (torch.autograd.function.NestedIOFunction method)
  * save_for_forward() (torch.autograd.function.BackwardCFunction method)
    * (torch.autograd.function.InplaceFunction method)
    * (torch.autograd.function.NestedIOFunction method)
  * save_module() (torch.package.PackageExporter method)
  * save_on_cpu (class in torch.autograd.graph)
  * save_pickle() (torch.package.PackageExporter method)
  * save_source_file() (torch.package.PackageExporter method)
  * save_source_string() (torch.package.PackageExporter method)
  * save_state_dict() (in module torch.distributed.checkpoint.state_dict_saver)
  * save_storage() (torch.cuda.gds.GdsFile method)
  * save_text() (torch.package.PackageExporter method)
  * save_to_buffer() (torch.jit.ScriptFunction method)
  * saved_tensors (torch.autograd.function.NestedIOFunction property)
  * saved_tensors_hooks (class in torch.autograd.graph)
  * SavePlan (class in torch.distributed.checkpoint)
  * SavePlanner (class in torch.distributed.checkpoint)
  * scalar_name() (torch.onnx.JitScalarType method)
  * scale (torch.distributions.half_cauchy.HalfCauchy property)
    * (torch.distributions.half_normal.HalfNormal property)
    * (torch.distributions.log_normal.LogNormal property)
  * scale_fn() (torch.optim.lr_scheduler.CyclicLR method)
  * scale_tril (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property)
    * (torch.distributions.multivariate_normal.MultivariateNormal property)
    * (torch.distributions.wishart.Wishart property)
  * scaled_dot_product_attention() (in module torch.nn.functional)
  * scaled_modified_bessel_k0() (in module torch.special)
  * scaled_modified_bessel_k1() (in module torch.special)
  * scatter() (in module torch)
    * (in module torch.cuda.comm)
    * (in module torch.distributed)
    * (torch.Tensor method)
  * scatter_() (torch.Tensor method)
  * scatter_add() (in module torch)
    * (torch.Tensor method)
  * scatter_add_() (torch.Tensor method)
  * scatter_full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * scatter_object_list() (in module torch.distributed)
  * scatter_reduce() (in module torch)
    * (torch.Tensor method)
  * scatter_reduce_() (torch.Tensor method)
  * schedule() (in module torch.profiler)
  * Schedule1F1B (class in torch.distributed.pipelining.schedules)
  * ScheduleGPipe (class in torch.distributed.pipelining.schedules)
  * ScheduleInterleaved1F1B (class in torch.distributed.pipelining.schedules)
  * ScheduleInterleavedZeroBubble (class in torch.distributed.pipelining.schedules)
  * ScheduleLoopedBFS (class in torch.distributed.pipelining.schedules)
  * ScheduleZBVZeroBubble (class in torch.distributed.pipelining.schedules)
  * script() (in module torch.jit)
  * script_if_tracing() (in module torch.jit)
  * ScriptFunction (class in torch.jit)
  * ScriptModule (class in torch.jit)
  * sdp_kernel() (in module torch.backends.cuda)
  * sdpa_kernel() (in module torch.nn.attention)
  * SDPAParams (class in torch.backends.cuda)
  * SDPBackend (class in torch.nn.attention)
  * searchsorted() (in module torch)
  * see() (torch.autograd.profiler.EnforceUnique method)
  * seed() (in module torch)
    * (in module torch.cuda)
    * (in module torch.mps)
    * (in module torch.random)
    * (in module torch.xpu)
    * (torch.Generator method)
  * seed_all() (in module torch.cuda)
    * (in module torch.xpu)
  * select() (in module torch)
    * (torch.Tensor method)
  * select_model_mode_for_export() (in module torch.onnx)
  * select_scatter() (in module torch)
    * (torch.Tensor method)
  * SelectiveCheckpointContext (class in torch.utils.checkpoint)
  * self_cpu_time_total (torch.autograd.profiler.profile property)
  * SELU (class in torch.nn)
  * selu() (in module torch.nn.functional)
  * send() (in module torch.distributed)
  * send_object_list() (in module torch.distributed)
  * seq_lengths (torch.nn.attention.flex_attention.BlockMask attribute)
  * SequenceParallel (class in torch.distributed.tensor.parallel)
  * Sequential (class in torch.nn)
  * SequentialLR (class in torch.optim.lr_scheduler)
  * SequentialSampler (class in torch.utils.data)
  * set() (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method)
    * (torch.distributed.Store method)
  * set_() (torch.Tensor method)
  * set_all_reduce_hook() (torch.distributed.fsdp.FSDPModule method)
  * set_backend_pattern_config() (torch.ao.quantization.backend_config.BackendConfig method)
  * set_backend_pattern_configs() (torch.ao.quantization.backend_config.BackendConfig method)
  * set_buffer() (in module torch.distributed.GradBucket)
  * set_checkpoint_debug_enabled() (in module torch.utils.checkpoint)
  * set_closed() (torch.distributed.elastic.rendezvous.RendezvousHandler method)
  * set_codegen() (torch.fx.Graph method)
  * set_crc32_options() (in module torch.serialization)
  * set_custom_trace_id_callback() (torch.profiler.profile method)
  * set_default_device() (in module torch)
  * set_default_dtype() (in module torch)
  * set_default_load_endianness() (in module torch.serialization)
  * set_default_mmap_options() (in module torch.serialization)
  * set_default_tensor_type() (in module torch)
  * set_default_validate_args() (torch.distributions.distribution.Distribution static method)
  * set_detect_anomaly (class in torch.autograd)
  * set_deterministic_debug_mode() (in module torch)
  * set_device() (in module torch.cpu)
    * (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * set_device_idx() (in module torch.accelerator)
  * set_device_index() (in module torch.accelerator)
  * set_device_map() (torch.distributed.rpc.TensorPipeRpcBackendOptions method)
  * set_devices() (torch.distributed.rpc.TensorPipeRpcBackendOptions method)
  * set_dir() (in module torch.hub)
  * set_dtype_configs() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_exception() (torch.futures.Future method)
  * set_extra_state() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * set_fastpath_enabled() (in module torch.backends.mha)
  * set_filename() (in module torch.cuda.tunable)
  * set_flags() (in module torch.backends.nnpack)
  * set_float32_matmul_precision() (in module torch)
  * set_float_to_observed_mapping() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_flush_denormal() (in module torch)
  * set_fused_module() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_fuser_method() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_fusion_strategy() (in module torch.jit)
  * set_global() (torch.ao.quantization.qconfig_mapping.QConfigMapping method)
  * set_grad_enabled (class in torch.autograd.grad_mode)
  * set_input_quantized_indexes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_is_last_backward() (torch.distributed.fsdp.FSDPModule method)
  * set_kernel_enabled() (torch._library.custom_ops.CustomOpDef method)
  * set_logs() (in module torch._logging)
  * set_materialize_grads() (torch.autograd.function.BackwardCFunction method)
    * (torch.autograd.function.FunctionCtx method)
    * (torch.autograd.function.InplaceFunction method)
    * (torch.autograd.function.NestedIOFunction method)
  * set_max_tuning_duration() (in module torch.cuda.tunable)
  * set_max_tuning_iterations() (in module torch.cuda.tunable)
  * set_model_state_dict() (in module torch.distributed.checkpoint.state_dict)
  * set_module() (in module torch.utils)
  * set_module_name() (torch.ao.quantization.qconfig_mapping.QConfigMapping method)
  * set_module_name_object_type_order() (torch.ao.quantization.qconfig_mapping.QConfigMapping method)
  * set_module_name_regex() (torch.ao.quantization.qconfig_mapping.QConfigMapping method)
  * set_modules_to_backward_prefetch() (torch.distributed.fsdp.FSDPModule method)
  * set_modules_to_forward_prefetch() (torch.distributed.fsdp.FSDPModule method)
  * set_multithreading_enabled (class in torch.autograd.grad_mode)
  * set_name() (torch.ao.quantization.backend_config.BackendConfig method)
  * set_non_traceable_module_classes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_non_traceable_module_names() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_num_interop_threads() (in module torch)
  * set_num_threads() (in module torch)
  * set_object_type() (torch.ao.quantization.qconfig_mapping.QConfigMapping method)
  * set_observation_type() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_observed_to_quantized_mapping() (torch.ao.quantization.fx.custom_config.ConvertCustomConfig method)
  * set_optimizer_state_dict() (in module torch.distributed.checkpoint.state_dict)
  * set_output_quantized_indexes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_overwrite_module_params_on_conversion() (in module torch.__future__)
  * set_pattern() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_per_process_memory_fraction() (in module torch.cuda)
    * (in module torch.mps)
  * set_post_optim_event() (torch.distributed.fsdp.FSDPModule method)
  * set_preserved_attributes() (torch.ao.quantization.fx.custom_config.ConvertCustomConfig method)
    * (torch.ao.quantization.fx.custom_config.FuseCustomConfig method)
    * (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_printoptions() (in module torch)
  * set_qat_module() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_reduce_scatter_divide_factor() (torch.distributed.fsdp.FSDPModule method)
  * set_reference_quantized_module() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_requires_all_reduce() (torch.distributed.fsdp.FSDPModule method)
  * set_requires_gradient_sync() (torch.distributed.fsdp.FSDPModule method)
  * set_reshard_after_backward() (torch.distributed.fsdp.FSDPModule method)
  * set_result() (torch.futures.Future method)
  * set_rng_state() (in module torch)
    * (in module torch.cuda)
    * (in module torch.mps)
    * (in module torch.mtia)
    * (in module torch.random)
    * (in module torch.xpu)
  * set_rng_state_all() (in module torch.cuda)
    * (in module torch.xpu)
  * set_root_module() (torch.ao.quantization.backend_config.BackendPatternConfig method)
  * set_rotating_buffer_size() (in module torch.cuda.tunable)
  * set_sharing_strategy() (in module torch.multiprocessing)
  * set_stance() (in module torch.compiler)
  * set_standalone_module_class() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_standalone_module_name() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
  * set_state() (torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend method)
    * (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend method)
    * (torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend method)
    * (torch.Generator method)
  * set_state_dict() (in module torch.distributed.checkpoint.state_dict)
  * set_state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * set_stream() (in module torch.accelerator)
    * (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * set_submodule() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * set_swap_module_params_on_conversion() (in module torch.__future__)
  * set_sync_debug_mode() (in module torch.cuda)
  * set_timeout() (torch.distributed.Store method)
  * set_unbacked_var_to_val() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * set_unshard_in_backward() (torch.distributed.fsdp.FSDPModule method)
  * set_up_planner() (torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner method)
    * (torch.distributed.checkpoint.LoadPlanner method)
    * (torch.distributed.checkpoint.SavePlanner method)
  * set_up_storage_reader() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method)
    * (torch.distributed.checkpoint.StorageReader method)
  * set_up_storage_writer() (torch.distributed.checkpoint.StorageWriter method)
  * set_warn_always() (in module torch)
  * setdefault() (torch.autograd.profiler_util.StringTable method)
    * (torch.nn.ParameterDict method)
  * setup_context() (torch.autograd.function.InplaceFunction static method)
    * (torch.autograd.function.NestedIOFunction static method)
  * SGD (class in torch.optim)
  * sgn() (in module torch)
    * (torch.Tensor method)
  * sgn_() (torch.Tensor method)
  * Shadow (class in torch.ao.ns._numeric_suite)
  * ShadowLogger (class in torch.ao.ns._numeric_suite)
  * shape (torch.nn.attention.flex_attention.BlockMask property)
    * (torch.Tensor attribute)
  * ShapeEnv (class in torch.fx.experimental.symbolic_shapes)
  * ShapeEnvSettings (class in torch.fx.experimental.symbolic_shapes)
  * ShapesCollection (class in torch.export.dynamic_shapes)
  * Shard (class in torch.distributed.tensor.placement_types)
  * shard_full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * sharded_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * ShardedOptimStateDictConfig (class in torch.distributed.fsdp)
  * ShardedStateDictConfig (class in torch.distributed.fsdp)
  * ShardingStrategy (class in torch.distributed.fsdp)
  * share_memory() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * share_memory_() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * short() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * ShortStorage (class in torch)
  * should_synchronize_after_execute (torch.distributed.checkpoint.staging.AsyncStager property)
  * show() (in module torch.__config__)
  * shutdown() (in module torch.distributed.rpc)
    * (torch.distributed.elastic.rendezvous.RendezvousHandler method)
  * Sigmoid (class in torch.ao.nn.quantized)
    * (class in torch.nn)
  * sigmoid() (in module torch)
    * (in module torch.nn.functional)
    * (torch.Tensor method)
  * sigmoid_() (torch.Tensor method)
  * SigmoidTransform (class in torch.distributions.transforms)
  * sign (torch.distributions.transforms.Transform property)
  * sign() (in module torch)
    * (torch.Tensor method)
  * sign_() (torch.Tensor method)
  * signbit() (in module torch)
    * (torch.Tensor method)
  * significant_figures (torch.utils.benchmark.Measurement property)
  * SiLU (class in torch.nn)
  * silu() (in module torch.nn.functional)
  * SimpleElasticAgent (class in torch.distributed.elastic.agent.server)
  * simplify() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * sin() (in module torch)
    * (torch.Tensor method)
  * sin_() (torch.Tensor method)
  * sinc() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)
  * sinc_() (torch.Tensor method)
  * sinh() (in module torch)
    * (torch.Tensor method)
  * sinh_() (torch.Tensor method)
  * Size (class in torch)
  * size (in module torch.backends.cuda.cufft_plan_cache)
  * size() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * size_hint() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * skip_data (class in torch.serialization)
  * skip_init() (in module torch.nn.utils)
  * slice_scatter() (in module torch)
    * (torch.Tensor method)
  * slogdet() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)

| 
  * smm() (in module torch)
    * (torch.Tensor method)
  * smooth_l1_loss() (in module torch.nn.functional)
  * SmoothL1Loss (class in torch.nn)
  * snapshot() (in module torch.mtia)
    * (torch.cuda.MemPool method)
  * SobolEngine (class in torch.quasirandom)
  * soft_margin_loss() (in module torch.nn.functional)
  * SoftMarginLoss (class in torch.nn)
  * Softmax (class in torch.nn)
  * softmax() (in module torch)
    * (in module torch.nn.functional)
    * (in module torch.sparse)
    * (in module torch.special)
    * (torch.Tensor method)
  * Softmax2d (class in torch.nn)
  * SoftmaxTransform (class in torch.distributions.transforms)
  * Softmin (class in torch.nn)
  * softmin() (in module torch.nn.functional)
  * Softplus (class in torch.nn)
  * softplus() (in module torch.nn.functional)
  * SoftplusTransform (class in torch.distributions.transforms)
  * Softshrink (class in torch.nn)
  * softshrink() (in module torch.nn.functional)
  * Softsign (class in torch.nn)
  * softsign() (in module torch.nn.functional)
  * solve() (in module torch.linalg)
    * (torch.fx.experimental.symbolic_shapes.DimConstraints method)
  * solve_ex() (in module torch.linalg)
  * solve_triangular() (in module torch.linalg)
  * sort() (in module torch)
    * (torch.Tensor method)
  * sorted_indices (torch.nn.utils.rnn.PackedSequence attribute)
  * source_rank() (torch.distributed.Work method)
  * sparse_() (in module torch.nn.init)
  * sparse_bsc_tensor() (in module torch)
  * sparse_bsr_tensor() (in module torch)
  * sparse_compressed_tensor() (in module torch)
  * sparse_coo_tensor() (in module torch)
  * sparse_csc_tensor() (in module torch)
  * sparse_csr_tensor() (in module torch)
  * sparse_dim() (torch.Tensor method)
  * sparse_mask() (torch.Tensor method)
  * sparse_resize_() (torch.Tensor method)
  * sparse_resize_and_clear_() (torch.Tensor method)
  * SparseAdam (class in torch.optim)
  * sparsity() (torch.nn.attention.flex_attention.BlockMask method)
  * spawn() (in module torch.multiprocessing.spawn)
  * SpawnContext (class in torch.multiprocessing)
  * spdiags() (in module torch.sparse)
  * spectral_norm() (in module torch.nn.utils)
    * (in module torch.nn.utils.parametrizations)
  * spherical_bessel_j0() (in module torch.special)
  * split() (in module torch)
    * (torch.Tensor method)
  * split_args_kwargs_into_chunks() (in module torch.distributed.pipelining.microbatch)
  * SplitPoint (class in torch.distributed.pipelining)
  * spsolve() (in module torch.sparse)
  * sqrt() (in module torch)
    * (torch.Tensor method)
  * sqrt_() (torch.Tensor method)
  * square() (in module torch)
    * (torch.Tensor method)
  * square_() (torch.Tensor method)
  * squeeze() (in module torch)
    * (torch.Tensor method)
  * squeeze_() (torch.Tensor method)
  * sspaddmm() (in module torch)
    * (torch.Tensor method)
  * stack (in module torch.distributions.constraints)
  * stack() (in module torch)
  * stack_module_state() (in module torch.func)
  * stack_trace (torch.fx.Node property)
  * StackDataset (class in torch.utils.data)
  * StackTransform (class in torch.distributions.transforms)
  * stage() (torch.distributed.checkpoint.FileSystemWriter method)
    * (torch.distributed.checkpoint.staging.AsyncStager method)
    * (torch.distributed.checkpoint.staging.BlockingAsyncStager method)
  * StandaloneModuleConfigEntry (class in torch.ao.quantization.fx.custom_config)
  * start() (in module torch.mps.profiler)
    * (torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer method)
  * start_processes() (in module torch.distributed.elastic.multiprocessing)
  * Stat (class in torch.monitor)
  * state_dict() (torch.distributed.checkpoint.stateful.Stateful method)
    * (torch.distributed.optim.PostLocalSGDOptimizer method)
    * (torch.distributed.optim.ZeroRedundancyOptimizer method)
    * (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.lr_scheduler.ChainedScheduler method)
    * (torch.optim.lr_scheduler.ConstantLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method)
    * (torch.optim.lr_scheduler.ExponentialLR method)
    * (torch.optim.lr_scheduler.LambdaLR method)
    * (torch.optim.lr_scheduler.LinearLR method)
    * (torch.optim.lr_scheduler.LRScheduler method)
    * (torch.optim.lr_scheduler.MultiplicativeLR method)
    * (torch.optim.lr_scheduler.MultiStepLR method)
    * (torch.optim.lr_scheduler.OneCycleLR method)
    * (torch.optim.lr_scheduler.PolynomialLR method)
    * (torch.optim.lr_scheduler.SequentialLR method)
    * (torch.optim.lr_scheduler.StepLR method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.optim.swa_utils.SWALR method)
  * state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * StateDictConfig (class in torch.distributed.fsdp)
  * StateDictOptions (class in torch.distributed.checkpoint.state_dict)
  * StateDictSettings (class in torch.distributed.fsdp)
  * Stateful (class in torch.distributed.checkpoint.stateful)
  * StatefulSymbolicContext (class in torch.fx.experimental.symbolic_shapes)
  * StatelessSymbolicContext (class in torch.fx.experimental.symbolic_shapes)
  * statically_known_true() (in module torch.fx.experimental.symbolic_shapes)
  * stats() (torch.utils.benchmark.CallgrindStats method)
  * std() (in module torch)
    * (torch.Tensor method)
  * std_mean() (in module torch)
  * stddev (torch.distributions.continuous_bernoulli.ContinuousBernoulli property)
    * (torch.distributions.distribution.Distribution property)
    * (torch.distributions.exponential.Exponential property)
    * (torch.distributions.gumbel.Gumbel property)
    * (torch.distributions.laplace.Laplace property)
    * (torch.distributions.normal.Normal property)
    * (torch.distributions.uniform.Uniform property)
  * step() (torch.distributed.optim.DistributedOptimizer method)
    * (torch.distributed.optim.PostLocalSGDOptimizer method)
    * (torch.distributed.optim.ZeroRedundancyOptimizer method)
    * (torch.distributed.pipelining.schedules.PipelineScheduleMulti method)
    * (torch.distributed.pipelining.schedules.PipelineScheduleSingle method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.lr_scheduler.ChainedScheduler method)
    * (torch.optim.lr_scheduler.ConstantLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingLR method)
    * (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method)
    * (torch.optim.lr_scheduler.CyclicLR method)
    * (torch.optim.lr_scheduler.ExponentialLR method)
    * (torch.optim.lr_scheduler.LambdaLR method)
    * (torch.optim.lr_scheduler.LinearLR method)
    * (torch.optim.lr_scheduler.LRScheduler method)
    * (torch.optim.lr_scheduler.MultiplicativeLR method)
    * (torch.optim.lr_scheduler.MultiStepLR method)
    * (torch.optim.lr_scheduler.OneCycleLR method)
    * (torch.optim.lr_scheduler.PolynomialLR method)
    * (torch.optim.lr_scheduler.ReduceLROnPlateau method)
    * (torch.optim.lr_scheduler.SequentialLR method)
    * (torch.optim.lr_scheduler.StepLR method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.SWALR method)
    * (torch.profiler.profile method)
  * StepLR (class in torch.optim.lr_scheduler)
  * stft() (in module torch)
    * (torch.Tensor method)
  * StickBreakingTransform (class in torch.distributions.transforms)
  * stop() (in module torch.mps.profiler)
    * (torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer method)
  * storage() (torch.Tensor method)
  * storage_meta() (torch.distributed.checkpoint.StorageWriter method)
  * storage_offset() (torch.Tensor method)
  * storage_type() (torch.Tensor method)
  * StorageReader (class in torch.distributed.checkpoint)
  * StorageWriter (class in torch.distributed.checkpoint)
  * Store (class in torch.distributed)
  * strategy (in module torch.backends.opt_einsum)
  * Stream (class in torch)
    * (class in torch.cpu)
    * (class in torch.cuda)
    * (class in torch.mtia)
    * (class in torch.xpu)
  * stream() (in module torch.cpu)
    * (in module torch.cuda)
    * (in module torch.mtia)
    * (in module torch.xpu)
  * StreamContext (class in torch.cpu)
    * (class in torch.cuda)
    * (class in torch.mtia)
    * (class in torch.xpu)
  * strict_fusion (class in torch.jit)
  * StrictMinMaxConstraint (class in torch.fx.experimental.symbolic_shapes)
  * stride() (torch.Tensor method)
  * StringTable (class in torch.autograd.profiler_util)
  * StudentT (class in torch.distributions.studentT)
  * sub() (in module torch)
    * (torch.Tensor method)
  * sub_() (torch.Tensor method)
  * SubclassSymbolicContext (class in torch.fx.experimental.symbolic_shapes)
  * SubprocessContext (class in torch.distributed.elastic.multiprocessing.api)
  * SubprocessHandler (class in torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler)
  * Subset (class in torch.utils.data)
  * SubsetRandomSampler (class in torch.utils.data)
  * substitute_in_graph() (in module torch.compiler)
  * subtract() (in module torch)
    * (torch.Tensor method)
  * subtract_() (torch.Tensor method)
  * sum() (in module torch)
    * (in module torch.sparse)
    * (torch.Tensor method)
  * sum_to_size() (torch.Tensor method)
  * SummaryWriter (class in torch.utils.tensorboard.writer)
  * summon_full_params() (torch.distributed.fsdp.FullyShardedDataParallel static method)
  * support (torch.distributions.bernoulli.Bernoulli attribute)
    * (torch.distributions.beta.Beta attribute)
    * (torch.distributions.binomial.Binomial property)
    * (torch.distributions.categorical.Categorical property)
    * (torch.distributions.cauchy.Cauchy attribute)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli attribute)
    * (torch.distributions.dirichlet.Dirichlet attribute)
    * (torch.distributions.distribution.Distribution property)
    * (torch.distributions.exponential.Exponential attribute)
    * (torch.distributions.fishersnedecor.FisherSnedecor attribute)
    * (torch.distributions.gamma.Gamma attribute)
    * (torch.distributions.geometric.Geometric attribute)
    * (torch.distributions.gumbel.Gumbel attribute)
    * (torch.distributions.half_cauchy.HalfCauchy attribute)
    * (torch.distributions.half_normal.HalfNormal attribute)
    * (torch.distributions.independent.Independent property)
    * (torch.distributions.inverse_gamma.InverseGamma attribute)
    * (torch.distributions.kumaraswamy.Kumaraswamy attribute)
    * (torch.distributions.laplace.Laplace attribute)
    * (torch.distributions.lkj_cholesky.LKJCholesky attribute)
    * (torch.distributions.log_normal.LogNormal attribute)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal attribute)
    * (torch.distributions.mixture_same_family.MixtureSameFamily property)
    * (torch.distributions.multinomial.Multinomial property)
    * (torch.distributions.multivariate_normal.MultivariateNormal attribute)
    * (torch.distributions.negative_binomial.NegativeBinomial attribute)
    * (torch.distributions.normal.Normal attribute)
    * (torch.distributions.one_hot_categorical.OneHotCategorical attribute)
    * (torch.distributions.pareto.Pareto property)
    * (torch.distributions.poisson.Poisson attribute)
    * (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli attribute)
    * (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)
    * (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)
    * (torch.distributions.studentT.StudentT attribute)
    * (torch.distributions.transformed_distribution.TransformedDistribution property)
    * (torch.distributions.uniform.Uniform property)
    * (torch.distributions.von_mises.VonMises attribute)
    * (torch.distributions.weibull.Weibull attribute)
    * (torch.distributions.wishart.Wishart attribute)
  * suppress_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method)
  * svd() (in module torch)
    * (in module torch.linalg)
    * (torch.Tensor method)
  * svd_lowrank() (in module torch)
  * svdvals() (in module torch.linalg)
  * SWALR (class in torch.optim.swa_utils)
  * swap_module (class in torch.ao.quantization)
  * swap_tensors() (in module torch.utils)
  * swapaxes() (in module torch)
    * (torch.Tensor method)
  * swapdims() (in module torch)
    * (torch.Tensor method)
  * SyclExtension() (in module torch.utils.cpp_extension)
  * sym_eq() (in module torch.fx.experimental.symbolic_shapes)
  * sym_float() (in module torch)
  * sym_fresh_size() (in module torch)
  * sym_int() (in module torch)
  * sym_ite() (in module torch)
  * sym_max() (in module torch)
  * sym_min() (in module torch)
  * sym_not() (in module torch)
  * sym_sum() (in module torch)
  * symbolic_trace() (in module torch.fx)
  * SymbolicContext (class in torch.fx.experimental.symbolic_shapes)
  * SymBool (class in torch)
  * SymBoolArgument (class in torch.export.graph_signature)
  * SymFloat (class in torch)
  * SymFloatArgument (class in torch.export.graph_signature)
  * SymInt (class in torch)
  * SymIntArgument (class in torch.export.graph_signature)
  * SyncBatchNorm (class in torch.nn)
  * synchronize() (in module torch.accelerator)
    * (in module torch.cpu)
    * (in module torch.cuda)
    * (in module torch.mps)
    * (in module torch.mtia)
    * (in module torch.xpu)
    * (torch.cuda.Event method)
    * (torch.cuda.ExternalStream method)
    * (torch.cuda.Stream method)
    * (torch.distributed.Work method)
    * (torch.Event method)
    * (torch.mps.event.Event method)
    * (torch.mtia.Event method)
    * (torch.mtia.Stream method)
    * (torch.Stream method)
    * (torch.xpu.Event method)
    * (torch.xpu.Stream method)
  * synchronize_staging() (torch.distributed.checkpoint.staging.AsyncStager method)
    * (torch.distributed.checkpoint.staging.BlockingAsyncStager method)

  
---|---  
## T
  * T (torch.Tensor attribute)
  * t() (in module torch)
    * (torch.Tensor method)
  * t_() (torch.Tensor method)
  * Tag (class in torch)
  * take() (in module torch)
    * (torch.Tensor method)
  * take_along_dim() (in module torch)
    * (torch.Tensor method)
  * tan() (in module torch)
    * (torch.Tensor method)
  * tan_() (torch.Tensor method)
  * tangent (torch.autograd.forward_ad.UnpackedDualTensor attribute)
  * Tanh (class in torch.nn)
  * tanh() (in module torch)
    * (in module torch.nn.functional)
    * (torch.Tensor method)
  * tanh_() (torch.Tensor method)
  * Tanhshrink (class in torch.nn)
  * tanhshrink() (in module torch.nn.functional)
  * TanhTransform (class in torch.distributions.transforms)
  * TCPStore (class in torch.distributed)
  * temperature (torch.distributions.relaxed_bernoulli.RelaxedBernoulli property)
    * (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical property)
  * temperature() (in module torch.cuda)
  * Tensor (class in torch)
  * tensor() (in module torch)
  * tensor_split() (in module torch)
    * (torch.Tensor method)
  * tensor_storage_size() (torch.distributed.checkpoint.planner.WriteItem method)
  * tensorboard_trace_handler() (in module torch.profiler)
  * TensorboardEventHandler (class in torch.monitor)
  * TensorChunkSpec (class in torch.distributed.pipelining.microbatch)
  * TensorDataset (class in torch.utils.data)
  * tensordot() (in module torch)
  * tensorinv() (in module torch.linalg)
  * TensorPipeRpcBackendOptions (class in torch.distributed.rpc)
  * tensorsolve() (in module torch.linalg)
  * then() (torch.futures.Future method)
  * threshold (class in torch.ao.nn.quantized.functional)
  * Threshold (class in torch.nn)
  * threshold() (in module torch.nn.functional)
  * threshold_() (in module torch.nn.functional)
  * tile() (in module torch)
    * (torch.Tensor method)
  * timeit() (torch.utils.benchmark.Timer method)
  * timeout (torch.distributed.Store property)
  * Timer (class in torch.utils.benchmark)
  * TimerClient (class in torch.distributed.elastic.timer)
  * TimerRequest (class in torch.distributed.elastic.timer)
  * TimerServer (class in torch.distributed.elastic.timer)
  * timestamp (torch.monitor.Event property)
  * to() (torch.jit.ScriptModule method)
    * (torch.nn.attention.flex_attention.BlockMask method)
    * (torch.nn.Module method)
    * (torch.nn.utils.rnn.PackedSequence method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * to_bool() (torch.fx.Tracer method)
  * to_dense() (torch.nn.attention.flex_attention.BlockMask method)
    * (torch.Tensor method)
  * to_dict() (torch.ao.quantization.backend_config.BackendConfig method)
    * (torch.ao.quantization.backend_config.BackendPatternConfig method)
    * (torch.ao.quantization.backend_config.DTypeConfig method)
    * (torch.ao.quantization.fx.custom_config.ConvertCustomConfig method)
    * (torch.ao.quantization.fx.custom_config.FuseCustomConfig method)
    * (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method)
    * (torch.ao.quantization.qconfig_mapping.QConfigMapping method)
  * to_dlpack() (in module torch.utils.dlpack)
  * to_empty() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * to_folder() (torch.fx.GraphModule method)
  * to_here() (torch.distributed.rpc.PyRRef method)
  * to_local() (torch.distributed.tensor.DTensor method)
  * to_mkldnn() (torch.Tensor method)
  * to_padded_tensor() (in module torch.nested)
  * to_sparse() (torch.Tensor method)
  * to_sparse_bsc() (torch.Tensor method)
  * to_sparse_bsr() (torch.Tensor method)
  * to_sparse_coo() (torch.Tensor method)
  * to_sparse_csc() (torch.Tensor method)
  * to_sparse_csr() (torch.Tensor method)
  * to_string() (torch.nn.attention.flex_attention.BlockMask method)
  * toggle_collection_dynamic() (torch.profiler._KinetoProfile method)
  * tolist() (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * topk() (in module torch)
    * (torch.Tensor method)
  * torch 
    * module
  * torch.__config__ 
    * module
  * torch.__future__ 
    * module
  * torch._logging 
    * module
  * torch.accelerator 
    * module
  * torch.amp 
    * module
  * torch.amp.autocast_mode 
    * module
  * torch.amp.grad_scaler 
    * module
  * torch.ao 
    * module
  * torch.ao.nn 
    * module
  * torch.ao.nn.intrinsic 
    * module
  * torch.ao.nn.intrinsic.modules 
    * module
  * torch.ao.nn.intrinsic.modules.fused 
    * module
  * torch.ao.nn.intrinsic.qat 
    * module
  * torch.ao.nn.intrinsic.qat.modules 
    * module
  * torch.ao.nn.intrinsic.qat.modules.conv_fused 
    * module
  * torch.ao.nn.intrinsic.qat.modules.linear_fused 
    * module
  * torch.ao.nn.intrinsic.qat.modules.linear_relu 
    * module
  * torch.ao.nn.intrinsic.quantized 
    * module
  * torch.ao.nn.intrinsic.quantized.dynamic 
    * module
  * torch.ao.nn.intrinsic.quantized.dynamic.modules 
    * module
  * torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu 
    * module
  * torch.ao.nn.intrinsic.quantized.modules 
    * module
  * torch.ao.nn.intrinsic.quantized.modules.bn_relu 
    * module
  * torch.ao.nn.intrinsic.quantized.modules.conv_add 
    * module
  * torch.ao.nn.intrinsic.quantized.modules.conv_relu 
    * module
  * torch.ao.nn.intrinsic.quantized.modules.linear_relu 
    * module
  * torch.ao.nn.qat 
    * module
  * torch.ao.nn.qat.dynamic 
    * module
  * torch.ao.nn.qat.dynamic.modules 
    * module
  * torch.ao.nn.qat.dynamic.modules.linear 
    * module
  * torch.ao.nn.qat.modules 
    * module
  * torch.ao.nn.qat.modules.conv 
    * module
  * torch.ao.nn.qat.modules.embedding_ops 
    * module
  * torch.ao.nn.qat.modules.linear 
    * module
  * torch.ao.nn.quantizable 
    * module
  * torch.ao.nn.quantizable.modules 
    * module
  * torch.ao.nn.quantizable.modules.activation 
    * module
  * torch.ao.nn.quantizable.modules.rnn 
    * module
  * torch.ao.nn.quantized 
    * module
  * torch.ao.nn.quantized.dynamic 
    * module
  * torch.ao.nn.quantized.dynamic.modules 
    * module
  * torch.ao.nn.quantized.dynamic.modules.conv 
    * module
  * torch.ao.nn.quantized.dynamic.modules.linear 
    * module
  * torch.ao.nn.quantized.dynamic.modules.rnn 
    * module
  * torch.ao.nn.quantized.functional 
    * module
  * torch.ao.nn.quantized.modules 
    * module
  * torch.ao.nn.quantized.modules.activation 
    * module
  * torch.ao.nn.quantized.modules.batchnorm 
    * module
  * torch.ao.nn.quantized.modules.conv 
    * module
  * torch.ao.nn.quantized.modules.dropout 
    * module
  * torch.ao.nn.quantized.modules.embedding_ops 
    * module
  * torch.ao.nn.quantized.modules.functional_modules 
    * module
  * torch.ao.nn.quantized.modules.linear 
    * module
  * torch.ao.nn.quantized.modules.normalization 
    * module
  * torch.ao.nn.quantized.modules.rnn 
    * module
  * torch.ao.nn.quantized.modules.utils 
    * module
  * torch.ao.nn.quantized.reference 
    * module
  * torch.ao.nn.quantized.reference.modules 
    * module
  * torch.ao.nn.quantized.reference.modules.conv 
    * module
  * torch.ao.nn.quantized.reference.modules.linear 
    * module
  * torch.ao.nn.quantized.reference.modules.rnn 
    * module
  * torch.ao.nn.quantized.reference.modules.sparse 
    * module
  * torch.ao.nn.quantized.reference.modules.utils 
    * module
  * torch.ao.nn.sparse 
    * module
  * torch.ao.nn.sparse.quantized 
    * module
  * torch.ao.nn.sparse.quantized.dynamic 
    * module
  * torch.ao.nn.sparse.quantized.dynamic.linear 
    * module
  * torch.ao.nn.sparse.quantized.linear 
    * module
  * torch.ao.nn.sparse.quantized.utils 
    * module
  * torch.ao.ns 
    * module
  * torch.ao.ns._numeric_suite 
    * module
  * torch.ao.ns._numeric_suite_fx 
    * module
  * torch.ao.ns.fx 
    * module
  * torch.ao.ns.fx.graph_matcher 
    * module
  * torch.ao.ns.fx.graph_passes 
    * module
  * torch.ao.ns.fx.mappings 
    * module
  * torch.ao.ns.fx.n_shadows_utils 
    * module
  * torch.ao.ns.fx.ns_types 
    * module
  * torch.ao.ns.fx.pattern_utils 
    * module
  * torch.ao.ns.fx.qconfig_multi_mapping 
    * module
  * torch.ao.ns.fx.utils 
    * module
  * torch.ao.ns.fx.weight_utils 
    * module
  * torch.ao.pruning 
    * module
  * torch.ao.pruning.scheduler 
    * module
  * torch.ao.pruning.scheduler.base_scheduler 
    * module
  * torch.ao.pruning.scheduler.cubic_scheduler 
    * module
  * torch.ao.pruning.scheduler.lambda_scheduler 
    * module
  * torch.ao.pruning.sparsifier 
    * module
  * torch.ao.pruning.sparsifier.base_sparsifier 
    * module
  * torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier 
    * module
  * torch.ao.pruning.sparsifier.utils 
    * module
  * torch.ao.pruning.sparsifier.weight_norm_sparsifier 
    * module
  * torch.ao.quantization 
    * module
  * torch.ao.quantization.backend_config 
    * module
  * torch.ao.quantization.backend_config.backend_config 
    * module
  * torch.ao.quantization.backend_config.executorch 
    * module
  * torch.ao.quantization.backend_config.fbgemm 
    * module
  * torch.ao.quantization.backend_config.native 
    * module
  * torch.ao.quantization.backend_config.observation_type 
    * module
  * torch.ao.quantization.backend_config.onednn 
    * module
  * torch.ao.quantization.backend_config.qnnpack 
    * module
  * torch.ao.quantization.backend_config.tensorrt 
    * module
  * torch.ao.quantization.backend_config.utils 
    * module
  * torch.ao.quantization.backend_config.x86 
    * module
  * torch.ao.quantization.fake_quantize 
    * module
  * torch.ao.quantization.fuse_modules 
    * module
  * torch.ao.quantization.fuser_method_mappings 
    * module
  * torch.ao.quantization.fx 
    * module
  * torch.ao.quantization.fx.convert 
    * module
  * torch.ao.quantization.fx.custom_config 
    * module
  * torch.ao.quantization.fx.fuse 
    * module
  * torch.ao.quantization.fx.fuse_handler 
    * module
  * torch.ao.quantization.fx.graph_module 
    * module
  * torch.ao.quantization.fx.lower_to_fbgemm 
    * module
  * torch.ao.quantization.fx.lower_to_qnnpack 
    * module
  * torch.ao.quantization.fx.lstm_utils 
    * module
  * torch.ao.quantization.fx.match_utils 
    * module
  * torch.ao.quantization.fx.pattern_utils 
    * module
  * torch.ao.quantization.fx.prepare 
    * module
  * torch.ao.quantization.fx.qconfig_mapping_utils 
    * module
  * torch.ao.quantization.fx.quantize_handler 
    * module
  * torch.ao.quantization.fx.tracer 
    * module
  * torch.ao.quantization.fx.utils 
    * module
  * torch.ao.quantization.observer 
    * module
  * torch.ao.quantization.pt2e 
    * module
  * torch.ao.quantization.pt2e.duplicate_dq_pass 
    * module
  * torch.ao.quantization.pt2e.export_utils 
    * module
  * torch.ao.quantization.pt2e.graph_utils 
    * module
  * torch.ao.quantization.pt2e.port_metadata_pass 
    * module
  * torch.ao.quantization.pt2e.prepare 
    * module
  * torch.ao.quantization.pt2e.qat_utils 
    * module
  * torch.ao.quantization.pt2e.representation 
    * module
  * torch.ao.quantization.pt2e.representation.rewrite 
    * module
  * torch.ao.quantization.pt2e.utils 
    * module
  * torch.ao.quantization.qconfig 
    * module
  * torch.ao.quantization.qconfig_mapping 
    * module
  * torch.ao.quantization.quant_type 
    * module
  * torch.ao.quantization.quantization_mappings 
    * module
  * torch.ao.quantization.quantize_fx 
    * module
  * torch.ao.quantization.quantize_jit 
    * module
  * torch.ao.quantization.quantize_pt2e 
    * module
  * torch.ao.quantization.quantizer 
    * module
  * torch.ao.quantization.quantizer.composable_quantizer 
    * module
  * torch.ao.quantization.quantizer.embedding_quantizer 
    * module
  * torch.ao.quantization.quantizer.quantizer 
    * module
  * torch.ao.quantization.quantizer.utils 
    * module
  * torch.ao.quantization.quantizer.x86_inductor_quantizer 
    * module
  * torch.ao.quantization.quantizer.xnnpack_quantizer 
    * module
  * torch.ao.quantization.quantizer.xnnpack_quantizer_utils 
    * module
  * torch.ao.quantization.quantizer.xpu_inductor_quantizer 
    * module
  * torch.ao.quantization.stubs 
    * module
  * torch.ao.quantization.utils 
    * module
  * torch.autograd 
    * module
  * torch.autograd.anomaly_mode 
    * module
  * torch.autograd.forward_ad 
    * module
  * torch.autograd.function 
    * module
  * torch.autograd.functional 
    * module
  * torch.autograd.grad_mode 
    * module
  * torch.autograd.gradcheck 
    * module
  * torch.autograd.graph 
    * module
  * torch.autograd.profiler 
    * module
  * torch.autograd.profiler_legacy 
    * module
  * torch.autograd.profiler_util 
    * module
  * torch.autograd.variable 
    * module
  * torch.backends 
    * module
  * torch.backends.cpu 
    * module
  * torch.backends.cuda 
    * module
  * torch.backends.cudnn 
    * module
  * torch.backends.cudnn.rnn 
    * module
  * torch.backends.cusparselt 
    * module
  * torch.backends.kleidiai 
    * module
  * torch.backends.mha 
    * module
  * torch.backends.mkl 
    * module
  * torch.backends.mkldnn 
    * module
  * torch.backends.mps 
    * module
  * torch.backends.nnpack 
    * module
  * torch.backends.openmp 
    * module
  * torch.backends.opt_einsum 
    * module
  * torch.backends.quantized 
    * module
  * torch.backends.xeon 
    * module
  * torch.backends.xeon.run_cpu 
    * module
  * torch.backends.xnnpack 
    * module
  * torch.compiler 
    * module
  * torch.compiler.config 
    * module
  * torch.contrib 
    * module
  * torch.cpu 
    * module
  * torch.cpu.amp 
    * module
  * torch.cpu.amp.autocast_mode 
    * module
  * torch.cpu.amp.grad_scaler 
    * module
  * torch.cuda 
    * module
  * torch.cuda._sanitizer 
    * module
  * torch.cuda.amp 
    * module
  * torch.cuda.amp.autocast_mode 
    * module
  * torch.cuda.amp.common 
    * module
  * torch.cuda.amp.grad_scaler 
    * module
  * torch.cuda.comm 
    * module
  * torch.cuda.error 
    * module
  * torch.cuda.gds 
    * module
  * torch.cuda.graphs 
    * module
  * torch.cuda.jiterator 
    * module
  * torch.cuda.memory 
    * module
  * torch.cuda.nccl 
    * module
  * torch.cuda.nvtx 
    * module
  * torch.cuda.profiler 
    * module
  * torch.cuda.random 
    * module
  * torch.cuda.sparse 
    * module
  * torch.cuda.streams 
    * module
  * torch.cuda.tunable 
    * module
  * torch.distributed 
    * module
  * torch.distributed.algorithms 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.default_hooks 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook 
    * module
  * torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks 
    * module
  * torch.distributed.algorithms.join 
    * module
  * torch.distributed.algorithms.model_averaging 
    * module
  * torch.distributed.algorithms.model_averaging.averagers 
    * module
  * torch.distributed.algorithms.model_averaging.hierarchical_model_averager 
    * module
  * torch.distributed.algorithms.model_averaging.utils 
    * module
  * torch.distributed.argparse_util 
    * module
  * torch.distributed.autograd 
    * module
  * torch.distributed.c10d_logger 
    * module
  * torch.distributed.checkpoint 
    * module
  * torch.distributed.checkpoint.api 
    * module
  * torch.distributed.checkpoint.default_planner 
    * module
  * torch.distributed.checkpoint.filesystem 
    * module
  * torch.distributed.checkpoint.format_utils 
    * module
  * torch.distributed.checkpoint.logger 
    * module
  * torch.distributed.checkpoint.logging_handlers 
    * module
  * torch.distributed.checkpoint.metadata 
    * module
  * torch.distributed.checkpoint.optimizer 
    * module
  * torch.distributed.checkpoint.planner 
    * module
  * torch.distributed.checkpoint.planner_helpers 
    * module
  * torch.distributed.checkpoint.resharding 
    * module
  * torch.distributed.checkpoint.staging 
    * module
  * torch.distributed.checkpoint.state_dict 
    * module
  * torch.distributed.checkpoint.state_dict_loader 
    * module
  * torch.distributed.checkpoint.state_dict_saver 
    * module
  * torch.distributed.checkpoint.stateful 
    * module
  * torch.distributed.checkpoint.storage 
    * module
  * torch.distributed.checkpoint.utils 
    * module
  * torch.distributed.collective_utils 
    * module
  * torch.distributed.constants 
    * module
  * torch.distributed.device_mesh 
    * module
  * torch.distributed.distributed_c10d 
    * module
  * torch.distributed.elastic 
    * module
  * torch.distributed.elastic.agent 
    * module
  * torch.distributed.elastic.agent.server 
    * module
  * torch.distributed.elastic.agent.server.api 
    * module
  * torch.distributed.elastic.agent.server.health_check_server 
    * module
  * torch.distributed.elastic.agent.server.local_elastic_agent 
    * module
  * torch.distributed.elastic.control_plane 
    * module
  * torch.distributed.elastic.events 
    * module
  * torch.distributed.elastic.events.api 
    * module
  * torch.distributed.elastic.events.handlers 
    * module
  * torch.distributed.elastic.metrics 
    * module
  * torch.distributed.elastic.metrics.api 
    * module
  * torch.distributed.elastic.multiprocessing 
    * module
  * torch.distributed.elastic.multiprocessing.api 
    * module
  * torch.distributed.elastic.multiprocessing.errors 
    * module
  * torch.distributed.elastic.multiprocessing.errors.error_handler 
    * module
  * torch.distributed.elastic.multiprocessing.errors.handlers 
    * module
  * torch.distributed.elastic.multiprocessing.redirects 
    * module
  * torch.distributed.elastic.multiprocessing.subprocess_handler 
    * module
  * torch.distributed.elastic.multiprocessing.subprocess_handler.handlers 
    * module
  * torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler 
    * module
  * torch.distributed.elastic.multiprocessing.tail_log 
    * module
  * torch.distributed.elastic.rendezvous 
    * module
  * torch.distributed.elastic.rendezvous.api 
    * module
  * torch.distributed.elastic.rendezvous.c10d_rendezvous_backend 
    * module
  * torch.distributed.elastic.rendezvous.dynamic_rendezvous 
    * module
  * torch.distributed.elastic.rendezvous.etcd_rendezvous 
    * module
  * torch.distributed.elastic.rendezvous.etcd_rendezvous_backend 
    * module
  * torch.distributed.elastic.rendezvous.etcd_server 
    * module
  * torch.distributed.elastic.rendezvous.etcd_store 
    * module
  * torch.distributed.elastic.rendezvous.registry 
    * module
  * torch.distributed.elastic.rendezvous.static_tcp_rendezvous 
    * module
  * torch.distributed.elastic.rendezvous.utils 
    * module
  * torch.distributed.elastic.timer 
    * module
  * torch.distributed.elastic.timer.api 
    * module
  * torch.distributed.elastic.timer.debug_info_logging 
    * module
  * torch.distributed.elastic.timer.file_based_local_timer 
    * module
  * torch.distributed.elastic.timer.local_timer 
    * module
  * torch.distributed.elastic.utils 
    * module
  * torch.distributed.elastic.utils.api 
    * module
  * torch.distributed.elastic.utils.data 
    * module
  * torch.distributed.elastic.utils.data.cycling_iterator 
    * module
  * torch.distributed.elastic.utils.data.elastic_distributed_sampler 
    * module
  * torch.distributed.elastic.utils.distributed 
    * module
  * torch.distributed.elastic.utils.log_level 
    * module
  * torch.distributed.elastic.utils.logging 
    * module
  * torch.distributed.elastic.utils.store 
    * module
  * torch.distributed.fsdp 
    * module
  * torch.distributed.fsdp.api 
    * module
  * torch.distributed.fsdp.fully_sharded_data_parallel 
    * module
  * torch.distributed.fsdp.sharded_grad_scaler 
    * module
  * torch.distributed.fsdp.wrap 
    * module
  * torch.distributed.launch 
    * module
  * torch.distributed.launcher 
    * module
  * torch.distributed.launcher.api 
    * module
  * torch.distributed.logging_handlers 
    * module
  * torch.distributed.nn 
    * module
  * torch.distributed.nn.api 
    * module
  * torch.distributed.nn.api.remote_module 
    * module
  * torch.distributed.nn.functional 
    * module
  * torch.distributed.nn.jit 
    * module
  * torch.distributed.nn.jit.instantiator 
    * module
  * torch.distributed.nn.jit.templates 
    * module
  * torch.distributed.nn.jit.templates.remote_module_template 
    * module
  * torch.distributed.optim 
    * module
  * torch.distributed.optim.apply_optimizer_in_backward 
    * module
  * torch.distributed.optim.functional_adadelta 
    * module
  * torch.distributed.optim.functional_adagrad 
    * module
  * torch.distributed.optim.functional_adam 
    * module
  * torch.distributed.optim.functional_adamax 
    * module
  * torch.distributed.optim.functional_adamw 
    * module
  * torch.distributed.optim.functional_rmsprop 
    * module
  * torch.distributed.optim.functional_rprop 
    * module
  * torch.distributed.optim.functional_sgd 
    * module
  * torch.distributed.optim.named_optimizer 
    * module
  * torch.distributed.optim.optimizer 
    * module
  * torch.distributed.optim.post_localSGD_optimizer 
    * module
  * torch.distributed.optim.utils 
    * module
  * torch.distributed.optim.zero_redundancy_optimizer 
    * module
  * torch.distributed.pipelining 
    * module
  * torch.distributed.pipelining.microbatch 
    * module
  * torch.distributed.pipelining.schedules 
    * module
  * torch.distributed.pipelining.stage 
    * module
  * torch.distributed.remote_device 
    * module
  * torch.distributed.rendezvous 
    * module
  * torch.distributed.rpc 
    * module
  * torch.distributed.rpc.api 
    * module
  * torch.distributed.rpc.backend_registry 
    * module
  * torch.distributed.rpc.constants 
    * module
  * torch.distributed.rpc.functions 
    * module
  * torch.distributed.rpc.internal 
    * module
  * torch.distributed.rpc.options 
    * module
  * torch.distributed.rpc.rref_proxy 
    * module
  * torch.distributed.rpc.server_process_global_profiler 
    * module
  * torch.distributed.run 
    * module
  * torch.distributed.tensor 
    * module
  * torch.distributed.tensor.debug 
    * module
  * torch.distributed.tensor.device_mesh 
    * module
  * torch.distributed.tensor.experimental 
    * module
  * torch.distributed.tensor.parallel 
    * module
  * torch.distributed.tensor.parallel.api 
    * module
  * torch.distributed.tensor.parallel.ddp 
    * module
  * torch.distributed.tensor.parallel.fsdp 
    * module
  * torch.distributed.tensor.parallel.input_reshard 
    * module
  * torch.distributed.tensor.parallel.loss 
    * module
  * torch.distributed.tensor.parallel.style 
    * module
  * torch.distributed.tensor.placement_types 
    * module
  * torch.distributed.utils 
    * module
  * torch.distributions 
    * module
  * torch.distributions.bernoulli 
    * module
  * torch.distributions.beta 
    * module
  * torch.distributions.binomial 
    * module
  * torch.distributions.categorical 
    * module
  * torch.distributions.cauchy 
    * module
  * torch.distributions.chi2 
    * module
  * torch.distributions.constraint_registry 
    * module
  * torch.distributions.constraints 
    * module
  * torch.distributions.continuous_bernoulli 
    * module
  * torch.distributions.dirichlet 
    * module
  * torch.distributions.distribution 
    * module
  * torch.distributions.exp_family 
    * module
  * torch.distributions.exponential 
    * module
  * torch.distributions.fishersnedecor 
    * module
  * torch.distributions.gamma 
    * module
  * torch.distributions.geometric 
    * module
  * torch.distributions.gumbel 
    * module
  * torch.distributions.half_cauchy 
    * module
  * torch.distributions.half_normal 
    * module
  * torch.distributions.independent 
    * module
  * torch.distributions.inverse_gamma 
    * module
  * torch.distributions.kl 
    * module
  * torch.distributions.kumaraswamy 
    * module
  * torch.distributions.laplace 
    * module
  * torch.distributions.lkj_cholesky 
    * module
  * torch.distributions.log_normal 
    * module
  * torch.distributions.logistic_normal 
    * module
  * torch.distributions.lowrank_multivariate_normal 
    * module
  * torch.distributions.mixture_same_family 
    * module
  * torch.distributions.multinomial 
    * module
  * torch.distributions.multivariate_normal 
    * module
  * torch.distributions.negative_binomial 
    * module
  * torch.distributions.normal 
    * module
  * torch.distributions.one_hot_categorical 
    * module
  * torch.distributions.pareto 
    * module
  * torch.distributions.poisson 
    * module
  * torch.distributions.relaxed_bernoulli 
    * module
  * torch.distributions.relaxed_categorical 
    * module
  * torch.distributions.studentT 
    * module
  * torch.distributions.transformed_distribution 
    * module
  * torch.distributions.transforms 
    * module
  * torch.distributions.uniform 
    * module
  * torch.distributions.utils 
    * module
  * torch.distributions.von_mises 
    * module
  * torch.distributions.weibull 
    * module
  * torch.distributions.wishart 
    * module
  * torch.export 
    * module
  * torch.export.custom_obj 
    * module
  * torch.export.custom_ops 
    * module
  * torch.export.decomp_utils 
    * module
  * torch.export.dynamic_shapes 
    * module

| 
  * torch.export.experimental 
    * module
  * torch.export.exported_program 
    * module
  * torch.export.graph_signature 
    * module
  * torch.export.passes 
    * module
  * torch.export.unflatten 
    * module
  * torch.fft 
    * module
  * torch.finfo (class in torch)
  * torch.func 
    * module
  * torch.functional 
    * module
  * torch.futures 
    * module
  * torch.fx 
    * module
  * torch.fx.annotate 
    * module
  * torch.fx.config 
    * module
  * torch.fx.experimental 
    * module
  * torch.fx.experimental.accelerator_partitioner 
    * module
  * torch.fx.experimental.const_fold 
    * module
  * torch.fx.experimental.debug 
    * module
  * torch.fx.experimental.graph_gradual_typechecker 
    * module
  * torch.fx.experimental.merge_matmul 
    * module
  * torch.fx.experimental.meta_tracer 
    * module
  * torch.fx.experimental.migrate_gradual_types 
    * module
  * torch.fx.experimental.migrate_gradual_types.constraint 
    * module
  * torch.fx.experimental.migrate_gradual_types.constraint_generator 
    * module
  * torch.fx.experimental.migrate_gradual_types.constraint_transformation 
    * module
  * torch.fx.experimental.migrate_gradual_types.operation 
    * module
  * torch.fx.experimental.migrate_gradual_types.transform_to_z3 
    * module
  * torch.fx.experimental.migrate_gradual_types.util 
    * module
  * torch.fx.experimental.migrate_gradual_types.z3_types 
    * module
  * torch.fx.experimental.normalize 
    * module
  * torch.fx.experimental.optimization 
    * module
  * torch.fx.experimental.partitioner_utils 
    * module
  * torch.fx.experimental.proxy_tensor 
    * module
  * torch.fx.experimental.recording 
    * module
  * torch.fx.experimental.refinement_types 
    * module
  * torch.fx.experimental.rewriter 
    * module
  * torch.fx.experimental.schema_type_annotation 
    * module
  * torch.fx.experimental.sym_node 
    * module
  * torch.fx.experimental.symbolic_shapes 
    * module
  * torch.fx.experimental.unification 
    * module
  * torch.fx.experimental.unification.core 
    * module
  * torch.fx.experimental.unification.dispatch 
    * module
  * torch.fx.experimental.unification.match 
    * module
  * torch.fx.experimental.unification.more 
    * module
  * torch.fx.experimental.unification.multipledispatch 
    * module
  * torch.fx.experimental.unification.multipledispatch.conflict 
    * module
  * torch.fx.experimental.unification.multipledispatch.core 
    * module
  * torch.fx.experimental.unification.multipledispatch.dispatcher 
    * module
  * torch.fx.experimental.unification.multipledispatch.utils 
    * module
  * torch.fx.experimental.unification.multipledispatch.variadic 
    * module
  * torch.fx.experimental.unification.unification_tools 
    * module
  * torch.fx.experimental.unification.utils 
    * module
  * torch.fx.experimental.unification.variable 
    * module
  * torch.fx.experimental.unify_refinements 
    * module
  * torch.fx.experimental.validator 
    * module
  * torch.fx.graph 
    * module
  * torch.fx.graph_module 
    * module
  * torch.fx.immutable_collections 
    * module
  * torch.fx.interpreter 
    * module
  * torch.fx.node 
    * module
  * torch.fx.operator_schemas 
    * module
  * torch.fx.passes 
    * module
  * torch.fx.passes.annotate_getitem_nodes 
    * module
  * torch.fx.passes.backends 
    * module
  * torch.fx.passes.backends.cudagraphs 
    * module
  * torch.fx.passes.dialect 
    * module
  * torch.fx.passes.dialect.common 
    * module
  * torch.fx.passes.dialect.common.cse_pass 
    * module
  * torch.fx.passes.fake_tensor_prop 
    * module
  * torch.fx.passes.graph_drawer 
    * module
  * torch.fx.passes.graph_manipulation 
    * module
  * torch.fx.passes.graph_transform_observer 
    * module
  * torch.fx.passes.infra 
    * module
  * torch.fx.passes.infra.partitioner 
    * module
  * torch.fx.passes.infra.pass_base 
    * module
  * torch.fx.passes.infra.pass_manager 
    * module
  * torch.fx.passes.net_min_base 
    * module
  * torch.fx.passes.operator_support 
    * module
  * torch.fx.passes.param_fetch 
    * module
  * torch.fx.passes.pass_manager 
    * module
  * torch.fx.passes.reinplace 
    * module
  * torch.fx.passes.runtime_assert 
    * module
  * torch.fx.passes.shape_prop 
    * module
  * torch.fx.passes.split_module 
    * module
  * torch.fx.passes.split_utils 
    * module
  * torch.fx.passes.splitter_base 
    * module
  * torch.fx.passes.tests 
    * module
  * torch.fx.passes.tests.test_pass_manager 
    * module
  * torch.fx.passes.tools_common 
    * module
  * torch.fx.passes.utils 
    * module
  * torch.fx.passes.utils.common 
    * module
  * torch.fx.passes.utils.fuser_utils 
    * module
  * torch.fx.passes.utils.matcher_utils 
    * module
  * torch.fx.passes.utils.matcher_with_name_node_map_utils 
    * module
  * torch.fx.passes.utils.source_matcher_utils 
    * module
  * torch.fx.proxy 
    * module
  * torch.fx.subgraph_rewriter 
    * module
  * torch.fx.tensor_type 
    * module
  * torch.fx.traceback 
    * module
  * torch.hub 
    * module
  * torch.iinfo (class in torch)
  * torch.jit 
    * module
  * torch.jit.annotations 
    * module
  * torch.jit.frontend 
    * module
  * torch.jit.generate_bytecode 
    * module
  * torch.jit.mobile 
    * module
  * torch.jit.quantized 
    * module
  * torch.jit.supported_ops 
    * module
  * torch.jit.unsupported_tensor_ops 
    * module
  * torch.library 
    * module
  * torch.linalg 
    * module
  * torch.masked 
    * module
  * torch.masked.maskedtensor 
    * module
  * torch.masked.maskedtensor.binary 
    * module
  * torch.masked.maskedtensor.core 
    * module
  * torch.masked.maskedtensor.creation 
    * module
  * torch.masked.maskedtensor.passthrough 
    * module
  * torch.masked.maskedtensor.reductions 
    * module
  * torch.masked.maskedtensor.unary 
    * module
  * torch.monitor 
    * module
  * torch.mps 
    * module
  * torch.mps.event 
    * module
  * torch.mps.profiler 
    * module
  * torch.mtia 
    * module
  * torch.mtia.memory 
    * module
  * torch.multiprocessing 
    * module
  * torch.multiprocessing.pool 
    * module
  * torch.multiprocessing.queue 
    * module
  * torch.multiprocessing.reductions 
    * module
  * torch.multiprocessing.spawn 
    * module
  * torch.nested 
    * module
  * torch.nn 
    * module
  * torch.nn.attention 
    * module
  * torch.nn.attention.bias 
    * module
  * torch.nn.attention.experimental 
    * module
  * torch.nn.attention.flex_attention 
    * module
  * torch.nn.backends 
    * module
  * torch.nn.backends.thnn 
    * module
  * torch.nn.common_types 
    * module
  * torch.nn.cpp 
    * module
  * torch.nn.functional 
    * module
  * torch.nn.grad 
    * module
  * torch.nn.init 
    * module
  * torch.nn.intrinsic 
    * module
  * torch.nn.intrinsic.modules 
    * module
  * torch.nn.intrinsic.modules.fused 
    * module
  * torch.nn.intrinsic.qat 
    * module
  * torch.nn.intrinsic.qat.modules 
    * module
  * torch.nn.intrinsic.qat.modules.conv_fused 
    * module
  * torch.nn.intrinsic.qat.modules.linear_fused 
    * module
  * torch.nn.intrinsic.qat.modules.linear_relu 
    * module
  * torch.nn.intrinsic.quantized 
    * module
  * torch.nn.intrinsic.quantized.dynamic 
    * module
  * torch.nn.intrinsic.quantized.dynamic.modules 
    * module
  * torch.nn.intrinsic.quantized.dynamic.modules.linear_relu 
    * module
  * torch.nn.intrinsic.quantized.modules 
    * module
  * torch.nn.intrinsic.quantized.modules.bn_relu 
    * module
  * torch.nn.intrinsic.quantized.modules.conv_relu 
    * module
  * torch.nn.intrinsic.quantized.modules.linear_relu 
    * module
  * torch.nn.modules 
    * module
  * torch.nn.modules.activation 
    * module
  * torch.nn.modules.adaptive 
    * module
  * torch.nn.modules.batchnorm 
    * module
  * torch.nn.modules.channelshuffle 
    * module
  * torch.nn.modules.container 
    * module
  * torch.nn.modules.conv 
    * module
  * torch.nn.modules.distance 
    * module
  * torch.nn.modules.dropout 
    * module
  * torch.nn.modules.flatten 
    * module
  * torch.nn.modules.fold 
    * module
  * torch.nn.modules.instancenorm 
    * module
  * torch.nn.modules.lazy 
    * module
  * torch.nn.modules.linear 
    * module
  * torch.nn.modules.loss 
    * module
  * torch.nn.modules.module 
    * module
  * torch.nn.modules.normalization 
    * module
  * torch.nn.modules.padding 
    * module
  * torch.nn.modules.pixelshuffle 
    * module
  * torch.nn.modules.pooling 
    * module
  * torch.nn.modules.rnn 
    * module
  * torch.nn.modules.sparse 
    * module
  * torch.nn.modules.transformer 
    * module
  * torch.nn.modules.upsampling 
    * module
  * torch.nn.modules.utils 
    * module
  * torch.nn.parallel 
    * module
  * torch.nn.parallel.comm 
    * module
  * torch.nn.parallel.distributed 
    * module
  * torch.nn.parallel.parallel_apply 
    * module
  * torch.nn.parallel.replicate 
    * module
  * torch.nn.parallel.scatter_gather 
    * module
  * torch.nn.parameter 
    * module
  * torch.nn.qat 
    * module
  * torch.nn.qat.dynamic 
    * module
  * torch.nn.qat.dynamic.modules 
    * module
  * torch.nn.qat.dynamic.modules.linear 
    * module
  * torch.nn.qat.modules 
    * module
  * torch.nn.qat.modules.conv 
    * module
  * torch.nn.qat.modules.embedding_ops 
    * module
  * torch.nn.qat.modules.linear 
    * module
  * torch.nn.quantizable 
    * module
  * torch.nn.quantizable.modules 
    * module
  * torch.nn.quantizable.modules.activation 
    * module
  * torch.nn.quantizable.modules.rnn 
    * module
  * torch.nn.quantized 
    * module
  * torch.nn.quantized.dynamic 
    * module
  * torch.nn.quantized.dynamic.modules 
    * module
  * torch.nn.quantized.dynamic.modules.conv 
    * module
  * torch.nn.quantized.dynamic.modules.linear 
    * module
  * torch.nn.quantized.dynamic.modules.rnn 
    * module
  * torch.nn.quantized.functional 
    * module
  * torch.nn.quantized.modules 
    * module
  * torch.nn.quantized.modules.activation 
    * module
  * torch.nn.quantized.modules.batchnorm 
    * module
  * torch.nn.quantized.modules.conv 
    * module
  * torch.nn.quantized.modules.dropout 
    * module
  * torch.nn.quantized.modules.embedding_ops 
    * module
  * torch.nn.quantized.modules.functional_modules 
    * module
  * torch.nn.quantized.modules.linear 
    * module
  * torch.nn.quantized.modules.normalization 
    * module
  * torch.nn.quantized.modules.rnn 
    * module
  * torch.nn.quantized.modules.utils 
    * module
  * torch.nn.utils 
    * module
  * torch.nn.utils.clip_grad 
    * module
  * torch.nn.utils.convert_parameters 
    * module
  * torch.nn.utils.fusion 
    * module
  * torch.nn.utils.init 
    * module
  * torch.nn.utils.memory_format 
    * module
  * torch.nn.utils.parametrizations 
    * module
  * torch.nn.utils.parametrize 
    * module
  * torch.nn.utils.prune 
    * module
  * torch.nn.utils.rnn 
    * module
  * torch.nn.utils.stateless 
    * module
  * torch.onnx 
    * module
  * torch.onnx.errors 
    * module
  * torch.onnx.operators 
    * module
  * torch.onnx.symbolic_caffe2 
    * module
  * torch.onnx.symbolic_helper 
    * module
  * torch.onnx.symbolic_opset10 
    * module
  * torch.onnx.symbolic_opset11 
    * module
  * torch.onnx.symbolic_opset12 
    * module
  * torch.onnx.symbolic_opset13 
    * module
  * torch.onnx.symbolic_opset14 
    * module
  * torch.onnx.symbolic_opset15 
    * module
  * torch.onnx.symbolic_opset16 
    * module
  * torch.onnx.symbolic_opset17 
    * module
  * torch.onnx.symbolic_opset18 
    * module
  * torch.onnx.symbolic_opset19 
    * module
  * torch.onnx.symbolic_opset20 
    * module
  * torch.onnx.symbolic_opset7 
    * module
  * torch.onnx.symbolic_opset8 
    * module
  * torch.onnx.symbolic_opset9 
    * module
  * torch.onnx.utils 
    * module
  * torch.onnx.verification 
    * module
  * torch.optim 
    * module
  * torch.optim.adadelta 
    * module
  * torch.optim.adagrad 
    * module
  * torch.optim.adam 
    * module
  * torch.optim.adamax 
    * module
  * torch.optim.adamw 
    * module
  * torch.optim.asgd 
    * module
  * torch.optim.lbfgs 
    * module
  * torch.optim.lr_scheduler 
    * module
  * torch.optim.nadam 
    * module
  * torch.optim.optimizer 
    * module
  * torch.optim.radam 
    * module
  * torch.optim.rmsprop 
    * module
  * torch.optim.rprop 
    * module
  * torch.optim.sgd 
    * module
  * torch.optim.sparse_adam 
    * module
  * torch.optim.swa_utils 
    * module
  * torch.overrides 
    * module
  * torch.package 
    * module
  * torch.package.analyze 
    * module
  * torch.package.analyze.find_first_use_of_broken_modules 
    * module
  * torch.package.analyze.is_from_package 
    * module
  * torch.package.analyze.trace_dependencies 
    * module
  * torch.package.file_structure_representation 
    * module
  * torch.package.find_file_dependencies 
    * module
  * torch.package.glob_group 
    * module
  * torch.package.importer 
    * module
  * torch.package.package_exporter 
    * module
  * torch.package.package_importer 
    * module
  * torch.profiler 
    * module
  * torch.profiler.itt 
    * module
  * torch.profiler.profiler 
    * module
  * torch.profiler.python_tracer 
    * module
  * torch.quantization 
    * module
  * torch.quantization.fake_quantize 
    * module
  * torch.quantization.fuse_modules 
    * module
  * torch.quantization.fuser_method_mappings 
    * module
  * torch.quantization.fx 
    * module
  * torch.quantization.fx.convert 
    * module
  * torch.quantization.fx.fuse 
    * module
  * torch.quantization.fx.fusion_patterns 
    * module
  * torch.quantization.fx.graph_module 
    * module
  * torch.quantization.fx.match_utils 
    * module
  * torch.quantization.fx.pattern_utils 
    * module
  * torch.quantization.fx.prepare 
    * module
  * torch.quantization.fx.quantization_patterns 
    * module
  * torch.quantization.fx.quantization_types 
    * module
  * torch.quantization.fx.utils 
    * module
  * torch.quantization.observer 
    * module
  * torch.quantization.qconfig 
    * module
  * torch.quantization.quant_type 
    * module
  * torch.quantization.quantization_mappings 
    * module
  * torch.quantization.quantize 
    * module
  * torch.quantization.quantize_fx 
    * module
  * torch.quantization.quantize_jit 
    * module
  * torch.quantization.stubs 
    * module
  * torch.quantization.utils 
    * module
  * torch.quasirandom 
    * module
  * torch.random 
    * module
  * torch.return_types 
    * module
  * torch.serialization 
    * module
  * torch.signal 
    * module
  * torch.signal.windows 
    * module
  * torch.signal.windows.windows 
    * module
  * torch.sparse 
    * module
  * torch.sparse.semi_structured 
    * module
  * torch.special 
    * module
  * torch.storage 
    * module
  * torch.testing 
    * module
  * torch.torch_version 
    * module
  * torch.types 
    * module
  * torch.utils 
    * module
  * torch.utils.backcompat 
    * module
  * torch.utils.backend_registration 
    * module
  * torch.utils.benchmark 
    * module
  * torch.utils.benchmark.examples 
    * module
  * torch.utils.benchmark.examples.blas_compare_setup 
    * module
  * torch.utils.benchmark.examples.compare 
    * module
  * torch.utils.benchmark.examples.fuzzer 
    * module
  * torch.utils.benchmark.examples.op_benchmark 
    * module
  * torch.utils.benchmark.examples.simple_timeit 
    * module
  * torch.utils.benchmark.examples.spectral_ops_fuzz_test 
    * module
  * torch.utils.benchmark.op_fuzzers 
    * module
  * torch.utils.benchmark.op_fuzzers.binary 
    * module
  * torch.utils.benchmark.op_fuzzers.sparse_binary 
    * module
  * torch.utils.benchmark.op_fuzzers.sparse_unary 
    * module
  * torch.utils.benchmark.op_fuzzers.spectral 
    * module
  * torch.utils.benchmark.op_fuzzers.unary 
    * module
  * torch.utils.benchmark.utils 
    * module
  * torch.utils.benchmark.utils.common 
    * module
  * torch.utils.benchmark.utils.compare 
    * module
  * torch.utils.benchmark.utils.compile 
    * module
  * torch.utils.benchmark.utils.cpp_jit 
    * module
  * torch.utils.benchmark.utils.fuzzer 
    * module
  * torch.utils.benchmark.utils.sparse_fuzzer 
    * module
  * torch.utils.benchmark.utils.timer 
    * module
  * torch.utils.benchmark.utils.valgrind_wrapper 
    * module
  * torch.utils.benchmark.utils.valgrind_wrapper.timer_interface 
    * module
  * torch.utils.bottleneck 
    * module
  * torch.utils.bundled_inputs 
    * module
  * torch.utils.checkpoint 
    * module
  * torch.utils.collect_env 
    * module
  * torch.utils.cpp_backtrace 
    * module
  * torch.utils.cpp_extension 
    * module
  * torch.utils.data 
    * module
  * torch.utils.data.backward_compatibility 
    * module
  * torch.utils.data.dataloader 
    * module
  * torch.utils.data.datapipes 
    * module
  * torch.utils.data.datapipes.dataframe 
    * module
  * torch.utils.data.datapipes.dataframe.dataframe_wrapper 
    * module
  * torch.utils.data.datapipes.dataframe.dataframes 
    * module
  * torch.utils.data.datapipes.dataframe.datapipes 
    * module
  * torch.utils.data.datapipes.dataframe.structures 
    * module
  * torch.utils.data.datapipes.datapipe 
    * module
  * torch.utils.data.datapipes.gen_pyi 
    * module
  * torch.utils.data.datapipes.iter 
    * module
  * torch.utils.data.datapipes.iter.callable 
    * module
  * torch.utils.data.datapipes.iter.combinatorics 
    * module
  * torch.utils.data.datapipes.iter.combining 
    * module
  * torch.utils.data.datapipes.iter.filelister 
    * module
  * torch.utils.data.datapipes.iter.fileopener 
    * module
  * torch.utils.data.datapipes.iter.grouping 
    * module
  * torch.utils.data.datapipes.iter.routeddecoder 
    * module
  * torch.utils.data.datapipes.iter.selecting 
    * module
  * torch.utils.data.datapipes.iter.sharding 
    * module
  * torch.utils.data.datapipes.iter.streamreader 
    * module
  * torch.utils.data.datapipes.iter.utils 
    * module
  * torch.utils.data.datapipes.map 
    * module
  * torch.utils.data.datapipes.map.callable 
    * module
  * torch.utils.data.datapipes.map.combinatorics 
    * module
  * torch.utils.data.datapipes.map.combining 
    * module
  * torch.utils.data.datapipes.map.grouping 
    * module
  * torch.utils.data.datapipes.map.utils 
    * module
  * torch.utils.data.datapipes.utils 
    * module
  * torch.utils.data.datapipes.utils.common 
    * module
  * torch.utils.data.datapipes.utils.decoder 
    * module
  * torch.utils.data.datapipes.utils.snapshot 
    * module
  * torch.utils.data.dataset 
    * module
  * torch.utils.data.distributed 
    * module
  * torch.utils.data.graph 
    * module
  * torch.utils.data.graph_settings 
    * module
  * torch.utils.data.sampler 
    * module
  * torch.utils.deterministic 
    * module
  * torch.utils.dlpack 
    * module
  * torch.utils.file_baton 
    * module
  * torch.utils.flop_counter 
    * module
  * torch.utils.hipify 
    * module
  * torch.utils.hipify.constants 
    * module
  * torch.utils.hipify.cuda_to_hip_mappings 
    * module
  * torch.utils.hipify.hipify_python 
    * module
  * torch.utils.hipify.version 
    * module
  * torch.utils.hooks 
    * module
  * torch.utils.jit 
    * module
  * torch.utils.jit.log_extract 
    * module
  * torch.utils.mkldnn 
    * module
  * torch.utils.mobile_optimizer 
    * module
  * torch.utils.model_dump 
    * module
  * torch.utils.model_zoo 
    * module
  * torch.utils.module_tracker 
    * module
  * torch.utils.serialization 
    * module
  * torch.utils.serialization.config 
    * module
  * torch.utils.show_pickle 
    * module
  * torch.utils.tensorboard 
    * module
  * torch.utils.tensorboard.summary 
    * module
  * torch.utils.tensorboard.writer 
    * module
  * torch.utils.throughput_benchmark 
    * module
  * torch.utils.viz 
    * module
  * torch.utils.weak 
    * module
  * torch.version 
    * module
  * torch.xpu 
    * module
  * torch.xpu.memory 
    * module
  * torch.xpu.random 
    * module
  * torch.xpu.streams 
    * module
  * TORCH_COMPILE_JOB_ID
  * torch_name() (torch.onnx.JitScalarType method)
  * torch_save_to_dcp() (in module torch.distributed.checkpoint.format_utils)
  * TorchAODType (class in torch.ao.quantization.observer)
  * total_average() (torch.autograd.profiler.profile method)
  * total_count (torch.distributions.multinomial.Multinomial attribute)
  * trace() (in module torch)
    * (in module torch.jit)
    * (torch.fx.Tracer method)
    * (torch.Tensor method)
  * trace_module() (in module torch.jit)
  * Tracer (class in torch.fx)
  * train() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
  * Transform (class in torch.distributions.transforms)
  * transform() (torch.fx.Transformer method)
    * (torch.utils.benchmark.FunctionCounts method)
  * transform_object() (torch.distributed.checkpoint.DefaultSavePlanner method)
  * transform_tensor() (torch.distributed.checkpoint.DefaultLoadPlanner method)
  * TransformedDistribution (class in torch.distributions.transformed_distribution)
  * Transformer (class in torch.fx)
    * (class in torch.nn)
  * TransformerDecoder (class in torch.nn)
  * TransformerDecoderLayer (class in torch.nn)
  * TransformerEncoder (class in torch.nn)
  * TransformerEncoderLayer (class in torch.nn)
  * transpose() (in module torch)
    * (torch.Tensor method)
  * transpose_() (torch.Tensor method)
  * trapezoid() (in module torch)
  * trapz() (in module torch)
  * triangular_solve() (in module torch)
    * (torch.Tensor method)
  * tril() (in module torch)
    * (torch.Tensor method)
  * tril_() (torch.Tensor method)
  * tril_indices() (in module torch)
  * trim_significant_figures() (torch.utils.benchmark.Compare method)
  * triplet_margin_loss() (in module torch.nn.functional)
  * triplet_margin_with_distance_loss() (in module torch.nn.functional)
  * TripletMarginLoss (class in torch.nn)
  * TripletMarginWithDistanceLoss (class in torch.nn)
  * triton_op() (in module torch.library)
  * triu() (in module torch)
    * (torch.Tensor method)
  * triu_() (torch.Tensor method)
  * triu_indices() (in module torch)
  * true_divide() (in module torch)
    * (torch.Tensor method)
  * true_divide_() (torch.Tensor method)
  * trunc() (in module torch)
    * (torch.Tensor method)
  * trunc_() (torch.Tensor method)
  * trunc_normal_() (in module torch.nn.init)
  * tune_gemm_in_file() (in module torch.cuda.tunable)
  * tuning_enable() (in module torch.cuda.tunable)
  * tuning_is_enabled() (in module torch.cuda.tunable)
  * type (torch.jit.Attribute attribute)
  * type() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)
    * (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * type_as() (torch.Tensor method)
  * TypedStorage (class in torch)

  
---|---  
## U
  * unbind() (in module torch)
    * (torch.Tensor method)
  * unbox() (torch.distributed.Work static method)
  * underlying_store (torch.distributed.PrefixStore property)
  * Unflatten (class in torch.nn)
  * unflatten() (in module torch)
    * (in module torch.export.unflatten)
    * (torch.Tensor method)
  * Unfold (class in torch.nn)
  * unfold() (in module torch.nn.functional)
    * (torch.Tensor method)
  * Uniform (class in torch.distributions.uniform)
  * uniform_() (in module torch.nn.init)
    * (torch.Tensor method)
  * UninitializedBuffer (class in torch.nn.parameter)
  * UninitializedParameter (class in torch.nn.parameter)
  * unique() (in module torch)
    * (torch.Tensor method)
  * unique_consecutive() (in module torch)
    * (torch.Tensor method)
  * unpack_dual() (in module torch.autograd.forward_ad)
  * unpack_sequence() (in module torch.nn.utils.rnn)
  * UnpackedDualTensor (class in torch.autograd.forward_ad)
  * unpad_sequence() (in module torch.nn.utils.rnn)
  * unravel_index() (in module torch)
  * unregister_custom_op_symbolic() (in module torch.onnx)
  * unregister_event_handler() (in module torch.monitor)
  * unshard() (torch.distributed.fsdp.FSDPModule method)
  * UnshardHandle (class in torch.distributed.fsdp)
  * unsorted_indices (torch.nn.utils.rnn.PackedSequence attribute)

| 
  * unsqueeze() (in module torch)
    * (torch.Tensor method)
  * unsqueeze_() (torch.Tensor method)
  * untyped() (torch.TypedStorage method)
    * (torch.UntypedStorage method)
  * untyped_storage() (torch.Tensor method)
  * UntypedStorage (class in torch)
  * unused() (in module torch.jit)
  * update() (torch.autograd.profiler_util.StringTable method)
    * (torch.export.decomp_utils.CustomDecompTable method)
    * (torch.nn.ModuleDict method)
    * (torch.nn.ParameterDict method)
  * update_arg() (torch.fx.Node method)
  * update_bn() (in module torch.optim.swa_utils)
  * update_bn_stats (class in torch.ao.nn.intrinsic.qat)
  * update_kwarg() (torch.fx.Node method)
  * update_parameters() (torch.optim.swa_utils.AveragedModel method)
  * upsample (class in torch.ao.nn.quantized.functional)
  * Upsample (class in torch.nn)
  * upsample() (in module torch.nn.functional)
  * upsample_bilinear (class in torch.ao.nn.quantized.functional)
  * upsample_bilinear() (in module torch.nn.functional)
  * upsample_nearest (class in torch.ao.nn.quantized.functional)
  * upsample_nearest() (in module torch.nn.functional)
  * UpsamplingBilinear2d (class in torch.nn)
  * UpsamplingNearest2d (class in torch.nn)
  * use_agent_store (torch.distributed.elastic.rendezvous.RendezvousHandler property)
  * use_count() (torch.cuda.MemPool method)
  * use_deterministic_algorithms() (in module torch)
  * use_mem_pool (class in torch.cuda)
  * utilization() (in module torch.cuda)

  
---|---  
## V
  * validate_checkpoint_id() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader class method)
    * (torch.distributed.checkpoint.StorageReader class method)
    * (torch.distributed.checkpoint.StorageWriter class method)
  * value (torch.jit.Attribute attribute)
  * value() (torch.futures.Future method)
  * values() (torch.autograd.profiler_util.StringTable method)
    * (torch.nn.ModuleDict method)
    * (torch.nn.ParameterDict method)
    * (torch.Tensor method)
  * vander() (in module torch)
    * (in module torch.linalg)
  * var() (in module torch)
    * (torch.Tensor method)
  * var_mean() (in module torch)
  * variance (torch.distributions.bernoulli.Bernoulli property)
    * (torch.distributions.beta.Beta property)
    * (torch.distributions.binomial.Binomial property)
    * (torch.distributions.categorical.Categorical property)
    * (torch.distributions.cauchy.Cauchy property)
    * (torch.distributions.continuous_bernoulli.ContinuousBernoulli property)
    * (torch.distributions.dirichlet.Dirichlet property)
    * (torch.distributions.distribution.Distribution property)
    * (torch.distributions.exponential.Exponential property)
    * (torch.distributions.fishersnedecor.FisherSnedecor property)
    * (torch.distributions.gamma.Gamma property)
    * (torch.distributions.geometric.Geometric property)
    * (torch.distributions.gumbel.Gumbel property)
    * (torch.distributions.half_cauchy.HalfCauchy property)
    * (torch.distributions.half_normal.HalfNormal property)
    * (torch.distributions.independent.Independent property)
    * (torch.distributions.inverse_gamma.InverseGamma property)
    * (torch.distributions.kumaraswamy.Kumaraswamy property)
    * (torch.distributions.laplace.Laplace property)
    * (torch.distributions.log_normal.LogNormal property)
    * (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property)
    * (torch.distributions.mixture_same_family.MixtureSameFamily property)
    * (torch.distributions.multinomial.Multinomial property)
    * (torch.distributions.multivariate_normal.MultivariateNormal property)
    * (torch.distributions.negative_binomial.NegativeBinomial property)
    * (torch.distributions.normal.Normal property)
    * (torch.distributions.one_hot_categorical.OneHotCategorical property)
    * (torch.distributions.pareto.Pareto property)
    * (torch.distributions.poisson.Poisson property)
    * (torch.distributions.studentT.StudentT property)
    * (torch.distributions.uniform.Uniform property)
    * (torch.distributions.von_mises.VonMises property)
    * (torch.distributions.weibull.Weibull property)
    * (torch.distributions.wishart.Wishart property)

| 
  * vdot() (in module torch)
    * (torch.Tensor method)
  * vecdot() (in module torch.linalg)
  * vector_norm() (in module torch.linalg)
  * vector_to_parameters() (in module torch.nn.utils)
  * verbose (class in torch.backends.mkl)
    * (class in torch.backends.mkldnn)
  * VerificationInfo (class in torch.onnx.verification)
  * VerificationOptions (class in torch.onnx.verification)
  * verify() (in module torch.onnx.verification)
  * verify_aten_graph() (in module torch.onnx.verification)
  * verify_ninja_availability() (in module torch.utils.cpp_extension)
  * verify_onnx_program() (in module torch.onnx.verification)
  * version() (in module torch.backends.cudnn)
    * (in module torch.backends.cusparselt)
  * vhp() (in module torch.autograd.functional)
  * view() (torch.Tensor method)
  * view_as() (torch.Tensor method)
  * view_as_complex() (in module torch)
  * view_as_real() (in module torch)
  * visualize_sharding() (in module torch.distributed.tensor.debug)
  * vjp() (in module torch.autograd.functional)
    * (in module torch.func)
    * (torch.autograd.function.InplaceFunction static method)
    * (torch.autograd.function.NestedIOFunction static method)
  * vmap() (in module torch)
    * (in module torch.func)
    * (torch.autograd.Function static method)
    * (torch.autograd.function.InplaceFunction static method)
    * (torch.autograd.function.NestedIOFunction static method)
  * VonMises (class in torch.distributions.von_mises)
  * vsplit() (in module torch)
    * (torch.Tensor method)
  * vstack() (in module torch)

  
---|---  
## W
  * wait() (in module torch.jit)
    * (torch.cuda.Event method)
    * (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method)
    * (torch.distributed.fsdp.UnshardHandle method)
    * (torch.distributed.Store method)
    * (torch.distributed.Work method)
    * (torch.Event method)
    * (torch.futures.Future method)
    * (torch.mps.event.Event method)
    * (torch.mtia.Event method)
    * (torch.xpu.Event method)
  * wait_all() (in module torch.futures)
  * wait_event() (torch.cuda.ExternalStream method)
    * (torch.cuda.Stream method)
    * (torch.mtia.Stream method)
    * (torch.Stream method)
    * (torch.xpu.Stream method)
  * wait_stream() (torch.cuda.ExternalStream method)
    * (torch.cuda.Stream method)
    * (torch.mtia.Stream method)
    * (torch.Stream method)
    * (torch.xpu.Stream method)
  * Weibull (class in torch.distributions.weibull)

| 
  * weight_norm() (in module torch.nn.utils)
    * (in module torch.nn.utils.parametrizations)
  * WeightedRandomSampler (class in torch.utils.data)
  * where() (in module torch)
    * (torch.Tensor method)
  * Wishart (class in torch.distributions.wishart)
  * with_args() (torch.ao.quantization.observer.AffineQuantizedObserverBase class method)
    * (torch.ao.quantization.observer.ObserverBase class method)
  * with_callable_args() (torch.ao.quantization.observer.ObserverBase class method)
  * Work (class in torch.distributed)
  * Worker (class in torch.distributed.elastic.agent.server)
  * worker_main() (in module torch.distributed.elastic.control_plane)
  * WorkerGroup (class in torch.distributed.elastic.agent.server)
  * WorkerInfo (class in torch.distributed.rpc)
  * WorkerSpec (class in torch.distributed.elastic.agent.server)
  * WorkerState (class in torch.distributed.elastic.agent.server)
  * wrap() (in module torch.fx)
  * wrap_torch_function() (in module torch.overrides)
  * wrap_triton() (in module torch.library)
  * write_data() (torch.distributed.checkpoint.StorageWriter method)
  * write_file() (in module torch.cuda.tunable)
  * write_file_on_exit() (in module torch.cuda.tunable)
  * WriteItem (class in torch.distributed.checkpoint.planner)

  
---|---  
## X
  * xavier_normal_() (in module torch.nn.init)
  * xavier_uniform_() (in module torch.nn.init)
  * xlog1py() (in module torch.special)
  * xlogy() (in module torch)
    * (in module torch.special)
    * (torch.Tensor method)

| 
  * xlogy_() (torch.Tensor method)
  * xpu() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.swa_utils.AveragedModel method)
    * (torch.Tensor method)

  
---|---  
## Z
  * zero_() (torch.Tensor method)
  * zero_grad() (torch.jit.ScriptModule method)
    * (torch.nn.Module method)
    * (torch.optim.Adadelta method)
    * (torch.optim.Adafactor method)
    * (torch.optim.Adagrad method)
    * (torch.optim.Adam method)
    * (torch.optim.Adamax method)
    * (torch.optim.AdamW method)
    * (torch.optim.ASGD method)
    * (torch.optim.LBFGS method)
    * (torch.optim.NAdam method)
    * (torch.optim.Optimizer method)
    * (torch.optim.RAdam method)
    * (torch.optim.RMSprop method)
    * (torch.optim.Rprop method)
    * (torch.optim.SGD method)
    * (torch.optim.SparseAdam method)
    * (torch.optim.swa_utils.AveragedModel method)

| 
  * ZeroPad1d (class in torch.nn)
  * ZeroPad2d (class in torch.nn)
  * ZeroPad3d (class in torch.nn)
  * ZeroPointDomain (class in torch.ao.quantization.observer)
  * ZeroRedundancyOptimizer (class in torch.distributed.optim)
  * zeros() (in module torch)
    * (in module torch.distributed.tensor)
  * zeros_() (in module torch.nn.init)
  * zeros_like() (in module torch)
  * zeta() (in module torch.special)

  
---|---  
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.linalg
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.linalg
Common linear algebra operations.
See Linear algebra (torch.linalg) for some common numerical edge-cases.
## Matrix Properties
`norm` | Computes a vector or matrix norm.  
---|---  
`vector_norm` | Computes a vector norm.  
`matrix_norm` | Computes a matrix norm.  
`diagonal` | Alias for `torch.diagonal()` with defaults `dim1`= -2, `dim2`= -1.  
`det` | Computes the determinant of a square matrix.  
`slogdet` | Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.  
`cond` | Computes the condition number of a matrix with respect to a matrix norm.  
`matrix_rank` | Computes the numerical rank of a matrix.  
## Decompositions
`cholesky` | Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.  
---|---  
`qr` | Computes the QR decomposition of a matrix.  
`lu` | Computes the LU decomposition with partial pivoting of a matrix.  
`lu_factor` | Computes a compact representation of the LU factorization with partial pivoting of a matrix.  
`eig` | Computes the eigenvalue decomposition of a square matrix if it exists.  
`eigvals` | Computes the eigenvalues of a square matrix.  
`eigh` | Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.  
`eigvalsh` | Computes the eigenvalues of a complex Hermitian or real symmetric matrix.  
`svd` | Computes the singular value decomposition (SVD) of a matrix.  
`svdvals` | Computes the singular values of a matrix.  
## Solvers
`solve` | Computes the solution of a square system of linear equations with a unique solution.  
---|---  
`solve_triangular` | Computes the solution of a triangular system of linear equations with a unique solution.  
`lu_solve` | Computes the solution of a square system of linear equations with a unique solution given an LU decomposition.  
`lstsq` | Computes a solution to the least squares problem of a system of linear equations.  
## Inverses
`inv` | Computes the inverse of a square matrix if it exists.  
---|---  
`pinv` | Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.  
## Matrix Functions
`matrix_exp` | Computes the matrix exponential of a square matrix.  
---|---  
`matrix_power` | Computes the n-th power of a square matrix for an integer n.  
## Matrix Products
`cross` | Computes the cross product of two 3-dimensional vectors.  
---|---  
`matmul` | Alias for `torch.matmul()`  
`vecdot` | Computes the dot product of two batches of vectors along a dimension.  
`multi_dot` | Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.  
`householder_product` | Computes the first n columns of a product of Householder matrices.  
## Tensor Operations
`tensorinv` | Computes the multiplicative inverse of `torch.tensordot()`.  
---|---  
`tensorsolve` | Computes the solution X to the system torch.tensordot(A, X) = B.  
## Misc
`vander` | Generates a Vandermonde matrix.  
---|---  
## Experimental Functions
`cholesky_ex` | Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.  
---|---  
`inv_ex` | Computes the inverse of a square matrix if it is invertible.  
`solve_ex` | A version of `solve()` that does not perform error checks unless `check_errors`= True.  
`lu_factor_ex` | This is a version of `lu_factor()` that does not perform error checks unless `check_errors`= True.  
`ldl_factor` | Computes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.  
`ldl_factor_ex` | This is a version of `ldl_factor()` that does not perform error checks unless `check_errors`= True.  
`ldl_solve` | Computes the solution of a system of linear equations using the LDL factorization.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.linalg
    * Matrix Properties
    * Decompositions
    * Solvers
    * Inverses
    * Matrix Functions
    * Matrix Products
    * Tensor Operations
    * Misc
    * Experimental Functions


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.compiler >
  * AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models >
  * torch._logging
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch._logging
PyTorch has a configurable logging system, where different components can be given different log level settings. For instance, one component’s log messages can be completely disabled, while another component’s log messages can be set to maximum verbosity.
Warning
This feature is in beta and may have compatibility breaking changes in the future.
Warning
This feature has not been expanded to control the log messages of all components in PyTorch yet.
There are two ways to configure the logging system: through the environment variable `TORCH_LOGS` or the python API torch._logging.set_logs.
`set_logs` | Sets the log level for individual components and toggles individual log artifact types.  
---|---  
The environment variable `TORCH_LOGS` is a comma-separated list of `[+-]<component>` pairs, where `<component>` is a component specified below. The `+` prefix will decrease the log level of the component, displaying more log messages while the `-` prefix will increase the log level of the component and display fewer log messages. The default setting is the behavior when a component is not specified in `TORCH_LOGS`. In addition to components, there are also artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed, so prefixing an artifact with `+` or `-` will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact, unless that artifact was specified to be off_by_default. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled. The following components and artifacts are configurable through the `TORCH_LOGS` environment variable (see torch._logging.set_logs for the python API): 

Components:
     

`all`
    
Special component which configures the default log level of all components. Default: `logging.WARN` 

`dynamo`
    
The log level for the TorchDynamo component. Default: `logging.WARN` 

`aot`
    
The log level for the AOTAutograd component. Default: `logging.WARN` 

`inductor`
    
The log level for the TorchInductor component. Default: `logging.WARN` 

`your.custom.module`
    
The log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default: `logging.WARN` 

Artifacts:
     

`bytecode`
    
Whether to emit the original and generated bytecode from TorchDynamo. Default: `False` 

`aot_graphs`
    
Whether to emit the graphs generated by AOTAutograd. Default: `False` 

`aot_joint_graph`
    
Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: `False` 

`compiled_autograd`
    
Whether to emit logs from compiled_autograd. Defaults: `False` 

`ddp_graphs`
    
Whether to emit graphs generated by DDPOptimizer. Default: `False` 

`graph`
    
Whether to emit the graph captured by TorchDynamo in tabular format. Default: `False` 

`graph_code`
    
Whether to emit the python source of the graph captured by TorchDynamo. Default: `False` 

`graph_breaks`
    
Whether to emit a message when a unique graph break is encountered during TorchDynamo tracing. Default: `False` 

`guards`
    
Whether to emit the guards generated by TorchDynamo for each compiled function. Default: `False` 

`recompiles`
    
Whether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default: `False` 

`output_code`
    
Whether to emit the TorchInductor output code. Default: `False` 

`schedule`
    
Whether to emit the TorchInductor schedule. Default: `False` 

Examples:
    
`TORCH_LOGS="+dynamo,aot"` will set the log level of TorchDynamo to `logging.DEBUG` and AOT to `logging.INFO`
`TORCH_LOGS="-dynamo,+inductor"` will set the log level of TorchDynamo to `logging.ERROR` and TorchInductor to `logging.DEBUG`
`TORCH_LOGS="aot_graphs"` will enable the `aot_graphs` artifact
`TORCH_LOGS="+dynamo,schedule"` will enable set the log level of TorchDynamo to `logging.DEBUG` and enable the `schedule` artifact
`TORCH_LOGS="+some.random.module,schedule"` will set the log level of some.random.module to `logging.DEBUG` and enable the `schedule` artifact
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch._logging


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.module_tracker
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.module_tracker
This utility can be used to track the current position inside an `torch.nn.Module` hierarchy. It can be used within other tracking tools to be able to easily associate measured quantities to user-friendly names. This is used in particular in the FlopCounterMode today. 

_class_ torch.utils.module_tracker.ModuleTracker[source][source]
    
`ModuleTracker` is a context manager that tracks the nn.Module hierarchy during execution so that other system can query which Module is currently being executed (or its backward is being executed).
You can access the `parents` attribute on this context manager to get the set of all the Modules currently being executed via their fqn (fully qualified name, also used as the key within the state_dict). You can access the `is_bw` attribute to know if you are currently running in backward or not.
Note that `parents` is never empty and always contains the “Global” key. The `is_bw` flag will remain `True` after the forward until another Module is executed. If you need it to be more accurate, please submit an issue requesting this. Adding a map from fqn to the module instance is possible but not done yet, please submit an issue requesting this if you need it.
Example usage
```
mod = torch.nn.Linear(2, 2)
with ModuleTracker() as tracker:
  # Access anything during the forward pass
  def my_linear(m1, m2, bias):
    print(f"Current modules: {tracker.parents}")
    return torch.mm(m1, m2.t()) + bias
  torch.nn.functional.linear = my_linear
  mod(torch.rand(2, 2))

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.module_tracker
    * `ModuleTracker`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.masked
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.masked
## Introduction
### Motivation
Warning
The PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.
MaskedTensor serves as an extension to `torch.Tensor` that provides the user with the ability to:
  * use any masked semantics (e.g. variable length tensors, nan* operators, etc.)
  * differentiate between 0 and NaN gradients
  * various sparse applications (see tutorial below)


“Specified” and “unspecified” have a long history in PyTorch without formal semantics and certainly without consistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla `torch.Tensor` class could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for said “specified” and “unspecified” values in PyTorch where they are a first class citizen instead of an afterthought. In turn, this should further unlock sparsity’s potential, enable safer and more consistent operators, and provide a smoother and more intuitive experience for users and developers alike.
### What is a MaskedTensor?
A MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us which entries from the input should be included or ignored.
By way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray) and take the max:
![_images/tensor_comparison.jpg](https://pytorch.org/docs/stable/_images/tensor_comparison.jpg)
On top is the vanilla tensor example while the bottom is MaskedTensor where all the 0’s are masked out. This clearly yields a different result depending on whether we have the mask, but this flexible structure allows the user to systematically ignore any elements they’d like during computation.
There are already a number of existing tutorials that we’ve written to help users onboard, such as:
  * Overview - the place to start for new users, discusses how to use MaskedTensors and why they’re useful
  * Sparsity - MaskedTensor supports sparse COO and CSR data and mask Tensors
  * Adagrad sparse semantics - a practical example of how MaskedTensor can simplify sparse semantics and implementations
  * Advanced semantics - discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy’s MaskedArray, and reduction semantics


## Supported Operators
### Unary Operators
Unary operators are operators that only contain only a single input. Applying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index, we apply the operator, otherwise we’ll continue to mask out the data.
The available unary operators are:
`abs` | Computes the absolute value of each element in `input`.  
---|---  
`absolute` | Alias for `torch.abs()`  
`acos` | Computes the inverse cosine of each element in `input`.  
`arccos` | Alias for `torch.acos()`.  
`acosh` | Returns a new tensor with the inverse hyperbolic cosine of the elements of `input`.  
`arccosh` | Alias for `torch.acosh()`.  
`angle` | Computes the element-wise angle (in radians) of the given `input` tensor.  
`asin` | Returns a new tensor with the arcsine of the elements of `input`.  
`arcsin` | Alias for `torch.asin()`.  
`asinh` | Returns a new tensor with the inverse hyperbolic sine of the elements of `input`.  
`arcsinh` | Alias for `torch.asinh()`.  
`atan` | Returns a new tensor with the arctangent of the elements of `input`.  
`arctan` | Alias for `torch.atan()`.  
`atanh` | Returns a new tensor with the inverse hyperbolic tangent of the elements of `input`.  
`arctanh` | Alias for `torch.atanh()`.  
`bitwise_not` | Computes the bitwise NOT of the given input tensor.  
`ceil` | Returns a new tensor with the ceil of the elements of `input`, the smallest integer greater than or equal to each element.  
`clamp` | Clamps all elements in `input` into the range [ `min`, `max` ].  
`clip` | Alias for `torch.clamp()`.  
`conj_physical` | Computes the element-wise conjugate of the given `input` tensor.  
`cos` | Returns a new tensor with the cosine of the elements of `input`.  
`cosh` | Returns a new tensor with the hyperbolic cosine of the elements of `input`.  
`deg2rad` | Returns a new tensor with each of the elements of `input` converted from angles in degrees to radians.  
`digamma` | Alias for `torch.special.digamma()`.  
`erf` | Alias for `torch.special.erf()`.  
`erfc` | Alias for `torch.special.erfc()`.  
`erfinv` | Alias for `torch.special.erfinv()`.  
`exp` | Returns a new tensor with the exponential of the elements of the input tensor `input`.  
`exp2` | Alias for `torch.special.exp2()`.  
`expm1` | Alias for `torch.special.expm1()`.  
`fix` | Alias for `torch.trunc()`  
`floor` | Returns a new tensor with the floor of the elements of `input`, the largest integer less than or equal to each element.  
`frac` | Computes the fractional portion of each element in `input`.  
`lgamma` | Computes the natural logarithm of the absolute value of the gamma function on `input`.  
`log` | Returns a new tensor with the natural logarithm of the elements of `input`.  
`log10` | Returns a new tensor with the logarithm to the base 10 of the elements of `input`.  
`log1p` | Returns a new tensor with the natural logarithm of (1 + `input`).  
`log2` | Returns a new tensor with the logarithm to the base 2 of the elements of `input`.  
`logit` | Alias for `torch.special.logit()`.  
`i0` | Alias for `torch.special.i0()`.  
`isnan` | Returns a new tensor with boolean elements representing if each element of `input` is NaN or not.  
`nan_to_num` | Replaces `NaN`, positive infinity, and negative infinity values in `input` with the values specified by `nan`, `posinf`, and `neginf`, respectively.  
`neg` | Returns a new tensor with the negative of the elements of `input`.  
`negative` | Alias for `torch.neg()`  
`positive` | Returns `input`.  
`pow` | Takes the power of each element in `input` with `exponent` and returns a tensor with the result.  
`rad2deg` | Returns a new tensor with each of the elements of `input` converted from angles in radians to degrees.  
`reciprocal` | Returns a new tensor with the reciprocal of the elements of `input`  
`round` | Rounds elements of `input` to the nearest integer.  
`rsqrt` | Returns a new tensor with the reciprocal of the square-root of each of the elements of `input`.  
`sigmoid` | Alias for `torch.special.expit()`.  
`sign` | Returns a new tensor with the signs of the elements of `input`.  
`sgn` | This function is an extension of torch.sign() to complex tensors.  
`signbit` | Tests if each element of `input` has its sign bit set or not.  
`sin` | Returns a new tensor with the sine of the elements of `input`.  
`sinc` | Alias for `torch.special.sinc()`.  
`sinh` | Returns a new tensor with the hyperbolic sine of the elements of `input`.  
`sqrt` | Returns a new tensor with the square-root of the elements of `input`.  
`square` | Returns a new tensor with the square of the elements of `input`.  
`tan` | Returns a new tensor with the tangent of the elements of `input`.  
`tanh` | Returns a new tensor with the hyperbolic tangent of the elements of `input`.  
`trunc` | Returns a new tensor with the truncated integer values of the elements of `input`.  
The available inplace unary operators are all of the above **except** :
`angle` | Computes the element-wise angle (in radians) of the given `input` tensor.  
---|---  
`positive` | Returns `input`.  
`signbit` | Tests if each element of `input` has its sign bit set or not.  
`isnan` | Returns a new tensor with boolean elements representing if each element of `input` is NaN or not.  
### Binary Operators
As you may have seen in the tutorial, `MaskedTensor` also has binary operations implemented with the caveat that the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you need support for a particular operator or have proposed semantics for how they should behave instead, please open an issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users know exactly what is going on and are being intentional about their decisions with masked semantics.
The available binary operators are:
`add` | Adds `other`, scaled by `alpha`, to `input`.  
---|---  
`atan2` | Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​ with consideration of the quadrant.  
`arctan2` | Alias for `torch.atan2()`.  
`bitwise_and` | Computes the bitwise AND of `input` and `other`.  
`bitwise_or` | Computes the bitwise OR of `input` and `other`.  
`bitwise_xor` | Computes the bitwise XOR of `input` and `other`.  
`bitwise_left_shift` | Computes the left arithmetic shift of `input` by `other` bits.  
`bitwise_right_shift` | Computes the right arithmetic shift of `input` by `other` bits.  
`div` | Divides each element of the input `input` by the corresponding element of `other`.  
`divide` | Alias for `torch.div()`.  
`floor_divide` |   
`fmod` | Applies C++'s std::fmod entrywise.  
`logaddexp` | Logarithm of the sum of exponentiations of the inputs.  
`logaddexp2` | Logarithm of the sum of exponentiations of the inputs in base-2.  
`mul` | Multiplies `input` by `other`.  
`multiply` | Alias for `torch.mul()`.  
`nextafter` | Return the next floating-point value after `input` towards `other`, elementwise.  
`remainder` | Computes Python's modulus operation entrywise.  
`sub` | Subtracts `other`, scaled by `alpha`, from `input`.  
`subtract` | Alias for `torch.sub()`.  
`true_divide` | Alias for `torch.div()` with `rounding_mode=None`.  
`eq` | Computes element-wise equality  
`ne` | Computes input≠other\text{input} \neq \text{other}input=other element-wise.  
`le` | Computes input≤other\text{input} \leq \text{other}input≤other element-wise.  
`ge` | Computes input≥other\text{input} \geq \text{other}input≥other element-wise.  
`greater` | Alias for `torch.gt()`.  
`greater_equal` | Alias for `torch.ge()`.  
`gt` | Computes input>other\text{input} > \text{other}input>other element-wise.  
`less_equal` | Alias for `torch.le()`.  
`lt` | Computes input<other\text{input} < \text{other}input<other element-wise.  
`less` | Alias for `torch.lt()`.  
`maximum` | Computes the element-wise maximum of `input` and `other`.  
`minimum` | Computes the element-wise minimum of `input` and `other`.  
`fmax` | Computes the element-wise maximum of `input` and `other`.  
`fmin` | Computes the element-wise minimum of `input` and `other`.  
`not_equal` | Alias for `torch.ne()`.  
The available inplace binary operators are all of the above **except** :
`logaddexp` | Logarithm of the sum of exponentiations of the inputs.  
---|---  
`logaddexp2` | Logarithm of the sum of exponentiations of the inputs in base-2.  
`equal` | `True` if two tensors have the same size and elements, `False` otherwise.  
`fmin` | Computes the element-wise minimum of `input` and `other`.  
`minimum` | Computes the element-wise minimum of `input` and `other`.  
`fmax` | Computes the element-wise maximum of `input` and `other`.  
### Reductions
The following reductions are available (with autograd support). For more information, the Overview tutorial details some examples of reductions, while the Advanced semantics tutorial has some further in-depth discussions about how we decided on certain reduction semantics.
`sum` | Returns the sum of all elements in the `input` tensor.  
---|---  
`mean` |   
`amin` | Returns the minimum value of each slice of the `input` tensor in the given dimension(s) `dim`.  
`amax` | Returns the maximum value of each slice of the `input` tensor in the given dimension(s) `dim`.  
`argmin` | Returns the indices of the minimum value(s) of the flattened tensor or along a dimension  
`argmax` | Returns the indices of the maximum value of all elements in the `input` tensor.  
`prod` | Returns the product of all elements in the `input` tensor.  
`all` | Tests if all elements in `input` evaluate to True.  
`norm` | Returns the matrix norm or vector norm of a given tensor.  
`var` | Calculates the variance over the dimensions specified by `dim`.  
`std` | Calculates the standard deviation over the dimensions specified by `dim`.  
### View and select functions
We’ve included a number of view and select functions as well; intuitively, these operators will apply to both the data and the mask and then wrap the result in a `MaskedTensor`. For a quick example, consider `select()`:
```
>>> data = torch.arange(12, dtype=torch.float).reshape(3, 4)
>>> data
tensor([[ 0., 1., 2., 3.],
    [ 4., 5., 6., 7.],
    [ 8., 9., 10., 11.]])
>>> mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]])
>>> mt = masked_tensor(data, mask)
>>> data.select(0, 1)
tensor([4., 5., 6., 7.])
>>> mask.select(0, 1)
tensor([False, True, False, False])
>>> mt.select(0, 1)
MaskedTensor(
 [   --,  5.0000,    --,    --]
)

```
Copy to clipboard
The following ops are currently supported:
`atleast_1d` | Returns a 1-dimensional view of each input tensor with zero dimensions.  
---|---  
`broadcast_tensors` | Broadcasts the given tensors according to Broadcasting semantics.  
`broadcast_to` | Broadcasts `input` to the shape `shape`.  
`cat` | Concatenates the given sequence of tensors in `tensors` in the given dimension.  
`chunk` | Attempts to split a tensor into the specified number of chunks.  
`column_stack` | Creates a new tensor by horizontally stacking the tensors in `tensors`.  
`dsplit` | Splits `input`, a tensor with three or more dimensions, into multiple tensors depthwise according to `indices_or_sections`.  
`flatten` | Flattens `input` by reshaping it into a one-dimensional tensor.  
`hsplit` | Splits `input`, a tensor with one or more dimensions, into multiple tensors horizontally according to `indices_or_sections`.  
`hstack` | Stack tensors in sequence horizontally (column wise).  
`kron` | Computes the Kronecker product, denoted by ⊗\otimes⊗, of `input` and `other`.  
`meshgrid` | Creates grids of coordinates specified by the 1D inputs in attr:tensors.  
`narrow` | Returns a new tensor that is a narrowed version of `input` tensor.  
`nn.functional.unfold` | Extract sliding local blocks from a batched input tensor.  
`ravel` | Return a contiguous flattened tensor.  
`select` | Slices the `input` tensor along the selected dimension at the given index.  
`split` | Splits the tensor into chunks.  
`stack` | Concatenates a sequence of tensors along a new dimension.  
`t` | Expects `input` to be <= 2-D tensor and transposes dimensions 0 and 1.  
`transpose` | Returns a tensor that is a transposed version of `input`.  
`vsplit` | Splits `input`, a tensor with two or more dimensions, into multiple tensors vertically according to `indices_or_sections`.  
`vstack` | Stack tensors in sequence vertically (row wise).  
`Tensor.expand` | Returns a new view of the `self` tensor with singleton dimensions expanded to a larger size.  
`Tensor.expand_as` | Expand this tensor to the same size as `other`.  
`Tensor.reshape` | Returns a tensor with the same data and number of elements as `self` but with the specified shape.  
`Tensor.reshape_as` | Returns this tensor as the same shape as `other`.  
`Tensor.unfold` | Returns a view of the original tensor which contains all slices of size `size` from `self` tensor in the dimension `dimension`.  
`Tensor.view` | Returns a new tensor with the same data as the `self` tensor but of a different `shape`.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.masked
    * Introduction
      * Motivation
      * What is a MaskedTensor?
    * Supported Operators
      * Unary Operators
      * Binary Operators
      * Reductions
      * View and select functions


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Meta device
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Meta device
The “meta” device is an abstract device which denotes a tensor which records only metadata, but no actual data. Meta tensors have two primary use cases:
  * Models can be loaded on the meta device, allowing you to load a representation of the model without actually loading the actual parameters into memory. This can be helpful if you need to make transformations on the model before you load the actual data.
  * Most operations can be performed on meta tensors, producing new meta tensors that describe what the result would have been if you performed the operation on a real tensor. You can use this to perform abstract analysis without needing to spend time on compute or space to represent the actual tensors. Because meta tensors do not have real data, you cannot perform data-dependent operations like `torch.nonzero()` or `item()`. In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we typically prefer representing the CUDA behavior faithfully in this situation.


Warning
Although in principle meta tensor computation should always be faster than an equivalent CPU/CUDA computation, many meta tensor implementations are implemented in Python and have not been ported to C++ for speed, so you may find that you get lower absolute framework latency with small CPU tensors.
## Idioms for working with meta tensors
An object can be loaded with `torch.load()` onto meta device by specifying `map_location='meta'`:
```
>>> torch.save(torch.randn(2), 'foo.pt')
>>> torch.load('foo.pt', map_location='meta')
tensor(..., device='meta', size=(2,))

```
Copy to clipboard
If you have some arbitrary code which performs some tensor construction without explicitly specifying a device, you can override it to instead construct on meta device by using the `torch.device()` context manager:
```
>>> with torch.device('meta'):
...   print(torch.randn(30, 30))
...
tensor(..., device='meta', size=(30, 30))

```
Copy to clipboard
This is especially helpful NN module construction, where you often are not able to explicitly pass in a device for initialization:
```
>>> from torch.nn.modules import Linear
>>> with torch.device('meta'):
...   print(Linear(20, 30))
...
Linear(in_features=20, out_features=30, bias=True)

```
Copy to clipboard
You cannot convert a meta tensor directly to a CPU/CUDA tensor, because the meta tensor stores no data and we do not know what the correct data values for your new tensor are:
```
>>> torch.ones(5, device='meta').to("cpu")
Traceback (most recent call last):
 File "<stdin>", line 1, in <module>
NotImplementedError: Cannot copy out of meta tensor; no data!

```
Copy to clipboard
Use a factory function like `torch.empty_like()` to explicitly specify how you would like the missing data to be filled in.
NN modules have a convenience method `torch.nn.Module.to_empty()` that allow you to the module to another device, leaving all parameters uninitialized. You are expected to explicitly reinitialize the parameters manually:
```
>>> from torch.nn.modules import Linear
>>> with torch.device('meta'):
...   m = Linear(20, 30)
>>> m.to_empty(device="cpu")
Linear(in_features=20, out_features=30, bias=True)

```
Copy to clipboard
`torch._subclasses.meta_utils` contains undocumented utilities for taking an arbitrary Tensor and constructing an equivalent meta Tensor with high fidelity. These APIs are experimental and may be changed in a BC breaking way at any time.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Meta device
    * Idioms for working with meta tensors


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.mtia
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.mtia
The MTIA backend is implemented out of the tree, only interfaces are be defined here.
This package enables an interface for accessing MTIA backend in python
`StreamContext` | Context-manager that selects a given stream.  
---|---  
`current_device` | Return the index of a currently selected device.  
`current_stream` | Return the currently selected `Stream` for a given device.  
`default_stream` | Return the default `Stream` for a given device.  
`device_count` | Return the number of MTIA devices available.  
`init` |   
`is_available` | Return true if MTIA device is available  
`is_initialized` | Return whether PyTorch's MTIA state has been initialized.  
`memory_stats` | Return a dictionary of MTIA memory allocator statistics for a given device.  
`get_device_capability` | Return capability of a given device as a tuple of (major version, minor version).  
`empty_cache` | Empty the MTIA device cache.  
`record_memory_history` | Enable/Disable the memory profiler on MTIA allocator  
`snapshot` | Return a dictionary of MTIA memory allocator history  
`set_device` | Set the current device.  
`set_stream` | Set the current stream.This is a wrapper API to set the stream.  
`stream` | Wrap around the Context-manager StreamContext that selects a given stream.  
`synchronize` | Waits for all jobs in all streams on a MTIA device to complete.  
`device` | Context-manager that changes the selected device.  
`set_rng_state` | Sets the random number generator state.  
`get_rng_state` | Returns the random number generator state as a ByteTensor.  
`DeferredMtiaCallError` |   
## Streams and events
`Event` | Query and record Stream status to identify or control dependencies across Stream and measure timing.  
---|---  
`Stream` | An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.mtia
    * Streams and events


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.mps
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.mps
This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apple’s API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See https://developer.apple.com/documentation/metalperformanceshaders for more details.
`device_count` | Returns the number of available MPS devices.  
---|---  
`synchronize` | Waits for all kernels in all streams on a MPS device to complete.  
`get_rng_state` | Returns the random number generator state as a ByteTensor.  
`set_rng_state` | Sets the random number generator state.  
`manual_seed` | Sets the seed for generating random numbers.  
`seed` | Sets the seed for generating random numbers to a random number.  
`empty_cache` | Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.  
`set_per_process_memory_fraction` | Set memory fraction for limiting process's memory allocation on MPS device.  
`current_allocated_memory` | Returns the current GPU memory occupied by tensors in bytes.  
`driver_allocated_memory` | Returns total GPU memory allocated by Metal driver for the process in bytes.  
`recommended_max_memory` | Returns recommended max Working set size for GPU memory in bytes.  
`compile_shader` | Compiles compute shader from source and allows one to invoke kernels defined there from the comfort of Python runtime Example.  
## MPS Profiler
`profiler.start` | Start OS Signpost tracing from MPS backend.  
---|---  
`profiler.stop` | Stops generating OS Signpost tracing from MPS backend.  
`profiler.profile` | Context Manager to enabling generating OS Signpost tracing from MPS backend.  
`profiler.is_capturing_metal` | Cheks if metal capture is in progress  
`profiler.is_metal_capture_enabled` | Checks if metal_capture context manager is usable To enable metal capture, set MTL_CAPTURE_ENABLED envvar  
`profiler.metal_capture` | Conext manager that enables capturing of Metal calls into gputrace  
## MPS Event
`event.Event` | Wrapper around an MPS event.  
---|---  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.mps
    * MPS Profiler
    * MPS Event


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.monitor
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.monitor
Warning
This module is a prototype release, and its interfaces and functionality may change without warning in future PyTorch releases.
`torch.monitor` provides an interface for logging events and counters from PyTorch.
The stat interfaces are designed to be used for tracking high level metrics that are periodically logged out to be used for monitoring system performance. Since the stats aggregate with a specific window size you can log to them from critical loops with minimal performance impact.
For more infrequent events or values such as loss, accuracy, usage tracking the event interface can be directly used.
Event handlers can be registered to handle the events and pass them to an external event sink.
## API Reference 

_class_ torch.monitor.Aggregation
    
> These are types of aggregations that can be used to accumulate stats.
Members:
> 

VALUE :
    
> VALUE returns the last value to be added. 

MEAN :
    
> MEAN computes the arithmetic mean of all the added values. 

COUNT :
    
> COUNT returns the total number of added values. 

SUM :
    
> SUM returns the sum of the added values. 

MAX :
    
> MAX returns the max of the added values. 

MIN :
    
> MIN returns the min of the added values. 

_property_ name


_class_ torch.monitor.Stat
    
Stat is used to compute summary statistics in a performant way over fixed intervals. Stat logs the statistics as an Event once every `window_size` duration. When the window closes the stats are logged via the event handlers as a `torch.monitor.Stat` event.
`window_size` should be set to something relatively high to avoid a huge number of events being logged. Ex: 60s. Stat uses millisecond precision.
If `max_samples` is set, the stat will cap the number of samples per window by discarding add calls once `max_samples` adds have occurred. If it’s not set, all `add` calls during the window will be included. This is an optional field to make aggregations more directly comparable across windows when the number of samples might vary.
When the Stat is destructed it will log any remaining data even if the window hasn’t elapsed. 

__init__(_self :torch._C._monitor.Stat_, _name :str_, _aggregations :list[torch._C._monitor.Aggregation]_, _window_size :datetime.timedelta_, _max_samples :int=9223372036854775807_) → None
    
Constructs the `Stat`. 

add(_self :torch._C._monitor.Stat_, _v :float_) → None
    
Adds a value to the stat to be aggregated according to the configured stat type and aggregations. 

_property_ count
    
Number of data points that have currently been collected. Resets once the event has been logged. 

get(_self :torch._C._monitor.Stat_) → dict[torch._C._monitor.Aggregation,float]
    
Returns the current value of the stat, primarily for testing purposes. If the stat has logged and no additional values have been added this will be zero. 

_property_ name
    
The name of the stat that was set during creation. 

_class_ torch.monitor.data_value_t
    
data_value_t is one of `str`, `float`, `int`, `bool`. 

_class_ torch.monitor.Event
    
Event represents a specific typed event to be logged. This can represent high-level data points such as loss or accuracy per epoch or more low-level aggregations such as through the Stats provided through this library.
All Events of the same type should have the same name so downstream handlers can correctly process them. 

__init__(_self :torch._C._monitor.Event_, _name :str_, _timestamp :datetime.datetime_, _data :dict[str,data_value_t]_) → None
    
Constructs the `Event`. 

_property_ data
    
The structured data contained within the `Event`. 

_property_ name
    
The name of the `Event`. 

_property_ timestamp
    
The timestamp when the `Event` happened. 

_class_ torch.monitor.EventHandlerHandle
    
EventHandlerHandle is a wrapper type returned by `register_event_handler` used to unregister the handler via `unregister_event_handler`. This cannot be directly initialized. 

torch.monitor.log_event(_event :torch._C._monitor.Event_) → None
    
log_event logs the specified event to all of the registered event handlers. It’s up to the event handlers to log the event out to the corresponding event sink.
If there are no event handlers registered this method is a no-op. 

torch.monitor.register_event_handler(_callback :Callable[[torch._C._monitor.Event],None]_) → torch._C._monitor.EventHandlerHandle
    
register_event_handler registers a callback to be called whenever an event is logged via `log_event`. These handlers should avoid blocking the main thread since that may interfere with training as they run during the `log_event` call. 

torch.monitor.unregister_event_handler(_handler :torch._C._monitor.EventHandlerHandle_) → None
    
unregister_event_handler unregisters the `EventHandlerHandle` returned after calling `register_event_handler`. After this returns the event handler will no longer receive events. 

_class_ torch.monitor.TensorboardEventHandler(_writer_)[source][source]
    
TensorboardEventHandler is an event handler that will write known events to the provided SummaryWriter.
This currently only supports `torch.monitor.Stat` events which are logged as scalars.
Example
```
>>> from torch.utils.tensorboard import SummaryWriter
>>> from torch.monitor import TensorboardEventHandler, register_event_handler
>>> writer = SummaryWriter("log_dir")
>>> register_event_handler(TensorboardEventHandler(writer))

```
Copy to clipboard 

__init__(_writer_)[source][source]
    
Constructs the `TensorboardEventHandler`.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.monitor
    * API Reference
      * `Aggregation`
        * `Aggregation.name`
      * `Stat`
        * `Stat.__init__()`
        * `Stat.add()`
        * `Stat.count`
        * `Stat.get()`
        * `Stat.name`
      * `data_value_t`
      * `Event`
        * `Event.__init__()`
        * `Event.data`
        * `Event.name`
        * `Event.timestamp`
      * `EventHandlerHandle`
      * `log_event()`
      * `register_event_handler()`
      * `unregister_event_handler()`
      * `TensorboardEventHandler`
        * `TensorboardEventHandler.__init__()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.model_zoo
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.model_zoo
Moved to torch.hub. 

torch.utils.model_zoo.load_url(_url_ , _model_dir =None_, _map_location =None_, _progress =True_, _check_hash =False_, _file_name =None_, _weights_only =False_)[source]
    
Loads the Torch serialized object at the given URL.
If downloaded file is a zip file, it will be automatically decompressed.
If the object is already present in model_dir, it’s deserialized and returned. The default value of `model_dir` is `<hub_dir>/checkpoints` where `hub_dir` is the directory returned by `get_dir()`. 

Parameters
    
  * **url** (_str_) – URL of the object to download
  * **model_dir** (_str_ _,__optional_) – directory in which to save the object
  * **map_location** (_optional_) – a function or a dict specifying how to remap storage locations (see torch.load)
  * **progress** (_bool_ _,__optional_) – whether or not to display a progress bar to stderr. Default: True
  * **check_hash** (_bool_ _,__optional_) – If True, the filename part of the URL should follow the naming convention `filename-<sha256>.ext` where `<sha256>` is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False
  * **file_name** (_str_ _,__optional_) – name for the downloaded file. Filename from `url` will be used if not set.
  * **weights_only** (_bool_ _,__optional_) – If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See `load()` for more details.



Return type
    
dict[str, _Any_]
Example
```
>>> state_dict = torch.hub.load_state_dict_from_url(
...   "https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth"
... )

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.model_zoo
    * `load_url()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.mobile_optimizer
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.mobile_optimizer
Warning
PyTorch Mobile is no longer actively supported. Please check out ExecuTorch, PyTorch’s all-new on-device inference library. You can also review documentation on XNNPACK and Vulkan delegates.
Torch mobile supports `torch.utils.mobile_optimizer.optimize_for_mobile` utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blocklisting optimization set, a preserved method list, and a backend. 

For CPU Backend, by default, if optimization blocklist is None or empty, `optimize_for_mobile` will run the following optimizations:
    
  * **Conv2D + BatchNorm fusion** (blocklisting option mobile_optimizer.MobileOptimizerType.CONV_BN_FUSION): This optimization pass folds `Conv2d-BatchNorm2d` into `Conv2d` in `forward` method of this module and all its submodules. The weight and bias of the `Conv2d` are correspondingly updated.
  * **Insert and Fold prepacked ops** (blocklisting option mobile_optimizer.MobileOptimizerType.INSERT_FOLD_PREPACK_OPS): This optimization pass rewrites the graph to replace 2D convolutions and linear ops with their prepacked counterparts. Prepacked ops are stateful ops in that, they require some state to be created, such as weight prepacking and use this state, i.e. prepacked weights, during op execution. XNNPACK is one such backend that provides prepacked ops, with kernels optimized for mobile platforms (such as ARM CPUs). Prepacking of weight enables efficient memory access and thus faster kernel execution. At the moment `optimize_for_mobile` pass rewrites the graph to replace `Conv2D/Linear` with 1) op that pre-packs weight for XNNPACK conv2d/linear ops and 2) op that takes pre-packed weight and activation as input and generates output activations. Since 1 needs to be done only once, we fold the weight pre-packing such that it is done only once at model load time. This pass of the `optimize_for_mobile` does 1 and 2 and then folds, i.e. removes, weight pre-packing ops.
  * **ReLU/Hardtanh fusion** : XNNPACK ops support fusion of clamping. That is clamping of output activation is done as part of the kernel, including for 2D convolution and linear op kernels. Thus clamping effectively comes for free. Thus any op that can be expressed as clamping op, such as `ReLU` or `hardtanh`, can be fused with previous `Conv2D` or `linear` op in XNNPACK. This pass rewrites graph by finding `ReLU/hardtanh` ops that follow XNNPACK `Conv2D/linear` ops, written by the previous pass, and fuses them together.
  * **Dropout removal** (blocklisting option mobile_optimizer.MobileOptimizerType.REMOVE_DROPOUT): This optimization pass removes `dropout` and `dropout_` nodes from this module when training is false.
  * **Conv packed params hoisting** (blocklisting option mobile_optimizer.MobileOptimizerType.HOIST_CONV_PACKED_PARAMS): This optimization pass moves convolution packed params to the root module, so that the convolution structs can be deleted. This decreases model size without impacting numerics.
  * **Add/ReLU fusion** (blocklisting option mobile_optimizer.MobileOptimizerType.FUSE_ADD_RELU): This pass finds instances of `relu` ops that follow `add` ops and fuses them into a single `add_relu`.



for Vulkan Backend, by default, if optimization blocklist is None or empty, `optimize_for_mobile` will run the following optimization:
    
  * **Automatic GPU Transfer** (blocklisting option mobile_optimizer.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER): This optimization pass rewrites the graph so that moving input and output data to and from the GPU becomes part of the model.


`optimize_for_mobile` will also invoke freeze_module pass which only preserves `forward` method. If you have other method to that needed to be preserved, add them into the preserved method list and pass into the method. 

torch.utils.mobile_optimizer.optimize_for_mobile(_script_module_ , _optimization_blocklist =None_, _preserved_methods =None_, _backend ='CPU'_)[source][source]
    
Optimize a torch script module for mobile deployment. 

Parameters
    
  * **script_module** (_ScriptModule_) – An instance of torch script module with type of ScriptModule.
  * **optimization_blocklist** (_Optional_ _[__set_ _[__torch._C._MobileOptimizerType_ _]__]_) – A set with type of MobileOptimizerType. When set is not passed, optimization method will run all the optimizer pass; otherwise, optimizer method will run the optimization pass that is not included inside optimization_blocklist.
  * **preserved_methods** (_Optional_ _[__list_ _[__~AnyStr_ _]__]_) – A list of methods that needed to be preserved when freeze_module pass is invoked
  * **backend** (_str_) – Device type to use for running the result model (‘CPU’(default), ‘Vulkan’ or ‘Metal’).



Returns
    
A new optimized torch script module 

Return type
    
_RecursiveScriptModule_
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.mobile_optimizer
    * `optimize_for_mobile()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.mtia.memory
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.mtia.memory
The MTIA backend is implemented out of the tree, only interfaces are be defined here.
This package adds support for device memory management implemented in MTIA.
`memory_stats` | Return a dictionary of MTIA memory allocator statistics for a given device.  
---|---  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.mtia.memory


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Named Tensors
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Named Tensors
Named Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support “broadcasting by name” rather than “broadcasting by position”.
Warning
The named tensor API is a prototype feature and subject to change.
## Creating named tensors
Factory functions now take a new `names` argument that associates a name with each dimension.
```
>>> torch.zeros(2, 3, names=('N', 'C'))
tensor([[0., 0., 0.],
    [0., 0., 0.]], names=('N', 'C'))

```
Copy to clipboard
Named dimensions, like regular Tensor dimensions, are ordered. `tensor.names[i]` is the name of dimension `i` of `tensor`.
The following factory functions support named tensors:
  * `torch.empty()`
  * `torch.rand()`
  * `torch.randn()`
  * `torch.ones()`
  * `torch.tensor()`
  * `torch.zeros()`


## Named dimensions
See `names` for restrictions on tensor names.
Use `names` to access the dimension names of a tensor and `rename()` to rename named dimensions.
```
>>> imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))
>>> imgs.names
('N', 'C', 'H', 'W')
>>> renamed_imgs = imgs.rename(H='height', W='width')
>>> renamed_imgs.names
('N', 'C', 'height', 'width)

```
Copy to clipboard
Named tensors can coexist with unnamed tensors; named tensors are instances of `torch.Tensor`. Unnamed tensors have `None`-named dimensions. Named tensors do not require all dimensions to be named.
```
>>> imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))
>>> imgs.names
(None, 'C', 'H', 'W')

```
Copy to clipboard
## Name propagation semantics
Named tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called _name inference_. More formally, name inference consists of the following two steps:
  * **Check names** : an operator may perform automatic checks at runtime that check that certain dimension names must match.
  * **Propagate names** : name inference propagates names to output tensors.


All operations that support named tensors propagate names.
```
>>> x = torch.randn(3, 3, names=('N', 'C'))
>>> x.abs().names
('N', 'C')

```
Copy to clipboard
### match semantics
Two names _match_ if they are equal (string equality) or if at least one is `None`. Nones are essentially a special “wildcard” name.
`unify(A, B)` determines which of the names `A` and `B` to propagate to the outputs. It returns the more _specific_ of the two names, if they match. If the names do not match, then it errors.
Note
In practice, when working with named tensors, one should avoid having unnamed dimensions because their handling can be complicated. It is recommended to lift all unnamed dimensions to be named dimensions by using `refine_names()`.
### Basic name inference rules
Let’s see how `match` and `unify` are used in name inference in the case of adding two one-dim tensors with no broadcasting.
```
x = torch.randn(3, names=('X',))
y = torch.randn(3)
z = torch.randn(3, names=('Z',))

```
Copy to clipboard
**Check names** : check that the names of the two tensors _match_.
For the following examples:
```
>>> # x + y # match('X', None) is True
>>> # x + z # match('X', 'Z') is False
>>> # x + x # match('X', 'X') is True
>>> x + z
Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match.

```
Copy to clipboard
**Propagate names** : _unify_ the names to select which one to propagate. In the case of `x + y`, `unify('X', None) = 'X'` because `'X'` is more specific than `None`.
```
>>> (x + y).names
('X',)
>>> (x + x).names
('X',)

```
Copy to clipboard
For a comprehensive list of name inference rules, see Named Tensors operator coverage. Here are two common operations that may be useful to go over:
  * Binary arithmetic ops: Unifies names from inputs
  * Matrix multiplication ops: Contracts away dims


## Explicit alignment by names
Use `align_as()` or `align_to()` to align tensor dimensions by name to a specified ordering. This is useful for performing “broadcasting by names”.
```
# This function is agnostic to the dimension ordering of `input`,
# as long as it has a `C` dimension somewhere.
def scale_channels(input, scale):
  scale = scale.refine_names('C')
  return input * scale.align_as(input)
>>> num_channels = 3
>>> scale = torch.randn(num_channels, names=('C',))
>>> imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))
>>> more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))
>>> videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D')
>>> scale_channels(imgs, scale)
>>> scale_channels(more_imgs, scale)
>>> scale_channels(videos, scale)

```
Copy to clipboard
## Manipulating dimensions
Use `align_to()` to permute large amounts of dimensions without mentioning all of them as in required by `permute()`.
```
>>> tensor = torch.randn(2, 2, 2, 2, 2, 2)
>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')
# Move the F (dim 5) and E dimension (dim 4) to the front while keeping
# the rest in the same order
>>> tensor.permute(5, 4, 0, 1, 2, 3)
>>> named_tensor.align_to('F', 'E', ...)

```
Copy to clipboard
Use `flatten()` and `unflatten()` to flatten and unflatten dimensions, respectively. These methods are more verbose than `view()` and `reshape()`, but have more semantic meaning to someone reading the code.
```
>>> imgs = torch.randn(32, 3, 128, 128)
>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')
>>> flat_imgs = imgs.view(32, -1)
>>> named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')
>>> named_flat_imgs.names
('N', 'features')
>>> unflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)])
>>> unflattened_named_imgs.names
('N', 'C', 'H', 'W')

```
Copy to clipboard
## Autograd support
Autograd currently supports named tensors in a limited manner: autograd ignores names on all tensors. Gradient computation is still correct but we lose the safety that names give us.
```
>>> x = torch.randn(3, names=('D',))
>>> weight = torch.randn(3, names=('D',), requires_grad=True)
>>> loss = (x - weight).abs()
>>> grad_loss = torch.randn(3)
>>> loss.backward(grad_loss)
>>> weight.grad # Unnamed for now. Will be named in the future
tensor([-1.8107, -0.6357, 0.0783])
>>> weight.grad.zero_()
>>> grad_loss = grad_loss.refine_names('C')
>>> loss = (x - weight).abs()
# Ideally we'd check that the names of loss and grad_loss match but we don't yet.
>>> loss.backward(grad_loss)
>>> weight.grad
tensor([-1.8107, -0.6357, 0.0783])

```
Copy to clipboard
## Currently supported operations and subsystems
### Operators
See Named Tensors operator coverage for a full list of the supported torch and tensor operations. We do not yet support the following that is not covered by the link:
  * indexing, advanced indexing.


For `torch.nn.functional` operators, we support the following:
  * `torch.nn.functional.relu()`
  * `torch.nn.functional.softmax()`
  * `torch.nn.functional.log_softmax()`
  * `torch.nn.functional.tanh()`
  * `torch.nn.functional.sigmoid()`
  * `torch.nn.functional.dropout()`


### Subsystems
Autograd is supported, see Autograd support. Because gradients are currently unnamed, optimizers may work but are untested.
NN modules are currently unsupported. This can lead to the following when calling modules with named tensor inputs:
  * NN module parameters are unnamed, so outputs may be partially named.
  * NN module forward passes have code that don’t support named tensors and will error out appropriately.


We also do not support the following subsystems, though some may work out of the box:
  * distributions
  * serialization (`torch.load()`, `torch.save()`)
  * multiprocessing
  * JIT
  * distributed
  * ONNX


If any of these would help your use case, please search if an issue has already been filed and if not, file one.
## Named tensor API reference
In this section please find the documentation for named tensor specific APIs. For a comprehensive reference for how names are propagated through other PyTorch operators, see Named Tensors operator coverage. 

_class_ torch.Tensor
     

names
    
Stores names for each of this tensor’s dimensions.
`names[idx]` corresponds to the name of tensor dimension `idx`. Names are either a string if the dimension is named or `None` if the dimension is unnamed.
Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).
Tensors may not have two named dimensions with the same name.
Warning
The named tensor API is experimental and subject to change. 

rename(_* names_, _** rename_map_)[source][source]
    
Renames dimension names of `self`.
There are two main usages:
`self.rename(**rename_map)` returns a view on tensor that has dims renamed as specified in the mapping `rename_map`.
`self.rename(*names)` returns a view on tensor, renaming all dimensions positionally using `names`. Use `self.rename(None)` to drop names on a tensor.
One cannot specify both positional args `names` and keyword args `rename_map`.
Examples:
```
>>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))
>>> renamed_imgs = imgs.rename(N='batch', C='channels')
>>> renamed_imgs.names
('batch', 'channels', 'H', 'W')
>>> renamed_imgs = imgs.rename(None)
>>> renamed_imgs.names
(None, None, None, None)
>>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')
>>> renamed_imgs.names
('batch', 'channel', 'height', 'width')

```
Copy to clipboard
Warning
The named tensor API is experimental and subject to change. 

rename_(_* names_, _** rename_map_)[source][source]
    
In-place version of `rename()`. 

refine_names(_* names_)[source][source]
    
Refines the dimension names of `self` according to `names`.
Refining is a special case of renaming that “lifts” unnamed dimensions. A `None` dim can be refined to have any name; a named dim can only be refined to have the same name.
Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.
`names` may contain up to one Ellipsis (`...`). The Ellipsis is expanded greedily; it is expanded in-place to fill `names` to the same length as `self.dim()` using names from the corresponding indices of `self.names`.
Python 2 does not support Ellipsis but one may use a string literal instead (`'...'`). 

Parameters
    
**names** (_iterable_ _of_ _str_) – The desired names of the output tensor. May contain up to one Ellipsis.
Examples:
```
>>> imgs = torch.randn(32, 3, 128, 128)
>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')
>>> named_imgs.names
('N', 'C', 'H', 'W')
>>> tensor = torch.randn(2, 3, 5, 7, 11)
>>> tensor = tensor.refine_names('A', ..., 'B', 'C')
>>> tensor.names
('A', None, None, 'B', 'C')

```
Copy to clipboard
Warning
The named tensor API is experimental and subject to change. 

align_as(_other_) → Tensor
    
Permutes the dimensions of the `self` tensor to match the dimension order in the `other` tensor, adding size-one dims for any new names.
This operation is useful for explicit broadcasting by names (see examples).
All of the dims of `self` must be named in order to use this method. The resulting tensor is a view on the original tensor.
All dimension names of `self` must be present in `other.names`. `other` may contain named dimensions that are not in `self.names`; the output tensor has a size-one dimension for each of those new names.
To align a tensor to a specific order, use `align_to()`.
Examples:
```
# Example 1: Applying a mask
>>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')
>>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))
>>> imgs.masked_fill_(mask.align_as(imgs), 0)

# Example 2: Applying a per-channel-scale
>>> def scale_channels(input, scale):
>>>  scale = scale.refine_names('C')
>>>  return input * scale.align_as(input)
>>> num_channels = 3
>>> scale = torch.randn(num_channels, names=('C',))
>>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))
>>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))
>>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))
# scale_channels is agnostic to the dimension order of the input
>>> scale_channels(imgs, scale)
>>> scale_channels(more_imgs, scale)
>>> scale_channels(videos, scale)

```
Copy to clipboard
Warning
The named tensor API is experimental and subject to change. 

align_to(_* names_)[source][source]
    
Permutes the dimensions of the `self` tensor to match the order specified in `names`, adding size-one dims for any new names.
All of the dims of `self` must be named in order to use this method. The resulting tensor is a view on the original tensor.
All dimension names of `self` must be present in `names`. `names` may contain additional names that are not in `self.names`; the output tensor has a size-one dimension for each of those new names.
`names` may contain up to one Ellipsis (`...`). The Ellipsis is expanded to be equal to all dimension names of `self` that are not mentioned in `names`, in the order that they appear in `self`.
Python 2 does not support Ellipsis but one may use a string literal instead (`'...'`). 

Parameters
    
**names** (_iterable_ _of_ _str_) – The desired dimension ordering of the output tensor. May contain up to one Ellipsis that is expanded to all unmentioned dim names of `self`.
Examples:
```
>>> tensor = torch.randn(2, 2, 2, 2, 2, 2)
>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')
# Move the F and E dims to the front while keeping the rest in order
>>> named_tensor.align_to('F', 'E', ...)

```
Copy to clipboard
Warning
The named tensor API is experimental and subject to change. 

flatten(_dims_ , _out_dim_) → Tensor
    
Flattens `dims` into a single dimension with name `out_dim`.
All of dims must be consecutive in order in the `self` tensor, but not necessary contiguous in memory.
Examples:
```
>>> imgs = torch.randn(32, 3, 128, 128, names=('N', 'C', 'H', 'W'))
>>> flat_imgs = imgs.flatten(['C', 'H', 'W'], 'features')
>>> flat_imgs.names, flat_imgs.shape
(('N', 'features'), torch.Size([32, 49152]))

```
Copy to clipboard
Warning
The named tensor API is experimental and subject to change.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Named Tensors
    * Creating named tensors
    * Named dimensions
    * Name propagation semantics
      * match semantics
      * Basic name inference rules
    * Explicit alignment by names
    * Manipulating dimensions
    * Autograd support
    * Currently supported operations and subsystems
      * Operators
      * Subsystems
    * Named tensor API reference
      * `Tensor.names`
      * `Tensor.rename()`
      * `Tensor.rename_()`
      * `Tensor.refine_names()`
      * `Tensor.align_as()`
      * `Tensor.align_to()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.nn.attention
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.nn.attention
This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention
## Utils
`sdpa_kernel` | Context manager to select which backend to use for scaled dot product attention.  
---|---  
`SDPBackend` | An enum-like class that contains the different backends for scaled dot product attention.  
## Submodules
`flex_attention` | This module implements the user facing API for flex_attention in PyTorch.  
---|---  
`bias` | Defines bias subclasses that work with scaled_dot_product_attention  
`experimental` |   
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.nn.attention
    * Utils
    * Submodules


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.nested
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.nested
## Introduction
Warning
The PyTorch API of nested tensors is in prototype stage and will change in the near future.
Nested tensors allow for ragged-shaped data to be contained within and operated upon as a single tensor. Such data is stored underneath in an efficient packed representation, while exposing a standard PyTorch tensor interface for applying operations.
A common application of nested tensors is for expressing batches of variable-length sequential data present in various domains, such as varying sentence lengths, image sizes, and audio / video clip lengths. Traditionally, such data has been handled by padding sequences to that of the max length within a batch, performing computation on the padded form, and subsequently masking to remove padding. This is inefficient and error-prone, and nested tensors exist to address these problems.
The API for calling operations on a nested tensor is no different from that of a regular `torch.Tensor`, allowing for seamless integration with existing models, with the main difference being construction of the inputs.
As this is a prototype feature, the set of operations supported is limited, but growing. We welcome issues, feature requests, and contributions. More information on contributing can be found in this Readme.
## Construction
Note
There are two forms of nested tensors present within PyTorch, distinguished by layout as specified during construction. Layout can be one of `torch.strided` or `torch.jagged`. We recommend utilizing the `torch.jagged` layout whenever possible. While it currently only supports a single ragged dimension, it has better op coverage, receives active development, and integrates well with `torch.compile`. These docs adhere to this recommendation and refer to nested tensors with the `torch.jagged` layout as “NJTs” for brevity throughout.
Construction is straightforward and involves passing a list of tensors to the `torch.nested.nested_tensor` constructor. A nested tensor with the `torch.jagged` layout (AKA an “NJT”) supports a single ragged dimension. This constructor will copy the input tensors into a packed, contiguous block of memory according to the layout described in the data_layout section below.
```
>>> a, b = torch.arange(3), torch.arange(5) + 3
>>> a
tensor([0, 1, 2])
>>> b
tensor([3, 4, 5, 6, 7])
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> print([component for component in nt])
[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]

```
Copy to clipboard
Each tensor in the list must have the same number of dimensions, but the shapes can otherwise vary along a single dimension. If the dimensionalities of the input components don’t match, the constructor throws an error.
```
>>> a = torch.randn(50, 128) # 2D tensor
>>> b = torch.randn(2, 50, 128) # 3D tensor
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
...
RuntimeError: When constructing a nested tensor, all tensors in list must have the same dim

```
Copy to clipboard
During construction, dtype, device, and whether gradients are required can be chosen via the usual keyword arguments.
```
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32, device="cuda", requires_grad=True)
>>> print([component for component in nt])
[tensor([0., 1., 2.], device='cuda:0',
    grad_fn=<UnbindBackwardAutogradNestedTensor0>), tensor([3., 4., 5., 6., 7.], device='cuda:0',
    grad_fn=<UnbindBackwardAutogradNestedTensor0>)]

```
Copy to clipboard
`torch.nested.as_nested_tensor` can be used to preserve autograd history from the tensors passed to the constructor. When this constructor is utilized, gradients will flow through the nested tensor back into the original components. Note that this constructor still copies the input components into a packed, contiguous block of memory.
```
>>> a = torch.randn(12, 512, requires_grad=True)
>>> b = torch.randn(23, 512, requires_grad=True)
>>> nt = torch.nested.as_nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt.sum().backward()
>>> a.grad
tensor([[1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    ...,
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.]])
>>> b.grad
tensor([[1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    ...,
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.]])

```
Copy to clipboard
The above functions all create contiguous NJTs, where a chunk of memory is allocated to store a packed form of the underlying components (see the data_layout section below for more details).
It is also possible to create a non-contiguous NJT view over a pre-existing dense tensor with padding, avoiding the memory allocation and copying. `torch.nested.narrow()` is the tool for accomplishing this.
```
>>> padded = torch.randn(3, 5, 4)
>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)
>>> nt = torch.nested.narrow(padded, dim=1, start=0, length=seq_lens, layout=torch.jagged)
>>> nt.shape
torch.Size([3, j1, 4])
>>> nt.is_contiguous()
False

```
Copy to clipboard
Note that the nested tensor acts as a view over the original padded dense tensor, referencing the same memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more limited, so if you run into support gaps, it’s always possible to convert to a contiguous NJT using `contiguous()`.
## Data Layout and Shape
For efficiency, nested tensors generally pack their tensor components into a contiguous chunk of memory and maintain additional metadata to specify batch item boundaries. For the `torch.jagged` layout, the contiguous chunk of memory is stored in the `values` component, with the `offsets` component delineating batch item boundaries for the ragged dimension.
![_images/njt_visual.png](https://pytorch.org/docs/stable/_images/njt_visual.png)
It’s possible to directly access the underlying NJT components when necessary.
```
>>> a = torch.randn(50, 128) # text 1
>>> b = torch.randn(32, 128) # text 2
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt.values().shape # note the "packing" of the ragged dimension; no padding needed
torch.Size([82, 128])
>>> nt.offsets()
tensor([ 0, 50, 82])

```
Copy to clipboard
It can also be useful to construct an NJT from the jagged `values` and `offsets` constituents directly; the `torch.nested.nested_tensor_from_jagged()` constructor serves this purpose.
```
>>> values = torch.randn(82, 128)
>>> offsets = torch.tensor([0, 50, 82], dtype=torch.int64)
>>> nt = torch.nested.nested_tensor_from_jagged(values=values, offsets=offsets)

```
Copy to clipboard
An NJT has a well-defined shape with dimensionality 1 greater than that of its components. The underlying structure of the ragged dimension is represented by a symbolic value (`j1` in the example below).
```
>>> a = torch.randn(50, 128)
>>> b = torch.randn(32, 128)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt.dim()
3
>>> nt.shape
torch.Size([2, j1, 128])

```
Copy to clipboard
NJTs must have the same ragged structure to be compatible with each other. For example, to run a binary operation involving two NJTs, the ragged structures must match (i.e. they must have the same ragged shape symbol in their shapes). In the details, each symbol corresponds with an exact `offsets` tensor, so both NJTs must have the same `offsets` tensor to be compatible with each other.
```
>>> a = torch.randn(50, 128)
>>> b = torch.randn(32, 128)
>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt2 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt1.offsets() is nt2.offsets()
False
>>> nt3 = nt1 + nt2
RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)

```
Copy to clipboard
In the above example, even though the conceptual shapes of the two NJTs are the same, they don’t share a reference to the same `offsets` tensor, so their shapes differ, and they are not compatible. We recognize that this behavior is unintuitive and are working hard to relax this restriction for the beta release of nested tensors. For a workaround, see the Troubleshooting section of this document.
In addition to the `offsets` metadata, NJTs can also compute and cache the minimum and maximum sequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA). There are currently no public APIs for accessing these, but this will change for the beta release.
## Supported Operations
This section contains a list of common operations over nested tensors that you may find useful. It is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While a sizeable subset of these are supported for nested tensors today, full support is a large task. The ideal state for nested tensors is full support of all PyTorch operations that are available for non-nested tensors. To help us accomplish this, please consider:
  * Requesting particular ops needed for your use case here to help us prioritize.
  * Contributing! It’s not too hard to add nested tensor support for a given PyTorch op; see the Contributions section below for details.


### Viewing nested tensor constituents
`unbind()` allows you to retrieve a view of the nested tensor’s constituents.
```
>>> import torch
>>> a = torch.randn(2, 3)
>>> b = torch.randn(3, 3)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> nt.unbind()
(tensor([[-0.9916, -0.3363, -0.2799],
    [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104, 1.4841],
    [ 2.0952, 0.2973, 0.2516],
    [ 0.9035, 1.3623, 0.2026]]))
>>> nt.unbind()[0] is not a
True
>>> nt.unbind()[0].mul_(3)
tensor([[ 3.6858, -3.7030, -4.4525],
    [-2.3481, 2.0236, 0.1975]])
>>> nt.unbind()
(tensor([[-2.9747, -1.0089, -0.8396],
    [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104, 1.4841],
    [ 2.0952, 0.2973, 0.2516],
    [ 0.9035, 1.3623, 0.2026]]))

```
Copy to clipboard
Note that `nt.unbind()[0]` is not a copy, but rather a slice of the underlying memory, which represents the first entry or constituent of the nested tensor.
### Conversions to / from padded
`torch.nested.to_padded_tensor()` converts an NJT to a padded dense tensor with the specified padding value. The ragged dimension will be padded out to the size of the maximum sequence length.
```
>>> import torch
>>> a = torch.randn(2, 3)
>>> b = torch.randn(6, 3)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> padded = torch.nested.to_padded_tensor(nt, padding=4.2)
>>> padded
tensor([[[ 1.6107, 0.5723, 0.3913],
     [ 0.0700, -0.4954, 1.8663],
     [ 4.2000, 4.2000, 4.2000],
     [ 4.2000, 4.2000, 4.2000],
     [ 4.2000, 4.2000, 4.2000],
     [ 4.2000, 4.2000, 4.2000]],
    [[-0.0479, -0.7610, -0.3484],
     [ 1.1345, 1.0556, 0.3634],
     [-1.7122, -0.5921, 0.0540],
     [-0.5506, 0.7608, 2.0606],
     [ 1.5658, -1.1934, 0.3041],
     [ 0.1483, -1.1284, 0.6957]]])

```
Copy to clipboard
This can be useful as an escape hatch to work around NJT support gaps, but ideally such conversions should be avoided when possible for optimal memory usage and performance, as the more efficient nested tensor layout does not materialize padding.
The reverse conversion can be accomplished using `torch.nested.narrow()`, which applies ragged structure to a given dense tensor to produce an NJT. Note that by default, this operation does not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be useful to explicitly call `contiguous()` here if a contiguous NJT is desired.
```
>>> padded = torch.randn(3, 5, 4)
>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)
>>> nt = torch.nested.narrow(padded, dim=1, length=seq_lens, layout=torch.jagged)
>>> nt.shape
torch.Size([3, j1, 4])
>>> nt = nt.contiguous()
>>> nt.shape
torch.Size([3, j2, 4])

```
Copy to clipboard
### Shape manipulations
Nested tensors support a wide array of operations for shape manipulation, including views.
```
>>> a = torch.randn(2, 6)
>>> b = torch.randn(4, 6)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> nt.shape
torch.Size([2, j1, 6])
>>> nt.unsqueeze(-1).shape
torch.Size([2, j1, 6, 1])
>>> nt.unflatten(-1, [2, 3]).shape
torch.Size([2, j1, 2, 3])
>>> torch.cat([nt, nt], dim=2).shape
torch.Size([2, j1, 12])
>>> torch.stack([nt, nt], dim=2).shape
torch.Size([2, j1, 2, 6])
>>> nt.transpose(-1, -2).shape
torch.Size([2, 6, j1])

```
Copy to clipboard
### Attention mechanisms
As variable-length sequences are common inputs to attention mechanisms, nested tensors support important attention operators Scaled Dot Product Attention (SDPA) and FlexAttention. See here for usage examples of NJT with SDPA and here for usage examples of NJT with FlexAttention.
## Usage with torch.compile
NJTs are designed to be used with `torch.compile()` for optimal performance, and we always recommend utilizing `torch.compile()` with NJTs when possible. NJTs work out-of-the-box and graph-break-free both when passed as inputs to a compiled function or module OR when instantiated in-line within the function.
Note
If you’re not able to utilize `torch.compile()` for your use case, performance and memory usage may still benefit from the use of NJTs, but it’s not as clear-cut whether this will be the case. It is important that the tensors being operated on are large enough so the performance gains are not outweighed by the overhead of python tensor subclasses.
```
>>> import torch
>>> a = torch.randn(2, 3)
>>> b = torch.randn(4, 3)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> def f(x): return x.sin() + 1
...
>>> compiled_f = torch.compile(f, fullgraph=True)
>>> output = compiled_f(nt)
>>> output.shape
torch.Size([2, j1, 3])
>>> def g(values, offsets): return torch.nested.nested_tensor_from_jagged(values, offsets) * 2.
...
>>> compiled_g = torch.compile(g, fullgraph=True)
>>> output2 = compiled_g(nt.values(), nt.offsets())
>>> output2.shape
torch.Size([2, j1, 3])

```
Copy to clipboard
Note that NJTs support Dynamic Shapes to avoid unnecessary recompiles with changing ragged structure.
```
>>> a = torch.randn(2, 3)
>>> b = torch.randn(4, 3)
>>> c = torch.randn(5, 3)
>>> d = torch.randn(6, 3)
>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged)
>>> def f(x): return x.sin() + 1
...
>>> compiled_f = torch.compile(f, fullgraph=True)
>>> output1 = compiled_f(nt1)
>>> output2 = compiled_f(nt2) # NB: No recompile needed even though ragged structure differs

```
Copy to clipboard
If you run into problems or arcane errors when utilizing NJT + `torch.compile`, please file a PyTorch issue. Full subclass support within `torch.compile` is a long-term effort and there may be some rough edges at this time.
## Troubleshooting
This section contains common errors that you may run into when utilizing nested tensors, alongside the reason for these errors and suggestions for how to address them.
### Unimplemented ops
This error is becoming rarer as nested tensor op support grows, but it’s still possible to hit it today given that there are a couple thousand ops within PyTorch.
```
NotImplementedError: aten.view_as_real.default

```
Copy to clipboard
The error is straightforward; we haven’t gotten around to adding op support for this particular op yet. If you’d like, you can contribute an implementation yourself OR simply request that we add support for this op in a future PyTorch release.
### Ragged structure incompatibility
```
RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)

```
Copy to clipboard
This error occurs when calling an op that operates over multiple NJTs with incompatible ragged structures. Currently, it is required that input NJTs have the exact same `offsets` constituent in order to have the same symbolic ragged structure symbol (e.g. `j1`).
As a workaround for this situation, it is possible to construct NJTs from the `values` and `offsets` components directly. With both NJTs referencing the same `offsets` components, they are considered to have the same ragged structure and are thus compatible.
```
>>> a = torch.randn(50, 128)
>>> b = torch.randn(32, 128)
>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt2 = torch.nested.nested_tensor_from_jagged(values=torch.randn(82, 128), offsets=nt1.offsets())
>>> nt3 = nt1 + nt2
>>> nt3.shape
torch.Size([2, j1, 128])

```
Copy to clipboard
### Data dependent operation within torch.compile
```
torch._dynamo.exc.Unsupported: data dependent operator: aten._local_scalar_dense.default; to enable, set torch._dynamo.config.capture_scalar_outputs = True

```
Copy to clipboard
This error occurs when calling an op that does data-dependent operation within torch.compile; this commonly occurs for ops that need to examine the values of the NJT’s `offsets` to determine the output shape. For example:
```
>>> a = torch.randn(50, 128)
>>> b = torch.randn(32, 128)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> def f(nt): return nt.chunk(2, dim=0)[0]
...
>>> compiled_f = torch.compile(f, fullgraph=True)
>>> output = compiled_f(nt)

```
Copy to clipboard
In this example, calling `chunk()` on the batch dimension of the NJT requires examination of the NJT’s `offsets` data to delineate batch item boundaries within the packed ragged dimension. As a workaround, there are a couple torch.compile flags that can be set:
```
>>> torch._dynamo.config.capture_dynamic_output_shape_ops = True
>>> torch._dynamo.config.capture_scalar_outputs = True

```
Copy to clipboard
If, after setting these, you still see data-dependent operator errors, please file an issue with PyTorch. This area of `torch.compile()` is still in heavy development and certain aspects of NJT support may be incomplete.
## Contributions
If you’d like to contribute to nested tensor development, one of the most impactful ways to do so is to add nested tensor support for a currently-unsupported PyTorch op. This process generally consists of a couple simple steps:
  1. Determine the name of the op to add; this should be something like `aten.view_as_real.default`. The signature for this op can be found in `aten/src/ATen/native/native_functions.yaml`.
  2. Register an op implementation in `torch/nested/_internal/ops.py`, following the pattern established there for other ops. Use the signature from `native_functions.yaml` for schema validation.


The most common way to implement an op is to unwrap the NJT into its constituents, redispatch the op on the underlying `values` buffer, and propagate the relevant NJT metadata (including `offsets`) to a new output NJT. If the output of the op is expected to have a different shape from the input, new `offsets`, etc. metadata must be computed.
When an op is applied over the batch or ragged dimension, these tricks can help quickly get a working implementation:
  * For _non-batchwise_ operation, an `unbind()`-based fallback should work.
  * For operation on the ragged dimension, consider converting to padded dense with a properly-selected padding value that won’t negatively bias the output, running the op, and converting back to NJT. Within `torch.compile`, these conversions can be fused to avoid materializing the padded intermediate.


## Detailed Docs for Construction and Conversion Functions 

torch.nested.nested_tensor(_tensor_list_ , _*_ , _dtype =None_, _layout =None_, _device =None_, _requires_grad =False_, _pin_memory =False_)[source][source]
    
Constructs a nested tensor with no autograd history (also known as a “leaf tensor”, see Autograd mechanics) from `tensor_list` a list of tensors. 

Parameters
    
  * **tensor_list** (_List_ _[__array_like_ _]_) – a list of tensors, or anything that can be passed to torch.tensor,
  * **dimensionality.** (_where each element_ _of_ _the list has the same_) – 



Keyword Arguments
    
  * **dtype** (`torch.dtype`, optional) – the desired type of returned nested tensor. Default: if None, same `torch.dtype` as leftmost tensor in the list.
  * **layout** (`torch.layout`, optional) – the desired layout of returned nested tensor. Only strided and jagged layouts are supported. Default: if None, the strided layout.
  * **device** (`torch.device`, optional) – the desired device of returned nested tensor. Default: if None, same `torch.device` as leftmost tensor in the list
  * **requires_grad** (_bool_ _,__optional_) – If autograd should record operations on the returned nested tensor. Default: `False`.
  * **pin_memory** (_bool_ _,__optional_) – If set, returned nested tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: `False`.



Return type
    
_Tensor_
Example:
```
>>> a = torch.arange(3, dtype=torch.float, requires_grad=True)
>>> b = torch.arange(5, dtype=torch.float, requires_grad=True)
>>> nt = torch.nested.nested_tensor([a, b], requires_grad=True)
>>> nt.is_leaf
True

```
Copy to clipboard 

torch.nested.nested_tensor_from_jagged(_values_ , _offsets =None_, _lengths =None_, _jagged_dim =None_, _min_seqlen =None_, _max_seqlen =None_)[source][source]
    
Constructs a jagged layout nested tensor from the given jagged components. The jagged layout consists of a required values buffer with the jagged dimension packed into a single dimension. The offsets / lengths metadata determines how this dimension is split into batch elements and are expected to be allocated on the same device as the values buffer. 

Expected metadata formats:
    
  * offsets: Indices within the packed dimension splitting it into heterogeneously-sized batch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6 should be conceptually split into batch elements of length [2, 1, 3]. Note that both the beginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).
  * lengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3] indicates that a packed jagged dim of size 6 should be conceptually split into batch elements of length [2, 1, 3].


Note that it can be useful to provide both offsets and lengths. This describes a nested tensor with “holes”, where the offsets indicate the start position of each batch item and the length specifies the total number of elements (see example below).
The returned jagged layout nested tensor will be a view of the input values tensor. 

Parameters
    
  * **values** (`torch.Tensor`) – The underlying buffer in the shape of (sum_B(*), D_1, …, D_N). The jagged dimension is packed into a single dimension, with the offsets / lengths metadata used to distinguish batch elements.
  * **offsets** (optional `torch.Tensor`) – Offsets into the jagged dimension of shape B + 1.
  * **lengths** (optional `torch.Tensor`) – Lengths of the batch elements of shape B.
  * **jagged_dim** (_optional python:int_) – Indicates which dimension in values is the packed jagged dimension. If None, this is set to dim=1 (i.e. the dimension immediately following the batch dimension). Default: None
  * **min_seqlen** (_optional python:int_) – If set, uses the specified value as the cached minimum sequence length for the returned nested tensor. This can be a useful alternative to computing this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None
  * **max_seqlen** (_optional python:int_) – If set, uses the specified value as the cached maximum sequence length for the returned nested tensor. This can be a useful alternative to computing this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None



Return type
    
_Tensor_
Example:
```
>>> values = torch.randn(12, 5)
>>> offsets = torch.tensor([0, 3, 5, 6, 10, 12])
>>> nt = nested_tensor_from_jagged(values, offsets)
>>> # 3D shape with the middle dimension jagged
>>> nt.shape
torch.Size([5, j2, 5])
>>> # Length of each item in the batch:
>>> offsets.diff()
tensor([3, 2, 1, 4, 2])
>>> values = torch.randn(6, 5)
>>> offsets = torch.tensor([0, 2, 3, 6])
>>> lengths = torch.tensor([1, 1, 2])
>>> # NT with holes
>>> nt = nested_tensor_from_jagged(values, offsets, lengths)
>>> a, b, c = nt.unbind()
>>> # Batch item 1 consists of indices [0, 1)
>>> torch.equal(a, values[0:1, :])
True
>>> # Batch item 2 consists of indices [2, 3)
>>> torch.equal(b, values[2:3, :])
True
>>> # Batch item 3 consists of indices [3, 5)
>>> torch.equal(c, values[3:5, :])
True

```
Copy to clipboard 

torch.nested.as_nested_tensor(_ts_ , _dtype =None_, _device =None_, _layout =None_)[source][source]
    
Constructs a nested tensor preserving autograd history from a tensor or a list / tuple of tensors.
If a nested tensor is passed, it will be returned directly unless the device / dtype / layout differ. Note that converting device / dtype will result in a copy, while converting layout is not currently supported by this function.
If a non-nested tensor is passed, it is treated as a batch of constituents of consistent size. A copy will be incurred if the passed device / dtype differ from those of the input OR if the input is non-contiguous. Otherwise, the input’s storage will be used directly.
If a tensor list is provided, tensors in the list are always copied during construction of the nested tensor. 

Parameters
    
**ts** (_Tensor_ _or_ _List_ _[__Tensor_ _] or_ _Tuple_ _[__Tensor_ _]_) – a tensor to treat as a nested tensor OR a list / tuple of tensors with the same ndim 

Keyword Arguments
    
  * **dtype** (`torch.dtype`, optional) – the desired type of returned nested tensor. Default: if None, same `torch.dtype` as leftmost tensor in the list.
  * **device** (`torch.device`, optional) – the desired device of returned nested tensor. Default: if None, same `torch.device` as leftmost tensor in the list
  * **layout** (`torch.layout`, optional) – the desired layout of returned nested tensor. Only strided and jagged layouts are supported. Default: if None, the strided layout.



Return type
    
_Tensor_
Example:
```
>>> a = torch.arange(3, dtype=torch.float, requires_grad=True)
>>> b = torch.arange(5, dtype=torch.float, requires_grad=True)
>>> nt = torch.nested.as_nested_tensor([a, b])
>>> nt.is_leaf
False
>>> fake_grad = torch.nested.nested_tensor([torch.ones_like(a), torch.zeros_like(b)])
>>> nt.backward(fake_grad)
>>> a.grad
tensor([1., 1., 1.])
>>> b.grad
tensor([0., 0., 0., 0., 0.])
>>> c = torch.randn(3, 5, requires_grad=True)
>>> nt2 = torch.nested.as_nested_tensor(c)

```
Copy to clipboard 

torch.nested.to_padded_tensor(_input_ , _padding_ , _output_size =None_, _out =None_) → Tensor
    
Returns a new (non-nested) Tensor by padding the `input` nested tensor. The leading entries will be filled with the nested data, while the trailing entries will be padded.
Warning
`to_padded_tensor()` always copies the underlying data, since the nested and the non-nested tensors differ in memory layout. 

Parameters
    
**padding** (_float_) – The padding value for the trailing entries. 

Keyword Arguments
    
  * **output_size** (_Tuple_ _[__int_ _]_) – The size of the output tensor. If given, it must be large enough to contain all nested data; else, will infer by taking the max size of each nested sub-tensor along each dimension.
  * **out** (_Tensor_ _,__optional_) – the output tensor.


Example:
```
>>> nt = torch.nested.nested_tensor([torch.randn((2, 5)), torch.randn((3, 4))])
nested_tensor([
 tensor([[ 1.6862, -1.1282, 1.1031, 0.0464, -1.3276],
     [-1.9967, -1.0054, 1.8972, 0.9174, -1.4995]]),
 tensor([[-1.8546, -0.7194, -0.2918, -0.1846],
     [ 0.2773, 0.8793, -0.5183, -0.6447],
     [ 1.8009, 1.8468, -0.9832, -1.5272]])
])
>>> pt_infer = torch.nested.to_padded_tensor(nt, 0.0)
tensor([[[ 1.6862, -1.1282, 1.1031, 0.0464, -1.3276],
     [-1.9967, -1.0054, 1.8972, 0.9174, -1.4995],
     [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],
    [[-1.8546, -0.7194, -0.2918, -0.1846, 0.0000],
     [ 0.2773, 0.8793, -0.5183, -0.6447, 0.0000],
     [ 1.8009, 1.8468, -0.9832, -1.5272, 0.0000]]])
>>> pt_large = torch.nested.to_padded_tensor(nt, 1.0, (2, 4, 6))
tensor([[[ 1.6862, -1.1282, 1.1031, 0.0464, -1.3276, 1.0000],
     [-1.9967, -1.0054, 1.8972, 0.9174, -1.4995, 1.0000],
     [ 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
     [ 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],
    [[-1.8546, -0.7194, -0.2918, -0.1846, 1.0000, 1.0000],
     [ 0.2773, 0.8793, -0.5183, -0.6447, 1.0000, 1.0000],
     [ 1.8009, 1.8468, -0.9832, -1.5272, 1.0000, 1.0000],
     [ 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]])
>>> pt_small = torch.nested.to_padded_tensor(nt, 2.0, (2, 2, 2))
RuntimeError: Value in output_size is less than NestedTensor padded size. Truncation is not supported.

```
Copy to clipboard 

torch.nested.masked_select(_tensor_ , _mask_)[source][source]
    
Constructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor will have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is represented with the offsets, this is unlike `masked_select()` where the output is collapsed to a 1D tensor.
Args: tensor (`torch.Tensor`): a strided tensor from which the jagged layout nested tensor is constructed from. mask (`torch.Tensor`): a strided mask tensor which is applied to the tensor input
Example:
```
>>> tensor = torch.randn(3, 3)
>>> mask = torch.tensor([[False, False, True], [True, False, True], [False, False, True]])
>>> nt = torch.nested.masked_select(tensor, mask)
>>> nt.shape
torch.Size([3, j4])
>>> # Length of each item in the batch:
>>> nt.offsets().diff()
tensor([1, 2, 1])
>>> tensor = torch.randn(6, 5)
>>> mask = torch.tensor([False])
>>> nt = torch.nested.masked_select(tensor, mask)
>>> nt.shape
torch.Size([6, j5])
>>> # Length of each item in the batch:
>>> nt.offsets().diff()
tensor([0, 0, 0, 0, 0, 0])

```
Copy to clipboard 

Return type
    
_Tensor_ 

torch.nested.narrow(_tensor_ , _dim_ , _start_ , _length_ , _layout =torch.strided_)[source][source]
    
Constructs a nested tensor (which might be a view) from `tensor`, a strided tensor. This follows similar semantics to torch.Tensor.narrow, where in the `dim`-th dimension the new nested tensor shows only the elements in the interval [start, start+length). As nested representations allow for a different start and length at each ‘row’ of that dimension, `start` and `length` can also be tensors of shape tensor.shape[0].
There’s some differences depending on the layout you use for the nested tensor. If using strided layout, torch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while jagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular representation is really useful for representing kv-caches in Transformer models, as specialized SDPA kernels can deal with format easily, resulting in performance improvements. 

Parameters
    
  * **tensor** (`torch.Tensor`) – a strided tensor, which will be used as the underlying data for the nested tensor if using the jagged layout or will be copied for the strided layout.
  * **dim** (_int_) – the dimension where narrow will be applied. Only dim=1 is supported for the jagged layout, while strided supports all dim
  * **start** (Union[int, `torch.Tensor`]) – starting element for the narrow operation
  * **length** (Union[int, `torch.Tensor`]) – number of elements taken during the narrow op



Keyword Arguments
    
**layout** (`torch.layout`, optional) – the desired layout of returned nested tensor. Only strided and jagged layouts are supported. Default: if None, the strided layout. 

Return type
    
_Tensor_
Example:
```
>>> starts = torch.tensor([0, 1, 2, 3, 4], dtype=torch.int64)
>>> lengths = torch.tensor([3, 2, 2, 1, 5], dtype=torch.int64)
>>> narrow_base = torch.randn(5, 10, 20)
>>> nt_narrowed = torch.nested.narrow(narrow_base, 1, starts, lengths, layout=torch.jagged)
>>> nt_narrowed.is_contiguous()
False

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.nested
    * Introduction
    * Construction
    * Data Layout and Shape
    * Supported Operations
      * Viewing nested tensor constituents
      * Conversions to / from padded
      * Shape manipulations
      * Attention mechanisms
    * Usage with torch.compile
    * Troubleshooting
      * Unimplemented ops
      * Ragged structure incompatibility
      * Data dependent operation within torch.compile
    * Contributions
    * Detailed Docs for Construction and Conversion Functions
      * `nested_tensor()`
      * `nested_tensor_from_jagged()`
      * `as_nested_tensor()`
      * `to_padded_tensor()`
      * `masked_select()`
      * `narrow()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Named Tensors operator coverage
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Named Tensors operator coverage
Please read Named Tensors first for an introduction to named tensors.
This document is a reference for _name inference_ , a process that defines how named tensors:
  1. use names to provide additional automatic runtime correctness checks
  2. propagate names from input tensors to output tensors


Below is a list of all operations that are supported with named tensors and their associated name inference rules.
If you don’t see an operation listed here, but it would help your use case, please search if an issue has already been filed and if not, file one.
Warning
The named tensor API is experimental and subject to change.
Supported Operations API | Name inference rule  
---|---  
`Tensor.abs()`, `torch.abs()` | Keeps input names  
`Tensor.abs_()` | Keeps input names  
`Tensor.acos()`, `torch.acos()` | Keeps input names  
`Tensor.acos_()` | Keeps input names  
`Tensor.add()`, `torch.add()` | Unifies names from inputs  
`Tensor.add_()` | Unifies names from inputs  
`Tensor.addmm()`, `torch.addmm()` | Contracts away dims  
`Tensor.addmm_()` | Contracts away dims  
`Tensor.addmv()`, `torch.addmv()` | Contracts away dims  
`Tensor.addmv_()` | Contracts away dims  
`Tensor.align_as()` | See documentation  
`Tensor.align_to()` | See documentation  
`Tensor.all()`, `torch.all()` | None  
`Tensor.any()`, `torch.any()` | None  
`Tensor.asin()`, `torch.asin()` | Keeps input names  
`Tensor.asin_()` | Keeps input names  
`Tensor.atan()`, `torch.atan()` | Keeps input names  
`Tensor.atan2()`, `torch.atan2()` | Unifies names from inputs  
`Tensor.atan2_()` | Unifies names from inputs  
`Tensor.atan_()` | Keeps input names  
`Tensor.bernoulli()`, `torch.bernoulli()` | Keeps input names  
`Tensor.bernoulli_()` | None  
`Tensor.bfloat16()` | Keeps input names  
`Tensor.bitwise_not()`, `torch.bitwise_not()` | Keeps input names  
`Tensor.bitwise_not_()` | None  
`Tensor.bmm()`, `torch.bmm()` | Contracts away dims  
`Tensor.bool()` | Keeps input names  
`Tensor.byte()` | Keeps input names  
`torch.cat()` | Unifies names from inputs  
`Tensor.cauchy_()` | None  
`Tensor.ceil()`, `torch.ceil()` | Keeps input names  
`Tensor.ceil_()` | None  
`Tensor.char()` | Keeps input names  
`Tensor.chunk()`, `torch.chunk()` | Keeps input names  
`Tensor.clamp()`, `torch.clamp()` | Keeps input names  
`Tensor.clamp_()` | None  
`Tensor.copy_()` | out function and in-place variants  
`Tensor.cos()`, `torch.cos()` | Keeps input names  
`Tensor.cos_()` | None  
`Tensor.cosh()`, `torch.cosh()` | Keeps input names  
`Tensor.cosh_()` | None  
`Tensor.acosh()`, `torch.acosh()` | Keeps input names  
`Tensor.acosh_()` | None  
`Tensor.cpu()` | Keeps input names  
`Tensor.cuda()` | Keeps input names  
`Tensor.cumprod()`, `torch.cumprod()` | Keeps input names  
`Tensor.cumsum()`, `torch.cumsum()` | Keeps input names  
`Tensor.data_ptr()` | None  
`Tensor.deg2rad()`, `torch.deg2rad()` | Keeps input names  
`Tensor.deg2rad_()` | None  
`Tensor.detach()`, `torch.detach()` | Keeps input names  
`Tensor.detach_()` | None  
`Tensor.device`, `torch.device()` | None  
`Tensor.digamma()`, `torch.digamma()` | Keeps input names  
`Tensor.digamma_()` | None  
`Tensor.dim()` | None  
`Tensor.div()`, `torch.div()` | Unifies names from inputs  
`Tensor.div_()` | Unifies names from inputs  
`Tensor.dot()`, `torch.dot()` | None  
`Tensor.double()` | Keeps input names  
`Tensor.element_size()` | None  
`torch.empty()` | Factory functions  
`torch.empty_like()` | Factory functions  
`Tensor.eq()`, `torch.eq()` | Unifies names from inputs  
`Tensor.erf()`, `torch.erf()` | Keeps input names  
`Tensor.erf_()` | None  
`Tensor.erfc()`, `torch.erfc()` | Keeps input names  
`Tensor.erfc_()` | None  
`Tensor.erfinv()`, `torch.erfinv()` | Keeps input names  
`Tensor.erfinv_()` | None  
`Tensor.exp()`, `torch.exp()` | Keeps input names  
`Tensor.exp_()` | None  
`Tensor.expand()` | Keeps input names  
`Tensor.expm1()`, `torch.expm1()` | Keeps input names  
`Tensor.expm1_()` | None  
`Tensor.exponential_()` | None  
`Tensor.fill_()` | None  
`Tensor.flatten()`, `torch.flatten()` | See documentation  
`Tensor.float()` | Keeps input names  
`Tensor.floor()`, `torch.floor()` | Keeps input names  
`Tensor.floor_()` | None  
`Tensor.frac()`, `torch.frac()` | Keeps input names  
`Tensor.frac_()` | None  
`Tensor.ge()`, `torch.ge()` | Unifies names from inputs  
`Tensor.get_device()`, `torch.get_device()` | None  
`Tensor.grad` | None  
`Tensor.gt()`, `torch.gt()` | Unifies names from inputs  
`Tensor.half()` | Keeps input names  
`Tensor.has_names()` | See documentation  
`Tensor.index_fill()`, `torch.index_fill()` | Keeps input names  
`Tensor.index_fill_()` | None  
`Tensor.int()` | Keeps input names  
`Tensor.is_contiguous()` | None  
`Tensor.is_cuda` | None  
`Tensor.is_floating_point()`, `torch.is_floating_point()` | None  
`Tensor.is_leaf` | None  
`Tensor.is_pinned()` | None  
`Tensor.is_shared()` | None  
`Tensor.is_signed()`, `torch.is_signed()` | None  
`Tensor.is_sparse` | None  
`Tensor.is_sparse_csr` | None  
`torch.is_tensor()` | None  
`Tensor.item()` | None  
`Tensor.itemsize` | None  
`Tensor.kthvalue()`, `torch.kthvalue()` | Removes dimensions  
`Tensor.le()`, `torch.le()` | Unifies names from inputs  
`Tensor.log()`, `torch.log()` | Keeps input names  
`Tensor.log10()`, `torch.log10()` | Keeps input names  
`Tensor.log10_()` | None  
`Tensor.log1p()`, `torch.log1p()` | Keeps input names  
`Tensor.log1p_()` | None  
`Tensor.log2()`, `torch.log2()` | Keeps input names  
`Tensor.log2_()` | None  
`Tensor.log_()` | None  
`Tensor.log_normal_()` | None  
`Tensor.logical_not()`, `torch.logical_not()` | Keeps input names  
`Tensor.logical_not_()` | None  
`Tensor.logsumexp()`, `torch.logsumexp()` | Removes dimensions  
`Tensor.long()` | Keeps input names  
`Tensor.lt()`, `torch.lt()` | Unifies names from inputs  
`torch.manual_seed()` | None  
`Tensor.masked_fill()`, `torch.masked_fill()` | Keeps input names  
`Tensor.masked_fill_()` | None  
`Tensor.masked_select()`, `torch.masked_select()` | Aligns mask up to input and then unifies_names_from_input_tensors  
`Tensor.matmul()`, `torch.matmul()` | Contracts away dims  
`Tensor.mean()`, `torch.mean()` | Removes dimensions  
`Tensor.median()`, `torch.median()` | Removes dimensions  
`Tensor.nanmedian()`, `torch.nanmedian()` | Removes dimensions  
`Tensor.mm()`, `torch.mm()` | Contracts away dims  
`Tensor.mode()`, `torch.mode()` | Removes dimensions  
`Tensor.mul()`, `torch.mul()` | Unifies names from inputs  
`Tensor.mul_()` | Unifies names from inputs  
`Tensor.mv()`, `torch.mv()` | Contracts away dims  
`Tensor.names` | See documentation  
`Tensor.narrow()`, `torch.narrow()` | Keeps input names  
`Tensor.nbytes` | None  
`Tensor.ndim` | None  
`Tensor.ndimension()` | None  
`Tensor.ne()`, `torch.ne()` | Unifies names from inputs  
`Tensor.neg()`, `torch.neg()` | Keeps input names  
`Tensor.neg_()` | None  
`torch.normal()` | Keeps input names  
`Tensor.normal_()` | None  
`Tensor.numel()`, `torch.numel()` | None  
`torch.ones()` | Factory functions  
`Tensor.pow()`, `torch.pow()` | Unifies names from inputs  
`Tensor.pow_()` | None  
`Tensor.prod()`, `torch.prod()` | Removes dimensions  
`Tensor.rad2deg()`, `torch.rad2deg()` | Keeps input names  
`Tensor.rad2deg_()` | None  
`torch.rand()` | Factory functions  
`torch.rand()` | Factory functions  
`torch.randn()` | Factory functions  
`torch.randn()` | Factory functions  
`Tensor.random_()` | None  
`Tensor.reciprocal()`, `torch.reciprocal()` | Keeps input names  
`Tensor.reciprocal_()` | None  
`Tensor.refine_names()` | See documentation  
`Tensor.register_hook()` | None  
`Tensor.register_post_accumulate_grad_hook()` | None  
`Tensor.rename()` | See documentation  
`Tensor.rename_()` | See documentation  
`Tensor.requires_grad` | None  
`Tensor.requires_grad_()` | None  
`Tensor.resize_()` | Only allow resizes that do not change shape  
`Tensor.resize_as_()` | Only allow resizes that do not change shape  
`Tensor.round()`, `torch.round()` | Keeps input names  
`Tensor.round_()` | None  
`Tensor.rsqrt()`, `torch.rsqrt()` | Keeps input names  
`Tensor.rsqrt_()` | None  
`Tensor.select()`, `torch.select()` | Removes dimensions  
`Tensor.short()` | Keeps input names  
`Tensor.sigmoid()`, `torch.sigmoid()` | Keeps input names  
`Tensor.sigmoid_()` | None  
`Tensor.sign()`, `torch.sign()` | Keeps input names  
`Tensor.sign_()` | None  
`Tensor.sgn()`, `torch.sgn()` | Keeps input names  
`Tensor.sgn_()` | None  
`Tensor.sin()`, `torch.sin()` | Keeps input names  
`Tensor.sin_()` | None  
`Tensor.sinh()`, `torch.sinh()` | Keeps input names  
`Tensor.sinh_()` | None  
`Tensor.asinh()`, `torch.asinh()` | Keeps input names  
`Tensor.asinh_()` | None  
`Tensor.size()` | None  
`Tensor.softmax()`, `torch.softmax()` | Keeps input names  
`Tensor.split()`, `torch.split()` | Keeps input names  
`Tensor.sqrt()`, `torch.sqrt()` | Keeps input names  
`Tensor.sqrt_()` | None  
`Tensor.squeeze()`, `torch.squeeze()` | Removes dimensions  
`Tensor.std()`, `torch.std()` | Removes dimensions  
`torch.std_mean()` | Removes dimensions  
`Tensor.stride()` | None  
`Tensor.sub()`, `torch.sub()` | Unifies names from inputs  
`Tensor.sub_()` | Unifies names from inputs  
`Tensor.sum()`, `torch.sum()` | Removes dimensions  
`Tensor.tan()`, `torch.tan()` | Keeps input names  
`Tensor.tan_()` | None  
`Tensor.tanh()`, `torch.tanh()` | Keeps input names  
`Tensor.tanh_()` | None  
`Tensor.atanh()`, `torch.atanh()` | Keeps input names  
`Tensor.atanh_()` | None  
`torch.tensor()` | Factory functions  
`Tensor.to()` | Keeps input names  
`Tensor.topk()`, `torch.topk()` | Removes dimensions  
`Tensor.transpose()`, `torch.transpose()` | Permutes dimensions  
`Tensor.trunc()`, `torch.trunc()` | Keeps input names  
`Tensor.trunc_()` | None  
`Tensor.type()` | None  
`Tensor.type_as()` | Keeps input names  
`Tensor.unbind()`, `torch.unbind()` | Removes dimensions  
`Tensor.unflatten()` | See documentation  
`Tensor.uniform_()` | None  
`Tensor.var()`, `torch.var()` | Removes dimensions  
`torch.var_mean()` | Removes dimensions  
`Tensor.zero_()` | None  
`torch.zeros()` | Factory functions  
## Keeps input names
All pointwise unary functions follow this rule as well as some other unary functions.
  * Check names: None
  * Propagate names: input tensor’s names are propagated to the output.


```
>>> x = torch.randn(3, 3, names=('N', 'C'))
>>> x.abs().names
('N', 'C')

```
Copy to clipboard
## Removes dimensions
All reduction ops like `sum()` remove dimensions by reducing over the desired dimensions. Other operations like `select()` and `squeeze()` remove dimensions.
Wherever one can pass an integer dimension index to an operator, one can also pass a dimension name. Functions that take lists of dimension indices can also take in a list of dimension names.
  * Check names: If `dim` or `dims` is passed in as a list of names, check that those names exist in `self`.
  * Propagate names: If the dimensions of the input tensor specified by `dim` or `dims` are not present in the output tensor, then the corresponding names of those dimensions do not appear in `output.names`.


```
>>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))
>>> x.squeeze('N').names
('C', 'H', 'W')
>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
>>> x.sum(['N', 'C']).names
('H', 'W')
# Reduction ops with keepdim=True don't actually remove dimensions.
>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
>>> x.sum(['N', 'C'], keepdim=True).names
('N', 'C', 'H', 'W')

```
Copy to clipboard
## Unifies names from inputs
All binary arithmetic ops follow this rule. Operations that broadcast still broadcast positionally from the right to preserve compatibility with unnamed tensors. To perform explicit broadcasting by names, use `Tensor.align_as()`.
  * Check names: All names must match positionally from the right. i.e., in `tensor + other`, `match(tensor.names[i], other.names[i])` must be true for all `i` in `(-min(tensor.dim(), other.dim()) + 1, -1]`.
  * Check names: Furthermore, all named dimensions must be aligned from the right. During matching, if we match a named dimension `A` with an unnamed dimension `None`, then `A` must not appear in the tensor with the unnamed dimension.
  * Propagate names: unify pairs of names from the right from both tensors to produce output names.


For example,
```
# tensor: Tensor[  N, None]
# other: Tensor[None,  C]
>>> tensor = torch.randn(3, 3, names=('N', None))
>>> other = torch.randn(3, 3, names=(None, 'C'))
>>> (tensor + other).names
('N', 'C')

```
Copy to clipboard
Check names:
  * `match(tensor.names[-1], other.names[-1])` is `True`
  * `match(tensor.names[-2], tensor.names[-2])` is `True`
  * Because we matched `None` in `tensor` with `'C'`, check to make sure `'C'` doesn’t exist in `tensor` (it does not).
  * Check to make sure `'N'` doesn’t exists in `other` (it does not).


Finally, the output names are computed with `[unify('N', None), unify(None, 'C')] = ['N', 'C']`
More examples:
```
# Dimensions don't match from the right:
# tensor: Tensor[N, C]
# other: Tensor[  N]
>>> tensor = torch.randn(3, 3, names=('N', 'C'))
>>> other = torch.randn(3, names=('N',))
>>> (tensor + other).names
RuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims
['N']: dim 'C' and dim 'N' are at the same position from the right but do
not match.
# Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]:
# tensor: Tensor[N, None]
# other: Tensor[   N]
>>> tensor = torch.randn(3, 3, names=('N', None))
>>> other = torch.randn(3, names=('N',))
>>> (tensor + other).names
RuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and
dims ['N', None]: dim 'N' appears in a different position from the right
across both lists.

```
Copy to clipboard
Note
In both of the last examples, it is possible to align the tensors by names and then perform the addition. Use `Tensor.align_as()` to align tensors by name or `Tensor.align_to()` to align tensors to a custom dimension ordering.
## Permutes dimensions
Some operations, like `Tensor.t()`, permute the order of dimensions. Dimension names are attached to individual dimensions so they get permuted as well.
If the operator takes in positional index `dim`, it is also able to take a dimension name as `dim`.
  * Check names: If `dim` is passed as a name, check that it exists in the tensor.
  * Propagate names: Permute dimension names in the same way as the dimensions that are being permuted.


```
>>> x = torch.randn(3, 3, names=('N', 'C'))
>>> x.transpose('N', 'C').names
('C', 'N')

```
Copy to clipboard
## Contracts away dims
Matrix multiply functions follow some variant of this. Let’s go through `torch.mm()` first and then generalize the rule for batch matrix multiplication.
For `torch.mm(tensor, other)`:
  * Check names: None
  * Propagate names: result names are `(tensor.names[-2], other.names[-1])`.


```
>>> x = torch.randn(3, 3, names=('N', 'D'))
>>> y = torch.randn(3, 3, names=('in', 'out'))
>>> x.mm(y).names
('N', 'out')

```
Copy to clipboard
Inherently, a matrix multiplication performs a dot product over two dimensions, collapsing them. When two tensors are matrix-multiplied, the contracted dimensions disappear and do not show up in the output tensor.
`torch.mv()`, `torch.dot()` work in a similar way: name inference does not check input names and removes the dimensions that are involved in the dot product:
```
>>> x = torch.randn(3, 3, names=('N', 'D'))
>>> y = torch.randn(3, names=('something',))
>>> x.mv(y).names
('N',)

```
Copy to clipboard
Now, let’s take a look at `torch.matmul(tensor, other)`. Assume that `tensor.dim() >= 2` and `other.dim() >= 2`.
  * Check names: Check that the batch dimensions of the inputs are aligned and broadcastable. See Unifies names from inputs for what it means for the inputs to be aligned.
  * Propagate names: result names are obtained by unifying the batch dimensions and removing the contracted dimensions: `unify(tensor.names[:-2], other.names[:-2]) + (tensor.names[-2], other.names[-1])`.


Examples:
```
# Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].
# 'A', 'B' are batch dimensions.
>>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))
>>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))
>>> torch.matmul(x, y).names
('A', 'B', 'C', 'F')

```
Copy to clipboard
Finally, there are fused `add` versions of many matmul functions. i.e., `addmm()` and `addmv()`. These are treated as composing name inference for i.e. `mm()` and name inference for `add()`.
## Factory functions
Factory functions now take a new `names` argument that associates a name with each dimension.
```
>>> torch.zeros(2, 3, names=('N', 'C'))
tensor([[0., 0., 0.],
    [0., 0., 0.]], names=('N', 'C'))

```
Copy to clipboard
## out function and in-place variants
A tensor specified as an `out=` tensor has the following behavior:
  * If it has no named dimensions, then the names computed from the operation get propagated to it.
  * If it has any named dimensions, then the names computed from the operation must be exactly equal to the existing names. Otherwise, the operation errors.


All in-place methods modify inputs to have names equal to the computed names from name inference. For example:
```
>>> x = torch.randn(3, 3)
>>> y = torch.randn(3, 3, names=('N', 'C'))
>>> x.names
(None, None)
>>> x += y
>>> x.names
('N', 'C')

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Named Tensors operator coverage
    * Keeps input names
    * Removes dimensions
    * Unifies names from inputs
    * Permutes dimensions
    * Contracts away dims
    * Factory functions
    * out function and in-place variants


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.nn.functional
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.nn.functional
## Convolution functions
`conv1d` | Applies a 1D convolution over an input signal composed of several input planes.  
---|---  
`conv2d` | Applies a 2D convolution over an input image composed of several input planes.  
`conv3d` | Applies a 3D convolution over an input image composed of several input planes.  
`conv_transpose1d` | Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called "deconvolution".  
`conv_transpose2d` | Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution".  
`conv_transpose3d` | Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution"  
`unfold` | Extract sliding local blocks from a batched input tensor.  
`fold` | Combine an array of sliding local blocks into a large containing tensor.  
## Pooling functions
`avg_pool1d` | Applies a 1D average pooling over an input signal composed of several input planes.  
---|---  
`avg_pool2d` | Applies 2D average-pooling operation in kH×kWkH \times kWkH×kW regions by step size sH×sWsH \times sWsH×sW steps.  
`avg_pool3d` | Applies 3D average-pooling operation in kT×kH×kWkT \times kH \times kWkT×kH×kW regions by step size sT×sH×sWsT \times sH \times sWsT×sH×sW steps.  
`max_pool1d` | Applies a 1D max pooling over an input signal composed of several input planes.  
`max_pool2d` | Applies a 2D max pooling over an input signal composed of several input planes.  
`max_pool3d` | Applies a 3D max pooling over an input signal composed of several input planes.  
`max_unpool1d` | Compute a partial inverse of `MaxPool1d`.  
`max_unpool2d` | Compute a partial inverse of `MaxPool2d`.  
`max_unpool3d` | Compute a partial inverse of `MaxPool3d`.  
`lp_pool1d` | Apply a 1D power-average pooling over an input signal composed of several input planes.  
`lp_pool2d` | Apply a 2D power-average pooling over an input signal composed of several input planes.  
`lp_pool3d` | Apply a 3D power-average pooling over an input signal composed of several input planes.  
`adaptive_max_pool1d` | Applies a 1D adaptive max pooling over an input signal composed of several input planes.  
`adaptive_max_pool2d` | Applies a 2D adaptive max pooling over an input signal composed of several input planes.  
`adaptive_max_pool3d` | Applies a 3D adaptive max pooling over an input signal composed of several input planes.  
`adaptive_avg_pool1d` | Applies a 1D adaptive average pooling over an input signal composed of several input planes.  
`adaptive_avg_pool2d` | Apply a 2D adaptive average pooling over an input signal composed of several input planes.  
`adaptive_avg_pool3d` | Apply a 3D adaptive average pooling over an input signal composed of several input planes.  
`fractional_max_pool2d` | Applies 2D fractional max pooling over an input signal composed of several input planes.  
`fractional_max_pool3d` | Applies 3D fractional max pooling over an input signal composed of several input planes.  
## Attention Mechanisms
The `torch.nn.attention.bias` module contains attention_biases that are designed to be used with scaled_dot_product_attention.
`scaled_dot_product_attention` | scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,  
---|---  
## Non-linear activation functions
`threshold` | Apply a threshold to each element of the input Tensor.  
---|---  
`threshold_` | In-place version of `threshold()`.  
`relu` | Applies the rectified linear unit function element-wise.  
`relu_` | In-place version of `relu()`.  
`hardtanh` | Applies the HardTanh function element-wise.  
`hardtanh_` | In-place version of `hardtanh()`.  
`hardswish` | Apply hardswish function, element-wise.  
`relu6` | Applies the element-wise function ReLU6(x)=min⁡(max⁡(0,x),6)\text{ReLU6}(x) = \min(\max(0,x), 6)ReLU6(x)=min(max(0,x),6).  
`elu` | Apply the Exponential Linear Unit (ELU) function element-wise.  
`elu_` | In-place version of `elu()`.  
`selu` | Applies element-wise, SELU(x)=scale∗(max⁡(0,x)+min⁡(0,α∗(exp⁡(x)−1)))\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))SELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1))), with α=1.6732632423543772848170429916717\alpha=1.6732632423543772848170429916717α=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.  
`celu` | Applies element-wise, CELU(x)=max⁡(0,x)+min⁡(0,α∗(exp⁡(x/α)−1))\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1)).  
`leaky_relu` | Applies element-wise, LeakyReLU(x)=max⁡(0,x)+negative_slope∗min⁡(0,x)\text{LeakyReLU}(x) = \max(0, x) + \text{negative\\_slope} * \min(0, x)LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)  
`leaky_relu_` | In-place version of `leaky_relu()`.  
`prelu` | Applies element-wise the function PReLU(x)=max⁡(0,x)+weight∗min⁡(0,x)\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)PReLU(x)=max(0,x)+weight∗min(0,x) where weight is a learnable parameter.  
`rrelu` | Randomized leaky ReLU.  
`rrelu_` | In-place version of `rrelu()`.  
`glu` | The gated linear unit.  
`gelu` | When the approximate argument is 'none', it applies element-wise the function GELU(x)=x∗Φ(x)\text{GELU}(x) = x * \Phi(x)GELU(x)=x∗Φ(x)  
`logsigmoid` | Applies element-wise LogSigmoid(xi)=log⁡(11+exp⁡(−xi))\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)LogSigmoid(xi​)=log(1+exp(−xi​)1​)  
`hardshrink` | Applies the hard shrinkage function element-wise  
`tanhshrink` | Applies element-wise, Tanhshrink(x)=x−Tanh(x)\text{Tanhshrink}(x) = x - \text{Tanh}(x)Tanhshrink(x)=x−Tanh(x)  
`softsign` | Applies element-wise, the function SoftSign(x)=x1+∣x∣\text{SoftSign}(x) = \frac{x}{1 + |x|}SoftSign(x)=1+∣x∣x​  
`softplus` | Applies element-wise, the function Softplus(x)=1β∗log⁡(1+exp⁡(β∗x))\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))Softplus(x)=β1​∗log(1+exp(β∗x)).  
`softmin` | Apply a softmin function.  
`softmax` | Apply a softmax function.  
`softshrink` | Applies the soft shrinkage function elementwise  
`gumbel_softmax` | Sample from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretize.  
`log_softmax` | Apply a softmax followed by a logarithm.  
`tanh` | Applies element-wise, Tanh(x)=tanh⁡(x)=exp⁡(x)−exp⁡(−x)exp⁡(x)+exp⁡(−x)\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(−x)exp(x)−exp(−x)​  
`sigmoid` | Applies the element-wise function Sigmoid(x)=11+exp⁡(−x)\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}Sigmoid(x)=1+exp(−x)1​  
`hardsigmoid` | Apply the Hardsigmoid function element-wise.  
`silu` | Apply the Sigmoid Linear Unit (SiLU) function, element-wise.  
`mish` | Apply the Mish function, element-wise.  
`batch_norm` | Apply Batch Normalization for each channel across a batch of data.  
`group_norm` | Apply Group Normalization for last certain number of dimensions.  
`instance_norm` | Apply Instance Normalization independently for each channel in every data sample within a batch.  
`layer_norm` | Apply Layer Normalization for last certain number of dimensions.  
`local_response_norm` | Apply local response normalization over an input signal.  
`rms_norm` | Apply Root Mean Square Layer Normalization.  
`normalize` | Perform LpL_pLp​ normalization of inputs over specified dimension.  
## Linear functions
`linear` | Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.  
---|---  
`bilinear` | Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T​Ax2​+b  
## Dropout functions
`dropout` | During training, randomly zeroes some elements of the input tensor with probability `p`.  
---|---  
`alpha_dropout` | Apply alpha dropout to the input.  
`feature_alpha_dropout` | Randomly masks out entire channels (a channel is a feature map).  
`dropout1d` | Randomly zero out entire channels (a channel is a 1D feature map).  
`dropout2d` | Randomly zero out entire channels (a channel is a 2D feature map).  
`dropout3d` | Randomly zero out entire channels (a channel is a 3D feature map).  
## Sparse functions
`embedding` | Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.  
---|---  
`embedding_bag` | Compute sums, means or maxes of bags of embeddings.  
`one_hot` | Takes LongTensor with index values of shape `(*)` and returns a tensor of shape `(*, num_classes)` that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.  
## Distance functions
`pairwise_distance` | See `torch.nn.PairwiseDistance` for details  
---|---  
`cosine_similarity` | Returns cosine similarity between `x1` and `x2`, computed along dim.  
`pdist` | Computes the p-norm distance between every pair of row vectors in the input.  
## Loss functions
`binary_cross_entropy` | Measure Binary Cross Entropy between the target and input probabilities.  
---|---  
`binary_cross_entropy_with_logits` | Calculate Binary Cross Entropy between target and input logits.  
`poisson_nll_loss` | Poisson negative log likelihood loss.  
`cosine_embedding_loss` | See `CosineEmbeddingLoss` for details.  
`cross_entropy` | Compute the cross entropy loss between input logits and target.  
`ctc_loss` | Apply the Connectionist Temporal Classification loss.  
`gaussian_nll_loss` | Gaussian negative log likelihood loss.  
`hinge_embedding_loss` | See `HingeEmbeddingLoss` for details.  
`kl_div` | Compute the KL Divergence loss.  
`l1_loss` | Function that takes the mean element-wise absolute value difference.  
`mse_loss` | Measures the element-wise mean squared error, with optional weighting.  
`margin_ranking_loss` | See `MarginRankingLoss` for details.  
`multilabel_margin_loss` | See `MultiLabelMarginLoss` for details.  
`multilabel_soft_margin_loss` | See `MultiLabelSoftMarginLoss` for details.  
`multi_margin_loss` | See `MultiMarginLoss` for details.  
`nll_loss` | Compute the negative log likelihood loss.  
`huber_loss` | Computes the Huber loss, with optional weighting.  
`smooth_l1_loss` | Compute the Smooth L1 loss.  
`soft_margin_loss` | See `SoftMarginLoss` for details.  
`triplet_margin_loss` | Compute the triplet loss between given input tensors and a margin greater than 0.  
`triplet_margin_with_distance_loss` | Compute the triplet margin loss for input tensors using a custom distance function.  
## Vision functions
`pixel_shuffle` | Rearranges elements in a tensor of shape (∗,C×r2,H,W)(*, C \times r^2, H, W)(∗,C×r2,H,W) to a tensor of shape (∗,C,H×r,W×r)(*, C, H \times r, W \times r)(∗,C,H×r,W×r), where r is the `upscale_factor`.  
---|---  
`pixel_unshuffle` | Reverses the `PixelShuffle` operation by rearranging elements in a tensor of shape (∗,C,H×r,W×r)(*, C, H \times r, W \times r)(∗,C,H×r,W×r) to a tensor of shape (∗,C×r2,H,W)(*, C \times r^2, H, W)(∗,C×r2,H,W), where r is the `downscale_factor`.  
`pad` | Pads tensor.  
`interpolate` | Down/up samples the input.  
`upsample` | Upsample input.  
`upsample_nearest` | Upsamples the input, using nearest neighbours' pixel values.  
`upsample_bilinear` | Upsamples the input, using bilinear upsampling.  
`grid_sample` | Compute grid sample.  
`affine_grid` | Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices `theta`.  
## DataParallel functions (multi-GPU, distributed)
### data_parallel
`torch.nn.parallel.data_parallel` | Evaluate module(input) in parallel across the GPUs given in device_ids.  
---|---  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.nn.functional
    * Convolution functions
    * Pooling functions
    * Attention Mechanisms
    * Non-linear activation functions
    * Linear functions
    * Dropout functions
    * Sparse functions
    * Distance functions
    * Loss functions
    * Vision functions
    * DataParallel functions (multi-GPU, distributed)
      * data_parallel


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Automatic Mixed Precision examples
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Automatic Mixed Precision examples
Ordinarily, “automatic mixed precision training” means training with `torch.autocast` and `torch.amp.GradScaler` together.
Instances of `torch.autocast` enable autocasting for chosen regions. Autocasting automatically chooses the precision for operations to improve performance while maintaining accuracy.
Instances of `torch.amp.GradScaler` help perform the steps of gradient scaling conveniently. Gradient scaling improves convergence for networks with `float16` (by default on CUDA and XPU) gradients by minimizing gradient underflow, as explained here.
`torch.autocast` and `torch.amp.GradScaler` are modular. In the samples below, each is used as its individual documentation suggests.
(Samples here are illustrative. See the Automatic Mixed Precision recipe for a runnable walkthrough.)
  * Typical Mixed Precision Training
  * Working with Unscaled Gradients
    * Gradient clipping
  * Working with Scaled Gradients
    * Gradient accumulation
    * Gradient penalty
  * Working with Multiple Models, Losses, and Optimizers
  * Working with Multiple GPUs
    * DataParallel in a single process
    * DistributedDataParallel, one GPU per process
    * DistributedDataParallel, multiple GPUs per process
  * Autocast and Custom Autograd Functions
    * Functions with multiple inputs or autocastable ops
    * Functions that need a particular `dtype`


## Typical Mixed Precision Training
```
# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)
# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()
for epoch in epochs:
  for input, target in data:
    optimizer.zero_grad()
    # Runs the forward pass with autocasting.
    with autocast(device_type='cuda', dtype=torch.float16):
      output = model(input)
      loss = loss_fn(output, target)
    # Scales loss. Calls backward() on scaled loss to create scaled gradients.
    # Backward passes under autocast are not recommended.
    # Backward ops run in the same dtype autocast chose for corresponding forward ops.
    scaler.scale(loss).backward()
    # scaler.step() first unscales the gradients of the optimizer's assigned params.
    # If these gradients do not contain infs or NaNs, optimizer.step() is then called,
    # otherwise, optimizer.step() is skipped.
    scaler.step(optimizer)
    # Updates the scale for next iteration.
    scaler.update()

```
Copy to clipboard
## Working with Unscaled Gradients
All gradients produced by `scaler.scale(loss).backward()` are scaled. If you wish to modify or inspect the parameters’ `.grad` attributes between `backward()` and `scaler.step(optimizer)`, you should unscale them first. For example, gradient clipping manipulates a set of gradients such that their global norm (see `torch.nn.utils.clip_grad_norm_()`) or maximum magnitude (see `torch.nn.utils.clip_grad_value_()`) is <=<=<= some user-imposed threshold. If you attempted to clip _without_ unscaling, the gradients’ norm/maximum magnitude would also be scaled, so your requested threshold (which was meant to be the threshold for _unscaled_ gradients) would be invalid.
`scaler.unscale_(optimizer)` unscales gradients held by `optimizer`’s assigned parameters. If your model or models contain other parameters that were assigned to another optimizer (say `optimizer2`), you may call `scaler.unscale_(optimizer2)` separately to unscale those parameters’ gradients as well.
### Gradient clipping
Calling `scaler.unscale_(optimizer)` before clipping enables you to clip unscaled gradients as usual:
```
scaler = GradScaler()
for epoch in epochs:
  for input, target in data:
    optimizer.zero_grad()
    with autocast(device_type='cuda', dtype=torch.float16):
      output = model(input)
      loss = loss_fn(output, target)
    scaler.scale(loss).backward()
    # Unscales the gradients of optimizer's assigned params in-place
    scaler.unscale_(optimizer)
    # Since the gradients of optimizer's assigned params are unscaled, clips as usual:
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
    # optimizer's gradients are already unscaled, so scaler.step does not unscale them,
    # although it still skips optimizer.step() if the gradients contain infs or NaNs.
    scaler.step(optimizer)
    # Updates the scale for next iteration.
    scaler.update()

```
Copy to clipboard
`scaler` records that `scaler.unscale_(optimizer)` was already called for this optimizer this iteration, so `scaler.step(optimizer)` knows not to redundantly unscale gradients before (internally) calling `optimizer.step()`.
Warning
`unscale_` should only be called once per optimizer per `step` call, and only after all gradients for that optimizer’s assigned parameters have been accumulated. Calling `unscale_` twice for a given optimizer between each `step` triggers a RuntimeError.
## Working with Scaled Gradients
### Gradient accumulation
Gradient accumulation adds gradients over an effective batch of size `batch_per_iter * iters_to_accumulate` (`* num_procs` if distributed). The scale should be calibrated for the effective batch, which means inf/NaN checking, step skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity. Also, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective batch are accumulated. If grads are unscaled (or the scale factor changes) before accumulation is complete, the next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor) after which it’s impossible to recover the accumulated unscaled grads `step` must apply.
Therefore, if you want to `unscale_` grads (e.g., to allow clipping unscaled grads), call `unscale_` just before `step`, after all (scaled) grads for the upcoming `step` have been accumulated. Also, only call `update` at the end of iterations where you called `step` for a full effective batch:
```
scaler = GradScaler()
for epoch in epochs:
  for i, (input, target) in enumerate(data):
    with autocast(device_type='cuda', dtype=torch.float16):
      output = model(input)
      loss = loss_fn(output, target)
      loss = loss / iters_to_accumulate
    # Accumulates scaled gradients.
    scaler.scale(loss).backward()
    if (i + 1) % iters_to_accumulate == 0:
      # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)
      scaler.step(optimizer)
      scaler.update()
      optimizer.zero_grad()

```
Copy to clipboard
### Gradient penalty
A gradient penalty implementation commonly creates gradients using `torch.autograd.grad()`, combines them to create the penalty value, and adds the penalty value to the loss.
Here’s an ordinary example of an L2 penalty without gradient scaling or autocasting:
```
for epoch in epochs:
  for input, target in data:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    # Creates gradients
    grad_params = torch.autograd.grad(outputs=loss,
                     inputs=model.parameters(),
                     create_graph=True)
    # Computes the penalty term and adds it to the loss
    grad_norm = 0
    for grad in grad_params:
      grad_norm += grad.pow(2).sum()
    grad_norm = grad_norm.sqrt()
    loss = loss + grad_norm
    loss.backward()
    # clip gradients here, if desired
    optimizer.step()

```
Copy to clipboard
To implement a gradient penalty _with_ gradient scaling, the `outputs` Tensor(s) passed to `torch.autograd.grad()` should be scaled. The resulting gradients will therefore be scaled, and should be unscaled before being combined to create the penalty value.
Also, the penalty term computation is part of the forward pass, and therefore should be inside an `autocast` context.
Here’s how that looks for the same L2 penalty:
```
scaler = GradScaler()
for epoch in epochs:
  for input, target in data:
    optimizer.zero_grad()
    with autocast(device_type='cuda', dtype=torch.float16):
      output = model(input)
      loss = loss_fn(output, target)
    # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params
    scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),
                         inputs=model.parameters(),
                         create_graph=True)
    # Creates unscaled grad_params before computing the penalty. scaled_grad_params are
    # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:
    inv_scale = 1./scaler.get_scale()
    grad_params = [p * inv_scale for p in scaled_grad_params]
    # Computes the penalty term and adds it to the loss
    with autocast(device_type='cuda', dtype=torch.float16):
      grad_norm = 0
      for grad in grad_params:
        grad_norm += grad.pow(2).sum()
      grad_norm = grad_norm.sqrt()
      loss = loss + grad_norm
    # Applies scaling to the backward call as usual.
    # Accumulates leaf gradients that are correctly scaled.
    scaler.scale(loss).backward()
    # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)
    # step() and update() proceed as usual.
    scaler.step(optimizer)
    scaler.update()

```
Copy to clipboard
## Working with Multiple Models, Losses, and Optimizers
If your network has multiple losses, you must call `scaler.scale` on each of them individually. If your network has multiple optimizers, you may call `scaler.unscale_` on any of them individually, and you must call `scaler.step` on each of them individually.
However, `scaler.update` should only be called once, after all optimizers used this iteration have been stepped:
```
scaler = torch.amp.GradScaler()
for epoch in epochs:
  for input, target in data:
    optimizer0.zero_grad()
    optimizer1.zero_grad()
    with autocast(device_type='cuda', dtype=torch.float16):
      output0 = model0(input)
      output1 = model1(input)
      loss0 = loss_fn(2 * output0 + 3 * output1, target)
      loss1 = loss_fn(3 * output0 - 5 * output1, target)
    # (retain_graph here is unrelated to amp, it's present because in this
    # example, both backward() calls share some sections of graph.)
    scaler.scale(loss0).backward(retain_graph=True)
    scaler.scale(loss1).backward()
    # You can choose which optimizers receive explicit unscaling, if you
    # want to inspect or modify the gradients of the params they own.
    scaler.unscale_(optimizer0)
    scaler.step(optimizer0)
    scaler.step(optimizer1)
    scaler.update()

```
Copy to clipboard
Each optimizer checks its gradients for infs/NaNs and makes an independent decision whether or not to skip the step. This may result in one optimizer skipping the step while the other one does not. Since step skipping occurs rarely (every several hundred iterations) this should not impede convergence. If you observe poor convergence after adding gradient scaling to a multiple-optimizer model, please report a bug.
## Working with Multiple GPUs
The issues described here only affect `autocast`. `GradScaler`‘s usage is unchanged.
### DataParallel in a single process
Even if `torch.nn.DataParallel` spawns threads to run the forward pass on each device. The autocast state is propagated in each one and the following will work:
```
model = MyModel()
dp_model = nn.DataParallel(model)
# Sets autocast in the main thread
with autocast(device_type='cuda', dtype=torch.float16):
  # dp_model's internal threads will autocast.
  output = dp_model(input)
  # loss_fn also autocast
  loss = loss_fn(output)

```
Copy to clipboard
### DistributedDataParallel, one GPU per process
`torch.nn.parallel.DistributedDataParallel`’s documentation recommends one GPU per process for best performance. In this case, `DistributedDataParallel` does not spawn threads internally, so usages of `autocast` and `GradScaler` are not affected.
### DistributedDataParallel, multiple GPUs per process
Here `torch.nn.parallel.DistributedDataParallel` may spawn a side thread to run the forward pass on each device, like `torch.nn.DataParallel`. The fix is the same: apply autocast as part of your model’s `forward` method to ensure it’s enabled in side threads.
## Autocast and Custom Autograd Functions
If your network uses custom autograd functions (subclasses of `torch.autograd.Function`), changes are required for autocast compatibility if any function
  * takes multiple floating-point Tensor inputs,
  * wraps any autocastable op (see the Autocast Op Reference), or
  * requires a particular `dtype` (for example, if it wraps CUDA extensions that were only compiled for `dtype`).


In all cases, if you’re importing the function and can’t alter its definition, a safe fallback is to disable autocast and force execution in `float32` ( or `dtype`) at any points of use where errors occur:
```
with autocast(device_type='cuda', dtype=torch.float16):
  ...
  with autocast(device_type='cuda', dtype=torch.float16, enabled=False):
    output = imported_function(input1.float(), input2.float())

```
Copy to clipboard
If you’re the function’s author (or can alter its definition) a better solution is to use the `torch.amp.custom_fwd()` and `torch.amp.custom_bwd()` decorators as shown in the relevant case below.
### Functions with multiple inputs or autocastable ops
Apply `custom_fwd` and `custom_bwd` (with no arguments) to `forward` and `backward` respectively. These ensure `forward` executes with the current autocast state and `backward` executes with the same autocast state as `forward` (which can prevent type mismatch errors):
```
class MyMM(torch.autograd.Function):
  @staticmethod
  @custom_fwd
  def forward(ctx, a, b):
    ctx.save_for_backward(a, b)
    return a.mm(b)
  @staticmethod
  @custom_bwd
  def backward(ctx, grad):
    a, b = ctx.saved_tensors
    return grad.mm(b.t()), a.t().mm(grad)

```
Copy to clipboard
Now `MyMM` can be invoked anywhere, without disabling autocast or manually casting inputs:
```
mymm = MyMM.apply
with autocast(device_type='cuda', dtype=torch.float16):
  output = mymm(input1, input2)

```
Copy to clipboard
### Functions that need a particular `dtype`
Consider a custom function that requires `torch.float32` inputs. Apply `custom_fwd(device_type='cuda', cast_inputs=torch.float32)` to `forward` and `custom_bwd(device_type='cuda')` to `backward`. If `forward` runs in an autocast-enabled region, the decorators cast floating-point Tensor inputs to `float32` on designated device assigned by the argument device_type, CUDA in this example, and locally disable autocast during `forward` and `backward`:
```
class MyFloat32Func(torch.autograd.Function):
  @staticmethod
  @custom_fwd(device_type='cuda', cast_inputs=torch.float32)
  def forward(ctx, input):
    ctx.save_for_backward(input)
    ...
    return fwd_output
  @staticmethod
  @custom_bwd(device_type='cuda')
  def backward(ctx, grad):
    ...

```
Copy to clipboard
Now `MyFloat32Func` can be invoked anywhere, without manually disabling autocast or casting inputs:
```
func = MyFloat32Func.apply
with autocast(device_type='cuda', dtype=torch.float16):
  # func will run in float32, regardless of the surrounding autocast state
  output = func(input)

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Automatic Mixed Precision examples
    * Typical Mixed Precision Training
    * Working with Unscaled Gradients
      * Gradient clipping
    * Working with Scaled Gradients
      * Gradient accumulation
      * Gradient penalty
    * Working with Multiple Models, Losses, and Optimizers
    * Working with Multiple GPUs
      * DataParallel in a single process
      * DistributedDataParallel, one GPU per process
      * DistributedDataParallel, multiple GPUs per process
    * Autocast and Custom Autograd Functions
      * Functions with multiple inputs or autocastable ops
      * Functions that need a particular `dtype`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.nn.init
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.nn.init
Warning
All the functions in this module are intended to be used to initialize neural network parameters, so they all run in `torch.no_grad()` mode and will not be taken into account by autograd. 

torch.nn.init.calculate_gain(_nonlinearity_ , _param =None_)[source][source]
    
Return the recommended gain value for the given nonlinearity function.
The values are as follows:
nonlinearity | gain  
---|---  
Linear / Identity | 111  
Conv{1,2,3}D | 111  
Sigmoid | 111  
Tanh | 53\frac{5}{3}35​  
ReLU | 2\sqrt{2}2​  
Leaky Relu | 21+negative_slope2\sqrt{\frac{2}{1 + \text{negative\\_slope}^2}}1+negative_slope22​​  
SELU | 34\frac{3}{4}43​  
Warning
In order to implement Self-Normalizing Neural Networks , you should use `nonlinearity='linear'` instead of `nonlinearity='selu'`. This gives the initial weights a variance of `1 / N`, which is necessary to induce a stable fixed point in the forward pass. In contrast, the default gain for `SELU` sacrifices the normalization effect for more stable gradient flow in rectangular layers. 

Parameters
    
  * **nonlinearity** – the non-linear function (nn.functional name)
  * **param** – optional parameter for the non-linear function


Examples
```
>>> gain = nn.init.calculate_gain('leaky_relu', 0.2) # leaky_relu with negative_slope=0.2

```
Copy to clipboard 

torch.nn.init.uniform_(_tensor_ , _a =0.0_, _b =1.0_, _generator =None_)[source][source]
    
Fill the input Tensor with values drawn from the uniform distribution.
U(a,b)\mathcal{U}(a, b)U(a,b). 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **a** (_float_) – the lower bound of the uniform distribution
  * **b** (_float_) – the upper bound of the uniform distribution
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)



Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.uniform_(w)

```
Copy to clipboard 

torch.nn.init.normal_(_tensor_ , _mean =0.0_, _std =1.0_, _generator =None_)[source][source]
    
Fill the input Tensor with values drawn from the normal distribution.
N(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2). 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **mean** (_float_) – the mean of the normal distribution
  * **std** (_float_) – the standard deviation of the normal distribution
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)



Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.normal_(w)

```
Copy to clipboard 

torch.nn.init.constant_(_tensor_ , _val_)[source][source]
    
Fill the input Tensor with the value val\text{val}val. 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **val** (_float_) – the value to fill the tensor with



Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.constant_(w, 0.3)

```
Copy to clipboard 

torch.nn.init.ones_(_tensor_)[source][source]
    
Fill the input Tensor with the scalar value 1. 

Parameters
    
**tensor** (_Tensor_) – an n-dimensional torch.Tensor 

Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.ones_(w)

```
Copy to clipboard 

torch.nn.init.zeros_(_tensor_)[source][source]
    
Fill the input Tensor with the scalar value 0. 

Parameters
    
**tensor** (_Tensor_) – an n-dimensional torch.Tensor 

Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.zeros_(w)

```
Copy to clipboard 

torch.nn.init.eye_(_tensor_)[source][source]
    
Fill the 2-dimensional input Tensor with the identity matrix.
Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible. 

Parameters
    
**tensor** – a 2-dimensional torch.Tensor
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.eye_(w)

```
Copy to clipboard 

torch.nn.init.dirac_(_tensor_ , _groups =1_)[source][source]
    
Fill the {3, 4, 5}-dimensional input Tensor with the Dirac delta function.
Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity 

Parameters
    
  * **tensor** – a {3, 4, 5}-dimensional torch.Tensor
  * **groups** (_int_ _,__optional_) – number of groups in the conv layer (default: 1)


Examples
```
>>> w = torch.empty(3, 16, 5, 5)
>>> nn.init.dirac_(w)
>>> w = torch.empty(3, 24, 5, 5)
>>> nn.init.dirac_(w, 3)

```
Copy to clipboard 

torch.nn.init.xavier_uniform_(_tensor_ , _gain =1.0_, _generator =None_)[source][source]
    
Fill the input Tensor with values using a Xavier uniform distribution.
The method is described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010). The resulting tensor will have values sampled from U(−a,a)\mathcal{U}(-a, a)U(−a,a) where
a=gain×6fan_in+fan_outa = \text{gain} \times \sqrt{\frac{6}{\text{fan\\_in} + \text{fan\\_out}}} a=gain×fan_in+fan_out6​​
Also known as Glorot initialization. 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **gain** (_float_) – an optional scaling factor
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)



Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))

```
Copy to clipboard
Note
Be aware that `fan_in` and `fan_out` are calculated assuming that the weight matrix is used in a transposed manner, (i.e., `x @ w.T` in `Linear` layers, where `w.shape = [fan_out, fan_in]`). This is important for correct initialization. If you plan to use `x @ w`, where `w.shape = [fan_in, fan_out]`, pass in a transposed weight matrix, i.e. `nn.init.xavier_uniform_(w.T, ...)`. 

torch.nn.init.xavier_normal_(_tensor_ , _gain =1.0_, _generator =None_)[source][source]
    
Fill the input Tensor with values using a Xavier normal distribution.
The method is described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010). The resulting tensor will have values sampled from N(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2) where
std=gain×2fan_in+fan_out\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\\_in} + \text{fan\\_out}}} std=gain×fan_in+fan_out2​​
Also known as Glorot initialization. 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **gain** (_float_) – an optional scaling factor
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)



Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.xavier_normal_(w)

```
Copy to clipboard
Note
Be aware that `fan_in` and `fan_out` are calculated assuming that the weight matrix is used in a transposed manner, (i.e., `x @ w.T` in `Linear` layers, where `w.shape = [fan_out, fan_in]`). This is important for correct initialization. If you plan to use `x @ w`, where `w.shape = [fan_in, fan_out]`, pass in a transposed weight matrix, i.e. `nn.init.xavier_normal_(w.T, ...)`. 

torch.nn.init.kaiming_uniform_(_tensor_ , _a =0_, _mode ='fan_in'_, _nonlinearity ='leaky_relu'_, _generator =None_)[source][source]
    
Fill the input Tensor with values using a Kaiming uniform distribution.
The method is described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015). The resulting tensor will have values sampled from U(−bound,bound)\mathcal{U}(-\text{bound}, \text{bound})U(−bound,bound) where
bound=gain×3fan_mode\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\\_mode}}} bound=gain×fan_mode3​​
Also known as He initialization. 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **a** (_float_) – the negative slope of the rectifier used after this layer (only used with `'leaky_relu'`)
  * **mode** (_str_) – either `'fan_in'` (default) or `'fan_out'`. Choosing `'fan_in'` preserves the magnitude of the variance of the weights in the forward pass. Choosing `'fan_out'` preserves the magnitudes in the backwards pass.
  * **nonlinearity** (_str_) – the non-linear function (nn.functional name), recommended to use only with `'relu'` or `'leaky_relu'` (default).
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)


Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')

```
Copy to clipboard
Note
Be aware that `fan_in` and `fan_out` are calculated assuming that the weight matrix is used in a transposed manner, (i.e., `x @ w.T` in `Linear` layers, where `w.shape = [fan_out, fan_in]`). This is important for correct initialization. If you plan to use `x @ w`, where `w.shape = [fan_in, fan_out]`, pass in a transposed weight matrix, i.e. `nn.init.kaiming_uniform_(w.T, ...)`. 

torch.nn.init.kaiming_normal_(_tensor_ , _a =0_, _mode ='fan_in'_, _nonlinearity ='leaky_relu'_, _generator =None_)[source][source]
    
Fill the input Tensor with values using a Kaiming normal distribution.
The method is described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015). The resulting tensor will have values sampled from N(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2) where
std=gainfan_mode\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\\_mode}}} std=fan_mode​gain​
Also known as He initialization. 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **a** (_float_) – the negative slope of the rectifier used after this layer (only used with `'leaky_relu'`)
  * **mode** (_str_) – either `'fan_in'` (default) or `'fan_out'`. Choosing `'fan_in'` preserves the magnitude of the variance of the weights in the forward pass. Choosing `'fan_out'` preserves the magnitudes in the backwards pass.
  * **nonlinearity** (_str_) – the non-linear function (nn.functional name), recommended to use only with `'relu'` or `'leaky_relu'` (default).
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)


Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')

```
Copy to clipboard
Note
Be aware that `fan_in` and `fan_out` are calculated assuming that the weight matrix is used in a transposed manner, (i.e., `x @ w.T` in `Linear` layers, where `w.shape = [fan_out, fan_in]`). This is important for correct initialization. If you plan to use `x @ w`, where `w.shape = [fan_in, fan_out]`, pass in a transposed weight matrix, i.e. `nn.init.kaiming_normal_(w.T, ...)`. 

torch.nn.init.trunc_normal_(_tensor_ , _mean =0.0_, _std =1.0_, _a =-2.0_, _b =2.0_, _generator =None_)[source][source]
    
Fill the input Tensor with values drawn from a truncated normal distribution.
The values are effectively drawn from the normal distribution N(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2) with values outside [a,b][a, b][a,b] redrawn until they are within the bounds. The method used for generating the random values works best when a≤mean≤ba \leq \text{mean} \leq ba≤mean≤b. 

Parameters
    
  * **tensor** (_Tensor_) – an n-dimensional torch.Tensor
  * **mean** (_float_) – the mean of the normal distribution
  * **std** (_float_) – the standard deviation of the normal distribution
  * **a** (_float_) – the minimum cutoff value
  * **b** (_float_) – the maximum cutoff value
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)



Return type
    
_Tensor_
Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.trunc_normal_(w)

```
Copy to clipboard 

torch.nn.init.orthogonal_(_tensor_ , _gain =1_, _generator =None_)[source][source]
    
Fill the input Tensor with a (semi) orthogonal matrix.
Described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened. 

Parameters
    
  * **tensor** – an n-dimensional torch.Tensor, where n≥2n \geq 2n≥2
  * **gain** – optional scaling factor
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)


Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.orthogonal_(w)

```
Copy to clipboard 

torch.nn.init.sparse_(_tensor_ , _sparsity_ , _std =0.01_, _generator =None_)[source][source]
    
Fill the 2D input Tensor as a sparse matrix.
The non-zero elements will be drawn from the normal distribution N(0,0.01)\mathcal{N}(0, 0.01)N(0,0.01), as described in Deep learning via Hessian-free optimization - Martens, J. (2010). 

Parameters
    
  * **tensor** – an n-dimensional torch.Tensor
  * **sparsity** – The fraction of elements in each column to be set to zero
  * **std** – the standard deviation of the normal distribution used to generate the non-zero values
  * **generator** (_Optional_ _[__Generator_ _]_) – the torch Generator to sample from (default: None)


Examples
```
>>> w = torch.empty(3, 5)
>>> nn.init.sparse_(w, sparsity=0.1)

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.nn.init
    * `calculate_gain()`
    * `uniform_()`
    * `normal_()`
    * `constant_()`
    * `ones_()`
    * `zeros_()`
    * `eye_()`
    * `dirac_()`
    * `xavier_uniform_()`
    * `xavier_normal_()`
    * `kaiming_uniform_()`
    * `kaiming_normal_()`
    * `trunc_normal_()`
    * `orthogonal_()`
    * `sparse_()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Autograd mechanics
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Autograd mechanics
This note will present an overview of how autograd works and records the operations. It’s not strictly necessary to understand all this, but we recommend getting familiar with it, as it will help you write more efficient, cleaner programs, and can aid you in debugging.
## How autograd encodes the history
Autograd is a reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.
Internally, autograd represents this graph as a graph of `Function` objects (really expressions), which can be `apply()` ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the `.grad_fn` attribute of each `torch.Tensor` is an entry point into this graph). When the forward pass is completed, we evaluate this graph in the backwards pass to compute the gradients.
An important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.
### Saved tensors
Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass. For example, the function x↦x2x\mapsto x^2x↦x2 saves the input xxx to compute the gradient.
When defining a custom Python `Function`, you can use `save_for_backward()` to save tensors during the forward pass and `saved_tensors` to retrieve them during the backward pass. See Extending PyTorch for more information.
For operations that PyTorch defines (e.g. `torch.pow()`), tensors are automatically saved as needed. You can explore (for educational or debugging purposes) which tensors are saved by a certain `grad_fn` by looking for its attributes starting with the prefix `_saved`.
```
x = torch.randn(5, requires_grad=True)
y = x.pow(2)
print(x.equal(y.grad_fn._saved_self)) # True
print(x is y.grad_fn._saved_self) # True

```
Copy to clipboard
In the previous code, `y.grad_fn._saved_self` refers to the same Tensor object as x. But that may not always be the case. For instance:
```
x = torch.randn(5, requires_grad=True)
y = x.exp()
print(y.equal(y.grad_fn._saved_result)) # True
print(y is y.grad_fn._saved_result) # False

```
Copy to clipboard
Under the hood, to prevent reference cycles, PyTorch has _packed_ the tensor upon saving and _unpacked_ it into a different tensor for reading. Here, the tensor you get from accessing `y.grad_fn._saved_result` is a different tensor object than `y` (but they still share the same storage).
Whether a tensor will be packed into a different tensor object depends on whether it is an output of its own grad_fn, which is an implementation detail subject to change and that users should not rely on.
You can control how PyTorch does packing / unpacking with Hooks for saved tensors.
## Gradients for non-differentiable functions
The gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable. Unfortunately many of the functions we use in practice do not have this property (`relu` or `sqrt` at `0`, for example). To try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order:
  1. If the function is differentiable and thus a gradient exists at the current point, use it.
  2. If the function is convex (at least locally), use the sub-gradient of minimum norm (it is the steepest descent direction).
  3. If the function is concave (at least locally), use the super-gradient of minimum norm (consider -f(x) and apply the previous point).
  4. If the function is defined, define the gradient at the current point by continuity (note that `inf` is possible here, for example for `sqrt(0)`). If multiple values are possible, pick one arbitrarily.
  5. If the function is not defined (`sqrt(-1)`, `log(-1)` or most functions when the input is `NaN`, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will use `NaN` as the gradient, but for performance reasons, some functions will use other values (`log(-1)`, for example).
  6. If the function is not a deterministic mapping (i.e. it is not a mathematical function), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of a `no_grad` environment.


## Locally disabling gradient computation
There are several mechanisms available from Python to locally disable gradient computation:
To disable gradients across entire blocks of code, there are context managers like no-grad mode and inference mode. For more fine-grained exclusion of subgraphs from gradient computation, there is setting the `requires_grad` field of a tensor.
Below, in addition to discussing the mechanisms above, we also describe evaluation mode (`nn.Module.eval()`), a method that is not used to disable gradient computation but, because of its name, is often mixed up with the three.
### Setting `requires_grad`
`requires_grad` is a flag, defaulting to false _unless wrapped in a_ `nn.Parameter`, that allows for fine-grained exclusion of subgraphs from gradient computation. It takes effect in both the forward and backward passes:
During the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. During the backward pass (`.backward()`), only leaf tensors with `requires_grad=True` will have gradients accumulated into their `.grad` fields.
It is important to note that even though every tensor has this flag, _setting_ it only makes sense for leaf tensors (tensors that do not have a `grad_fn`, e.g., a `nn.Module`’s parameters). Non-leaf tensors (tensors that do have `grad_fn`) are tensors that have a backward graph associated with them. Thus their gradients will be needed as an intermediary result to compute the gradient for a leaf tensor that requires grad. From this definition, it is clear that all non-leaf tensors will automatically have `require_grad=True`.
Setting `requires_grad` should be the main way you control which parts of the model are part of the gradient computation, for example, if you need to freeze parts of your pretrained model during model fine-tuning.
To freeze parts of your model, simply apply `.requires_grad_(False)` to the parameters that you don’t want updated. And as described above, since computations that use these parameters as inputs would not be recorded in the forward pass, they won’t have their `.grad` fields updated in the backward pass because they won’t be part of the backward graph in the first place, as desired.
Because this is such a common pattern, `requires_grad` can also be set at the module level with `nn.Module.requires_grad_()`. When applied to a module, `.requires_grad_()` takes effect on all of the module’s parameters (which have `requires_grad=True` by default).
### Grad Modes
Apart from setting `requires_grad` there are also three grad modes that can be selected from Python that can affect how computations in PyTorch are processed by autograd internally: default mode (grad mode), no-grad mode, and inference mode, all of which can be togglable via context managers and decorators.
Mode | Excludes operations from being recorded in backward graph | Skips additional autograd tracking overhead | Tensors created while the mode is enabled can be used in grad-mode later | Examples  
---|---|---|---|---  
default |  |  | ✓ | Forward pass  
no-grad | ✓ |  | ✓ | Optimizer updates  
inference | ✓ | ✓ |  | Data processing, model evaluation  
### Default Mode (Grad Mode)
The “default mode” is the mode we are implicitly in when no other modes like no-grad and inference mode are enabled. To be contrasted with “no-grad mode” the default mode is also sometimes called “grad mode”.
The most important thing to know about the default mode is that it is the only mode in which `requires_grad` takes effect. `requires_grad` is always overridden to be `False` in both the two other modes.
### No-grad Mode
Computations in no-grad mode behave as if none of the inputs require grad. In other words, computations in no-grad mode are never recorded in the backward graph even if there are inputs that have `require_grad=True`.
Enable no-grad mode when you need to perform operations that should not be recorded by autograd, but you’d still like to use the outputs of these computations in grad mode later. This context manager makes it convenient to disable gradients for a block of code or function without having to temporarily set tensors to have `requires_grad=False`, and then back to `True`.
For example, no-grad mode might be useful when writing an optimizer: when performing the training update you’d like to update parameters in-place without the update being recorded by autograd. You also intend to use the updated parameters for computations in grad mode in the next forward pass.
The implementations in torch.nn.init also rely on no-grad mode when initializing the parameters as to avoid autograd tracking when updating the initialized parameters in-place.
### Inference Mode
Inference mode is the extreme version of no-grad mode. Just like in no-grad mode, computations in inference mode are not recorded in the backward graph, but enabling inference mode will allow PyTorch to speed up your model even more. This better runtime comes with a drawback: tensors created in inference mode will not be able to be used in computations to be recorded by autograd after exiting inference mode.
Enable inference mode when you are performing computations that do not have interactions with autograd, AND you don’t plan on using the tensors created in inference mode in any computation that is to be recorded by autograd later.
It is recommended that you try out inference mode in the parts of your code that do not require autograd tracking (e.g., data processing and model evaluation). If it works out of the box for your use case it’s a free performance win. If you run into errors after enabling inference mode, check that you are not using tensors created in inference mode in computations that are recorded by autograd after exiting inference mode. If you cannot avoid such use in your case, you can always switch back to no-grad mode.
For details on inference mode please see Inference Mode.
For implementation details of inference mode see RFC-0011-InferenceMode.
### Evaluation Mode (`nn.Module.eval()`)
Evaluation mode is not a mechanism to locally disable gradient computation. It is included here anyway because it is sometimes confused to be such a mechanism.
Functionally, `module.eval()` (or equivalently `module.train(False)`) are completely orthogonal to no-grad mode and inference mode. How `model.eval()` affects your model depends entirely on the specific modules used in your model and whether they define any training-mode specific behavior.
You are responsible for calling `model.eval()` and `model.train()` if your model relies on modules such as `torch.nn.Dropout` and `torch.nn.BatchNorm2d` that may behave differently depending on training mode, for example, to avoid updating your BatchNorm running statistics on validation data.
It is recommended that you always use `model.train()` when training and `model.eval()` when evaluating your model (validation/testing) even if you aren’t sure your model has training-mode specific behavior, because a module you are using might be updated to behave differently in training and eval modes.
## In-place operations with autograd
Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.
There are two main reasons that limit the applicability of in-place operations:
  1. In-place operations can potentially overwrite values required to compute gradients.
  2. Every in-place operation requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to the `Function` representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will raise an error if the storage of modified inputs is referenced by any other `Tensor`.


### In-place correctness checks
Every tensor keeps a version counter, that is incremented every time it is marked dirty in any operation. When a Function saves any tensors for backward, a version counter of their containing Tensor is saved as well. Once you access `self.saved_tensors` it is checked, and if it is greater than the saved value an error is raised. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.
## Multithreaded Autograd
The autograd engine is responsible for running all the backward operations necessary to compute the backward pass. This section will describe all the details that can help you make the best use of it in a multithreaded environment. (This is relevant only for PyTorch 1.6+ as the behavior in previous version was different.)
User could train their model with multithreading code (e.g. Hogwild training), and does not block on the concurrent backward computations, example code could be:
```
# Define a train function to be used in different threads
def train_fn():
  x = torch.ones(5, 5, requires_grad=True)
  # forward
  y = (x + 3) * (x + 4) * 0.5
  # backward
  y.sum().backward()
  # potential optimizer update

# User write their own threading code to drive the train_fn
threads = []
for _ in range(10):
  p = threading.Thread(target=train_fn, args=())
  p.start()
  threads.append(p)
for p in threads:
  p.join()

```
Copy to clipboard
Note that some behaviors that user should be aware of:
### Concurrency on CPU
When you run `backward()` or `grad()` via python or C++ API in multiple threads on CPU, you are expecting to see extra concurrency instead of serializing all the backward calls in a specific order during execution (behavior before PyTorch 1.6).
### Non-determinism
If you are calling `backward()` from multiple threads concurrently and have shared inputs (i.e. Hogwild CPU training), then non-determinism should be expected. This can occur because parameters are automatically shared across threads, as such, multiple threads may access and try to accumulate the same `.grad` attribute during gradient accumulation. This is technically not safe, and it might result in race condition and the result might be invalid to use.
Users developing multithreaded models featuring shared parameters should have the threading model in mind and should understand the issues described above.
The functional API `torch.autograd.grad()` may be used to calculate the gradients instead of `backward()` to avoid non-determinism.
### Graph retaining
If part of the autograd graph is shared between threads, i.e. run first part of forward single thread, then run second part in multiple threads, then the first part of graph is shared. In this case different threads execute `grad()` or `backward()` on the same graph might have issue of destroying the graph on the fly of one thread, and the other thread will crash in this case. Autograd will error out to the user similar to what call `backward()` twice with out `retain_graph=True`, and let the user know they should use `retain_graph=True`.
### Thread Safety on Autograd Node
Since Autograd allows the caller thread to drive its backward execution for potential parallelism, it’s important that we ensure thread safety on CPU with parallel `backward()` calls that share part/whole of the GraphTask.
Custom Python `autograd.Function`s are automatically thread safe because of GIL. For built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and custom `autograd::Function`s, the Autograd Engine uses thread mutex locking to ensure thread safety on autograd Nodes that might have state write/read.
### No thread safety on C++ hooks
Autograd relies on the user to write thread safe C++ hooks. If you want the hook to be correctly applied in multithreading environment, you will need to write proper thread locking code to ensure the hooks are thread safe.
## Autograd for Complex Numbers
The short version:
  * When you use PyTorch to differentiate any function f(z)f(z)f(z) with complex domain and/or codomain, the gradients are computed under the assumption that the function is a part of a larger real-valued loss function g(input)=Lg(input)=Lg(input)=L. The gradient computed is ∂L∂z∗\frac{\partial L}{\partial z^*}∂z∗∂L​ (note the conjugation of z), the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, there is a viable path in making the existing optimizers work out of the box with complex parameters.
  * This convention matches TensorFlow’s convention for complex differentiation, but is different from JAX (which computes ∂L∂z\frac{\partial L}{\partial z}∂z∂L​).
  * If you have a real-to-real function which internally uses complex operations, the convention here doesn’t matter: you will always get the same result that you would have gotten if it had been implemented with only real operations.


If you are curious about the mathematical details, or want to know how to define complex derivatives in PyTorch, read on.
### What are complex derivatives?
The mathematical definition of complex-differentiability takes the limit definition of a derivative and generalizes it to operate on complex numbers. Consider a function f:C→Cf: ℂ → ℂf:C→C,
> f(z=x+yj)=u(x,y)+v(x,y)jf(z=x+yj) = u(x, y) + v(x, y)j f(z=x+yj)=u(x,y)+v(x,y)j
where uuu and vvv are two variable real valued functions and jjj is the imaginary unit.
Using the derivative definition, we can write:
> f′(z)=lim⁡h→0,h∈Cf(z+h)−f(z)hf'(z) = \lim_{h \to 0, h \in C} \frac{f(z+h) - f(z)}{h} f′(z)=h→0,h∈Clim​hf(z+h)−f(z)​
In order for this limit to exist, not only must uuu and vvv must be real differentiable, but fff must also satisfy the Cauchy-Riemann equations. In other words: the limit computed for real and imaginary steps (hhh) must be equal. This is a more restrictive condition.
The complex differentiable functions are commonly known as holomorphic functions. They are well behaved, have all the nice properties that you’ve seen from real differentiable functions, but are practically of no use in the optimization world. For optimization problems, only real valued objective functions are used in the research community since complex numbers are not part of any ordered field and so having complex valued loss does not make much sense.
It also turns out that no interesting real-valued objective fulfill the Cauchy-Riemann equations. So the theory with holomorphic function cannot be used for optimization and most people therefore use the Wirtinger calculus.
### Wirtinger Calculus comes into the picture …
So, we have this great theory of complex differentiability and holomorphic functions, and we can’t use any of it at all, because many of the commonly used functions are not holomorphic. What’s a poor mathematician to do? Well, Wirtinger observed that even if f(z)f(z)f(z) isn’t holomorphic, one could rewrite it as a two variable function f(z,z∗)f(z, z*)f(z,z∗) which is always holomorphic. This is because real and imaginary of the components of zzz can be expressed in terms of zzz and z∗z^*z∗ as:
> Re(z)=z+z∗2Im(z)=z−z∗2j\begin{aligned} \mathrm{Re}(z) &= \frac {z + z^*}{2} \\\ \mathrm{Im}(z) &= \frac {z - z^*}{2j} \end{aligned} Re(z)Im(z)​=2z+z∗​=2jz−z∗​​
Wirtinger calculus suggests to study f(z,z∗)f(z, z^*)f(z,z∗) instead, which is guaranteed to be holomorphic if fff was real differentiable (another way to think of it is as a change of coordinate system, from f(x,y)f(x, y)f(x,y) to f(z,z∗)f(z, z^*)f(z,z∗).) This function has partial derivatives ∂∂z\frac{\partial }{\partial z}∂z∂​ and ∂∂z∗\frac{\partial}{\partial z^{*}}∂z∗∂​. We can use the chain rule to establish a relationship between these partial derivatives and the partial derivatives w.r.t., the real and imaginary components of zzz.
> ∂∂x=∂z∂x∗∂∂z+∂z∗∂x∗∂∂z∗=∂∂z+∂∂z∗∂∂y=∂z∂y∗∂∂z+∂z∗∂y∗∂∂z∗=1j∗(∂∂z−∂∂z∗)\begin{aligned} \frac{\partial }{\partial x} &= \frac{\partial z}{\partial x} * \frac{\partial }{\partial z} + \frac{\partial z^*}{\partial x} * \frac{\partial }{\partial z^*} \\\ &= \frac{\partial }{\partial z} + \frac{\partial }{\partial z^*} \\\ \\\ \frac{\partial }{\partial y} &= \frac{\partial z}{\partial y} * \frac{\partial }{\partial z} + \frac{\partial z^*}{\partial y} * \frac{\partial }{\partial z^*} \\\ &= 1j * \left(\frac{\partial }{\partial z} - \frac{\partial }{\partial z^*}\right) \end{aligned} ∂x∂​∂y∂​​=∂x∂z​∗∂z∂​+∂x∂z∗​∗∂z∗∂​=∂z∂​+∂z∗∂​=∂y∂z​∗∂z∂​+∂y∂z∗​∗∂z∗∂​=1j∗(∂z∂​−∂z∗∂​)​
From the above equations, we get:
> ∂∂z=1/2∗(∂∂x−1j∗∂∂y)∂∂z∗=1/2∗(∂∂x+1j∗∂∂y)\begin{aligned} \frac{\partial }{\partial z} &= 1/2 * \left(\frac{\partial }{\partial x} - 1j * \frac{\partial }{\partial y}\right) \\\ \frac{\partial }{\partial z^*} &= 1/2 * \left(\frac{\partial }{\partial x} + 1j * \frac{\partial }{\partial y}\right) \end{aligned} ∂z∂​∂z∗∂​​=1/2∗(∂x∂​−1j∗∂y∂​)=1/2∗(∂x∂​+1j∗∂y∂​)​
which is the classic definition of Wirtinger calculus that you would find on Wikipedia.
There are a lot of beautiful consequences of this change.
  * For one, the Cauchy-Riemann equations translate into simply saying that ∂f∂z∗=0\frac{\partial f}{\partial z^*} = 0∂z∗∂f​=0 (that is to say, the function fff can be written entirely in terms of zzz, without making reference to z∗z^*z∗).
  * Another important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should take while making variable update is given by ∂Loss∂z∗\frac{\partial Loss}{\partial z^*}∂z∗∂Loss​ (not ∂Loss∂z\frac{\partial Loss}{\partial z}∂z∂Loss​).


For more reading, check out: https://arxiv.org/pdf/0906.4835.pdf
### How is Wirtinger Calculus useful in optimization?
Researchers in audio and other fields, more commonly, use gradient descent to optimize real valued loss functions with complex variables. Typically, these people treat the real and imaginary values as separate channels that can be updated. For a step size α/2\alpha/2α/2 and loss LLL, we can write the following equations in R2ℝ^2R2:
> xn+1=xn−(α/2)∗∂L∂xyn+1=yn−(α/2)∗∂L∂y\begin{aligned} x_{n+1} &= x_n - (\alpha/2) * \frac{\partial L}{\partial x} \\\ y_{n+1} &= y_n - (\alpha/2) * \frac{\partial L}{\partial y} \end{aligned} xn+1​yn+1​​=xn​−(α/2)∗∂x∂L​=yn​−(α/2)∗∂y∂L​​
How do these equations translate into complex space CℂC?
> zn+1=xn−(α/2)∗∂L∂x+1j∗(yn−(α/2)∗∂L∂y)=zn−α∗1/2∗(∂L∂x+j∂L∂y)=zn−α∗∂L∂z∗\begin{aligned} z_{n+1} &= x_n - (\alpha/2) * \frac{\partial L}{\partial x} + 1j * (y_n - (\alpha/2) * \frac{\partial L}{\partial y}) \\\ &= z_n - \alpha * 1/2 * \left(\frac{\partial L}{\partial x} + j \frac{\partial L}{\partial y}\right) \\\ &= z_n - \alpha * \frac{\partial L}{\partial z^*} \end{aligned} zn+1​​=xn​−(α/2)∗∂x∂L​+1j∗(yn​−(α/2)∗∂y∂L​)=zn​−α∗1/2∗(∂x∂L​+j∂y∂L​)=zn​−α∗∂z∗∂L​​
Something very interesting has happened: Wirtinger calculus tells us that we can simplify the complex variable update formula above to only refer to the conjugate Wirtinger derivative ∂L∂z∗\frac{\partial L}{\partial z^*}∂z∗∂L​, giving us exactly the step we take in optimization.
Because the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative when you differentiate a function with a real valued loss.
### How does PyTorch compute the conjugate Wirtinger derivative?
Typically, our derivative formulas take in grad_output as an input, representing the incoming Vector-Jacobian product that we’ve already computed, aka, ∂L∂s∗\frac{\partial L}{\partial s^*}∂s∗∂L​, where LLL is the loss of the entire computation (producing a real loss) and sss is the output of our function. The goal here is to compute ∂L∂z∗\frac{\partial L}{\partial z^*}∂z∗∂L​, where zzz is the input of the function. It turns out that in the case of real loss, we can get away with _only_ calculating ∂L∂s∗\frac{\partial L}{\partial s^*}∂s∗∂L​, even though the chain rule implies that we also need to have access to ∂L∂s\frac{\partial L}{\partial s}∂s∂L​. If you want to skip this derivation, look at the last equation in this section and then skip to the next section.
Let’s continue working with f:C→Cf: ℂ → ℂf:C→C defined as f(z)=f(x+yj)=u(x,y)+v(x,y)jf(z) = f(x+yj) = u(x, y) + v(x, y)jf(z)=f(x+yj)=u(x,y)+v(x,y)j. As discussed above, autograd’s gradient convention is centered around optimization for real valued loss functions, so let’s assume fff is a part of larger real valued loss function ggg. Using chain rule, we can write:
> (1)∂L∂z∗=∂L∂u∗∂u∂z∗+∂L∂v∗∂v∂z∗\frac{\partial L}{\partial z^*} = \frac{\partial L}{\partial u} * \frac{\partial u}{\partial z^*} + \frac{\partial L}{\partial v} * \frac{\partial v}{\partial z^*} ∂z∗∂L​=∂u∂L​∗∂z∗∂u​+∂v∂L​∗∂z∗∂v​
Now using Wirtinger derivative definition, we can write:
> ∂L∂s=1/2∗(∂L∂u−∂L∂vj)∂L∂s∗=1/2∗(∂L∂u+∂L∂vj)\begin{aligned} \frac{\partial L}{\partial s} = 1/2 * \left(\frac{\partial L}{\partial u} - \frac{\partial L}{\partial v} j\right) \\\ \frac{\partial L}{\partial s^*} = 1/2 * \left(\frac{\partial L}{\partial u} + \frac{\partial L}{\partial v} j\right) \end{aligned} ∂s∂L​=1/2∗(∂u∂L​−∂v∂L​j)∂s∗∂L​=1/2∗(∂u∂L​+∂v∂L​j)​
It should be noted here that since uuu and vvv are real functions, and LLL is real by our assumption that fff is a part of a real valued function, we have:
> (2)(∂L∂s)∗=∂L∂s∗\left( \frac{\partial L}{\partial s} \right)^* = \frac{\partial L}{\partial s^*} (∂s∂L​)∗=∂s∗∂L​
i.e., ∂L∂s\frac{\partial L}{\partial s}∂s∂L​ equals to grad_output∗grad\\_output^*grad_output∗.
Solving the above equations for ∂L∂u\frac{\partial L}{\partial u}∂u∂L​ and ∂L∂v\frac{\partial L}{\partial v}∂v∂L​, we get:
> (3)∂L∂u=∂L∂s+∂L∂s∗∂L∂v=1j∗(∂L∂s−∂L∂s∗)\begin{aligned} \frac{\partial L}{\partial u} = \frac{\partial L}{\partial s} + \frac{\partial L}{\partial s^*} \\\ \frac{\partial L}{\partial v} = 1j * \left(\frac{\partial L}{\partial s} - \frac{\partial L}{\partial s^*}\right) \end{aligned} ∂u∂L​=∂s∂L​+∂s∗∂L​∂v∂L​=1j∗(∂s∂L​−∂s∗∂L​)​
Substituting (3) in (1), we get:
> ∂L∂z∗=(∂L∂s+∂L∂s∗)∗∂u∂z∗+1j∗(∂L∂s−∂L∂s∗)∗∂v∂z∗=∂L∂s∗(∂u∂z∗+∂v∂z∗j)+∂L∂s∗∗(∂u∂z∗−∂v∂z∗j)=∂L∂s∗∂(u+vj)∂z∗+∂L∂s∗∗∂(u+vj)∗∂z∗=∂L∂s∗∂s∂z∗+∂L∂s∗∗∂s∗∂z∗\begin{aligned} \frac{\partial L}{\partial z^*} &= \left(\frac{\partial L}{\partial s} + \frac{\partial L}{\partial s^*}\right) * \frac{\partial u}{\partial z^*} + 1j * \left(\frac{\partial L}{\partial s} - \frac{\partial L}{\partial s^*}\right) * \frac{\partial v}{\partial z^*} \\\ &= \frac{\partial L}{\partial s} * \left(\frac{\partial u}{\partial z^*} + \frac{\partial v}{\partial z^*} j\right) + \frac{\partial L}{\partial s^*} * \left(\frac{\partial u}{\partial z^*} - \frac{\partial v}{\partial z^*} j\right) \\\ &= \frac{\partial L}{\partial s} * \frac{\partial (u + vj)}{\partial z^*} + \frac{\partial L}{\partial s^*} * \frac{\partial (u + vj)^*}{\partial z^*} \\\ &= \frac{\partial L}{\partial s} * \frac{\partial s}{\partial z^*} + \frac{\partial L}{\partial s^*} * \frac{\partial s^*}{\partial z^*} \\\ \end{aligned} ∂z∗∂L​​=(∂s∂L​+∂s∗∂L​)∗∂z∗∂u​+1j∗(∂s∂L​−∂s∗∂L​)∗∂z∗∂v​=∂s∂L​∗(∂z∗∂u​+∂z∗∂v​j)+∂s∗∂L​∗(∂z∗∂u​−∂z∗∂v​j)=∂s∂L​∗∂z∗∂(u+vj)​+∂s∗∂L​∗∂z∗∂(u+vj)∗​=∂s∂L​∗∂z∗∂s​+∂s∗∂L​∗∂z∗∂s∗​​
Using (2), we get:
> (4)∂L∂z∗=(∂L∂s∗)∗∗∂s∂z∗+∂L∂s∗∗(∂s∂z)∗=(grad_output)∗∗∂s∂z∗+grad_output∗(∂s∂z)∗\begin{aligned} \frac{\partial L}{\partial z^*} &= \left(\frac{\partial L}{\partial s^*}\right)^* * \frac{\partial s}{\partial z^*} + \frac{\partial L}{\partial s^*} * \left(\frac{\partial s}{\partial z}\right)^* \\\ &= \boxed{ (grad\\_output)^* * \frac{\partial s}{\partial z^*} + grad\\_output * \left(\frac{\partial s}{\partial z}\right)^* } \\\ \end{aligned} ∂z∗∂L​​=(∂s∗∂L​)∗∗∂z∗∂s​+∂s∗∂L​∗(∂z∂s​)∗=(grad_output)∗∗∂z∗∂s​+grad_output∗(∂z∂s​)∗​​
This last equation is the important one for writing your own gradients, as it decomposes our derivative formula into a simpler one that is easy to compute by hand.
### How can I write my own derivative formula for a complex function?
The above boxed equation gives us the general formula for all derivatives on complex functions. However, we still need to compute ∂s∂z\frac{\partial s}{\partial z}∂z∂s​ and ∂s∂z∗\frac{\partial s}{\partial z^*}∂z∗∂s​. There are two ways you could do this:
>   * The first way is to just use the definition of Wirtinger derivatives directly and calculate ∂s∂z\frac{\partial s}{\partial z}∂z∂s​ and ∂s∂z∗\frac{\partial s}{\partial z^*}∂z∗∂s​ by using ∂s∂x\frac{\partial s}{\partial x}∂x∂s​ and ∂s∂y\frac{\partial s}{\partial y}∂y∂s​ (which you can compute in the normal way).
>   * The second way is to use the change of variables trick and rewrite f(z)f(z)f(z) as a two variable function f(z,z∗)f(z, z^*)f(z,z∗), and compute the conjugate Wirtinger derivatives by treating zzz and z∗z^*z∗ as independent variables. This is often easier; for example, if the function in question is holomorphic, only zzz will be used (and ∂s∂z∗\frac{\partial s}{\partial z^*}∂z∗∂s​ will be zero).
> 

Let’s consider the function f(z=x+yj)=c∗z=c∗(x+yj)f(z = x + yj) = c * z = c * (x+yj)f(z=x+yj)=c∗z=c∗(x+yj) as an example, where c∈Rc \in ℝc∈R.
Using the first way to compute the Wirtinger derivatives, we have.
∂s∂z=1/2∗(∂s∂x−∂s∂yj)=1/2∗(c−(c∗1j)∗1j)=c∂s∂z∗=1/2∗(∂s∂x+∂s∂yj)=1/2∗(c+(c∗1j)∗1j)=0\begin{aligned} \frac{\partial s}{\partial z} &= 1/2 * \left(\frac{\partial s}{\partial x} - \frac{\partial s}{\partial y} j\right) \\\ &= 1/2 * (c - (c * 1j) * 1j) \\\ &= c \\\ \\\ \\\ \frac{\partial s}{\partial z^*} &= 1/2 * \left(\frac{\partial s}{\partial x} + \frac{\partial s}{\partial y} j\right) \\\ &= 1/2 * (c + (c * 1j) * 1j) \\\ &= 0 \\\ \end{aligned} ∂z∂s​∂z∗∂s​​=1/2∗(∂x∂s​−∂y∂s​j)=1/2∗(c−(c∗1j)∗1j)=c=1/2∗(∂x∂s​+∂y∂s​j)=1/2∗(c+(c∗1j)∗1j)=0​
Using (4), and grad_output = 1.0 (which is the default grad output value used when `backward()` is called on a scalar output in PyTorch), we get:
> ∂L∂z∗=1∗0+1∗c=c\frac{\partial L}{\partial z^*} = 1 * 0 + 1 * c = c ∂z∗∂L​=1∗0+1∗c=c
Using the second way to compute Wirtinger derivatives, we directly get:
> ∂s∂z=∂(c∗z)∂z=c∂s∂z∗=∂(c∗z)∂z∗=0\begin{aligned} \frac{\partial s}{\partial z} &= \frac{\partial (c*z)}{\partial z} \\\ &= c \\\ \frac{\partial s}{\partial z^*} &= \frac{\partial (c*z)}{\partial z^*} \\\ &= 0 \end{aligned} ∂z∂s​∂z∗∂s​​=∂z∂(c∗z)​=c=∂z∗∂(c∗z)​=0​
And using (4) again, we get ∂L∂z∗=c\frac{\partial L}{\partial z^*} = c∂z∗∂L​=c. As you can see, the second way involves lesser calculations, and comes in more handy for faster calculations.
### What about cross-domain functions?
Some functions map from complex inputs to real outputs, or vice versa. These functions form a special case of (4), which we can derive using the chain rule:
>   * For f:C→Rf: ℂ → ℝf:C→R, we get:
>> ∂L∂z∗=2∗grad_output∗∂s∂z∗\frac{\partial L}{\partial z^*} = 2 * grad\\_output * \frac{\partial s}{\partial z^{*}} ∂z∗∂L​=2∗grad_output∗∂z∗∂s​
>   * For f:R→Cf: ℝ → ℂf:R→C, we get:
>> ∂L∂z∗=2∗Re(grad_output∗∗∂s∂z∗)\frac{\partial L}{\partial z^*} = 2 * \mathrm{Re}(grad\\_output^* * \frac{\partial s}{\partial z^{*}}) ∂z∗∂L​=2∗Re(grad_output∗∗∂z∗∂s​)
> 

## Hooks for saved tensors
You can control how saved tensors are packed / unpacked by defining a pair of `pack_hook` / `unpack_hook` hooks. The `pack_hook` function should take a tensor as its single argument but can return any python object (e.g. another tensor, a tuple, or even a string containing a filename). The `unpack_hook` function takes as its single argument the output of `pack_hook` and should return a tensor to be used in the backward pass. The tensor returned by `unpack_hook` only needs to have the same content as the tensor passed as input to `pack_hook`. In particular, any autograd-related metadata can be ignored as they will be overwritten during unpacking.
An example of such pair is:
```
class SelfDeletingTempFile():
  def __init__(self):
    self.name = os.path.join(tmp_dir, str(uuid.uuid4()))
  def __del__(self):
    os.remove(self.name)
def pack_hook(tensor):
  temp_file = SelfDeletingTempFile()
  torch.save(tensor, temp_file.name)
  return temp_file
def unpack_hook(temp_file):
  return torch.load(temp_file.name)

```
Copy to clipboard
Notice that the `unpack_hook` should not delete the temporary file because it might be called multiple times: the temporary file should be alive for as long as the returned SelfDeletingTempFile object is alive. In the above example, we prevent leaking the temporary file by closing it when it is no longer needed (on deletion of the SelfDeletingTempFile object).
Note
We guarantee that `pack_hook` will only be called once but `unpack_hook` can be called as many times as the backward pass requires it and we expect it to return the same data each time.
Warning
Performing inplace operations on the input of any of the functions is forbidden as they may lead to unexpected side-effects. PyTorch will throw an error if the input to a pack hook is modified inplace but does not catch the case where the input to an unpack hook is modified inplace.
### Registering hooks for a saved tensor
You can register a pair of hooks on a saved tensor by calling the `register_hooks()` method on a `SavedTensor` object. Those objects are exposed as attributes of a `grad_fn` and start with the `_raw_saved_` prefix.
```
x = torch.randn(5, requires_grad=True)
y = x.pow(2)
y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)

```
Copy to clipboard
The `pack_hook` method is called as soon as the pair is registered. The `unpack_hook` method is called each time the saved tensor needs to be accessed, either by means of `y.grad_fn._saved_self` or during the backward pass.
Warning
If you maintain a reference to a `SavedTensor` after the saved tensors have been released (i.e. after backward has been called), calling its `register_hooks()` is forbidden. PyTorch will throw an error most of the time but it may fail to do so in some cases and undefined behavior may arise.
### Registering default hooks for saved tensors
Alternatively, you can use the context-manager `saved_tensors_hooks` to register a pair of hooks which will be applied to _all_ saved tensors that are created in that context.
Example:
```
# Only save on disk tensors that have size >= 1000
SAVE_ON_DISK_THRESHOLD = 1000
def pack_hook(x):
  if x.numel() < SAVE_ON_DISK_THRESHOLD:
    return x
  temp_file = SelfDeletingTempFile()
  torch.save(tensor, temp_file.name)
  return temp_file
def unpack_hook(tensor_or_sctf):
  if isinstance(tensor_or_sctf, torch.Tensor):
    return tensor_or_sctf
  return torch.load(tensor_or_sctf.name)
class Model(nn.Module):
  def forward(self, x):
    with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
     # ... compute output
     output = x
    return output
model = Model()
net = nn.DataParallel(model)

```
Copy to clipboard
The hooks defined with this context manager are thread-local. Hence, the following code will not produce the desired effects because the hooks do not go through DataParallel.
```
# Example what NOT to do
net = nn.DataParallel(model)
with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
  output = net(input)

```
Copy to clipboard
Note that using those hooks disables all the optimization in place to reduce Tensor object creation. For example:
```
with torch.autograd.graph.saved_tensors_hooks(lambda x: x, lambda x: x):
  x = torch.randn(5, requires_grad=True)
  y = x * x

```
Copy to clipboard
Without the hooks, `x`, `y.grad_fn._saved_self` and `y.grad_fn._saved_other` all refer to the same tensor object. With the hooks, PyTorch will pack and unpack x into two new tensor objects that share the same storage with the original x (no copy performed).
## Backward Hooks execution
This section will discuss when different hooks fire or don’t fire. Then it will discuss the order in which they are fired. The hooks that will be covered are: backward hooks registered to Tensor via `torch.Tensor.register_hook()`, post-accumulate-grad hooks registered to Tensor via `torch.Tensor.register_post_accumulate_grad_hook()`, post-hooks registered to Node via `torch.autograd.graph.Node.register_hook()`, and pre-hooks registered to Node via `torch.autograd.graph.Node.register_prehook()`.
### Whether a particular hook will be fired
Hooks registered to a Tensor via `torch.Tensor.register_hook()` are executed when gradients are being computed for that Tensor. (Note that this does not require the Tensor’s grad_fn to be executed. For example, if the Tensor is passed as part of the `inputs` argument to `torch.autograd.grad()`, the Tensor’s grad_fn may not be executed, but the hook register to that Tensor will always be executed.)
Hooks registered to a Tensor via `torch.Tensor.register_post_accumulate_grad_hook()` are executed after the gradients have been accumulated for that Tensor, meaning the Tensor’s grad field has been set. Whereas hooks registered via `torch.Tensor.register_hook()` are run as gradients are being computed, hooks registered via `torch.Tensor.register_post_accumulate_grad_hook()` are only triggered once the Tensor’s grad field is updated by autograd at the end of the backward pass. Thus, post-accumulate-grad hooks can only be registered for leaf Tensors. Registering a hook via `torch.Tensor.register_post_accumulate_grad_hook()` on a non-leaf Tensor will error, even if you call backward(retain_graph=True).
Hooks registered to `torch.autograd.graph.Node` using `torch.autograd.graph.Node.register_hook()` or `torch.autograd.graph.Node.register_prehook()` are only fired if the Node it was registered to is executed.
Whether a particular Node is executed may depend on whether the backward pass was called with `torch.autograd.grad()` or `torch.autograd.backward()`. Specifically, you should be aware of these differences when you register a hook on a Node corresponding to a Tensor that you are passing to `torch.autograd.grad()` or `torch.autograd.backward()` as part of the `inputs` argument.
If you are using `torch.autograd.backward()`, all of the above mentioned hooks will be executed, whether or not you specified the `inputs` argument. This is because .backward() executes all Nodes, even if they correspond to a Tensor specified as an input. (Note that the execution of this additional Node corresponding to Tensors passed as `inputs` is usually unnecessary, but done anyway. This behavior is subject to change; you should not depend on it.)
On the other hand, if you are using `torch.autograd.grad()`, the backward hooks registered to Nodes that correspond to the Tensors passed to `input` may not be executed, because those Nodes will not be executed unless there is another input that depends on the gradient result of this Node.
### The order in which the different hooks are fired
The order in which things happen are:
  1. hooks registered to Tensor are executed
  2. pre-hooks registered to Node are executed (if Node is executed).
  3. the `.grad` field is updated for Tensors that retain_grad
  4. Node is executed (subject to rules above)
  5. for leaf Tensors that have `.grad` accumulated, post-accumulate-grad hooks are executed
  6. post-hooks registered to Node are executed (if Node is executed)


If multiple hooks of the same type are registered on the same Tensor or Node they are executed in the order in which they are registered. Hooks that are executed later can observe the modifications to the gradient made by earlier hooks.
### Special hooks
`torch.autograd.graph.register_multi_grad_hook()` is implemented using hooks registered to Tensors. Each individual Tensor hook is fired following the Tensor hook ordering defined above and the registered multi-grad hook is called when the last Tensor gradient is computed.
`torch.nn.modules.module.register_module_full_backward_hook()` is implemented using hooks registered to Node. As the forward is computed, hooks are registered to grad_fn corresponding to the inputs and outputs of the module. Because a module may take multiple inputs and return multiple outputs, a dummy custom autograd Function is first applied to the inputs of the module before forward and the outputs of the module before the output of forward is returned to ensure that those Tensors share a single grad_fn, which we can then attach our hooks to.
### Behavior of Tensor hooks when Tensor is modified in-place
Usually hooks registered to a Tensor receive the gradient of the outputs with respect to that Tensor, where the value of the Tensor is taken to be its value at the time backward is computed.
However, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks registered before in-place modification similarly receive gradients of the outputs with respect to the Tensor, but the value of the Tensor is taken to be its value before in-place modification.
If you prefer the behavior in the former case, you should register them to the Tensor after all in-place modifications to it have been made. For example:
```
t = torch.tensor(1., requires_grad=True).sin()
t.cos_()
t.register_hook(fn)
t.backward()

```
Copy to clipboard
Furthermore, it can be helpful to know that under the hood, when hooks are registered to a Tensor, they actually become permanently bound to the grad_fn of that Tensor, so if that Tensor is then modified in-place, even though the Tensor now has a new grad_fn, hooks registered before it was modified in-place will continue to be associated with the old grad_fn, e.g. they will fire when that Tensor’s old grad_fn is reached in the graph by the autograd engine.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Autograd mechanics
    * How autograd encodes the history
      * Saved tensors
    * Gradients for non-differentiable functions
    * Locally disabling gradient computation
      * Setting `requires_grad`
      * Grad Modes
      * Default Mode (Grad Mode)
      * No-grad Mode
      * Inference Mode
      * Evaluation Mode (`nn.Module.eval()`)
    * In-place operations with autograd
      * In-place correctness checks
    * Multithreaded Autograd
      * Concurrency on CPU
      * Non-determinism
      * Graph retaining
      * Thread Safety on Autograd Node
      * No thread safety on C++ hooks
    * Autograd for Complex Numbers
      * What are complex derivatives?
      * Wirtinger Calculus comes into the picture …
      * How is Wirtinger Calculus useful in optimization?
      * How does PyTorch compute the conjugate Wirtinger derivative?
      * How can I write my own derivative formula for a complex function?
      * What about cross-domain functions?
    * Hooks for saved tensors
      * Registering hooks for a saved tensor
      * Registering default hooks for saved tensors
    * Backward Hooks execution
      * Whether a particular hook will be fired
      * The order in which the different hooks are fired
      * Special hooks
      * Behavior of Tensor hooks when Tensor is modified in-place


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.nn
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.nn
These are the basic building blocks for graphs:
torch.nn
  * Containers
  * Convolution Layers
  * Pooling layers
  * Padding Layers
  * Non-linear Activations (weighted sum, nonlinearity)
  * Non-linear Activations (other)
  * Normalization Layers
  * Recurrent Layers
  * Transformer Layers
  * Linear Layers
  * Dropout Layers
  * Sparse Layers
  * Distance Functions
  * Loss Functions
  * Vision Layers
  * Shuffle Layers
  * DataParallel Layers (multi-GPU, distributed)
  * Utilities
  * Quantized Functions
  * Lazy Modules Initialization
    * Aliases


`Buffer` | A kind of Tensor that should not be considered a model parameter.  
---|---  
`Parameter` | A kind of Tensor that is to be considered a module parameter.  
`UninitializedParameter` | A parameter that is not initialized.  
`UninitializedBuffer` | A buffer that is not initialized.  
## Containers
`Module` | Base class for all neural network modules.  
---|---  
`Sequential` | A sequential container.  
`ModuleList` | Holds submodules in a list.  
`ModuleDict` | Holds submodules in a dictionary.  
`ParameterList` | Holds parameters in a list.  
`ParameterDict` | Holds parameters in a dictionary.  
Global Hooks For Module
`register_module_forward_pre_hook` | Register a forward pre-hook common to all modules.  
---|---  
`register_module_forward_hook` | Register a global forward hook for all the modules.  
`register_module_backward_hook` | Register a backward hook common to all the modules.  
`register_module_full_backward_pre_hook` | Register a backward pre-hook common to all the modules.  
`register_module_full_backward_hook` | Register a backward hook common to all the modules.  
`register_module_buffer_registration_hook` | Register a buffer registration hook common to all modules.  
`register_module_module_registration_hook` | Register a module registration hook common to all modules.  
`register_module_parameter_registration_hook` | Register a parameter registration hook common to all modules.  
## Convolution Layers
`nn.Conv1d` | Applies a 1D convolution over an input signal composed of several input planes.  
---|---  
`nn.Conv2d` | Applies a 2D convolution over an input signal composed of several input planes.  
`nn.Conv3d` | Applies a 3D convolution over an input signal composed of several input planes.  
`nn.ConvTranspose1d` | Applies a 1D transposed convolution operator over an input image composed of several input planes.  
`nn.ConvTranspose2d` | Applies a 2D transposed convolution operator over an input image composed of several input planes.  
`nn.ConvTranspose3d` | Applies a 3D transposed convolution operator over an input image composed of several input planes.  
`nn.LazyConv1d` | A `torch.nn.Conv1d` module with lazy initialization of the `in_channels` argument.  
`nn.LazyConv2d` | A `torch.nn.Conv2d` module with lazy initialization of the `in_channels` argument.  
`nn.LazyConv3d` | A `torch.nn.Conv3d` module with lazy initialization of the `in_channels` argument.  
`nn.LazyConvTranspose1d` | A `torch.nn.ConvTranspose1d` module with lazy initialization of the `in_channels` argument.  
`nn.LazyConvTranspose2d` | A `torch.nn.ConvTranspose2d` module with lazy initialization of the `in_channels` argument.  
`nn.LazyConvTranspose3d` | A `torch.nn.ConvTranspose3d` module with lazy initialization of the `in_channels` argument.  
`nn.Unfold` | Extracts sliding local blocks from a batched input tensor.  
`nn.Fold` | Combines an array of sliding local blocks into a large containing tensor.  
## Pooling layers
`nn.MaxPool1d` | Applies a 1D max pooling over an input signal composed of several input planes.  
---|---  
`nn.MaxPool2d` | Applies a 2D max pooling over an input signal composed of several input planes.  
`nn.MaxPool3d` | Applies a 3D max pooling over an input signal composed of several input planes.  
`nn.MaxUnpool1d` | Computes a partial inverse of `MaxPool1d`.  
`nn.MaxUnpool2d` | Computes a partial inverse of `MaxPool2d`.  
`nn.MaxUnpool3d` | Computes a partial inverse of `MaxPool3d`.  
`nn.AvgPool1d` | Applies a 1D average pooling over an input signal composed of several input planes.  
`nn.AvgPool2d` | Applies a 2D average pooling over an input signal composed of several input planes.  
`nn.AvgPool3d` | Applies a 3D average pooling over an input signal composed of several input planes.  
`nn.FractionalMaxPool2d` | Applies a 2D fractional max pooling over an input signal composed of several input planes.  
`nn.FractionalMaxPool3d` | Applies a 3D fractional max pooling over an input signal composed of several input planes.  
`nn.LPPool1d` | Applies a 1D power-average pooling over an input signal composed of several input planes.  
`nn.LPPool2d` | Applies a 2D power-average pooling over an input signal composed of several input planes.  
`nn.LPPool3d` | Applies a 3D power-average pooling over an input signal composed of several input planes.  
`nn.AdaptiveMaxPool1d` | Applies a 1D adaptive max pooling over an input signal composed of several input planes.  
`nn.AdaptiveMaxPool2d` | Applies a 2D adaptive max pooling over an input signal composed of several input planes.  
`nn.AdaptiveMaxPool3d` | Applies a 3D adaptive max pooling over an input signal composed of several input planes.  
`nn.AdaptiveAvgPool1d` | Applies a 1D adaptive average pooling over an input signal composed of several input planes.  
`nn.AdaptiveAvgPool2d` | Applies a 2D adaptive average pooling over an input signal composed of several input planes.  
`nn.AdaptiveAvgPool3d` | Applies a 3D adaptive average pooling over an input signal composed of several input planes.  
## Padding Layers
`nn.ReflectionPad1d` | Pads the input tensor using the reflection of the input boundary.  
---|---  
`nn.ReflectionPad2d` | Pads the input tensor using the reflection of the input boundary.  
`nn.ReflectionPad3d` | Pads the input tensor using the reflection of the input boundary.  
`nn.ReplicationPad1d` | Pads the input tensor using replication of the input boundary.  
`nn.ReplicationPad2d` | Pads the input tensor using replication of the input boundary.  
`nn.ReplicationPad3d` | Pads the input tensor using replication of the input boundary.  
`nn.ZeroPad1d` | Pads the input tensor boundaries with zero.  
`nn.ZeroPad2d` | Pads the input tensor boundaries with zero.  
`nn.ZeroPad3d` | Pads the input tensor boundaries with zero.  
`nn.ConstantPad1d` | Pads the input tensor boundaries with a constant value.  
`nn.ConstantPad2d` | Pads the input tensor boundaries with a constant value.  
`nn.ConstantPad3d` | Pads the input tensor boundaries with a constant value.  
`nn.CircularPad1d` | Pads the input tensor using circular padding of the input boundary.  
`nn.CircularPad2d` | Pads the input tensor using circular padding of the input boundary.  
`nn.CircularPad3d` | Pads the input tensor using circular padding of the input boundary.  
## Non-linear Activations (weighted sum, nonlinearity)
`nn.ELU` | Applies the Exponential Linear Unit (ELU) function, element-wise.  
---|---  
`nn.Hardshrink` | Applies the Hard Shrinkage (Hardshrink) function element-wise.  
`nn.Hardsigmoid` | Applies the Hardsigmoid function element-wise.  
`nn.Hardtanh` | Applies the HardTanh function element-wise.  
`nn.Hardswish` | Applies the Hardswish function, element-wise.  
`nn.LeakyReLU` | Applies the LeakyReLU function element-wise.  
`nn.LogSigmoid` | Applies the Logsigmoid function element-wise.  
`nn.MultiheadAttention` | Allows the model to jointly attend to information from different representation subspaces.  
`nn.PReLU` | Applies the element-wise PReLU function.  
`nn.ReLU` | Applies the rectified linear unit function element-wise.  
`nn.ReLU6` | Applies the ReLU6 function element-wise.  
`nn.RReLU` | Applies the randomized leaky rectified linear unit function, element-wise.  
`nn.SELU` | Applies the SELU function element-wise.  
`nn.CELU` | Applies the CELU function element-wise.  
`nn.GELU` | Applies the Gaussian Error Linear Units function.  
`nn.Sigmoid` | Applies the Sigmoid function element-wise.  
`nn.SiLU` | Applies the Sigmoid Linear Unit (SiLU) function, element-wise.  
`nn.Mish` | Applies the Mish function, element-wise.  
`nn.Softplus` | Applies the Softplus function element-wise.  
`nn.Softshrink` | Applies the soft shrinkage function element-wise.  
`nn.Softsign` | Applies the element-wise Softsign function.  
`nn.Tanh` | Applies the Hyperbolic Tangent (Tanh) function element-wise.  
`nn.Tanhshrink` | Applies the element-wise Tanhshrink function.  
`nn.Threshold` | Thresholds each element of the input Tensor.  
`nn.GLU` | Applies the gated linear unit function.  
## Non-linear Activations (other)
`nn.Softmin` | Applies the Softmin function to an n-dimensional input Tensor.  
---|---  
`nn.Softmax` | Applies the Softmax function to an n-dimensional input Tensor.  
`nn.Softmax2d` | Applies SoftMax over features to each spatial location.  
`nn.LogSoftmax` | Applies the log⁡(Softmax(x))\log(\text{Softmax}(x))log(Softmax(x)) function to an n-dimensional input Tensor.  
`nn.AdaptiveLogSoftmaxWithLoss` | Efficient softmax approximation.  
## Normalization Layers
`nn.BatchNorm1d` | Applies Batch Normalization over a 2D or 3D input.  
---|---  
`nn.BatchNorm2d` | Applies Batch Normalization over a 4D input.  
`nn.BatchNorm3d` | Applies Batch Normalization over a 5D input.  
`nn.LazyBatchNorm1d` | A `torch.nn.BatchNorm1d` module with lazy initialization.  
`nn.LazyBatchNorm2d` | A `torch.nn.BatchNorm2d` module with lazy initialization.  
`nn.LazyBatchNorm3d` | A `torch.nn.BatchNorm3d` module with lazy initialization.  
`nn.GroupNorm` | Applies Group Normalization over a mini-batch of inputs.  
`nn.SyncBatchNorm` | Applies Batch Normalization over a N-Dimensional input.  
`nn.InstanceNorm1d` | Applies Instance Normalization.  
`nn.InstanceNorm2d` | Applies Instance Normalization.  
`nn.InstanceNorm3d` | Applies Instance Normalization.  
`nn.LazyInstanceNorm1d` | A `torch.nn.InstanceNorm1d` module with lazy initialization of the `num_features` argument.  
`nn.LazyInstanceNorm2d` | A `torch.nn.InstanceNorm2d` module with lazy initialization of the `num_features` argument.  
`nn.LazyInstanceNorm3d` | A `torch.nn.InstanceNorm3d` module with lazy initialization of the `num_features` argument.  
`nn.LayerNorm` | Applies Layer Normalization over a mini-batch of inputs.  
`nn.LocalResponseNorm` | Applies local response normalization over an input signal.  
`nn.RMSNorm` | Applies Root Mean Square Layer Normalization over a mini-batch of inputs.  
## Recurrent Layers
`nn.RNNBase` | Base class for RNN modules (RNN, LSTM, GRU).  
---|---  
`nn.RNN` | Apply a multi-layer Elman RNN with tanh⁡\tanhtanh or ReLU\text{ReLU}ReLU non-linearity to an input sequence.  
`nn.LSTM` | Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence.  
`nn.GRU` | Apply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.  
`nn.RNNCell` | An Elman RNN cell with tanh or ReLU non-linearity.  
`nn.LSTMCell` | A long short-term memory (LSTM) cell.  
`nn.GRUCell` | A gated recurrent unit (GRU) cell.  
## Transformer Layers
`nn.Transformer` | A transformer model.  
---|---  
`nn.TransformerEncoder` | TransformerEncoder is a stack of N encoder layers.  
`nn.TransformerDecoder` | TransformerDecoder is a stack of N decoder layers.  
`nn.TransformerEncoderLayer` | TransformerEncoderLayer is made up of self-attn and feedforward network.  
`nn.TransformerDecoderLayer` | TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.  
## Linear Layers
`nn.Identity` | A placeholder identity operator that is argument-insensitive.  
---|---  
`nn.Linear` | Applies an affine linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.  
`nn.Bilinear` | Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T​Ax2​+b.  
`nn.LazyLinear` | A `torch.nn.Linear` module where in_features is inferred.  
## Dropout Layers
`nn.Dropout` | During training, randomly zeroes some of the elements of the input tensor with probability `p`.  
---|---  
`nn.Dropout1d` | Randomly zero out entire channels.  
`nn.Dropout2d` | Randomly zero out entire channels.  
`nn.Dropout3d` | Randomly zero out entire channels.  
`nn.AlphaDropout` | Applies Alpha Dropout over the input.  
`nn.FeatureAlphaDropout` | Randomly masks out entire channels.  
## Sparse Layers
`nn.Embedding` | A simple lookup table that stores embeddings of a fixed dictionary and size.  
---|---  
`nn.EmbeddingBag` | Compute sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.  
## Distance Functions
`nn.CosineSimilarity` | Returns cosine similarity between x1x_1x1​ and x2x_2x2​, computed along dim.  
---|---  
`nn.PairwiseDistance` | Computes the pairwise distance between input vectors, or between columns of input matrices.  
## Loss Functions
`nn.L1Loss` | Creates a criterion that measures the mean absolute error (MAE) between each element in the input xxx and target yyy.  
---|---  
`nn.MSELoss` | Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xxx and target yyy.  
`nn.CrossEntropyLoss` | This criterion computes the cross entropy loss between input logits and target.  
`nn.CTCLoss` | The Connectionist Temporal Classification loss.  
`nn.NLLLoss` | The negative log likelihood loss.  
`nn.PoissonNLLLoss` | Negative log likelihood loss with Poisson distribution of target.  
`nn.GaussianNLLLoss` | Gaussian negative log likelihood loss.  
`nn.KLDivLoss` | The Kullback-Leibler divergence loss.  
`nn.BCELoss` | Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:  
`nn.BCEWithLogitsLoss` | This loss combines a Sigmoid layer and the BCELoss in one single class.  
`nn.MarginRankingLoss` | Creates a criterion that measures the loss given inputs x1x1x1, x2x2x2, two 1D mini-batch or 0D Tensors, and a label 1D mini-batch or 0D Tensor yyy (containing 1 or -1).  
`nn.HingeEmbeddingLoss` | Measures the loss given an input tensor xxx and a labels tensor yyy (containing 1 or -1).  
`nn.MultiLabelMarginLoss` | Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xxx (a 2D mini-batch Tensor) and output yyy (which is a 2D Tensor of target class indices).  
`nn.HuberLoss` | Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.  
`nn.SmoothL1Loss` | Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.  
`nn.SoftMarginLoss` | Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy (containing 1 or -1).  
`nn.MultiLabelSoftMarginLoss` | Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xxx and target yyy of size (N,C)(N, C)(N,C).  
`nn.CosineEmbeddingLoss` | Creates a criterion that measures the loss given input tensors x1x_1x1​, x2x_2x2​ and a Tensor label yyy with values 1 or -1.  
`nn.MultiMarginLoss` | Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xxx (a 2D mini-batch Tensor) and output yyy (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−10 \leq y \leq \text{x.size}(1)-10≤y≤x.size(1)−1):  
`nn.TripletMarginLoss` | Creates a criterion that measures the triplet loss given an input tensors x1x1x1, x2x2x2, x3x3x3 and a margin with a value greater than 000.  
`nn.TripletMarginWithDistanceLoss` | Creates a criterion that measures the triplet loss given input tensors aaa, ppp, and nnn (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function ("distance function") used to compute the relationship between the anchor and positive example ("positive distance") and the anchor and negative example ("negative distance").  
## Vision Layers
`nn.PixelShuffle` | Rearrange elements in a tensor according to an upscaling factor.  
---|---  
`nn.PixelUnshuffle` | Reverse the PixelShuffle operation.  
`nn.Upsample` | Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.  
`nn.UpsamplingNearest2d` | Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.  
`nn.UpsamplingBilinear2d` | Applies a 2D bilinear upsampling to an input signal composed of several input channels.  
## Shuffle Layers
`nn.ChannelShuffle` | Divides and rearranges the channels in a tensor.  
---|---  
## DataParallel Layers (multi-GPU, distributed)
`nn.DataParallel` | Implements data parallelism at the module level.  
---|---  
`nn.parallel.DistributedDataParallel` | Implement distributed data parallelism based on `torch.distributed` at module level.  
## Utilities
From the `torch.nn.utils` module:
Utility functions to clip parameter gradients.
`clip_grad_norm_` | Clip the gradient norm of an iterable of parameters.  
---|---  
`clip_grad_norm` | Clip the gradient norm of an iterable of parameters.  
`clip_grad_value_` | Clip the gradients of an iterable of parameters at specified value.  
`get_total_norm` | Compute the norm of an iterable of tensors.  
`clip_grads_with_norm_` | Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm.  
Utility functions to flatten and unflatten Module parameters to and from a single vector.
`parameters_to_vector` | Flatten an iterable of parameters into a single vector.  
---|---  
`vector_to_parameters` | Copy slices of a vector into an iterable of parameters.  
Utility functions to fuse Modules with BatchNorm modules.
`fuse_conv_bn_eval` | Fuse a convolutional module and a BatchNorm module into a single, new convolutional module.  
---|---  
`fuse_conv_bn_weights` | Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.  
`fuse_linear_bn_eval` | Fuse a linear module and a BatchNorm module into a single, new linear module.  
`fuse_linear_bn_weights` | Fuse linear module parameters and BatchNorm module parameters into new linear module parameters.  
Utility functions to convert Module parameter memory formats.
`convert_conv2d_weight_memory_format` | Convert `memory_format` of `nn.Conv2d.weight` to `memory_format`.  
---|---  
`convert_conv3d_weight_memory_format` | Convert `memory_format` of `nn.Conv3d.weight` to `memory_format` The conversion recursively applies to nested `nn.Module`, including `module`.  
Utility functions to apply and remove weight normalization from Module parameters.
`weight_norm` | Apply weight normalization to a parameter in the given module.  
---|---  
`remove_weight_norm` | Remove the weight normalization reparameterization from a module.  
`spectral_norm` | Apply spectral normalization to a parameter in the given module.  
`remove_spectral_norm` | Remove the spectral normalization reparameterization from a module.  
Utility functions for initializing Module parameters.
`skip_init` | Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.  
---|---  
Utility classes and functions for pruning Module parameters.
`prune.BasePruningMethod` | Abstract base class for creation of new pruning techniques.  
---|---  
`prune.PruningContainer` | Container holding a sequence of pruning methods for iterative pruning.  
`prune.Identity` | Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.  
`prune.RandomUnstructured` | Prune (currently unpruned) units in a tensor at random.  
`prune.L1Unstructured` | Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.  
`prune.RandomStructured` | Prune entire (currently unpruned) channels in a tensor at random.  
`prune.LnStructured` | Prune entire (currently unpruned) channels in a tensor based on their L`n`-norm.  
`prune.CustomFromMask` |   
`prune.identity` | Apply pruning reparametrization without pruning any units.  
`prune.random_unstructured` | Prune tensor by removing random (currently unpruned) units.  
`prune.l1_unstructured` | Prune tensor by removing units with the lowest L1-norm.  
`prune.random_structured` | Prune tensor by removing random channels along the specified dimension.  
`prune.ln_structured` | Prune tensor by removing channels with the lowest L`n`-norm along the specified dimension.  
`prune.global_unstructured` | Globally prunes tensors corresponding to all parameters in `parameters` by applying the specified `pruning_method`.  
`prune.custom_from_mask` | Prune tensor corresponding to parameter called `name` in `module` by applying the pre-computed mask in `mask`.  
`prune.remove` | Remove the pruning reparameterization from a module and the pruning method from the forward hook.  
`prune.is_pruned` | Check if a module is pruned by looking for pruning pre-hooks.  
Parametrizations implemented using the new parametrization functionality in `torch.nn.utils.parameterize.register_parametrization()`.
`parametrizations.orthogonal` | Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices.  
---|---  
`parametrizations.weight_norm` | Apply weight normalization to a parameter in the given module.  
`parametrizations.spectral_norm` | Apply spectral normalization to a parameter in the given module.  
Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations.
`parametrize.register_parametrization` | Register a parametrization to a tensor in a module.  
---|---  
`parametrize.remove_parametrizations` | Remove the parametrizations on a tensor in a module.  
`parametrize.cached` | Context manager that enables the caching system within parametrizations registered with `register_parametrization()`.  
`parametrize.is_parametrized` | Determine if a module has a parametrization.  
`parametrize.ParametrizationList` | A sequential container that holds and manages the original parameters or buffers of a parametrized `torch.nn.Module`.  
---|---  
Utility functions to call a given Module in a stateless manner.
`stateless.functional_call` | Perform a functional call on the module by replacing the module parameters and buffers with the provided ones.  
---|---  
Utility functions in other modules
`nn.utils.rnn.PackedSequence` | Holds the data and list of `batch_sizes` of a packed sequence.  
---|---  
`nn.utils.rnn.pack_padded_sequence` | Packs a Tensor containing padded sequences of variable length.  
`nn.utils.rnn.pad_packed_sequence` | Pad a packed batch of variable length sequences.  
`nn.utils.rnn.pad_sequence` | Pad a list of variable length Tensors with `padding_value`.  
`nn.utils.rnn.pack_sequence` | Packs a list of variable length Tensors.  
`nn.utils.rnn.unpack_sequence` | Unpack PackedSequence into a list of variable length Tensors.  
`nn.utils.rnn.unpad_sequence` | Unpad padded Tensor into a list of variable length Tensors.  
`nn.Flatten` | Flattens a contiguous range of dims into a tensor.  
---|---  
`nn.Unflatten` | Unflattens a tensor dim expanding it to a desired shape.  
## Quantized Functions
Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.
## Lazy Modules Initialization
`nn.modules.lazy.LazyModuleMixin` | A mixin for modules that lazily initialize parameters, also known as "lazy modules".  
---|---  
### Aliases
The following are aliases to their counterparts in `torch.nn`:
`nn.modules.normalization.RMSNorm` | Applies Root Mean Square Layer Normalization over a mini-batch of inputs.  
---|---  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.nn
    * Containers
    * Convolution Layers
    * Pooling layers
    * Padding Layers
    * Non-linear Activations (weighted sum, nonlinearity)
    * Non-linear Activations (other)
    * Normalization Layers
    * Recurrent Layers
    * Transformer Layers
    * Linear Layers
    * Dropout Layers
    * Sparse Layers
    * Distance Functions
    * Loss Functions
    * Vision Layers
    * Shuffle Layers
    * DataParallel Layers (multi-GPU, distributed)
    * Utilities
    * Quantized Functions
    * Lazy Modules Initialization
      * Aliases


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Broadcasting semantics
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Broadcasting semantics
Many PyTorch operations support NumPy’s broadcasting semantics. See https://numpy.org/doc/stable/user/basics.broadcasting.html for details.
In short, if a PyTorch operation supports broadcast, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).
## General semantics
Two tensors are “broadcastable” if the following rules hold:
  * Each tensor has at least one dimension.
  * When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.


For Example:
```
>>> x=torch.empty(5,7,3)
>>> y=torch.empty(5,7,3)
# same shapes are always broadcastable (i.e. the above rules always hold)
>>> x=torch.empty((0,))
>>> y=torch.empty(2,2)
# x and y are not broadcastable, because x does not have at least 1 dimension
# can line up trailing dimensions
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty( 3,1,1)
# x and y are broadcastable.
# 1st trailing dimension: both have size 1
# 2nd trailing dimension: y has size 1
# 3rd trailing dimension: x size == y size
# 4th trailing dimension: y dimension doesn't exist
# but:
>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty( 3,1,1)
# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3

```
Copy to clipboard
If two tensors `x`, `y` are “broadcastable”, the resulting tensor size is calculated as follows:
  * If the number of dimensions of `x` and `y` are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.
  * Then, for each dimension size, the resulting dimension size is the max of the sizes of `x` and `y` along that dimension.


For Example:
```
# can line up trailing dimensions to make reading easier
>>> x=torch.empty(5,1,4,1)
>>> y=torch.empty( 3,1,1)
>>> (x+y).size()
torch.Size([5, 3, 4, 1])
# but not necessary:
>>> x=torch.empty(1)
>>> y=torch.empty(3,1,7)
>>> (x+y).size()
torch.Size([3, 1, 7])
>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty(3,1,1)
>>> (x+y).size()
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1

```
Copy to clipboard
## In-place semantics
One complication is that in-place operations do not allow the in-place tensor to change shape as a result of the broadcast.
For Example:
```
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty(3,1,1)
>>> (x.add_(y)).size()
torch.Size([5, 3, 4, 1])
# but:
>>> x=torch.empty(1,3,1)
>>> y=torch.empty(3,1,7)
>>> (x.add_(y)).size()
RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.

```
Copy to clipboard
## Backwards compatibility
Prior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes, as long as the number of elements in each tensor was equal. The pointwise operation would then be carried out by viewing each tensor as 1-dimensional. PyTorch now supports broadcasting and the “1-dimensional” pointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are not broadcastable, but have the same number of elements.
Note that the introduction of broadcasting can cause backwards incompatible changes in the case where two tensors do not have the same shape, but are broadcastable and have the same number of elements. For Example:
```
>>> torch.add(torch.ones(4,1), torch.randn(4))

```
Copy to clipboard
would previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]). In order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist, you may set torch.utils.backcompat.broadcast_warning.enabled to True, which will generate a python warning in such cases.
For Example:
```
>>> torch.utils.backcompat.broadcast_warning.enabled=True
>>> torch.add(torch.ones(4,1), torch.ones(4))
__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.
Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Broadcasting semantics
    * General semantics
    * In-place semantics
    * Backwards compatibility


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Extending torch.func with autograd.Function
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Extending torch.func with autograd.Function
So you’d like to use `torch.autograd.Function` with the `torch.func` transforms like `torch.vmap()`, `torch.func.grad()`, etc.
There are two main use cases:
  * you wish to call code that does not contain PyTorch operations and have it work with function transforms. That is, the `torch.autograd.Function`’s forward/backward/etc calls into functions from other systems like C++, CUDA, numpy.
  * you wish to specify custom gradient rules, like JAX’s custom_vjp/custom_jvp


PyTorch combines both of these concepts into `torch.autograd.Function`.
## Basic Usage
This guide assumes you are familiar with Extending torch.autograd, which explains how to use `torch.autograd.Function`.
`torch.autograd.Function` can either have a `forward()` that accepts a ctx object, or it can have separate `forward()` (that does not accept `ctx`) and a `setup_context()` staticmethod that modifies the `ctx` object.
Only the latter is supported with function transforms:
  * `forward()` is the code that performs the operation and it should not accept a `ctx` object.
  * `setup_context(ctx, inputs, output)` is the code where you can call methods on `ctx`. Here is where you should save Tensors for backward (by calling `ctx.save_for_backward(*tensors)`), or save non-Tensors (by assigning them to the `ctx` object).


Because `setup_context()` accepts only `inputs` and `output`, the only quantities that can be saved are either objects (such as Tensors) in the inputs or outputs or quantities (like `Tensor.shape`) derived from them. If you wish to save a non-input intermediate activation from `Function.forward()` for backward, then you’ll need to return it as an output from `forward()` so that it gets passed to `setup_context()`.
Depending on the transform,
  * to support reverse-mode AD (`torch.func.grad()`, `torch.func.vjp()`), the `torch.autograd.Function` needs a `backward()` staticmethod.
  * to support `torch.vmap()`, the `torch.autograd.Function` needs a `vmap()` staticmethod.
  * to support `torch.func.jvp()`, the `torch.autograd.Function` needs a `jvp()` staticmethod.
  * to support compositions of transforms (like `torch.func.jacrev()`, `torch.func.jacfwd()`, `torch.func.hessian()`) – you may need multiple of the above.


In order for the `torch.autograd.Function` to be arbitrarily composable with function transforms, we recommend that all other staticmethods other than `forward()` and `setup_context()` must be transformable: that is, they must consist of only PyTorch operators or call other `torch.autograd.Function` (that may call into C++/CUDA/etc).
Let’s go over some examples of common use cases.
### Example 1: autograd.Function calls into another system
A common case is a `torch.autograd.Function` with both forward() and backward() calling into another system (like C++, CUDA, numpy, triton).
```
import torch
import numpy as np
def to_numpy(tensor):
  return tensor.cpu().numpy()
class NumpySort(torch.autograd.Function):
  # Note that forward does not take ctx
  @staticmethod
  def forward(x, dim):
    device = x.device
    x = to_numpy(x)
    ind = np.argsort(x, axis=dim)
    ind_inv = np.argsort(ind, axis=dim)
    result = np.take_along_axis(x, ind, axis=dim)
    # Any intermediates to be saved in backward must be returned as
    # outputs.
    return (
      # The desired output
      torch.tensor(result, device=device),
      # intermediate to save for backward
      torch.tensor(ind, device=device),
      # intermediate to save for backward
      torch.tensor(ind_inv, device=device),
    )
  # setup_context is responsible for calling methods and/or assigning to
  # the ctx object. Please do not do additional compute (e.g. add
  # Tensors together) in setup_context.
  @staticmethod
  def setup_context(ctx, inputs, output):
    x, dim = inputs
    # Note that output is whatever you returned from forward.
    # If you returned multiple values, then output is a Tuple of multiple values.
    # If you returned a single Tensor, then output is a Tensor.
    # If you returned a Tuple with a single Tensor, then output is a
    # Tuple with a single Tensor.
    _, ind, ind_inv = output
    ctx.mark_non_differentiable(ind, ind_inv)
    # Tensors must be saved via ctx.save_for_backward. Please do not
    # assign them directly onto the ctx object.
    ctx.save_for_backward(ind, ind_inv)
    # Non-tensors may be saved by assigning them as attributes on the ctx object.
    ctx.dim = dim
  @staticmethod
  def backward(ctx, grad_output, _0, _1):
    # For the autograd.Function to be arbitrarily composable with function
    # transforms, all staticmethod other than forward and setup_context
    # must be implemented in a "transformable" way; that is, they must
    # only consist of PyTorch operations or autograd.Function.
    #
    # For example, this allows us to do double backwards and/or compute
    # second order gradients.
    #
    # We've written the backward pass of NumpySort in terms of another
    # autograd.Function, NumpyTake.
    ind, ind_inv = ctx.saved_tensors
    return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None
class NumpyTake(torch.autograd.Function):
  @staticmethod
  def forward(x, ind, ind_inv, dim):
    device = x.device
    x = to_numpy(x)
    ind = to_numpy(ind)
    return torch.tensor(np.take_along_axis(x, ind, dim), device=device)
  @staticmethod
  def setup_context(ctx, inputs, output):
    x, ind, ind_inv, dim = inputs
    ctx.save_for_backward(ind, ind_inv)
    ctx.dim = dim
  @staticmethod
  def backward(ctx, grad_output):
    ind, ind_inv = ctx.saved_tensors
    result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)
    return result, None, None, None

```
Copy to clipboard
Now, to make it easier to use `NumpySort` (to hide away the intermediates we returned as outputs, as well as allow default args and kwargs), we create a new function that invokes it:
```
def numpy_sort(x, dim=-1):
  result, _, _ = NumpySort.apply(x, dim)
  return result

```
Copy to clipboard
And here’s a sanity check:
```
x = torch.randn(2, 3)
grad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)
assert torch.allclose(grad_x, torch.ones_like(x))

```
Copy to clipboard
### Example 2: autograd.Function specifies custom gradient rules
Another common case is an `torch.autograd.Function` that is implemented with PyTorch operations. PyTorch is able to compute gradients for PyTorch operations automatically, but perhaps we wish to customize how the gradients are computed. Some reasons why we may want a custom backward different from the one PyTorch gives us are:
  * improving numeric stability
  * changing the performance characteristics of the backward
  * changing how edge cases are handled (e.g. nans, inf)
  * modifying the gradient (e.g. gradient clipping)


Here’s an example of an `torch.autograd.Function` for the function `y = x ** 3` where we change the performance characteristics (some computation that would normally happen during the backward pass, computing dx, happens in the forward pass).
```
class MyCube(torch.autograd.Function):
  @staticmethod
  def forward(x):
    result = x ** 3
    # In regular PyTorch, if we had just run y = x ** 3, then the backward
    # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done
    # that computation here in the forward pass instead.
    dx = 3 * x ** 2
    return result, dx
  @staticmethod
  def setup_context(ctx, inputs, output):
    x, = inputs
    result, dx = output
    ctx.save_for_backward(x, dx)
  @staticmethod
  def backward(ctx, grad_output, grad_dx):
    x, dx = ctx.saved_tensors
    # In order for the autograd.Function to work with higher-order
    # gradients, we must add the gradient contribution of `dx`.
    result = grad_output * dx + grad_dx * 6 * x
    return result

```
Copy to clipboard
Now, to make it easier to use `NumpySort` (and hide away the intermediates we returned as outputs) we create a new function that invokes it:
```
def my_cube(x):
  result, _ = MyCube.apply(x)
  return result

```
Copy to clipboard
Here’s a sanity check computing the second-order gradients:
```
x = torch.randn([])
ggx = torch.func.grad(torch.func.grad(my_cube))(x)
assert torch.allclose(ggx, 6 * x)

```
Copy to clipboard
### Limitations and gotchas
Warning
Please read these limitations of `torch.autograd.Function` with torch.func transforms carefully. We are not able to catch many of these situations and error out gracefully so they will lead to undefined behavior.
Please do not capture Tensors that are being transformed over, have requires_grad=True, or are dual tensors, into the methods of the `torch.autograd.Function`. The way to be completely safe is to ensure that the only Tensors being used inside any method of the `torch.autograd.Function` must be directly passed as inputs (or via the ctx object) rather than come from outside the `torch.autograd.Function`.
`torch.autograd.Function` does not handle Tensors in pytrees (arbitrary nested Python data structures that may or may not contain Tensors). For those Tensors to be tracked by autograd, they must be passed directly as an argument to `torch.autograd.Function`. This is in contrast to jax.{custom_vjp, custom_jvp}, which do accept pytrees.
Please only use `save_for_backward()` or `save_for_forward()` to save Tensors. Please do not assign Tensors or collections of Tensors directly onto the ctx object - these Tensors will not get tracked
## `torch.vmap()` Support
To use an `torch.autograd.Function` with `torch.vmap()`, you must either:
  * provide a `vmap()` staticmethod that tells us the behavior of the `torch.autograd.Function` under `torch.vmap()`
  * ask us to autogenerate it by setting `generate_vmap_rule=True`.


### Automatically generate a vmap rule
If your `torch.autograd.Function` fulfills the following additional constraints, then we are able to generate a vmap rule for it. If it doesn’t fulfill the constraints or if you want custom behavior under vmap, please manually define a vmap staticmethod (see next section).
Warning
We are not easily able to check for the following constraints and error out gracefully. Violation of the constraints may lead to undefined behavior.
  * The `torch.autograd.Function`’s `forward()`, `backward()` (if it exists) and `jvp()` (if it exists) staticmethods must be transformable via `torch.vmap()`. That is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom CUDA kernels).


Example:
```
class MyCube(torch.autograd.Function):
  # Set generate_vmap_rule to True to ask PyTorch to automatically generate
  # a vmap rule.
  generate_vmap_rule = True
  @staticmethod
  def forward(x):
    result = x ** 3
    dx = 3 * x ** 2
    return result, dx
  @staticmethod
  def setup_context(ctx, inputs, output):
    x, = inputs
    result, dx = output
    ctx.save_for_backward(x, dx)
  @staticmethod
  def backward(ctx, grad_output, grad_dx):
    x, dx = ctx.saved_tensors
    result = grad_output * dx + grad_dx * 6 * x
    return result
def my_cube(x):
  result, dx = MyCube.apply(x)
  return result
x = torch.randn(3)
result = torch.vmap(my_cube)(x)
assert torch.allclose(result, x ** 3)

```
Copy to clipboard
### Defining the vmap staticmethod
If your `torch.autograd.Function` calls into another system (like NumPy, C++, CUDA, triton), then to get it to work with `torch.vmap()` or transforms that use it, you’ll need to manually define a `vmap()` staticmethod.
Depending on what transforms you want to use and your use case, you may not need to add a `vmap()` staticmethod to all of your `torch.autograd.Function`:
  * For example, `torch.func.jacrev()` performs `vmap()` over the backward pass. So if you’re only interested in using `torch.func.jacrev()`, only the `backward()` staticmethod needs to be vmappable.


We do recommend ensuring all of your `torch.autograd.Function` have support for `torch.vmap()` though, especially if you are writing a third-party library and you want your `torch.autograd.Function` to work with all combinations of `torch.func()` transforms.
Conceptually, the vmap staticmethod is responsible for defining how the `forward()` should behave under `torch.vmap()`. That is, it defines how to transform the `forward()` to run over inputs with an additional dimension (the dimension being vmapped over). This is similar to how `torch.vmap()` is implemented over PyTorch operations: for each operation, we define a vmap rule (sometimes also referred to as a “batching rule”).
Here’s how to define the `vmap()` staticmethod:
  * the signature is `vmap(info, in_dims: Tuple[Optional[int]], *args)`, where `*args` is the same as the args to `forward()`.
  * The vmap staticmethod is responsible for defining how the `forward()` should behave under `torch.vmap()`. That is, given inputs with an additional dimension (specified by `in_dims`), how do we compute the batched version of `forward()`?
  * For each arg in `args`, `in_dims` has a corresponding `Optional[int]`. It is `None` if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.
  * `info` is a collection of additional metadata that may be helpful: `info.batch_size` specifies the size of the dimension being vmapped over, while `info.randomness` is the `randomness` option that was passed to `torch.vmap()`.
  * The return of the vmap staticmethod is a tuple of `(output, out_dims)`. Similar to `in_dims`, `out_dims` should be of the same structure as `output` and contain one `out_dim` per output that specifies if the output has the vmapped dimension and what index it is in.


Example:
```
def to_numpy(tensor):
  return tensor.cpu().numpy()
class NumpySort(torch.autograd.Function):
  @staticmethod
  def forward(x, dim):
    device = x.device
    x = to_numpy(x)
    ind = np.argsort(x, axis=dim)
    ind_inv = np.argsort(ind, axis=dim)
    result = np.take_along_axis(x, ind, axis=dim)
    return (
      torch.tensor(result, device=device),
      torch.tensor(ind, device=device),
      torch.tensor(ind_inv, device=device),
    )
  @staticmethod
  def setup_context(ctx, inputs, output):
    x, dim = inputs
    _, ind, ind_inv = output
    ctx.mark_non_differentiable(ind, ind_inv)
    ctx.save_for_backward(ind, ind_inv)
    ctx.dim = dim
  @staticmethod
  def backward(ctx, grad_output, _0, _1):
    ind, ind_inv = ctx.saved_tensors
    return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None
  # The signature of the vmap staticmethod is:
  # vmap(info, in_dims: Tuple[Optional[int]], *args)
  # where *args is the same as the arguments to `forward`.
  @staticmethod
  def vmap(info, in_dims, x, dim):
    # For every input (x and dim), in_dims stores an Optional[int]
    # that is:
    # - None if the input is not being vmapped over or if the input
    #  is not a Tensor
    # - an integer if the input is being vmapped over that represents
    #  the index of the dimension being vmapped over.
    x_bdim, _ = in_dims
    # A "vmap rule" is the logic of how to perform the operation given
    # inputs with one additional dimension. In NumpySort, x has an
    # additional dimension (x_bdim). The vmap rule is simply
    # to call NumpySort again but pass it a different `dim`.
    x = x.movedim(x_bdim, 0)
    # Handle negative dims correctly
    dim = dim if dim >= 0 else dim + x.dim() - 1
    result = NumpySort.apply(x, dim + 1)
    # The vmap rule must return a tuple of two things
    # 1. the output. Should be the same amount of things
    #  as returned by the forward().
    # 2. one Optional[int] for each output specifying if each output
    # is being vmapped over, and if so, the index of the
    # dimension being vmapped over.
    #
    # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the
    # dimension being vmapped over to the front of `x`, that appears at
    # dimension 0 of all outputs.
    # The return is (output, out_dims) -- output is a tuple of 3 Tensors
    # and out_dims is a Tuple of 3 Optional[int]
    return NumpySort.apply(x, dim + 1), (0, 0, 0)
class NumpyTake(torch.autograd.Function):
  @staticmethod
  def forward(x, ind, ind_inv, dim):
    device = x.device
    x = to_numpy(x)
    ind = to_numpy(ind)
    return torch.tensor(np.take_along_axis(x, ind, dim), device=device)
  @staticmethod
  def setup_context(ctx, inputs, output):
    x, ind, ind_inv, dim = inputs
    ctx.save_for_backward(ind, ind_inv)
    ctx.dim = dim
  @staticmethod
  def backward(ctx, grad_output):
    ind, ind_inv = ctx.saved_tensors
    result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)
    return result, None, None, None
  @staticmethod
  def vmap(info, in_dims, x, ind, ind_inv, dim):
    x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims
    # The strategy is: expand {x, ind, ind_inv} to all have the dimension
    # being vmapped over.
    # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).
    # Handle negative dims by wrapping them to be positive
    logical_dim = x.dim() if x_bdim is None else x_bdim - 1
    dim = dim if dim >= 0 else dim + logical_dim
    def maybe_expand_bdim_at_front(x, x_bdim):
      if x_bdim is None:
        return x.expand(info.batch_size, *x.shape)
      return x.movedim(x_bdim, 0)
    # If the Tensor doesn't have the dimension being vmapped over,
    # expand it out. Otherwise, move it to the front of the Tensor
    x = maybe_expand_bdim_at_front(x, x_bdim)
    ind = maybe_expand_bdim_at_front(ind, ind_bdim)
    ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)
    # The return is a tuple (output, out_dims). Since output is a Tensor,
    # then out_dims is an Optional[int] (instead of being a Tuple).
    return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0
def numpy_sort(x, dim=-1):
  result, _, _ = NumpySort.apply(x, dim)
  return result
x = torch.randn(2, 3)
result = torch.vmap(numpy_sort)(x)
assert torch.allclose(result, numpy_sort(result, 1))

```
Copy to clipboard
Note
The vmap staticmethod should aim to preserve the semantics of the entire `Function`. That is, (pseudocode) `grad(vmap(MyFunc))` should be replaceable with a `grad(map(MyFunc))`.
If your autograd.Function has any custom behavior in the backward pass, please keep this in mind.
Note
It is a legitimate use case to write a custom vmap staticmethod for a `Function` that PyTorch is able to generate a vmap rule for via `generate_vmap_rule=True`. You may wish to do this if the generated vmap rule doesn’t have the semantics you’re looking for.
## `torch.func.jvp()` Support
To support forward-mode AD, a `torch.autograd.Function` must have a `jvp()` staticmethod. Please see Forward mode AD for details.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Extending torch.func with autograd.Function
    * Basic Usage
      * Example 1: autograd.Function calls into another system
      * Example 2: autograd.Function specifies custom gradient rules
      * Limitations and gotchas
    * `torch.vmap()` Support
      * Automatically generate a vmap rule
      * Defining the vmap staticmethod
    * `torch.func.jvp()` Support


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * CPU threading and TorchScript inference
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# CPU threading and TorchScript inference
PyTorch allows using multiple CPU threads during TorchScript model inference. The following figure shows different levels of parallelism one would find in a typical application:
![../_images/cpu_threading_torchscript_inference.svg](https://pytorch.org/docs/stable/_images/cpu_threading_torchscript_inference.svg)
One or more inference threads execute a model’s forward pass on the given inputs. Each inference thread invokes a JIT interpreter that executes the ops of a model inline, one by one. A model can utilize a `fork` TorchScript primitive to launch an asynchronous task. Forking several operations at once results in a task that is executed in parallel. The `fork` operator returns a `Future` object which can be used to synchronize on later, for example:
```
@torch.jit.script
def compute_z(x):
  return torch.mm(x, self.w_z)
@torch.jit.script
def forward(x):
  # launch compute_z asynchronously:
  fut = torch.jit._fork(compute_z, x)
  # execute the next operation in parallel to compute_z:
  y = torch.mm(x, self.w_y)
  # wait for the result of compute_z:
  z = torch.jit._wait(fut)
  return y + z

```
Copy to clipboard
PyTorch uses a single thread pool for the inter-op parallelism, this thread pool is shared by all inference tasks that are forked within the application process.
In addition to the inter-op parallelism, PyTorch can also utilize multiple threads within the ops (intra-op parallelism). This can be useful in many cases, including element-wise ops on large tensors, convolutions, GEMMs, embedding lookups and others.
## Build options
PyTorch uses an internal ATen library to implement ops. In addition to that, PyTorch can also be built with support of external libraries, such as MKL and MKL-DNN, to speed up computations on CPU.
ATen, MKL and MKL-DNN support intra-op parallelism and depend on the following parallelization libraries to implement it:
  * OpenMP - a standard (and a library, usually shipped with a compiler), widely used in external libraries;
  * TBB - a newer parallelization library optimized for task-based parallelism and concurrent environments.


OpenMP historically has been used by a large number of libraries. It is known for a relative ease of use and support for loop-based parallelism and other primitives.
TBB is used to a lesser extent in external libraries, but, at the same time, is optimized for the concurrent environments. PyTorch’s TBB backend guarantees that there’s a separate, single, per-process intra-op thread pool used by all of the ops running in the application.
Depending of the use case, one might find one or another parallelization library a better choice in their application.
PyTorch allows selecting of the parallelization backend used by ATen and other libraries at the build time with the following build options:
Library | Build Option | Values | Notes  
---|---|---|---  
ATen | `ATEN_THREADING` | `OMP` (default), `TBB` |   
MKL | `MKL_THREADING` | (same) | To enable MKL use `BLAS=MKL`  
MKL-DNN | `MKLDNN_CPU_RUNTIME` | (same) | To enable MKL-DNN use `USE_MKLDNN=1`  
It is recommended not to mix OpenMP and TBB within one build.
Any of the `TBB` values above require `USE_TBB=1` build setting (default: OFF). A separate setting `USE_OPENMP=1` (default: ON) is required for OpenMP parallelism.
## Runtime API
The following API is used to control thread settings:
Type of parallelism | Settings | Notes  
---|---|---  
Inter-op parallelism | `at::set_num_interop_threads`, `at::get_num_interop_threads` (C++) `set_num_interop_threads`, `get_num_interop_threads` (Python, `torch` module) | Default number of threads: number of CPU cores.  
Intra-op parallelism | `at::set_num_threads`, `at::get_num_threads` (C++) `set_num_threads`, `get_num_threads` (Python, `torch` module) Environment variables: `OMP_NUM_THREADS` and `MKL_NUM_THREADS`  
For the intra-op parallelism settings, `at::set_num_threads`, `torch.set_num_threads` always take precedence over environment variables, `MKL_NUM_THREADS` variable takes precedence over `OMP_NUM_THREADS`.
## Tuning the number of threads
The following simple script shows how a runtime of matrix multiplication changes with the number of threads:
```
import timeit
runtimes = []
threads = [1] + [t for t in range(2, 49, 2)]
for t in threads:
  torch.set_num_threads(t)
  r = timeit.timeit(setup = "import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)", stmt="torch.mm(x, y)", number=100)
  runtimes.append(r)
# ... plotting (threads, runtimes) ...

```
Copy to clipboard
Running the script on a system with 24 physical CPU cores (Xeon E5-2680, MKL and OpenMP based build) results in the following runtimes:
![../_images/cpu_threading_runtimes.svg](https://pytorch.org/docs/stable/_images/cpu_threading_runtimes.svg)
The following considerations should be taken into account when tuning the number of intra- and inter-op threads:
  * When choosing the number of threads one needs to avoid oversubscription (using too many threads, leads to performance degradation). For example, in an application that uses a large application thread pool or heavily relies on inter-op parallelism, one might find disabling intra-op parallelism as a possible option (i.e. by calling `set_num_threads(1)`);
  * In a typical application one might encounter a trade off between latency (time spent on processing an inference request) and throughput (amount of work done per unit of time). Tuning the number of threads can be a useful tool to adjust this trade off in one way or another. For example, in latency critical applications one might want to increase the number of intra-op threads to process each request as fast as possible. At the same time, parallel implementations of ops may add an extra overhead that increases amount work done per single request and thus reduces the overall throughput.


Warning
OpenMP does not guarantee that a single per-process intra-op thread pool is going to be used in the application. On the contrary, two different application or inter-op threads may use different OpenMP thread pools for intra-op work. This might result in a large number of threads used by the application. Extra care in tuning the number of threads is needed to avoid oversubscription in multi-threaded applications in OpenMP case.
Note
Pre-built PyTorch releases are compiled with OpenMP support.
Note
`parallel_info` utility prints information about thread settings and can be used for debugging. Similar output can be also obtained in Python with `torch.__config__.parallel_info()` call.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * CPU threading and TorchScript inference
    * Build options
    * Runtime API
    * Tuning the number of threads


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * PyTorch Custom Operators Landing Page
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# PyTorch Custom Operators Landing Page
This page has moved.
Redirecting to the new page…
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * PyTorch Custom Operators Landing Page


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Frequently Asked Questions
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Frequently Asked Questions
## My model reports “cuda runtime error(2): out of memory”
As the error message suggests, you have run out of memory on your GPU. Since we often deal with large amounts of data in PyTorch, small mistakes can rapidly cause your program to use up all of your GPU; fortunately, the fixes in these cases are often simple. Here are a few common things to check:
**Don’t accumulate history across your training loop.** By default, computations involving variables that require gradients will keep history. This means that you should avoid using such variables in computations which will live beyond your training loops, e.g., when tracking statistics. Instead, you should detach the variable or access its underlying data.
Sometimes, it can be non-obvious when differentiable variables can occur. Consider the following training loop (abridged from source):
```
total_loss = 0
for i in range(10000):
  optimizer.zero_grad()
  output = model(input)
  loss = criterion(output)
  loss.backward()
  optimizer.step()
  total_loss += loss

```
Copy to clipboard
Here, `total_loss` is accumulating history across your training loop, since `loss` is a differentiable variable with autograd history. You can fix this by writing total_loss += float(loss) instead.
Other instances of this problem: 1.
**Don’t hold onto tensors and variables you don’t need.** If you assign a Tensor or Variable to a local, Python will not deallocate until the local goes out of scope. You can free this reference by using `del x`. Similarly, if you assign a Tensor or Variable to a member variable of an object, it will not deallocate until the object goes out of scope. You will get the best memory usage if you don’t hold onto temporaries you don’t need.
The scopes of locals can be larger than you expect. For example:
```
for i in range(5):
  intermediate = f(input[i])
  result += g(intermediate)
output = h(result)
return output

```
Copy to clipboard
Here, `intermediate` remains live even while `h` is executing, because its scope extrudes past the end of the loop. To free it earlier, you should `del intermediate` when you are done with it.
**Avoid running RNNs on sequences that are too large.** The amount of memory required to backpropagate through an RNN scales linearly with the length of the RNN input; thus, you will run out of memory if you try to feed an RNN a sequence that is too long.
The technical term for this phenomenon is backpropagation through time, and there are plenty of references for how to implement truncated BPTT, including in the word language model example; truncation is handled by the `repackage` function as described in this forum post.
**Don’t use linear layers that are too large.** A linear layer `nn.Linear(m, n)` uses O(nm)O(nm)O(nm) memory: that is to say, the memory requirements of the weights scales quadratically with the number of features. It is very easy to blow through your memory this way (and remember that you will need at least twice the size of the weights, since you also need to store the gradients.)
**Consider checkpointing.** You can trade-off memory for compute by using checkpoint.
## My GPU memory isn’t freed properly
PyTorch uses a caching memory allocator to speed up memory allocations. As a result, the values shown in `nvidia-smi` usually don’t reflect the true memory usage. See Memory management for more details about GPU memory management.
If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via `ps -elf | grep python` and manually kill them with `kill -9 [pid]`.
## My out of memory exception handler can’t allocate memory
You may have some code that tries to recover from out of memory errors.
```
try:
  run_model(batch_size)
except RuntimeError: # Out of memory
  for _ in range(batch_size):
    run_model(1)

```
Copy to clipboard
But find that when you do run out of memory, your recovery code can’t allocate either. That’s because the python exception object holds a reference to the stack frame where the error was raised. Which prevents the original tensor objects from being freed. The solution is to move you OOM recovery code outside of the `except` clause.
```
oom = False
try:
  run_model(batch_size)
except RuntimeError: # Out of memory
  oom = True
if oom:
  for _ in range(batch_size):
    run_model(1)

```
Copy to clipboard
## My data loader workers return identical random numbers
You are likely using other libraries to generate random numbers in the dataset and worker subprocesses are started via `fork`. See `torch.utils.data.DataLoader`’s documentation for how to properly set up random seeds in workers with its `worker_init_fn` option.
## My recurrent network doesn’t work with data parallelism
There is a subtlety in using the `pack sequence -> recurrent network -> unpack sequence` pattern in a `Module` with `DataParallel` or `data_parallel()`. Input to each the `forward()` on each device will only be part of the entire input. Because the unpack operation `torch.nn.utils.rnn.pad_packed_sequence()` by default only pads up to the longest input it sees, i.e., the longest on that particular device, size mismatches will happen when results are gathered together. Therefore, you can instead take advantage of the `total_length` argument of `pad_packed_sequence()` to make sure that the `forward()` calls return sequences of same length. For example, you can write:
```
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
class MyModule(nn.Module):
  # ... __init__, other methods, etc.
  # padded_input is of shape [B x T x *] (batch_first mode) and contains
  # the sequences sorted by lengths
  #  B is the batch size
  #  T is max sequence length
  def forward(self, padded_input, input_lengths):
    total_length = padded_input.size(1) # get the max sequence length
    packed_input = pack_padded_sequence(padded_input, input_lengths,
                      batch_first=True)
    packed_output, _ = self.my_lstm(packed_input)
    output, _ = pad_packed_sequence(packed_output, batch_first=True,
                    total_length=total_length)
    return output

m = MyModule().cuda()
dp_m = nn.DataParallel(m)

```
Copy to clipboard
Additionally, extra care needs to be taken when batch dimension is dim `1` (i.e., `batch_first=False`) with data parallelism. In this case, the first argument of pack_padded_sequence `padding_input` will be of shape `[T x B x *]` and should be scattered along dim `1`, but the second argument `input_lengths` will be of shape `[B]` and should be scattered along dim `0`. Extra code to manipulate the tensor shapes will be needed.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Frequently Asked Questions
    * My model reports “cuda runtime error(2): out of memory”
    * My GPU memory isn’t freed properly
    * My out of memory exception handler can’t allocate memory
    * My data loader workers return identical random numbers
    * My recurrent network doesn’t work with data parallelism


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * CUDA semantics
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# CUDA semantics
`torch.cuda` is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device. The selected device can be changed with a `torch.cuda.device` context manager.
However, once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed on the same device as the tensor.
Cross-GPU operations are not allowed by default, with the exception of `copy_()` and other methods with copy-like functionality such as `to()` and `cuda()`. Unless you enable peer-to-peer memory access, any attempts to launch ops on tensors spread across different devices will raise an error.
Below you can find a small example showcasing this:
```
cuda = torch.device('cuda')   # Default CUDA device
cuda0 = torch.device('cuda:0')
cuda2 = torch.device('cuda:2') # GPU 2 (these are 0-indexed)
x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)
with torch.cuda.device(1):
  # allocates a tensor on GPU 1
  a = torch.tensor([1., 2.], device=cuda)
  # transfers a tensor from CPU to GPU 1
  b = torch.tensor([1., 2.]).cuda()
  # a.device and b.device are device(type='cuda', index=1)
  # You can also use ``Tensor.to`` to transfer a tensor:
  b2 = torch.tensor([1., 2.]).to(device=cuda)
  # b.device and b2.device are device(type='cuda', index=1)
  c = a + b
  # c.device is device(type='cuda', index=1)
  z = x + y
  # z.device is device(type='cuda', index=0)
  # even within a context, you can specify the device
  # (or give a GPU index to the .cuda call)
  d = torch.randn(2, device=cuda2)
  e = torch.randn(2).to(cuda2)
  f = torch.randn(2).cuda(cuda2)
  # d.device, e.device, and f.device are all device(type='cuda', index=2)

```
Copy to clipboard
## TensorFloat-32 (TF32) on Ampere (and later) devices
Starting in PyTorch 1.7, there is a new flag called allow_tf32. This flag defaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later. This flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores, available on NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies and batched matrix multiplies) and convolutions.
TF32 tensor cores are designed to achieve better performance on matmul and convolutions on torch.float32 tensors by rounding input data to have 10 bits of mantissa, and accumulating results with FP32 precision, maintaining FP32 dynamic range.
matmuls and convolutions are controlled separately, and their corresponding flags can be accessed at:
```
# The flag below controls whether to allow TF32 on matmul. This flag defaults to False
# in PyTorch 1.12 and later.
torch.backends.cuda.matmul.allow_tf32 = True
# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
torch.backends.cudnn.allow_tf32 = True

```
Copy to clipboard
The precision of matmuls can also be set more broadly (limited not just to CUDA) via `set_float_32_matmul_precision()`. Note that besides matmuls and convolutions themselves, functions and nn modules that internally uses matmuls or convolutions are also affected. These include nn.Linear, nn.Conv*, cdist, tensordot, affine grid and grid sample, adaptive log softmax, GRU and LSTM.
To get an idea of the precision and speed, see the example code and benchmark data (on A100) below:
```
a_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
b_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
ab_full = a_full @ b_full
mean = ab_full.abs().mean() # 80.7277
a = a_full.float()
b = b_full.float()
# Do matmul at TF32 mode.
torch.backends.cuda.matmul.allow_tf32 = True
ab_tf32 = a @ b # takes 0.016s on GA100
error = (ab_tf32 - ab_full).abs().max() # 0.1747
relative_error = error / mean # 0.0022
# Do matmul with TF32 disabled.
torch.backends.cuda.matmul.allow_tf32 = False
ab_fp32 = a @ b # takes 0.11s on GA100
error = (ab_fp32 - ab_full).abs().max() # 0.0031
relative_error = error / mean # 0.000039

```
Copy to clipboard
From the above example, we can see that with TF32 enabled, the speed is ~7x faster on A100, and that relative error compared to double precision is approximately 2 orders of magnitude larger. Note that the exact ratio of TF32 to single precision speed depends on the hardware generation, as properties such as the ratio of memory bandwidth to compute as well as the ratio of TF32 to FP32 matmul throughput may vary from generation to generation or model to model. If full FP32 precision is needed, users can disable TF32 by:
```
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

```
Copy to clipboard
To toggle the TF32 flags off in C++, you can do
```
at::globalContext().setAllowTF32CuBLAS(false);
at::globalContext().setAllowTF32CuDNN(false);

```
Copy to clipboard
For more information about TF32, see:
  * TensorFloat-32
  * CUDA 11
  * Ampere architecture


## Reduced Precision Reduction in FP16 GEMMs
(Distinct from full FP16 accumulation that is intended for hardware that has higher throughput with FP16 accumulation than FP32 accumulation, see Full FP16 accumulation)
fp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a large k dimension) and GPU architectures at the cost of numerical precision and potential for overflow.
Some example benchmark data on V100:
```
[--------------------------- bench_gemm_transformer --------------------------]
   [ m , k , n ]  | allow_fp16_reduc=True | allow_fp16_reduc=False
1 threads: --------------------------------------------------------------------
   [4096, 4048, 4096]  |      1634.6    |      1639.8
   [4096, 4056, 4096]  |      1670.8    |      1661.9
   [4096, 4080, 4096]  |      1664.2    |      1658.3
   [4096, 4096, 4096]  |      1639.4    |      1651.0
   [4096, 4104, 4096]  |      1677.4    |      1674.9
   [4096, 4128, 4096]  |      1655.7    |      1646.0
   [4096, 4144, 4096]  |      1796.8    |      2519.6
   [4096, 5096, 4096]  |      2094.6    |      3190.0
   [4096, 5104, 4096]  |      2144.0    |      2663.5
   [4096, 5112, 4096]  |      2149.1    |      2766.9
   [4096, 5120, 4096]  |      2142.8    |      2631.0
   [4096, 9728, 4096]  |      3875.1    |      5779.8
   [4096, 16384, 4096]  |      6182.9    |      9656.5
(times in microseconds).

```
Copy to clipboard
If full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with:
```
torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False

```
Copy to clipboard
To toggle the reduced precision reduction flags in C++, one can do
```
at::globalContext().setAllowFP16ReductionCuBLAS(false);

```
Copy to clipboard
## Reduced Precision Reduction in BF16 GEMMs
A similar flag (as above) exists for BFloat16 GEMMs. Note that this switch is set to True by default for BF16, if you observe numerical instability in your workload, you may wish to set it to False.
If reduced precision reductions are not desired, users can disable reduced precision reductions in bf16 GEMMs with:
```
torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False

```
Copy to clipboard
To toggle the reduced precision reduction flags in C++, one can do
```
at::globalContext().setAllowBF16ReductionCuBLAS(true);

```
Copy to clipboard
## Full FP16 Accmumulation in FP16 GEMMs
Certain GPUs have increased performance when doing _all_ FP16 GEMM accumulation in FP16, at the cost of numerical precision and greater likelihood of overflow. Note that this setting only has an effect on GPUs of compute capability 7.0 (Volta) or newer.
This behavior can be enabled via:
```
torch.backends.cuda.matmul.allow_fp16_accumulation = True

```
Copy to clipboard
To toggle the reduced precision reduction flags in C++, one can do
```
at::globalContext().setAllowFP16AccumulationCuBLAS(true);

```
Copy to clipboard
## Asynchronous execution
By default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are _enqueued_ to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on CPU or other GPUs.
In general, the effect of asynchronous computation is invisible to the caller, because (1) each device executes operations in the order they are queued, and (2) PyTorch automatically performs necessary synchronization when copying data between CPU and GPU or between two GPUs. Hence, computation will proceed as if every operation was executed synchronously.
You can force synchronous computation by setting environment variable `CUDA_LAUNCH_BLOCKING=1`. This can be handy when an error occurs on the GPU. (With asynchronous execution, such an error isn’t reported until after the operation is actually executed, so the stack trace does not show where it was requested.)
A consequence of the asynchronous computation is that time measurements without synchronizations are not accurate. To get precise measurements, one should either call `torch.cuda.synchronize()` before measuring, or use `torch.cuda.Event` to record times as following:
```
start_event = torch.cuda.Event(enable_timing=True)
end_event = torch.cuda.Event(enable_timing=True)
start_event.record()
# Run some things here
end_event.record()
torch.cuda.synchronize() # Wait for the events to be recorded!
elapsed_time_ms = start_event.elapsed_time(end_event)

```
Copy to clipboard
As an exception, several functions such as `to()` and `copy_()` admit an explicit `non_blocking` argument, which lets the caller bypass synchronization when it is unnecessary. Another exception is CUDA streams, explained below.
### CUDA streams
A CUDA stream is a linear sequence of execution that belongs to a specific device. You normally do not need to create one explicitly: by default, each device uses its own “default” stream.
Operations inside each stream are serialized in the order they are created, but operations from different streams can execute concurrently in any relative order, unless explicit synchronization functions (such as `synchronize()` or `wait_stream()`) are used. For example, the following code is incorrect:
```
cuda = torch.device('cuda')
s = torch.cuda.Stream() # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
with torch.cuda.stream(s):
  # sum() may start execution before normal_() finishes!
  B = torch.sum(A)

```
Copy to clipboard
When the “current stream” is the default stream, PyTorch automatically performs necessary synchronization when data is moved around, as explained above. However, when using non-default streams, it is the user’s responsibility to ensure proper synchronization. The fixed version of this example is:
```
cuda = torch.device('cuda')
s = torch.cuda.Stream() # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
s.wait_stream(torch.cuda.default_stream(cuda)) # NEW!
with torch.cuda.stream(s):
  B = torch.sum(A)
A.record_stream(s) # NEW!

```
Copy to clipboard
There are two new additions. The `torch.cuda.Stream.wait_stream()` call ensures that the `normal_()` execution has finished before we start running `sum(A)` on a side stream. The `torch.Tensor.record_stream()` (see for more details) ensures that we do not deallocate A before `sum(A)` has completed. You can also manually wait on the stream at some later point in time with `torch.cuda.default_stream(cuda).wait_stream(s)` (note that it is pointless to wait immediately, since that will prevent the stream execution from running in parallel with other work on the default stream.) See the documentation for `torch.Tensor.record_stream()` on more details on when to use one or another.
Note that this synchronization is necessary even when there is no read dependency, e.g., as seen in this example:
```
cuda = torch.device('cuda')
s = torch.cuda.Stream() # Create a new stream.
A = torch.empty((100, 100), device=cuda)
s.wait_stream(torch.cuda.default_stream(cuda)) # STILL REQUIRED!
with torch.cuda.stream(s):
  A.normal_(0.0, 1.0)
  A.record_stream(s)

```
Copy to clipboard
Despite the computation on `s` not reading the contents of `A` and no other uses of `A`, it is still necessary to synchronize, because `A` may correspond to memory reallocated by the CUDA caching allocator, with pending operations from the old (deallocated) memory.
### Stream semantics of backward passes
Each backward CUDA op runs on the same stream that was used for its corresponding forward op. If your forward pass runs independent ops in parallel on different streams, this helps the backward pass exploit that same parallelism.
The stream semantics of a backward call with respect to surrounding ops are the same as for any other call. The backward pass inserts internal syncs to ensure this even when backward ops run on multiple streams as described in the previous paragraph. More concretely, when calling `autograd.backward`, `autograd.grad`, or `tensor.backward`, and optionally supplying CUDA tensor(s) as the initial gradient(s) (e.g., `autograd.backward(..., grad_tensors=initial_grads)`, `autograd.grad(..., grad_outputs=initial_grads)`, or `tensor.backward(..., gradient=initial_grad)`), the acts of
  1. optionally populating initial gradient(s),
  2. invoking the backward pass, and
  3. using the gradients


have the same stream-semantics relationship as any group of ops:
```
s = torch.cuda.Stream()
# Safe, grads are used in the same stream context as backward()
with torch.cuda.stream(s):
  loss.backward()
  use grads
# Unsafe
with torch.cuda.stream(s):
  loss.backward()
use grads
# Safe, with synchronization
with torch.cuda.stream(s):
  loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads
# Safe, populating initial grad and invoking backward are in the same stream context
with torch.cuda.stream(s):
  loss.backward(gradient=torch.ones_like(loss))
# Unsafe, populating initial_grad and invoking backward are in different stream contexts,
# without synchronization
initial_grad = torch.ones_like(loss)
with torch.cuda.stream(s):
  loss.backward(gradient=initial_grad)
# Safe, with synchronization
initial_grad = torch.ones_like(loss)
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
  initial_grad.record_stream(s)
  loss.backward(gradient=initial_grad)

```
Copy to clipboard
#### BC note: Using grads on the default stream
In prior versions of PyTorch (1.9 and earlier), the autograd engine always synced the default stream with all backward ops, so the following pattern:
```
with torch.cuda.stream(s):
  loss.backward()
use grads

```
Copy to clipboard
was safe as long as `use grads` happened on the default stream. In present PyTorch, that pattern is no longer safe. If `backward()` and `use grads` are in different stream contexts, you must sync the streams:
```
with torch.cuda.stream(s):
  loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads

```
Copy to clipboard
even if `use grads` is on the default stream.
## Memory management
PyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in `nvidia-smi`. You can use `memory_allocated()` and `max_memory_allocated()` to monitor memory occupied by tensors, and use `memory_reserved()` and `max_memory_reserved()` to monitor the total amount of memory managed by the caching allocator. Calling `empty_cache()` releases all **unused** cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.
To better understand how CUDA memory is being used over time, Understanding CUDA Memory Usage describes tools for capturing and visualizing traces of memory use.
For more advanced users, we offer more comprehensive memory benchmarking via `memory_stats()`. We also offer the capability to capture a complete snapshot of the memory allocator state via `memory_snapshot()`, which can help you understand the underlying allocation patterns produced by your code.
### Optimizing memory usage with `PYTORCH_CUDA_ALLOC_CONF`
Use of a caching allocator can interfere with memory checking tools such as `cuda-memcheck`. To debug memory errors using `cuda-memcheck`, set `PYTORCH_NO_CUDA_MEMORY_CACHING=1` in your environment to disable caching.
The behavior of the caching allocator can be controlled via the environment variable `PYTORCH_CUDA_ALLOC_CONF`. The format is `PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...` Available options:
  * `backend` allows selecting the underlying allocator implementation. Currently, valid options are `native`, which uses PyTorch’s native implementation, and `cudaMallocAsync`, which uses CUDA’s built-in asynchronous allocator. `cudaMallocAsync` requires CUDA 11.4 or newer. The default is `native`. `backend` applies to all devices used by the process, and can’t be specified on a per-device basis.
  * `max_split_size_mb` prevents the native allocator from splitting blocks larger than this size (in MB). This can reduce fragmentation and may allow some borderline workloads to complete without running out of memory. Performance cost can range from ‘zero’ to ‘substantial’ depending on allocation patterns. Default value is unlimited, i.e. all blocks can be split. The `memory_stats()` and `memory_summary()` methods are useful for tuning. This option should be used as a last resort for a workload that is aborting due to ‘out of memory’ and showing a large amount of inactive split blocks. `max_split_size_mb` is only meaningful with `backend:native`. With `backend:cudaMallocAsync`, `max_split_size_mb` is ignored.
  * `roundup_power2_divisions` helps with rounding the requested allocation size to nearest power-2 division and making better use of the blocks. In the native CUDACachingAllocator, the sizes are rounded up in multiple of blocks size of 512, so this works fine for smaller sizes. However, this can be inefficient for large near-by allocations as each will go to different size of blocks and re-use of those blocks are minimized. This might create lots of unused blocks and will waste GPU memory capacity. This option enables the rounding of allocation size to nearest power-2 division. For example, if we need to round-up size of 1200 and if number of divisions is 4, the size 1200 lies between 1024 and 2048 and if we do 4 divisions between them, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200 will be rounded to 1280 as the nearest ceiling of power-2 division. Specify a single value to apply for all allocation sizes or specify an array of key value pairs to set power-2 division individually for each power of two interval. For example to set 1 division for all allocations under 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions for allocations between 512MB and 1GB and 8 divisions for any larger allocations, set the knob value to: [256:1,512:2,1024:4,>:8]. `roundup_power2_divisions` is only meaningful with `backend:native`. With `backend:cudaMallocAsync`, `roundup_power2_divisions` is ignored.
  * 

`max_non_split_rounding_mb` will allow non-split blocks for better reuse, eg,
    
a 1024MB cached block can be re-used for a 512MB allocation request. In the default case, we only allow up to 20MB of rounding of non-split blocks, so a 512MB block can only be served with between 512-532 MB size block. If we set the value of this option to 1024, it will alow 512-1536 MB size blocks to be used for a 512MB block which increases reuse of larger blocks. This will also help in reducing the stalls in avoiding expensive cudaMalloc calls.
  * `garbage_collection_threshold` helps actively reclaiming unused GPU memory to avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks), which can be unfavorable to latency-critical GPU applications (e.g., servers). Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming GPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e., 80% of the total memory allocated to the GPU application). The algorithm prefers to free old & unused blocks first to avoid freeing blocks that are actively being reused. The threshold value should be between greater than 0.0 and less than 1.0. `garbage_collection_threshold` is only meaningful with `backend:native`. With `backend:cudaMallocAsync`, `garbage_collection_threshold` is ignored.
  * `expandable_segments` (experimental, default: False) If set to True, this setting instructs the allocator to create CUDA allocations that can later be expanded to better handle cases where a job changing allocation sizes frequently, such as having a changing batch size. Normally for large (>2MB) allocations, the allocator calls cudaMalloc to get allocations that are the same size as what the user requests. In the future, parts of these allocations can be reused for other requests if they are free. This works well when the program makes many requests of exactly the same size or of sizes that even multiples of that size. Many deep learning models follow this behavior. However, one common exception is when the batch size changes slightly from one iteration to the next, e.g. in batched inference. When the program runs initially with batch size N, it will make allocations appropriate for that size. If in the future, it runs at size N - 1, the existing allocations will still be big enough. However, if it runs at size N + 1, then it will have to make new allocations that are slightly larger. Not all the tensors are the same size. Some might be (N + 1)*A and others (N + 1)*A*B where A and B are some non-batch dimensions in the model. Because the allocator reuses existing allocations when they are big enough, some number of (N + 1)*A allocations will actually fit in the already existing N*B*A segments, though not perfectly. As the model runs it will partially fill up all of these segments leaving unusable free slices of memory at the end of these segments. The allocator at some point will need to cudaMalloc a new (N + 1)*A*B segment. If there is not enough memory, there is now no way to recover the slices of memory that are free at the end of existing segments. With models 50+ layers deep, this pattern might repeat 50+ times creating many slivers.
expandable_segments allows the allocator to create a segment initially and then expand its size later when more memory is needed. Instead of making one segment per allocation, it tries to make one segment (per stream) that grows as necessary. Now when the N + 1 case runs, the allocations will tile nicely into the one large segment until it fills up. Then more memory is requested and appended to the end of the segment. This process does not create as many slivers of unusable memory, so it is more likely to succeed at finding this memory.
pinned_use_cuda_host_register option is a boolean flag that determines whether to use the CUDA API’s cudaHostRegister function for allocating pinned memory instead of the default cudaHostAlloc. When set to True, the memory is allocated using regular malloc and then pages are mapped to the memory before calling cudaHostRegister. This pre-mapping of pages helps reduce the lock time during the execution of cudaHostRegister.
pinned_num_register_threads option is only valid when pinned_use_cuda_host_register is set to True. By default, one thread is used to map the pages. This option allows using more threads to parallelize the page mapping operations to reduce the overall allocation time of pinned memory. A good value for this option is 8 based on benchmarking results.
pinned_use_background_threads option is a boolean flag to enable background thread for processing events. This avoids any slow path associated with querying/processing of events in the fast allocation path. This feature is disabled by default.


Note
Some stats reported by the CUDA memory management API are specific to `backend:native`, and are not meaningful with `backend:cudaMallocAsync`. See each function’s docstring for details.
## Using custom memory allocators for CUDA
It is possible to define allocators as simple functions in C/C++ and compile them as a shared library, the code below shows a basic allocator that just traces all the memory operations.
```
#include<sys/types.h>
#include<cuda_runtime_api.h>
#include<iostream>
// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC
extern"C"{
void*my_malloc(ssize_tsize,intdevice,cudaStream_tstream){
void*ptr;
cudaMalloc(&ptr,size);
std::cout<<"alloc "<<ptr<<size<<std::endl;
returnptr;
}
voidmy_free(void*ptr,ssize_tsize,intdevice,cudaStream_tstream){
std::cout<<"free "<<ptr<<" "<<stream<<std::endl;
cudaFree(ptr);
}
}

```
Copy to clipboard
This can be used in python through the `torch.cuda.memory.CUDAPluggableAllocator`. The user is responsible for supplying the path to the .so file and the name of the alloc/free functions that match the signatures specified above.
```
import torch
# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
  'alloc.so', 'my_malloc', 'my_free')
# Swap the current allocator
torch.cuda.memory.change_current_allocator(new_alloc)
# This will allocate memory in the device using the new allocator
b = torch.zeros(10, device='cuda')

```
Copy to clipboard
```
import torch
# Do an initial memory allocator
b = torch.zeros(10, device='cuda')
# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
  'alloc.so', 'my_malloc', 'my_free')
# This will error since the current allocator was already instantiated
torch.cuda.memory.change_current_allocator(new_alloc)

```
Copy to clipboard
## Mixing different CUDA system allocators in the same program
Depending on your use case, `change_current_allocator()` may not be what you want to use, since it swaps the CUDA allocator for the entire program (similar to `PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync`). For instance, if the swapped allocator doesn’t have caching mechanism, you will lose all the benefits of PyTorch’s CUDACachingAllocator. Instead, you can selectively mark a region of PyTorch code to use a custom allocator using `torch.cuda.MemPool`. This will let you use multiple CUDA system allocators in the same PyTorch program, along with most of the benefits of the CUDACachingAllocator (e.g. caching). Using `torch.cuda.MemPool`, you can utilize custom allocators that enable several features, such as:
  * Allocating output buffers for an all-reduce using `ncclMemAlloc` allocator can enable NVLink Switch Reductions (NVLS). This can reduce contention between overlapping compute and communication kernels on GPU resources (SMs, and Copy Engines), especially on tensor-parallel workloads.
  * For Grace CPU based systems, allocating host outputs buffers for an all-gather using `cuMemCreate` and specifying `CU_MEM_LOCATION_TYPE_HOST_NUMA` can enable Extended GPU Memory (EGM) based memory transfers from source GPUs to the destination CPU. This accelerates the all-gather since the transfer happens over NVLinks, which otherwise would have happened over bandwidth-limited, Network Interface Card (NIC) links. Such an accelerated all-gather can in turn speed up model checkpointing.
  * If you are crafting a model and don’t want to think about the optimal memory placements of a memory intensive module at first (e.g. an embedding table), or perhaps you have a module which is not performance sensitive and doesn’t fit in the GPU, then you could just allocate that module with `cudaMallocManaged` with preferred CPU location and get your model working first.


Note
While `cudaMallocManaged` offers convenient automatic memory management using CUDA Unified Virtual Memory (UVM), it is not recommended for DL workloads. For DL workloads that fit in GPU memory, explicit placement consistently outperforms UVM, since there are no page faults and access patterns remain predictable. When GPU memory gets saturated, UVM has to perform costly double transfers, evicting pages to CPU before bringing in new ones.
The code below shows `ncclMemAlloc` wrapped in a `torch.cuda.memory.CUDAPluggableAllocator`.
```
import os
import torch
import torch.distributed as dist
from torch.cuda.memory import CUDAPluggableAllocator
from torch.distributed.distributed_c10d import _get_default_group
from torch.utils import cpp_extension

# create allocator
nccl_allocator_source = """
#include <nccl.h>
#include <iostream>
extern "C" {
void* nccl_alloc_plug(size_t size, int device, void* stream) {
 std::cout << "Using ncclMemAlloc" << std::endl;
 void* ptr;
 ncclResult_t err = ncclMemAlloc(&ptr, size);
 return ptr;
}
void nccl_free_plug(void* ptr, size_t size, int device, void* stream) {
 std::cout << "Using ncclMemFree" << std::endl;
 ncclResult_t err = ncclMemFree(ptr);
}
}
"""
nccl_allocator_libname = "nccl_allocator"
nccl_allocator = torch.utils.cpp_extension.load_inline(
  name=nccl_allocator_libname,
  cpp_sources=nccl_allocator_source,
  with_cuda=True,
  extra_ldflags=["-lnccl"],
  verbose=True,
  is_python_module=False,
  build_directory="./",
)
allocator = CUDAPluggableAllocator(
  f"./{nccl_allocator_libname}.so", "nccl_alloc_plug", "nccl_free_plug"
).allocator()
# setup distributed
rank = int(os.getenv("RANK"))
local_rank = int(os.getenv("LOCAL_RANK"))
world_size = int(os.getenv("WORLD_SIZE"))
torch.cuda.set_device(local_rank)
dist.init_process_group(backend="nccl")
device = torch.device(f"cuda:{local_rank}")
default_pg = _get_default_group()
backend = default_pg._get_backend(device)
# Note: for convenience, ProcessGroupNCCL backend provides
# the ncclMemAlloc allocator as backend.mem_allocator
allocator = backend.mem_allocator

```
Copy to clipboard
You can now define a new memory pool by passing this allocator to `torch.cuda.MemPool`:
```
pool = torch.cuda.MemPool(allocator)

```
Copy to clipboard
The pool can then be used with the `torch.cuda.use_mem_pool` context manager to allocate tensors into that pool:
```
with torch.cuda.use_mem_pool(pool):
  # tensor gets allocated with ncclMemAlloc passed in the pool
  tensor = torch.arange(1024 * 1024 * 2, device=device)
  print(f"tensor ptr on rank {rank} is {hex(tensor.data_ptr())}")
# register user buffers using ncclCommRegister (called under the hood)
backend.register_mem_pool(pool)
# Collective uses Zero Copy NVLS
dist.all_reduce(tensor[0:4])
torch.cuda.synchronize()
print(tensor[0:4])

```
Copy to clipboard
Note the usage of `register_mem_pool` in the above example. This is an extra step for NVLS reductions, where the user buffers need to be registered with NCCL. A user can de-register the buffers with a similar `deregister_mem_pool` call.
To reclaim memory, users will first need to ensure nothing is using the pool. When none of the tensors are holding a reference to the pool, `empty_cache()` will be called internally on deletion of the pool, hence returning all the memory to the system.
```
del tensor, del pool

```
Copy to clipboard
The following `torch.cuda.MemPool.use_count()` and `torch.cuda.MemPool.snapshot()` APIs can be used for debugging purposes:
```
pool = torch.cuda.MemPool(allocator)
# pool's use count should be 1 at this point as MemPool object
# holds a reference
assert pool.use_count() == 1
nelem_1mb = 1024 * 1024 // 4
with torch.cuda.use_mem_pool(pool):
  out_0 = torch.randn(nelem_1mb, device="cuda")
  # pool's use count should be 2 at this point as use_mem_pool
  # holds a reference
  assert pool.use_count() == 2
# pool's use count should be back to 1 at this point as use_mem_pool
# released its reference
assert pool.use_count() == 1
with torch.cuda.use_mem_pool(pool):
  # pool should have 1 segment since we made a small allocation (1 MB)
  # above and so the CUDACachingAllocator packed it into a 2 MB buffer
  assert len(pool.snapshot()) == 1
  out_1 = torch.randn(nelem_1mb, device="cuda")
  # pool should still have 1 segment since we made another small allocation
  # (1 MB) that got packed into the existing 2 MB buffer
  assert len(pool.snapshot()) == 1
  out_2 = torch.randn(nelem_1mb, device="cuda")
  # pool now should have 2 segments since the CUDACachingAllocator had
  # to make a new 2 MB buffer to accomodate out_2
  assert len(pool.snapshot()) == 2

```
Copy to clipboard
Note
  * `torch.cuda.MemPool` holds a reference to the pool. When you use the `torch.cuda.use_mem_pool` context manager, it will also acquire another reference to the pool. On exit of the context manager, it will release its reference. After that, ideally it should only be tensors holding references to the pool. Once the tensors release their references, the use count of the pool will be 1, reflecting that only the `torch.cuda.MemPool` object is holding a reference. Only at that point, can the memory held by the pool be returned to the system when the pool’s destructor is called using `del`.
  * `torch.cuda.MemPool` doesn’t currently support `expandable_segments` mode of CUDACachingAllocator.
  * NCCL has specific requirements for a buffer to be compatible with NVLS reductions. These requirements can be broken in a dynamic workload, for instance, the buffer being sent to NCCL by the CUDACachingAllocator might be split and hence, not correctly aligned. In those cases, NCCL can use a fallback algorithm instead of NVLS.
  * Allocators like `ncclMemAlloc` can use more memory than requested, due to alignment requirements (`CU_MULTICAST_GRANULARITY_RECOMMENDED`, `CU_MULTICAST_GRANULARITY_MINIMUM`), and can cause your workload to run out of memory.


## cuBLAS workspaces
For each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will be allocated if that handle and stream combination executes a cuBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless `torch._C._cuda_clearCublasWorkspaces()` is called. The workspace size per allocation can be specified via the environment variable `CUBLAS_WORKSPACE_CONFIG` with the format `:[SIZE]:[COUNT]`. As an example, the default workspace size per allocation is `CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8` which specifies a total size of `2 * 4096 + 8 * 16 KiB`. To force cuBLAS to avoid using workspaces, set `CUBLAS_WORKSPACE_CONFIG=:0:0`.
## cuFFT plan cache
For each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly running FFT methods (e.g., `torch.fft.fft()`) on CUDA tensors of same geometry with same configuration. Because some cuFFT plans may allocate GPU memory, these caches have a maximum capacity.
You may control and query the properties of the cache of current device with the following APIs:
  * `torch.backends.cuda.cufft_plan_cache.max_size` gives the capacity of the cache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions). Setting this value directly modifies the capacity.
  * `torch.backends.cuda.cufft_plan_cache.size` gives the number of plans currently residing in the cache.
  * `torch.backends.cuda.cufft_plan_cache.clear()` clears the cache.


To control and query plan caches of a non-default device, you can index the `torch.backends.cuda.cufft_plan_cache` object with either a `torch.device` object or a device index, and access one of the above attributes. E.g., to set the capacity of the cache for device `1`, one can write `torch.backends.cuda.cufft_plan_cache[1].max_size = 10`.
## Just-in-Time Compilation
PyTorch just-in-time compiles some operations, like torch.special.zeta, when performed on CUDA tensors. This compilation can be time consuming (up to a few seconds depending on your hardware and software) and may occur multiple times for a single operator since many PyTorch operators actually select from a variety of kernels, each of which must be compiled once, depending on their input. This compilation occurs once per process, or just once if a kernel cache is used.
By default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if XDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it’s not (except on Windows, where the kernel cache is not yet supported). The caching behavior can be directly controlled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no cache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used as a kernel cache instead of the default location.
## Best practices
### Device-agnostic code
Due to the structure of PyTorch, you may need to explicitly write device-agnostic (CPU or GPU) code; an example may be creating a new tensor as the initial hidden state of a recurrent neural network.
The first step is to determine whether the GPU should be used or not. A common pattern is to use Python’s `argparse` module to read in user arguments, and have a flag that can be used to disable CUDA, in combination with `is_available()`. In the following, `args.device` results in a `torch.device` object that can be used to move tensors to CPU or CUDA.
```
import argparse
import torch
parser = argparse.ArgumentParser(description='PyTorch Example')
parser.add_argument('--disable-cuda', action='store_true',
          help='Disable CUDA')
args = parser.parse_args()
args.device = None
if not args.disable_cuda and torch.cuda.is_available():
  args.device = torch.device('cuda')
else:
  args.device = torch.device('cpu')

```
Copy to clipboard
Note
When assessing the availability of CUDA in a given environment (`is_available()`), PyTorch’s default behavior is to call the CUDA Runtime API method cudaGetDeviceCount. Because this call in turn initializes the CUDA Driver API (via cuInit) if it is not already initialized, subsequent forks of a process that has run `is_available()` will fail with a CUDA initialization error.
One can set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in your environment before importing PyTorch modules that execute `is_available()` (or before executing it directly) in order to direct `is_available()` to attempt an NVML-based assessment (nvmlDeviceGetCount_v2). If the NVML-based assessment is successful (i.e. NVML discovery/initialization does not fail), `is_available()` calls will not poison subsequent forks.
If NVML discovery/initialization fails, `is_available()` will fallback to the standard CUDA Runtime API assessment and the aforementioned fork constraint will apply.
Note that the above NVML-based CUDA availability assessment provides a weaker guarantee than the default CUDA Runtime API approach (which requires CUDA initialization to succeed). In some circumstances, the NVML-based check may succeed while later CUDA initialization fails.
Now that we have `args.device`, we can use it to create a Tensor on the desired device.
```
x = torch.empty((8, 42), device=args.device)
net = Network().to(device=args.device)

```
Copy to clipboard
This can be used in a number of cases to produce device agnostic code. Below is an example when using a dataloader:
```
cuda0 = torch.device('cuda:0') # CUDA GPU 0
for i, x in enumerate(train_loader):
  x = x.to(cuda0)

```
Copy to clipboard
When working with multiple GPUs on a system, you can use the `CUDA_VISIBLE_DEVICES` environment flag to manage which GPUs are available to PyTorch. As mentioned above, to manually control which GPU a tensor is created on, the best practice is to use a `torch.cuda.device` context manager.
```
print("Outside device is 0") # On device 0 (default in most scenarios)
with torch.cuda.device(1):
  print("Inside device is 1") # On device 1
print("Outside device is still 0") # On device 0

```
Copy to clipboard
If you have a tensor and would like to create a new tensor of the same type on the same device, then you can use a `torch.Tensor.new_*` method (see `torch.Tensor`). Whilst the previously mentioned `torch.*` factory functions (Creation Ops) depend on the current GPU context and the attributes arguments you pass in, `torch.Tensor.new_*` methods preserve the device and other attributes of the tensor.
This is the recommended practice when creating modules in which new tensors need to be created internally during the forward pass.
```
cuda = torch.device('cuda')
x_cpu = torch.empty(2)
x_gpu = torch.empty(2, device=cuda)
x_cpu_long = torch.empty(2, dtype=torch.int64)
y_cpu = x_cpu.new_full([3, 2], fill_value=0.3)
print(y_cpu)
  tensor([[ 0.3000, 0.3000],
      [ 0.3000, 0.3000],
      [ 0.3000, 0.3000]])
y_gpu = x_gpu.new_full([3, 2], fill_value=-5)
print(y_gpu)
  tensor([[-5.0000, -5.0000],
      [-5.0000, -5.0000],
      [-5.0000, -5.0000]], device='cuda:0')
y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])
print(y_cpu_long)
  tensor([[ 1, 2, 3]])

```
Copy to clipboard
If you want to create a tensor of the same type and size of another tensor, and fill it with either ones or zeros, `ones_like()` or `zeros_like()` are provided as convenient helper functions (which also preserve `torch.device` and `torch.dtype` of a Tensor).
```
x_cpu = torch.empty(2, 3)
x_gpu = torch.empty(2, 3)
y_cpu = torch.ones_like(x_cpu)
y_gpu = torch.zeros_like(x_gpu)

```
Copy to clipboard
### Use pinned memory buffers
Warning
This is an advanced tip. If you overuse pinned memory, it can cause serious problems when running low on RAM, and you should be aware that pinning is often an expensive operation.
Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a `pin_memory()` method, that returns a copy of the object, with data put in a pinned region.
Also, once you pin a tensor or storage, you can use asynchronous GPU copies. Just pass an additional `non_blocking=True` argument to a `to()` or a `cuda()` call. This can be used to overlap data transfers with computation.
You can make the `DataLoader` return batches placed in pinned memory by passing `pin_memory=True` to its constructor.
### Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel
Most use cases involving batched inputs and multiple GPUs should default to using `DistributedDataParallel` to utilize more than one GPU.
There are significant caveats to using CUDA models with `multiprocessing`; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.
It is recommended to use `DistributedDataParallel`, instead of `DataParallel` to do multi-GPU training, even if there is only a single node.
The difference between `DistributedDataParallel` and `DataParallel` is: `DistributedDataParallel` uses multiprocessing where a process is created for each GPU, while `DataParallel` uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.
If you use `DistributedDataParallel`, you could use torch.distributed.launch utility to launch your program, see Third-party backends.
## CUDA Graphs
A CUDA graph is a record of the work (mostly kernels and their arguments) that a CUDA stream and its dependent streams perform. For general principles and details on the underlying CUDA API, see Getting Started with CUDA Graphs and the Graphs section of the CUDA C Programming Guide.
PyTorch supports the construction of CUDA graphs using stream capture, which puts a CUDA stream in _capture mode_. CUDA work issued to a capturing stream doesn’t actually run on the GPU. Instead, the work is recorded in a graph.
After capture, the graph can be _launched_ to run the GPU work as many times as needed. Each replay runs the same kernels with the same arguments. For pointer arguments this means the same memory addresses are used. By filling input memory with new data (e.g., from a new batch) before each replay, you can rerun the same work on new data.
### Why CUDA Graphs?
Replaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange for **greatly reduced CPU overhead**. A graph’s arguments and kernels are fixed, so a graph replay skips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver overheads. Under the hood, a replay submits the entire graph’s work to the GPU with a single call to cudaGraphLaunch. Kernels in a replay also execute slightly faster on the GPU, but eliding CPU overhead is the main benefit.
You should try CUDA graphs if all or part of your network is graph-safe (usually this means static shapes and static control flow, but see the other constraints) and you suspect its runtime is at least somewhat CPU-limited.
### PyTorch API
Warning
This API is in beta and may change in future releases.
PyTorch exposes graphs via a raw `torch.cuda.CUDAGraph` class and two convenience wrappers, `torch.cuda.graph` and `torch.cuda.make_graphed_callables`.
`torch.cuda.graph` is a simple, versatile context manager that captures CUDA work in its context. Before capture, warm up the workload to be captured by running a few eager iterations. Warmup must occur on a side stream. Because the graph reads from and writes to the same memory addresses in every replay, you must maintain long-lived references to tensors that hold input and output data during capture. To run the graph on new input data, copy new data to the capture’s input tensor(s), replay the graph, then read the new output from the capture’s output tensor(s). Example:
```
g = torch.cuda.CUDAGraph()
# Placeholder input used for capture
static_input = torch.empty((5,), device="cuda")
# Warmup before capture
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
  for _ in range(3):
    static_output = static_input * 2
torch.cuda.current_stream().wait_stream(s)
# Captures the graph
# To allow capture, automatically sets a side stream as the current stream in the context
with torch.cuda.graph(g):
  static_output = static_input * 2
# Fills the graph's input memory with new data to compute on
static_input.copy_(torch.full((5,), 3, device="cuda"))
g.replay()
# static_output holds the results
print(static_output) # full of 3 * 2 = 6
# Fills the graph's input memory with more data to compute on
static_input.copy_(torch.full((5,), 4, device="cuda"))
g.replay()
print(static_output) # full of 4 * 2 = 8

```
Copy to clipboard
See Whole-network capture, Usage with torch.cuda.amp, and Usage with multiple streams for realistic and advanced patterns.
`make_graphed_callables` is more sophisticated. `make_graphed_callables` accepts Python functions and `torch.nn.Module`s. For each passed function or Module, it creates separate graphs of the forward-pass and backward-pass work. See Partial-network capture.
#### Constraints
A set of ops is _capturable_ if it doesn’t violate any of the following constraints.
Constraints apply to all work in a `torch.cuda.graph` context and all work in the forward and backward passes of any callable you pass to `torch.cuda.make_graphed_callables()`.
Violating any of these will likely cause a runtime error:
  * Capture must occur on a non-default stream. (This is only a concern if you use the raw `CUDAGraph.capture_begin` and `CUDAGraph.capture_end` calls. `graph` and `make_graphed_callables()` set a side stream for you.)
  * Ops that synchronize the CPU with the GPU (e.g., `.item()` calls) are prohibited.
  * CUDA RNG operations are permitted, and when using multiple `torch.Generator` instances within a graph, they must be registered using `CUDAGraph.register_generator_state` before graph capture. Avoid using `Generator.get_state` and `Generator.set_state` during capture; instead, utilize `Generator.graphsafe_set_state` and `Generator.graphsafe_get_state` for managing generator states safely within the graph context. This ensures proper RNG operation and generator management within CUDA graphs.


Violating any of these will likely cause silent numerical errors or undefined behavior:
  * Within a process, only one capture may be underway at a time.
  * No non-captured CUDA work may run in this process (on any thread) while capture is underway.
  * CPU work is not captured. If the captured ops include CPU work, that work will be elided during replay.
  * Every replay reads from and writes to the same (virtual) memory addresses.
  * Dynamic control flow (based on CPU or GPU data) is prohibited.
  * Dynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence has the same size and layout in every replay.
  * Using multiple streams in a capture is allowed, but there are restrictions.


#### Non-constraints
  * Once captured, the graph may be replayed on any stream.


### Whole-network capture
If your entire network is capturable, you can capture and replay an entire iteration:
```
N, D_in, H, D_out = 640, 4096, 2048, 1024
model = torch.nn.Sequential(torch.nn.Linear(D_in, H),
              torch.nn.Dropout(p=0.2),
              torch.nn.Linear(H, D_out),
              torch.nn.Dropout(p=0.1)).cuda()
loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
# Placeholders used for capture
static_input = torch.randn(N, D_in, device='cuda')
static_target = torch.randn(N, D_out, device='cuda')
# warmup
# Uses static_input and static_target here for convenience,
# but in a real setting, because the warmup includes optimizer.step()
# you must use a few batches of real data.
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
  for i in range(3):
    optimizer.zero_grad(set_to_none=True)
    y_pred = model(static_input)
    loss = loss_fn(y_pred, static_target)
    loss.backward()
    optimizer.step()
torch.cuda.current_stream().wait_stream(s)
# capture
g = torch.cuda.CUDAGraph()
# Sets grads to None before capture, so backward() will create
# .grad attributes with allocations from the graph's private pool
optimizer.zero_grad(set_to_none=True)
with torch.cuda.graph(g):
  static_y_pred = model(static_input)
  static_loss = loss_fn(static_y_pred, static_target)
  static_loss.backward()
  optimizer.step()
real_inputs = [torch.rand_like(static_input) for _ in range(10)]
real_targets = [torch.rand_like(static_target) for _ in range(10)]
for data, target in zip(real_inputs, real_targets):
  # Fills the graph's input memory with new data to compute on
  static_input.copy_(data)
  static_target.copy_(target)
  # replay() includes forward, backward, and step.
  # You don't even need to call optimizer.zero_grad() between iterations
  # because the captured backward refills static .grad tensors in place.
  g.replay()
  # Params have been updated. static_y_pred, static_loss, and .grad
  # attributes hold values from computing on this iteration's data.

```
Copy to clipboard
### Partial-network capture
If some of your network is unsafe to capture (e.g., due to dynamic control flow, dynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe part(s) eagerly and use `torch.cuda.make_graphed_callables()` to graph only the capture-safe part(s).
By default, callables returned by `make_graphed_callables()` are autograd-aware, and can be used in the training loop as direct replacements for the functions or `nn.Module`s you passed.
`make_graphed_callables()` internally creates `CUDAGraph` objects, runs warmup iterations, and maintains static inputs and outputs as needed. Therefore (unlike with `torch.cuda.graph`) you don’t need to handle those manually.
In the following example, data-dependent dynamic control flow means the network isn’t capturable end-to-end, but `make_graphed_callables()` lets us capture and run graph-safe sections as graphs regardless:
```
N, D_in, H, D_out = 640, 4096, 2048, 1024
module1 = torch.nn.Linear(D_in, H).cuda()
module2 = torch.nn.Linear(H, D_out).cuda()
module3 = torch.nn.Linear(H, D_out).cuda()
loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD(chain(module1.parameters(),
                 module2.parameters(),
                 module3.parameters()),
              lr=0.1)
# Sample inputs used for capture
# requires_grad state of sample inputs must match
# requires_grad state of real inputs each callable will see.
x = torch.randn(N, D_in, device='cuda')
h = torch.randn(N, H, device='cuda', requires_grad=True)
module1 = torch.cuda.make_graphed_callables(module1, (x,))
module2 = torch.cuda.make_graphed_callables(module2, (h,))
module3 = torch.cuda.make_graphed_callables(module3, (h,))
real_inputs = [torch.rand_like(x) for _ in range(10)]
real_targets = [torch.randn(N, D_out, device="cuda") for _ in range(10)]
for data, target in zip(real_inputs, real_targets):
  optimizer.zero_grad(set_to_none=True)
  tmp = module1(data) # forward ops run as a graph
  if tmp.sum().item() > 0:
    tmp = module2(tmp) # forward ops run as a graph
  else:
    tmp = module3(tmp) # forward ops run as a graph
  loss = loss_fn(tmp, target)
  # module2's or module3's (whichever was chosen) backward ops,
  # as well as module1's backward ops, run as graphs
  loss.backward()
  optimizer.step()

```
Copy to clipboard
### Usage with torch.cuda.amp
For typical optimizers, `GradScaler.step` syncs the CPU with the GPU, which is prohibited during capture. To avoid errors, either use partial-network capture, or (if forward, loss, and backward are capture-safe) capture forward, loss, and backward but not the optimizer step:
```
# warmup
# In a real setting, use a few batches of real data.
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
  for i in range(3):
    optimizer.zero_grad(set_to_none=True)
    with torch.cuda.amp.autocast():
      y_pred = model(static_input)
      loss = loss_fn(y_pred, static_target)
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
torch.cuda.current_stream().wait_stream(s)
# capture
g = torch.cuda.CUDAGraph()
optimizer.zero_grad(set_to_none=True)
with torch.cuda.graph(g):
  with torch.cuda.amp.autocast():
    static_y_pred = model(static_input)
    static_loss = loss_fn(static_y_pred, static_target)
  scaler.scale(static_loss).backward()
  # don't capture scaler.step(optimizer) or scaler.update()
real_inputs = [torch.rand_like(static_input) for _ in range(10)]
real_targets = [torch.rand_like(static_target) for _ in range(10)]
for data, target in zip(real_inputs, real_targets):
  static_input.copy_(data)
  static_target.copy_(target)
  g.replay()
  # Runs scaler.step and scaler.update eagerly
  scaler.step(optimizer)
  scaler.update()

```
Copy to clipboard
### Usage with multiple streams
Capture mode automatically propagates to any streams that sync with a capturing stream. Within capture, you may expose parallelism by issuing calls to different streams, but the overall stream dependency DAG must branch out from the initial capturing stream after capture begins and rejoin the initial stream before capture ends:
```
with torch.cuda.graph(g):
  # at context manager entrance, torch.cuda.current_stream()
  # is the initial capturing stream
  # INCORRECT (does not branch out from or rejoin initial stream)
  with torch.cuda.stream(s):
    cuda_work()
  # CORRECT:
  # branches out from initial stream
  s.wait_stream(torch.cuda.current_stream())
  with torch.cuda.stream(s):
    cuda_work()
  # rejoins initial stream before capture ends
  torch.cuda.current_stream().wait_stream(s)

```
Copy to clipboard
Note
To avoid confusion for power users looking at replays in nsight systems or nvprof: Unlike eager execution, the graph interprets a nontrivial stream DAG in capture as a hint, not a command. During replay, the graph may reorganize independent ops onto different streams or enqueue them in a different order (while respecting your original DAG’s overall dependencies).
### Usage with DistributedDataParallel
#### NCCL < 2.9.6
NCCL versions earlier than 2.9.6 don’t allow collectives to be captured. You must use partial-network capture, which defers allreduces to happen outside graphed sections of backward.
Call `make_graphed_callables()` on graphable network sections _before_ wrapping the network with DDP.
#### NCCL >= 2.9.6
NCCL versions 2.9.6 or later allow collectives in the graph. Approaches that capture an entire backward pass are a viable option, but need three setup steps.
  1. Disable DDP’s internal async error handling:
```
os.environ["NCCL_ASYNC_ERROR_HANDLING"] = "0"
torch.distributed.init_process_group(...)

```
Copy to clipboard
  2. Before full-backward capture, DDP must be constructed in a side-stream context:
```
with torch.cuda.stream(s):
  model = DistributedDataParallel(model)

```
Copy to clipboard
  3. Your warmup must run at least 11 DDP-enabled eager iterations before capture.


### Graph memory management
A captured graph acts on the same virtual addresses every time it replays. If PyTorch frees the memory, a later replay can hit an illegal memory access. If PyTorch reassigns the memory to new tensors, the replay can corrupt the values seen by those tensors. Therefore, the virtual addresses used by the graph must be reserved for the graph across replays. The PyTorch caching allocator achieves this by detecting when capture is underway and satisfying the capture’s allocations from a graph-private memory pool. The private pool stays alive until its `CUDAGraph` object and all tensors created during capture go out of scope.
Private pools are maintained automatically. By default, the allocator creates a separate private pool for each capture. If you capture multiple graphs, this conservative approach ensures graph replays never corrupt each other’s values, but sometimes needlessly wastes memory.
#### Sharing memory across captures
To economize the memory stashed in private pools, `torch.cuda.graph` and `torch.cuda.make_graphed_callables()` optionally allow different captures to share the same private pool. It’s safe for a set of graphs to share a private pool if you know they’ll always be replayed in the same order they were captured, and never be replayed concurrently.
`torch.cuda.graph`’s `pool` argument is a hint to use a particular private pool, and can be used to share memory across graphs as shown:
```
g1 = torch.cuda.CUDAGraph()
g2 = torch.cuda.CUDAGraph()
# (create static inputs for g1 and g2, run warmups of their workloads...)
# Captures g1
with torch.cuda.graph(g1):
  static_out_1 = g1_workload(static_in_1)
# Captures g2, hinting that g2 may share a memory pool with g1
with torch.cuda.graph(g2, pool=g1.pool()):
  static_out_2 = g2_workload(static_in_2)
static_in_1.copy_(real_data_1)
static_in_2.copy_(real_data_2)
g1.replay()
g2.replay()

```
Copy to clipboard
With `torch.cuda.make_graphed_callables()`, if you want to graph several callables and you know they’ll always run in the same order (and never concurrently) pass them as a tuple in the same order they’ll run in the live workload, and `make_graphed_callables()` will capture their graphs using a shared private pool.
If, in the live workload, your callables will run in an order that occasionally changes, or if they’ll run concurrently, passing them as a tuple to a single invocation of `make_graphed_callables()` is not allowed. Instead, you must call `make_graphed_callables()` separately for each one.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * CUDA semantics
    * TensorFloat-32 (TF32) on Ampere (and later) devices
    * Reduced Precision Reduction in FP16 GEMMs
    * Reduced Precision Reduction in BF16 GEMMs
    * Full FP16 Accmumulation in FP16 GEMMs
    * Asynchronous execution
      * CUDA streams
      * Stream semantics of backward passes
        * BC note: Using grads on the default stream
    * Memory management
      * Optimizing memory usage with `PYTORCH_CUDA_ALLOC_CONF`
    * Using custom memory allocators for CUDA
    * Mixing different CUDA system allocators in the same program
    * cuBLAS workspaces
    * cuFFT plan cache
    * Just-in-Time Compilation
    * Best practices
      * Device-agnostic code
      * Use pinned memory buffers
      * Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel
    * CUDA Graphs
      * Why CUDA Graphs?
      * PyTorch API
        * Constraints
        * Non-constraints
      * Whole-network capture
      * Partial-network capture
      * Usage with torch.cuda.amp
      * Usage with multiple streams
      * Usage with DistributedDataParallel
        * NCCL < 2.9.6
        * NCCL >= 2.9.6
      * Graph memory management
        * Sharing memory across captures


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * FSDP Notes
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# FSDP Notes
## FSDP Prefetch Nuances
For overlapping `forward` all-gathers with `forward` compute, there are two possible mechanisms:
  1. Implicit forward prefetching (always enabled)
  2. Explicit forward prefetching (`forward_prefetch=True`)


Implicit `forward` prefetching refers to relying on issuing the all-gathers from a separate CUDA stream to allow for overlapping an all-gather with `forward` compute issued before it (from the CPU perspective). For example, if we have layer 0 all-gather -> layer 0 `forward` compute -> layer 1 all-gather -> …, then layer 1 all-gather can overlap with layer 0 `forward` compute even though the CPU thread issued it afterwards. (The 1st all-gather will not be able to overlap with anything.)
Explicit `forward` prefetching refers to changing the CPU thread’s issue order: e.g. layer 0 all-gather -> layer 1 all-gather -> layer 0 `forward` compute -> …. In eager mode, there is no way to know in general which layer is the next layer (e.g. layer 1 in the example) when still executing on layer 0. Therefore, explicit `forward` prefetching should only be used for models whose execution order is fixed from iteration to iteration (which we sometimes call “static graph”). An example of a model that does not satisfy this constraint is FLAVA).
Explicit `forward` prefetching only saves the time taken to issue a layer’s `forward` compute kernels at the cost that the next all-gather’s output tensor must be allocated while the current one is still in use. By issuing the next all- gather before the current `forward` compute kernels, the next all-gather can start sooner on GPU. For most LLM workloads, this is not the case, so there is no motivation for enabling `forward_prefetch=True`.
In contrast, for `backward`, we must use explicit `backward` prefetching or else there will be 0 overlap of communication and computation. The reason is because we use a single NCCL process group for both all-gather and reduce-scatter (partially because in earlier NCCL versions, it was not safe to use multiple concurrently on the same device over the same ranks). A single NCCL process group means a single internal NCCL stream on which reduce-scatters and all-gathers run serially. As such, unless we explicitly reorder the CPU issue order to be next all-gather -> current reduce-scatter, then the current reduce-scatter would block the next all-gather and hence the next `backward` computation, preventing the current reduce-scatter from overlapping.
## Communication payload size
In FSDP the communications are:
  1. all-gather on parameters in `forward`
  2. all-gather on parameters in `backward`
  3. reduce-scatter on gradients in `backward`


If activation checkpointing (`checkpoint()`) is used there is no additional communication since the parameters are prefetched anyway during `backward`.
In the FSDP design, the communication payload per rank is determined as follows: Each call to `FullyShardedDataParallel` creates one communication group consisting of the parameters in `module.parameters()` except any already assigned to a nested `FullyShardedDataParallel` instance. For example, for Llama, if you apply `FullyShardedDataParallel` to every transformer block and also to the root module, then there is one communication group for each transformer block and finally one communication group with the initial embedding and final linear. Each communication group corresponds to a single all-gather call and single reduce-scatter call. In that way, how you apply `FullyShardedDataParallel` determines the communication size. In general, applying FSDP to each transformer block is a good heuristic for LLMs, and it is hard to do better than that given the current design.
Let’s consider an example where we have a Transformer-based model sharded over 8 GPUs, where the sharding happens at the transformer block-level only, and each transformer block contains 1.6B parameters and the parameters are in fp32 (4 bytes each). Which means that once sharded, each transformer block will contain 0.2B parameters on each rank.
  * The `forward` pass will communicate in chunks of `0.2*4 = 0.8GB` in all-gather
  * The `backward` pass will communicate 2 times `0.8GB` each (1x all-gather and 1x reduce-scatter)


In other words there will be 3 communications with a payload of `0.8GB` each. If the model was comprised of 10 transformer blocks there would be a total of 30 communications for a total of `30*0.8=24GB`.
To formalize the payload size per communication per rank is `total_transformer_block_params_in_B*dtype_bytes/num_gpus` (GBs).
Please note that in this example we didn’t include the additional communications required for the embedding, which should be accounted for as well. And the math would depend on whether the input and output embeddings are tied or not. If they aren’t tied there will be 2x more communications.
## FSDP buffers sizes
First, let’s cover the buffers allocated for communications:
`forward` currently requires 2x all-gather buffer size. Here is why:
As explained in FSDP Prefetch Nuances in the case of explicit `forward` prefetching (`forward_prefetch=True`) case of layer 0 all-gather -> layer 0 forward compute -> layer 1 all-gather there is a need for 2 all-gather-sized buffers, because one buffer is used in the current ``forward` while the other is used to do the prefetching.
While the implicit `forward` prefetching (`forward_prefetch=False`, default) case of the same sequence in theory should need only 1 buffer, in reality it’s still 2x all-gather-sized buffers. The reason is that in the flat-parameter FSDP design, we do not copy-out of the all-gather buffer. The parameters used for compute are directly viewed into the all-gather buffer (in fact, the main benefit of the “flat parameter” is exactly this reason). In that case, while ‘layer 1 all-gather’ is overlapping with ‘layer 0 forward compute’, the ‘layer 0 forward compute’ is using the parameters viewed into the ‘layer 0 all-gather’ buffer.
A natural question then is, when would you want `forward_prefetch=False`? For static-graph models (like most LLMs), there is a major technical reason. It is more that, practically, we added this option quickly for some CPU-bound internal models and have not tested every code path with it in unit testing, so we are less confident in it. `forward_prefetching=False` can be slightly easier to reason about since we do not have to check the recorded forward order as a possible ‘failure mode’; a module’s all-gather can always be found under its own `record_function` label in its profiler trace.
`backward` currently requires at least 2x all-gather buffer size and potentially a bit more. Here is why:
The current FSDP design uses `recordStream` to manage allocations produced in one stream consumed in another, which can lead to more memory usage than expected. How much more can be “non-deterministic” in that it depends on GPU kernel timing relative to the CPU. The `limit_all_gathers=True` argument is a mitigation to that - for more details refer to this discussion is FSDP & CUDACachingAllocator.
The way existing FSDP works with autograd:
  * Existing FSDP all-gathers the `flat_param`, which is the autograd leaf.
  * It calls `torch.split` to get 1D views into the `flat_param` corresponding to its constituent original parameters.
  * It calls `torch.view` on each 1D split to view back to ND.
  * This means that in `backward`, we end up with `ViewBackward` (ND -> 1D) and `SplitWithSizesBackward` (which is a concat). In particular, each individual gradient is computed as a separate allocation, and an explicit concat happens to construct the reduce-scatter input buffer. This implies actually a 2x buffer size for reduce-scatter at that peak memory point.


In summary, for `backward`, it is about 2x buffer size for reduce-scatter plus any `recordStream` effects.
Second, let’s discuss the additional buffers:
Once the sharded parameters are gathered from all ranks, they require an additional buffer of total_transformer_block_params_in_B*dtype_bytes for the full parameters - so continuing the example from earlier if each transformer block is 1.6B parameters and the parameters are in fp32, then it’d be 1.6*4=6.4GB buffer.
And there is a need for 2 of those buffers, since there is one currently being used and another being prefetched.
To summarize, we have:
  1. 2 times communication buffers of `total_transformer_block_params_in_B*dtype_bytes/num_gpus`
  2. 2 times unsharded transformer block parameters buffer ```total_transformer_block_params_in_B*dtype_bytes`


or if you have been following the example:
  1. `2*1.6*4/8=1.6GB`
  2. `2**1.6*4=12.8GB`


and the total of `14.4GB`.
Now let’s briefly discuss what happens to the embeddings as we have left those out from the calculations:
Given the rule we discussed that you included in the note starting with “the communication buffer size is determined as follows”, we can analyze as follows:
  * Suppose we apply FSDP to the root module (e.g. the `Transformer` class). Suppose we further apply FSDP to each transformer block (e.g. the `TransformerBlock` class).
  * Most commonly, the embedding and final linear projection are direct children of the root `Transformer` class.
  * Following our rule, that means that the embedding and final linear projection are assigned to the root `Transformer`’s flat parameter.
  * We have _another_ special rule, which is that the root does not free its parameters after forward because they will be anyways immediately all-gathered in backward.
  * Putting this together, this means that the root’s flat parameter including the embedding and final projection are all-gathered to begin forward and kept in GPU memory until the end of backward.
  * If the embedding and final linear are not weight-tied, then we _could_ further apply FSDP to the embedding and to the final linear. For weight-tied parameters, we require them to be part of the same flat parameter (or else it would get double-counted). That would allow the embedding to be freed after its usage in forward and only all-gathered toward the end of backward.
  * Hopefully, this gives a better sense – each FSDP module gets assigned parameters in its `module.parameters` except those already assigned to another nested FSDP module, and the FSDP module’s `forward` defines the ‘live’ interval for its parameters. Hence, the nested `nn.Module` structure can affect the all-gather/free schedule and hence the memory/throughput performance.


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * FSDP Notes
    * FSDP Prefetch Nuances
    * Communication payload size
    * FSDP buffers sizes


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Extending PyTorch
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Extending PyTorch
In this note we’ll cover ways of extending `torch.nn`, `torch.autograd`, `torch`, and writing custom C++ extensions.
## Adding new operators
PyTorch offers a large library of operators that work on Tensors (e.g. `torch.add()`, `torch.sum()`, etc). However, you may wish to bring a new custom operation to PyTorch and have it behave like PyTorch’s built-in operators. In order to do so, you must register the custom operation with PyTorch via the Python torch.library or C++ TORCH_LIBRARY APIs.
Please see PyTorch Custom Operators Landing Page for more details.
## Extending `torch.autograd`
Adding operations to `autograd` requires implementing a new `Function` subclass for each operation. Recall that Functions are what `autograd` uses to encode the operation history and compute gradients.
The first part of this doc is focused on backward mode AD as it is the most widely used feature. A section at the end discusses the extensions for forward mode AD.
### When to use
In general, implement a custom function if you want to perform computations in your model that are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but still wish for your operation to chain with other ops and work with the autograd engine.
In some situations, custom functions can also be used to improve performance and memory usage: If you implemented your forward and backward passes using a C++ extension, you can wrap them in `Function` to interface with the autograd engine. If you’d like to reduce the number of buffers saved for the backward pass, custom functions can be used to combine ops together.
### When not to use
If you can already write your function in terms of PyTorch’s built-in ops, its backward graph is (most likely) already able to be recorded by autograd. In this case, you do not need to implement the backward function yourself. Consider using a plain old Python function.
If you need to maintain state, i.e., trainable parameters, you should (also) use a custom module. See the section below for more information on extending `torch.nn`.
If you’d like to alter the gradients during the backward pass or perform a side effect, consider registering a tensor or Module hook.
### How to use
Take the following steps: 1. Subclass `Function` and implement the `forward()`, (optional) `setup_context()` and `backward()` methods. 2. Call the proper methods on the ctx argument. 3. Declare whether your function supports double backward. 4. Validate whether your gradients are correct using gradcheck.
**Step 1:** After subclassing `Function`, you’ll need to define 3 methods:
  * `forward()` is the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here. `Tensor` arguments that track history (i.e., with `requires_grad=True`) will be converted to ones that don’t track history before the call, and their use will be registered in the graph. Note that this logic won’t traverse lists/dicts/any other data structures and will only consider tensors that are direct arguments to the call. You can return either a single `Tensor` output, or a `tuple` of tensors if there are multiple outputs. Also, please refer to the docs of `Function` to find descriptions of useful methods that can be called only from `forward()`.
  * `setup_context()` (optional). One can either write a “combined” `forward()` that accepts a `ctx` object or (as of PyTorch 2.0) a separate `forward()` that does not accept `ctx` and a `setup_context()` method where the `ctx` modification happens. The `forward()` should have the compute and `setup_context()` should only be responsible for the `ctx` modification (and not have any compute). In general the separate `forward()` and `setup_context()` is closer to how PyTorch native operations work and therefore more composable with various PyTorch subsystems. See Combined or separate forward() and setup_context() for more details.
  * `backward()` (or `vjp()`) defines the gradient formula. It will be given as many `Tensor` arguments as there were outputs, with each of them representing gradient w.r.t. that output. It is important NEVER to modify these in-place. It should return as many tensors as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn’t require gradient (`needs_input_grad` is a tuple of booleans indicating whether each input needs gradient computation), or were non-`Tensor` objects, you can return `python:None`. Also, if you have optional arguments to `forward()` you can return more gradients than there were inputs, as long as they’re all `None`.


**Step 2:** It is your responsibility to use the functions in `ctx` properly in order to ensure that the new `Function` works properly with the autograd engine.
  * `save_for_backward()` must be used to save any tensors to be used in the backward pass. Non-tensors should be stored directly on ctx. If tensors that are neither input nor output are saved for backward your `Function` may not support double backward (see step 3).
  * `mark_dirty()` must be used to mark any input that is modified inplace by the forward function.
  * `mark_non_differentiable()` must be used to tell the engine if an output is not differentiable. By default all output tensors that are of differentiable type will be set to require gradient. Tensors of non-differentiable type (i.e., integral types) are never marked as requiring gradients.
  * `set_materialize_grads()` can be used to tell the autograd engine to optimize gradient computations in the cases where the output does not depend on the input by not materializing grad tensors given to backward function. That is, if set to False, None object in Python or “undefined tensor” (tensor x for which x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior to calling backward, and so your code will need to handle such objects as if they were tensors filled with zeros. The default value of this setting is True.


**Step 3:** If your `Function` does not support double backward you should explicitly declare this by decorating backward with the `once_differentiable()`. With this decorator, attempts to perform double backward through your function will produce an error. See our double backward tutorial for more information on double backward.
**Step 4:** It is recommended that you use `torch.autograd.gradcheck()` to check whether your backward function correctly computes gradients of the forward by computing the Jacobian matrix using your backward function and comparing the value element-wise with the Jacobian computed numerically using finite-differencing.
### Example
Below you can find code for a `Linear` function, with additional comments:
```
# Inherit from Function
class LinearFunction(Function):
  # Note that forward, setup_context, and backward are @staticmethods
  @staticmethod
  def forward(input, weight, bias):
    output = input.mm(weight.t())
    if bias is not None:
      output += bias.unsqueeze(0).expand_as(output)
    return output
  @staticmethod
  # inputs is a Tuple of all of the inputs passed to forward.
  # output is the output of the forward().
  def setup_context(ctx, inputs, output):
    input, weight, bias = inputs
    ctx.save_for_backward(input, weight, bias)
  # This function has only a single output, so it gets only one gradient
  @staticmethod
  def backward(ctx, grad_output):
    # This is a pattern that is very convenient - at the top of backward
    # unpack saved_tensors and initialize all gradients w.r.t. inputs to
    # None. Thanks to the fact that additional trailing Nones are
    # ignored, the return statement is simple even when the function has
    # optional inputs.
    input, weight, bias = ctx.saved_tensors
    grad_input = grad_weight = grad_bias = None
    # These needs_input_grad checks are optional and there only to
    # improve efficiency. If you want to make your code simpler, you can
    # skip them. Returning gradients for inputs that don't require it is
    # not an error.
    if ctx.needs_input_grad[0]:
      grad_input = grad_output.mm(weight)
    if ctx.needs_input_grad[1]:
      grad_weight = grad_output.t().mm(input)
    if bias is not None and ctx.needs_input_grad[2]:
      grad_bias = grad_output.sum(0)
    return grad_input, grad_weight, grad_bias

```
Copy to clipboard
Now, to make it easier to use these custom ops, we recommend either aliasing them or wrapping them in a function. Wrapping in a function lets us support default arguments and keyword arguments:
```
# Option 1: alias
linear = LinearFunction.apply
# Option 2: wrap in a function, to support default args and keyword args.
def linear(input, weight, bias=None):
  return LinearFunction.apply(input, weight, bias)

```
Copy to clipboard
Here, we give an additional example of a function that is parametrized by non-Tensor arguments:
```
class MulConstant(Function):
  @staticmethod
  def forward(tensor, constant):
    return tensor * constant
  @staticmethod
  def setup_context(ctx, inputs, output):
    # ctx is a context object that can be used to stash information
    # for backward computation
    tensor, constant = inputs
    ctx.constant = constant
  @staticmethod
  def backward(ctx, grad_output):
    # We return as many input gradients as there were arguments.
    # Gradients of non-Tensor arguments to forward must be None.
    return grad_output * ctx.constant, None

```
Copy to clipboard
And here, we optimize the above example by calling set_materialize_grads(False):
```
class MulConstant(Function):
  @staticmethod
  def forward(tensor, constant):
    return tensor * constant
  @staticmethod
  def setup_context(ctx, inputs, output):
    tensor, constant = inputs
    ctx.set_materialize_grads(False)
    ctx.constant = constant
  @staticmethod
  def backward(ctx, grad_output):
    # Here we must handle None grad_output tensor. In this case we
    # can skip unnecessary computations and just return None.
    if grad_output is None:
      return None, None
    # We return as many input gradients as there were arguments.
    # Gradients of non-Tensor arguments to forward must be None.
    return grad_output * ctx.constant, None

```
Copy to clipboard
If you need any “intermediate” Tensors computed in `forward()` to be saved, either they must be returned as outputs, or combine `forward` and `setup_context()` (see Combined or separate forward() and setup_context()). Note that this means if you want gradients to flow through those intermediate values, you need to define the gradient formula for them (see also the double backward tutorial ):
```
class MyCube(torch.autograd.Function):
  @staticmethod
  def forward(x):
    # We wish to save dx for backward. In order to do so, it must
    # be returned as an output.
    dx = 3 * x ** 2
    result = x ** 3
    return result, dx
  @staticmethod
  def setup_context(ctx, inputs, output):
    x, = inputs
    result, dx = output
    ctx.save_for_backward(x, dx)
  @staticmethod
  def backward(ctx, grad_output, grad_dx):
    x, dx = ctx.saved_tensors
    # In order for the autograd.Function to work with higher-order
    # gradients, we must add the gradient contribution of `dx`,
    # which is grad_dx * 6 * x.
    result = grad_output * dx + grad_dx * 6 * x
    return result
# Wrap MyCube in a function so that it is clearer what the output is
def my_cube(x):
  result, dx = MyCube.apply(x)
  return result

```
Copy to clipboard
Note
Inputs to `backward`, i.e., `grad_output`, can also be tensors that track history. So if `backward` is implemented with differentiable operations, (e.g., invocation of another custom `Function`), higher order derivatives will work. In this case, the tensors saved with `save_for_backward` can also be used in the backward and have gradients flowing back but tensors saved in the `ctx` won’t have gradients flowing back for them. If you need gradients to flow back for a Tensor saved in the `ctx`, you should make it an output of the custom `Function` and save it with `save_for_backward`.
You probably want to check if the backward method you implemented actually computes the derivatives of your function. It is possible by comparing with numerical approximations using small finite differences:
```
from torch.autograd import gradcheck
# gradcheck takes a tuple of tensors as input, check if your gradient
# evaluated with these tensors are close enough to numerical
# approximations and returns True if they all verify this condition.
input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))
test = gradcheck(linear, input, eps=1e-6, atol=1e-4)
print(test)

```
Copy to clipboard
See Numerical gradient checking for more details on finite-difference gradient comparisons. If your function is used in higher order derivatives (differentiating the backward pass) you can use the `gradgradcheck` function from the same package to check higher order derivatives.
### Combined or separate `forward()` and `setup_context()`
There are two main ways to define `Function`. Either:
  * define a `forward()` that combines the forward compute logic with `setup_context()`
  * (as of PyTorch 2.0) define a separate `forward()` and `setup_context()`


We recommend the second option (separate `forward()` and `setup_context()`) because that is closer to how PyTorch native operations are implemented and it composes with `torch.func` transforms. However, we plan to support both approaches going forward; combining `forward()` with `setup_context()`: leads to more flexibility since you are able to save intermediates without returning them as output.
Please see the previous section for how to define `Function` with separate `forward()` and `setup_context()`.
Here is an example of how to define a `Function` with combined `forward()` and `setup_context()`:
```
class LinearFunction(Function):
  @staticmethod
  # ctx is the first argument to forward
  def forward(ctx, input, weight, bias=None):
    # The forward pass can use ctx.
    ctx.save_for_backward(input, weight, bias)
    output = input.mm(weight.t())
    if bias is not None:
      output += bias.unsqueeze(0).expand_as(output)
    return output
  @staticmethod
  def backward(ctx, grad_output):
    input, weight, bias = ctx.saved_tensors
    grad_input = grad_weight = grad_bias = None
    if ctx.needs_input_grad[0]:
      grad_input = grad_output.mm(weight)
    if ctx.needs_input_grad[1]:
      grad_weight = grad_output.t().mm(input)
    if bias is not None and ctx.needs_input_grad[2]:
      grad_bias = grad_output.sum(0)
    return grad_input, grad_weight, grad_bias

```
Copy to clipboard
### Forward mode AD
Overriding the forward mode AD formula has a very similar API with some different subtleties. You can implement the `jvp()` function.
It will be given as many `Tensor` arguments as there were inputs, with each of them representing gradient w.r.t. that input. It should return as many tensors as there were outputs, with each of them containing the gradient w.r.t. its corresponding output. The `jvp()` will be called just after the `forward()` method, before the `apply()` returns.
`jvp()` has a few subtle differences with the `backward()` function:
  * You can use the ctx to pass any data from the `forward()` to the `jvp()` function. If that state will not be needed for the `backward()`, you can explicitly free it by doing `del ctx.foo` at the end of the `jvp()` function.
  * The implementation of `jvp()` must be backward differentiable or explicitly check that none of the given forward mode gradient has `requires_grad` set.
  * The `jvp()` function must match the view/inplace behavior of `forward()`. For example, if the `i` th input is modified inplace, then the `i` th gradient must be updated inplace. Similarly, if the `j` th output is a view of the `k` th input. Then the returned `j` th output gradient must be a view of the given `k` th input gradient.
  * Because the user cannot specify which gradient needs to be computed, the `jvp()` function should always compute gradients for all the outputs.
  * The forward mode gradients do respect the flag set by `set_materialize_grads()` and you can get None input gradients when this is disabled.


### `torch.func` transforms and/or `torch.vmap()`
Please see Extending torch.func with autograd.Function for details.
## Extending `torch.nn`
`nn` exports two kinds of interfaces - modules and their functional versions. You can extend it in both ways, but we recommend using modules for all kinds of layers, that hold any parameters or buffers, and recommend using a functional form parameter-less operations like activation functions, pooling, etc.
Adding a functional version of an operation is already fully covered in the section above.
### Adding a `Module`
Since `nn` heavily utilizes `autograd`, adding a new `Module` requires implementing a `Function` that performs the operation and can compute the gradient. From now on let’s assume that we want to implement a `Linear` module and we have the function implemented as in the listing above. There’s very little code required to add this. Now, there are two functions that need to be implemented:
  * `__init__` (_optional_) - takes in arguments such as kernel sizes, numbers of features, etc. and initializes parameters and buffers.
  * `forward()` - instantiates a `Function` and uses it to perform the operation. It’s very similar to a functional wrapper shown above.


This is how a `Linear` module can be implemented:
```
class Linear(nn.Module):
  def __init__(self, input_features, output_features, bias=True):
    super().__init__()
    self.input_features = input_features
    self.output_features = output_features
    # nn.Parameter is a special kind of Tensor, that will get
    # automatically registered as Module's parameter once it's assigned
    # as an attribute. Parameters and buffers need to be registered, or
    # they won't appear in .parameters() (doesn't apply to buffers), and
    # won't be converted when e.g. .cuda() is called. You can use
    # .register_buffer() to register buffers.
    # nn.Parameters require gradients by default.
    self.weight = nn.Parameter(torch.empty(output_features, input_features))
    if bias:
      self.bias = nn.Parameter(torch.empty(output_features))
    else:
      # You should always register all possible parameters, but the
      # optional ones can be None if you want.
      self.register_parameter('bias', None)
    # Not a very smart way to initialize weights
    nn.init.uniform_(self.weight, -0.1, 0.1)
    if self.bias is not None:
      nn.init.uniform_(self.bias, -0.1, 0.1)
  def forward(self, input):
    # See the autograd section for explanation of what happens here.
    return LinearFunction.apply(input, self.weight, self.bias)
  def extra_repr(self):
    # (Optional)Set the extra information about this module. You can test
    # it by printing an object of this class.
    return 'input_features={}, output_features={}, bias={}'.format(
      self.input_features, self.output_features, self.bias is not None
    )

```
Copy to clipboard
## Extending `torch` Python API
You can create custom types that emulate `Tensor` by defining a custom class with methods that match `Tensor`. But what if you want to be able to pass these types to functions like `torch.add()` in the top-level `torch` namespace that accept `Tensor` operands?
If your custom Python type defines a method named `__torch_function__`, PyTorch will invoke your `__torch_function__` implementation when an instance of your custom class is passed to a function in the `torch` namespace. This makes it possible to define custom implementations for any of the functions in the `torch` namespace which your `__torch_function__` implementation can call, allowing your users to make use of your custom type with existing PyTorch workflows that they have already written for `Tensor`. This works with “duck” types that are unrelated to `Tensor` as well as user-defined subclasses of `Tensor`.
### Extending `torch` with a `Tensor`-like type
Note
This functionality is inspired by the NumPy `__array_function__` protocol. See the NumPy documentation and NEP-0018 for more details.
To make this concrete, let’s begin with a simple example that illustrates the API dispatch mechanism. We’ll create a custom type that represents a 2D scalar tensor, parametrized by the order `N` and value along the diagonal entries, `value`:
```
class ScalarTensor(object):
  def __init__(self, N, value):
    self._N = N
    self._value = value
  def __repr__(self):
    return "ScalarTensor(N={}, value={})".format(self._N, self._value)
  def tensor(self):
    return self._value * torch.eye(self._N)

```
Copy to clipboard
This first iteration of the design isn’t very useful. The main functionality of `ScalarTensor` is to provide a more compact string representation of a scalar tensor than in the base tensor class:
```
>>> d = ScalarTensor(5, 2)
>>> d
ScalarTensor(N=5, value=2)
>>> d.tensor()
tensor([[2., 0., 0., 0., 0.],
    [0., 2., 0., 0., 0.],
    [0., 0., 2., 0., 0.],
    [0., 0., 0., 2., 0.],
    [0., 0., 0., 0., 2.]])

```
Copy to clipboard
If we try to use this object with the `torch` API, we will run into issues:
```
>>> import torch
>>> torch.mean(d)
TypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor

```
Copy to clipboard
Adding a `__torch_function__` implementation to `ScalarTensor` makes it possible for the above operation to succeed. Let’s re-do our implementation, this time adding a `__torch_function__` implementation:
```
HANDLED_FUNCTIONS = {}
class ScalarTensor(object):
  def __init__(self, N, value):
    self._N = N
    self._value = value
  def __repr__(self):
    return "ScalarTensor(N={}, value={})".format(self._N, self._value)
  def tensor(self):
    return self._value * torch.eye(self._N)
  @classmethod
  def __torch_function__(cls, func, types, args=(), kwargs=None):
    if kwargs is None:
      kwargs = {}
    if func not in HANDLED_FUNCTIONS or not all(
      issubclass(t, (torch.Tensor, ScalarTensor))
      for t in types
    ):
      return NotImplemented
    return HANDLED_FUNCTIONS[func](*args, **kwargs)

```
Copy to clipboard
The `__torch_function__` method takes four arguments: `func`, a reference to the torch API function that is being overridden, `types`, the list of types of Tensor-likes that implement `__torch_function__`, `args`, the tuple of arguments passed to the function, and `kwargs`, the dict of keyword arguments passed to the function. It uses a global dispatch table named `HANDLED_FUNCTIONS` to store custom implementations. The keys of this dictionary are functions in the `torch` namespace and the values are implementations for `ScalarTensor`.
Note
Using a global dispatch table is not a mandated part of the `__torch_function__` API, it is just a useful design pattern for structuring your override implementations.
This class definition isn’t quite enough to make `torch.mean` do the right thing when we pass it a `ScalarTensor` – we also need to define an implementation for `torch.mean` for `ScalarTensor` operands and add the implementation to the `HANDLED_FUNCTIONS` dispatch table dictionary. One way of doing this is to define a decorator:
```
import functools
def implements(torch_function):
"""Register a torch function override for ScalarTensor"""
  def decorator(func):
    functools.update_wrapper(func, torch_function)
    HANDLED_FUNCTIONS[torch_function] = func
    return func
  return decorator

```
Copy to clipboard
which can be applied to the implementation of our override:
```
@implements(torch.mean)
def mean(input):
  return float(input._value) / input._N

```
Copy to clipboard
With this change we can now use `torch.mean` with `ScalarTensor`:
```
>>> d = ScalarTensor(5, 2)
>>> torch.mean(d)
0.4

```
Copy to clipboard
Of course `torch.mean` is an example of the simplest kind of function to override since it only takes one operand. We can use the same machinery to override a function that takes more than one operand, any one of which might be a tensor or tensor-like that defines `__torch_function__`, for example for `torch.add()`:
```
def ensure_tensor(data):
  if isinstance(data, ScalarTensor):
    return data.tensor()
  return torch.as_tensor(data)
@implements(torch.add)
def add(input, other):
  try:
    if input._N == other._N:
      return ScalarTensor(input._N, input._value + other._value)
    else:
      raise ValueError("Shape mismatch!")
  except AttributeError:
    return torch.add(ensure_tensor(input), ensure_tensor(other))

```
Copy to clipboard
This version has a fast path for when both operands are `ScalarTensor` instances and also a slower path which degrades to converting the data to tensors when either operand is not a `ScalarTensor`. That makes the override function correctly when either operand is a `ScalarTensor` or a regular `Tensor`:
```
>>> s = ScalarTensor(2, 2)
>>> torch.add(s, s)
ScalarTensor(N=2, value=4)
>>> t = torch.tensor([[1, 1,], [1, 1]])
>>> torch.add(s, t)
tensor([[3., 1.],
    [1., 3.]])

```
Copy to clipboard
Note that our implementation of `add` does not take `alpha` or `out` as keyword arguments like `torch.add()` does:
```
>>> torch.add(s, s, alpha=2)
TypeError: add() got an unexpected keyword argument 'alpha'

```
Copy to clipboard
For speed and flexibility the `__torch_function__` dispatch mechanism does not check that the signature of an override function matches the signature of the function being overridden in the `torch` API. For some applications ignoring optional arguments would be fine but to ensure full compatibility with `Tensor`, user implementations of torch API functions should take care to exactly emulate the API of the function that is being overridden.
Functions in the `torch` API that do not have explicit overrides will return `NotImplemented` from `__torch_function__`. If all operands with `__torch_function__` defined on them return `NotImplemented`, PyTorch will raise a `TypeError`. This means that most of the time operations that do not have explicit overrides for a type will raise a `TypeError` when an instance of such a type is passed:
```
>>> torch.mul(s, 3)
TypeError: no implementation found for 'torch.mul' on types that
implement __torch_function__: [ScalarTensor]

```
Copy to clipboard
In practice this means that if you would like to implement your overrides using a `__torch_function__` implementation along these lines, you will need to explicitly implement the full `torch` API or the entire subset of the API that you care about for your use case. This may be a tall order as the full `torch` API is quite extensive.
Another option is to not return `NotImplemented` for operations that are not handled but to instead pass a `Tensor` to the original `torch` function when no override is available. For example, if we change our implementation of `__torch_function__` for `ScalarTensor` to the one below:
```
@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
  if kwargs is None:
    kwargs = {}
  if func not in HANDLED_FUNCTIONS or not all(
      issubclass(t, (torch.Tensor, ScalarTensor))
      for t in types
    ):
    args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]
    return func(*args, **kwargs)
  return HANDLED_FUNCTIONS[func](*args, **kwargs)

```
Copy to clipboard
Then `torch.mul()` will work correctly, although the return type will always be a `Tensor` rather than a `ScalarTensor`, even if both operands are `ScalarTensor` instances:
```
>>> s = ScalarTensor(2, 2)
>>> torch.mul(s, s)
tensor([[4., 0.],
    [0., 4.]])

```
Copy to clipboard
Also see the `MetadataTensor` example below for another variation on this pattern but instead always returns a `MetadataTensor` to propagate metadata through operations in the `torch` API.
The `__torch_function__` protocol is designed for full coverage of the API, partial coverage may lead to undesirable results, in particular, certain functions raising a `TypeError`. This is especially true for subclasses, where all three of torch.add, torch.Tensor.__add__ and torch.Tensor.add must be covered, even if they return exactly the same result. Failing to do this may also lead to infinite recursion. If one requires the implementation of a function from `torch.Tensor` subclasses, they must use `super().__torch_function__` inside their implementation.
### Subclassing `torch.Tensor`
As of version 1.7.0, methods on `torch.Tensor` and functions in public `torch.*` namespaces applied on `torch.Tensor` subclasses will return subclass instances instead of `torch.Tensor` instances:
```
>>> class SubTensor(torch.Tensor):
...   pass
>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__
'SubTensor'
>>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__
'SubTensor'

```
Copy to clipboard
If multiple subclasses exist, the lowest one in the hierarchy will be chosen by default. If there is no unique way to determine such a case, then a `TypeError` is raised:
```
>>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__
'SubTensor2'
>>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__
'SubTensor2'
>>> torch.add(SubTensor([0]), OtherSubTensor([1]))
Traceback (most recent call last):
 File "<stdin>", line 1, in <module>
TypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]

```
Copy to clipboard
If one wishes to have a global override for all tensor methods, one can use `__torch_function__`. Here is an example that logs all function/method calls:
```
class LoggingTensor(torch.Tensor):
  @classmethod
  def __torch_function__(cls, func, types, args=(), kwargs=None):
    # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion
    if func is not torch.Tensor.__repr__:
      logging.info(f"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}")
    if kwargs is None:
      kwargs = {}
    return super().__torch_function__(func, types, args, kwargs)

```
Copy to clipboard
However, if one instead wishes to override a method on the Tensor subclass, there one can do so either by directly overriding the method (by defining it for a subclass), or by using `__torch_function__` and matching with `func`.
One should be careful within `__torch_function__` for subclasses to always call `super().__torch_function__(func, ...)` instead of `func` directly, as was the case before version 1.7.0. Failing to do this may cause `func` to recurse back into `__torch_function__` and therefore cause infinite recursion.
### Extending `torch` with a `Tensor` wrapper type
Another useful case is a type that wraps a `Tensor`, either as an attribute or via subclassing. Below we implement a special case of this sort of type, a `MetadataTensor` that attaches a dictionary of metadata to a `Tensor` that is propagated through `torch` operations. Since this is a generic sort of wrapping for the full `torch` API, we do not need to individually implement each override so we can make the `__torch_function__` implementation more permissive about what operations are allowed:
```
class MetadataTensor(object):
  def __init__(self, data, metadata=None, **kwargs):
    self._t = torch.as_tensor(data, **kwargs)
    self._metadata = metadata
  def __repr__(self):
    return "Metadata:\n{}\n\ndata:\n{}".format(self._metadata, self._t)
  @classmethod
  def __torch_function__(cls, func, types, args=(), kwargs=None):
    if kwargs is None:
      kwargs = {}
    metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))
    args = [getattr(a, '_t', a) for a in args]
    assert len(metadatas) > 0
    ret = func(*args, **kwargs)
    return MetadataTensor(ret, metadata=metadatas[0])

```
Copy to clipboard
This simple implementation won’t necessarily work with every function in the `torch` API but it is good enough to capture most common operations:
```
>>> metadata = {'owner': 'Ministry of Silly Walks'}
>>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata)
>>> t = torch.tensor([[1, 2], [1, 2]])
>>> torch.add(t, m)
Metadata:
{'owner': 'Ministry of Silly Walks'}
data:
tensor([[2, 4],
    [4, 6]])
>>> torch.mul(t, m)
Metadata:
{'owner': 'Ministry of Silly Walks'}
data:
tensor([[1, 4],
    [3, 8]])

```
Copy to clipboard
### Operations on multiple types that define `__torch_function__`
It is possible to use the torch API with multiple distinct types that each have a `__torch_function__` implementation, but special care must be taken. In such a case the rules are:
  * The dispatch operation gathers all distinct implementations of `__torch_function__` for each operand and calls them in order: subclasses before superclasses, and otherwise left to right in the operator expression.
  * If any value other than `NotImplemented` is returned, that value is returned as the result. Implementations can register that they do not implement an operation by returning `NotImplemented`.
  * If all of the `__torch_function__` implementations return `NotImplemented`, PyTorch raises a `TypeError`.


### Testing Coverage of Overrides for the PyTorch API
One troublesome aspect of implementing `__torch_function__` is that if some operations do and others do not have overrides, users will at best see an inconsistent experience, or at worst will see errors raised at runtime when they use a function that does not have an override. To ease this process, PyTorch provides a developer-facing API for ensuring full support for `__torch_function__` overrides. This API is private and may be subject to changes without warning in the future.
First, to get a listing of all overridable functions, use `torch.overrides._get_overridable_functions`. This returns a dictionary whose keys are namespaces in the `PyTorch` Python API and whose values are a list of functions in that namespace that can be overridden. For example, let’s print the names of the first 5 functions in `torch.nn.functional` that can be overridden:
```
>>> from torch.overrides import get_overridable_functions
>>> func_dict = get_overridable_functions()
>>> nn_funcs = func_dict[torch.nn.functional]
>>> print([f.__name__ for f in nn_funcs[:5])
['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d',
 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices']

```
Copy to clipboard
This listing of functions makes it possible to iterate over all overridable functions, however in practice this is not enough to write tests for all of these functions without laboriously and manually copying the signature of each function for each test. To ease this process, the `torch.overrides._get_testing_overrides` function returns a dictionary mapping overridable functions in the `PyTorch` API to dummy lambda functions that have the same signature as the original function but unconditionally return -1. These functions are most useful to use with `inspect` to analyze the function signature of the original `PyTorch` function:
```
>>> import inspect
>>> from torch.overrides import get_testing_overrides
>>> override_dict = get_testing_overrides()
>>> dummy_add = override_dict[torch.add]
>>> inspect.signature(dummy_add)
<Signature (input, other, out=None)>

```
Copy to clipboard
Finally, `torch.overrides.get_ignored_functions` returns a tuple of functions that explicitly cannot be overridden by `__torch_function__`. This list can be useful to confirm that a function that isn’t present in the dictionary returned by `get_overridable_functions` cannot be overridden.
## Extending `torch` native API
While `__torch_function__` allows one to effectively extend PyTorch’s pure Python components’ behavior, it does not allow one to extend the parts of PyTorch implemented in C++. To that end, a `Tensor` subclass can also define `__torch_dispatch__` which will be able to override the behavior at the C++ level.
To effectively use this feature, it is important to know how the native part of PyTorch is implemented. The most important component there is what we call the “dispatcher” (the best description can be found in this blog post even though it is slightly outdated). As hinted by its name, it is responsible for calling the right backend function for a specific call of a function. For example, when calling `torch.add(a, b)`, the dispatcher will inspect both arguments, figure out which “feature” (autograd, autocast, functionalization, etc) and which “backend” (CPU, CUDA, MPS, etc) should be used for this specific call and finally call all the right kernels. A very common thing done by a kernel is to “redispatch”. For example, when running your neural network on GPU with autocast, the first call will be the autocast kernel that will handle any potential autocast logic and redispatch down. The next feature in line will be autograd that will properly create the autograd graph and then redispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA kernel and return the final result. On the way out, autograd will attach the graph to the output and, finally, autocast will have a chance to do any update it needs on exit.
One configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found in `DispatchKey.h` inside the `DispatchKey` enum. For the purpose of extending torch, the important subset of the ordering for this discussion is:
vmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python -> Backends
The most important key for the purpose of this discussion is `Python` as every Tensor subclass with the `__torch_dispatch__` method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the provided `func` again will perform a “redispatch”.
Some important implications of this implementation are:
  * This code runs “below all features”. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc).
  * If any high level feature implements a given function without redispatching, it will never reach the `Python` key and so the `__torch_dispatch__` callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead.
  * When calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None).
  * Our native functions are lazily populated as `torch.ops.{namespace}.{func_name}.{overload_name}` as callable Python objects to enable easily interacting with them from Python. The `func` object given to `__torch_dispatch__` is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code.


In a similar way where `__torch_function__` is able to interpose on all of torch’s Python API and Tensor methods, `__torch_dispatch__` is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here: `torch.add(a, 2)` and `a + 2` will lead to exactly the same aten call. Most of these functions are defined in `native_functions.yaml` which specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen. Some more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions.
It is also possible to add new native functions using `torch.library`. This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions.
You can find many examples of `__torch_dispatch__`-based subclasses in the subclass zoo repo.
### `__torch_dispatch__` calling convention
```
@classmethod
def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
  pass

```
Copy to clipboard
When a user calls an operator with inputs that have `__torch_dispatch__`, that call may be forwarded to the `__torch_dispatch__`. args and kwargs get normalized before the call to `__torch_dispatch__`, that is:
  * the `kwargs` consist of keyword-only arguments in the operator’s schema. If a kwarg is equal to its default value (in the schema), it will not be passed.
  * the `args` consists of all other arguments, no matter how they were passed to the operator (positional vs keyword). If an arg is equal to its default value, and it is the right-most positional arg or all the args to the right of it are not passed, it will not be passed.


## Extending all `torch` API with Modes
Unfortunately, there are functions that do not take Tensor inputs. This means that the subclass approach described above cannot be used to override the behavior of all of PyTorch’s functions. Also, if the use case requires to intercept every function call, changing every Tensor to be a subclass can be overly intrusive.
To address this use case, we introduced the concept of “Mode”. These exist for `__torch_function__` and `__torch_dispatch__` overrides, are created by subclassing respectively `torch.overrides.TorchFunctionMode` and `torch.utils._python_dispatch.TorchDispatchMode`, and are used as a context manager.
To simplify the description of how it interacts with subclasses and other modes, whenever the context manager for a mode is entered, every function behaves as if there was an extra Tensor argument at the beginning of the argument list with the mode as a subclass. This means in particular that all modes handlers will be called before any subclass handler and that modes corresponding to the inner context manager will always run first.
It is also important to note that within a given mode handler, this specific mode is disabled and can be re-enabled manually by doing `with self:`.
Here is an example that shows logging modes of each type:
```
import torch
from torch.overrides import TorchFunctionMode, resolve_name
from torch.utils._python_dispatch import TorchDispatchMode
class FunctionLog(TorchFunctionMode):
  def __torch_function__(self, func, types, args, kwargs=None):
    print(f"Function Log: {resolve_name(func)}(*{args}, **{kwargs})")
    return func(*args, **(kwargs or {}))
class DispatchLog(TorchDispatchMode):
  def __torch_dispatch__(self, func, types, args, kwargs=None):
    print(f"Dispatch Log: {func}(*{args}, **{kwargs})")
    return func(*args, **(kwargs or {}))
def f():
  a = torch.rand(10, requires_grad=True)
  b = a * 2
  b.sum().backward()
print("TorchFunctionMode logging:")
with FunctionLog():
  f()
print("TorchDispatchMode logging:")
with DispatchLog():
  f()

```
Copy to clipboard
Which prints the following, with extra comments:
```
TorchFunctionMode logging:
Function Log: torch.rand(*(10,), **{'requires_grad': True})
Function Log: torch.Tensor.mul(*(tensor([0.7164, 0.9897, 0.1745, 0.9336, 0.4287, 0.7989, 0.2169, 0.7474, 0.5624,
    0.5970], requires_grad=True), 2), **None)
Function Log: torch.Tensor.sum(*(tensor([1.4328, 1.9794, 0.3490, 1.8671, 0.8573, 1.5977, 0.4338, 1.4948, 1.1249,
    1.1939], grad_fn=<MulBackward0>),), **None)
# Note that at the python level, we only see the call to backward but not what happens in the autograd engine.
Function Log: torch.Tensor.backward(*(tensor(12.3307, grad_fn=<SumBackward0>),), **{'gradient': None, 'retain_graph': None, 'create_graph': False, 'inputs': None})
TorchDispatchMode logging:
# Here the requires_grad flag from autograd is removed while default arguments were populated.
Dispatch Log: aten.rand.default(*([10],), **{'device': device(type='cpu'), 'pin_memory': False})
Dispatch Log: aten.mul.Tensor(*(tensor([0.2151, 0.6018, 0.8415, 0.9060, 0.2974, 0.7708, 0.6668, 0.0352, 0.7948,
    0.6023], requires_grad=True), 2), **{})
Dispatch Log: aten.sum.default(*(tensor([0.4303, 1.2036, 1.6831, 1.8120, 0.5949, 1.5416, 1.3335, 0.0705, 1.5897,
    1.2046], grad_fn=<MulBackward0>),), **{})
# Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient.
Dispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format})
# This is the backward of the sum
Dispatch Log: aten.expand.default(*(tensor(1.), [10]), **{})
Dispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{})
Dispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})
Dispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Extending PyTorch
    * Adding new operators
    * Extending `torch.autograd`
      * When to use
      * When not to use
      * How to use
      * Example
      * Combined or separate `forward()` and `setup_context()`
      * Forward mode AD
      * `torch.func` transforms and/or `torch.vmap()`
    * Extending `torch.nn`
      * Adding a `Module`
    * Extending `torch` Python API
      * Extending `torch` with a `Tensor`-like type
      * Subclassing `torch.Tensor`
      * Extending `torch` with a `Tensor` wrapper type
      * Operations on multiple types that define `__torch_function__`
      * Testing Coverage of Overrides for the PyTorch API
    * Extending `torch` native API
      * `__torch_dispatch__` calling convention
    * Extending all `torch` API with Modes


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Distributed Data Parallel
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Distributed Data Parallel
Warning
The implementation of `torch.nn.parallel.DistributedDataParallel` evolves over time. This design note is written based on the state as of v1.4.
`torch.nn.parallel.DistributedDataParallel` (DDP) transparently performs distributed data parallel training. This page describes how it works and reveals implementation details.
## Example
Let us start with a simple `torch.nn.parallel.DistributedDataParallel` example. This example uses a `torch.nn.Linear` as the local model, wraps it with DDP, and then runs one forward pass, one backward pass, and an optimizer step on the DDP model. After that, parameters on the local model will be updated, and all models on different processes should be exactly the same.
```
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
import os
from torch.nn.parallel import DistributedDataParallel as DDP

def example(rank, world_size):
  # create default process group
  dist.init_process_group("gloo", rank=rank, world_size=world_size)
  # create local model
  model = nn.Linear(10, 10).to(rank)
  # construct DDP model
  ddp_model = DDP(model, device_ids=[rank])
  # define loss function and optimizer
  loss_fn = nn.MSELoss()
  optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
  # forward pass
  outputs = ddp_model(torch.randn(20, 10).to(rank))
  labels = torch.randn(20, 10).to(rank)
  # backward pass
  loss_fn(outputs, labels).backward()
  # update parameters
  optimizer.step()
def main():
  world_size = 2
  mp.spawn(example,
    args=(world_size,),
    nprocs=world_size,
    join=True)
if __name__=="__main__":
  # Environment variables which need to be
  # set when using c10d's default "env"
  # initialization mode.
  os.environ["MASTER_ADDR"] = "localhost"
  os.environ["MASTER_PORT"] = "29500"
  main()

```
Copy to clipboard
DDP works with TorchDynamo. When used with TorchDynamo, apply the DDP model wrapper before compiling the model, such that torchdynamo can apply `DDPOptimizer` (graph-break optimizations) based on DDP bucket sizes. (See TorchDynamo DDPOptimizer for more information.)
```
ddp_model = DDP(model, device_ids=[rank])
ddp_model = torch.compile(ddp_model)

```
Copy to clipboard
## Internal Design
This section reveals how it works under the hood of `torch.nn.parallel.DistributedDataParallel` by diving into details of every step in one iteration.
  * **Prerequisite** : DDP relies on c10d `ProcessGroup` for communications. Hence, applications must create `ProcessGroup` instances before constructing DDP.
  * **Construction** : The DDP constructor takes a reference to the local module, and broadcasts `state_dict()` from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. Then, each DDP process creates a local `Reducer`, which later will take care of the gradients synchronization during the backward pass. To improve communication efficiency, the `Reducer` organizes parameter gradients into buckets, and reduces one bucket at a time. Bucket size can be configured by setting the bucket_cap_mb argument in DDP constructor. The mapping from parameter gradients to buckets is determined at the construction time, based on the bucket size limit and parameter sizes. Model parameters are allocated into buckets in (roughly) the reverse order of `Model.parameters()` from the given model. The reason for using the reverse order is because DDP expects gradients to become ready during the backward pass in approximately that order. The figure below shows an example. Note that, the `grad0` and `grad1` are in `bucket1`, and the other two gradients are in `bucket0`. Of course, this assumption might not always be true, and when that happens it could hurt DDP backward speed as the `Reducer` cannot kick off the communication at the earliest possible time. Besides bucketing, the `Reducer` also registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready.
  * **Forward Pass** : The DDP takes the input and passes it to the local model, and then analyzes the output from the local model if `find_unused_parameters` is set to `True`. This mode allows running backward on a subgraph of the model, and DDP finds out which parameters are involved in the backward pass by traversing the autograd graph from the model output and marking all unused parameters as ready for reduction. During the backward pass, the `Reducer` would only wait for unready parameters, but it would still reduce all buckets. Marking a parameter gradient as ready does not help DDP skip buckets as for now, but it will prevent DDP from waiting for absent gradients forever during the backward pass. Note that traversing the autograd graph introduces extra overheads, so applications should only set `find_unused_parameters` to `True` when necessary.
  * **Backward Pass** : The `backward()` function is directly invoked on the loss `Tensor`, which is out of DDP’s control, and DDP uses autograd hooks registered at construction time to trigger gradients synchronizations. When one gradient becomes ready, its corresponding DDP hook on that grad accumulator will fire, and DDP will then mark that parameter gradient as ready for reduction. When gradients in one bucket are all ready, the `Reducer` kicks off an asynchronous `allreduce` on that bucket to calculate mean of gradients across all processes. When all buckets are ready, the `Reducer` will block waiting for all `allreduce` operations to finish. When this is done, averaged gradients are written to the `param.grad` field of all parameters. So after the backward pass, the grad field on the same corresponding parameter across different DDP processes should be the same.
  * **Optimizer Step** : From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.

![ddp_grad_sync.png](https://user-images.githubusercontent.com/16999635/72401724-d296d880-371a-11ea-90ab-737f86543df9.png)
Note
DDP requires `Reducer` instances on all processes to invoke `allreduce` in exactly the same order, which is done by always running `allreduce` in the bucket index order instead of actual bucket ready order. Mismatched `allreduce` order across processes can lead to wrong results or DDP backward hang.
## Implementation
Below are pointers to the DDP implementation components. The stacked graph shows the structure of the code.
### ProcessGroup
  * ProcessGroup.hpp: contains the abstract API of all process group implementations. The `c10d` library provides 3 implementations out of the box, namely, ProcessGroupGloo, ProcessGroupNCCL, and ProcessGroupMPI. `DistributedDataParallel` uses `ProcessGroup::broadcast()` to send model states from the process with rank 0 to others during initialization and `ProcessGroup::allreduce()` to sum gradients.
  * Store.hpp: assists the rendezvous service for process group instances to find each other.


### DistributedDataParallel
  * distributed.py: is the Python entry point for DDP. It implements the initialization steps and the `forward` function for the `nn.parallel.DistributedDataParallel` module which call into C++ libraries. Its `_sync_param` function performs intra-process parameter synchronization when one DDP process works on multiple devices, and it also broadcasts model buffers from the process with rank 0 to all other processes. The inter-process parameter synchronization happens in `Reducer.cpp`.
  * comm.h: implements the coalesced broadcast helper function which is invoked to broadcast model states during initialization and synchronize model buffers before the forward pass.
  * reducer.h: provides the core implementation for gradient synchronization in the backward pass. It has three entry point functions:
    * `Reducer`: The constructor is called in `distributed.py` which registers `Reducer::autograd_hook()` to gradient accumulators.
    * `autograd_hook()` function will be invoked by the autograd engine when a gradient becomes ready.
    * `prepare_for_backward()` is called at the end of DDP forward pass in `distributed.py`. It traverses the autograd graph to find unused parameters when `find_unused_parameters` is set to `True` in DDP constructor.

![ddp_code.png](https://user-images.githubusercontent.com/16999635/72313120-4e7c1c80-3658-11ea-9c6d-44336b2daeac.png)
### TorchDynamo DDPOptimizer
DDP’s performance advantage comes from overlapping allreduce collectives with computations during backwards. AotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph, because allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes.
TorchDynamo’s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP’s allreduce buckets during backwards. Note: the goal is to break the graph during backwards, and the simplest implementation is to break the forward graphs and then call AotAutograd and compilation on each section. This allows DDP’s allreduce hooks to fire in-between sections of backwards, and schedule communications to overlap with compute.
See this blog post for a more in-depth explanation and experimental results, or read the docs and code at torch/_dynamo/optimizations/distributed.py
To Debug DDPOptimizer, set TORCH_LOGS=’ddp_graphs’ for full graph dumps. For logs without graphs, add any of ‘dynamo’, ‘distributed’, or ‘dist_ddp’ to TORCH_LOGS (for basic info about bucket boundaries). To disable DDPOptimizer, set torch._dynamo.config.optimize_ddp=False. DDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Distributed Data Parallel
    * Example
    * Internal Design
    * Implementation
      * ProcessGroup
      * DistributedDataParallel
      * TorchDynamo DDPOptimizer


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Getting Started on Intel GPU
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Getting Started on Intel GPU
## Hardware Prerequisite
For Intel Data Center GPU
Device | Red Hat* Enterprise Linux* 9.2 | SUSE Linux Enterprise Server* 15 SP5 | Ubuntu* Server 22.04 (>= 5.15 LTS kernel)  
---|---|---|---  
Intel® Data Center GPU Max Series (CodeName: Ponte Vecchio) | yes | yes | yes  
For Intel Client GPU
Supported OS | Validated Hardware  
---|---  
Windows 10/11 & Ubuntu 24.10 |  Intel® Arc A-Series Graphics (CodeName: Alchemist) Intel® Arc B-Series Graphics (CodeName: Battlemage) Intel® Core™ Ultra Processors with Intel® Arc™ Graphics (CodeName: Meteor Lake) Intel® Core™ Ultra 200V Series with Intel® Arc™ Graphics (CodeName: Lunar Lake) Intel® Core™ Ultra Series 2 Processors with Intel® Arc™ Graphics (CodeName: Arrow Lake)  
Ubuntu 24.04 & WSL2 (Ubuntu 24.04) |  Intel® Arc A-Series Graphics (CodeName: Alchemist) Intel® Core™ Ultra Processors with Intel® Arc™ Graphics (CodeName: Meteor Lake) Intel® Core™ Ultra 200V Series with Intel® Arc™ Graphics (CodeName: Lunar Lake) Intel® Core™ Ultra Series 2 Processors with Intel® Arc™ Graphics (CodeName: Arrow Lake)  
Intel GPUs support (Prototype) is ready from PyTorch* 2.5 for Intel® Client GPUs and Intel® Data Center GPU Max Series on both Linux and Windows, which brings Intel GPUs and the SYCL* software stack into the official PyTorch stack with consistent user experience to embrace more AI application scenarios.
## Software Prerequisite
To use PyTorch on Intel GPUs, you need to install the Intel GPUs driver first. For installation guide, visit Intel GPUs Driver Installation.
Please skip the Intel® Deep Learning Essentials installation section if you install from binaries. For building from source, please refer to PyTorch Installation Prerequisites for Intel GPUs for both Intel GPU Driver and Intel® Deep Learning Essentials Installation.
## Installation
### Binaries
Now that we have Intel GPU Driver installed, use the following commands to install `pytorch`, `torchvision`, `torchaudio` on Linux.
For release wheels
```
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu

```
Copy to clipboard
For nightly wheels
```
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu

```
Copy to clipboard
### From Source
Now that we have Intel GPU Driver and Intel® Deep Learning Essentials installed. Follow guides to build `pytorch`, `torchvision`, `torchaudio` from source.
Build from source for `torch` refer to PyTorch Installation Build from source.
Build from source for `torchvision` refer to Torchvision Installation Build from source.
Build from source for `torchaudio` refert to Torchaudio Installation Build from source.
## Check availability for Intel GPU
To check if your Intel GPU is available, you would typically use the following code:
```
import torch
torch.xpu.is_available() # torch.xpu is the API for Intel GPU support

```
Copy to clipboard
If the output is `False`, double check driver installation for Intel GPUs.
## Minimum Code Change
If you are migrating code from `cuda`, you would change references from `cuda` to `xpu`. For example:
```
# CUDA CODE
tensor = torch.tensor([1.0, 2.0]).to("cuda")
# CODE for Intel GPU
tensor = torch.tensor([1.0, 2.0]).to("xpu")

```
Copy to clipboard
The following points outline the support and limitations for PyTorch with Intel GPU:
  1. Both training and inference workflows are supported.
  2. Both eager mode and `torch.compile` is supported. The feature `torch.compile` is also supported on Windows from PyTorch* 2.7 with Intel GPU, refer to How to Use Inductor on Windows with CPU/XPU.
  3. Data types such as FP32, BF16, FP16, and Automatic Mixed Precision (AMP) are all supported.


## Examples
This section contains usage examples for both inference and training workflows.
### Inference Examples
Here is a few inference workflow examples.
#### Inference with FP32
```
import torch
import torchvision.models as models
model = models.resnet50(weights="ResNet50_Weights.DEFAULT")
model.eval()
data = torch.rand(1, 3, 224, 224)
model = model.to("xpu")
data = data.to("xpu")
with torch.no_grad():
  model(data)
print("Execution finished")

```
Copy to clipboard
#### Inference with AMP
```
import torch
import torchvision.models as models
model = models.resnet50(weights="ResNet50_Weights.DEFAULT")
model.eval()
data = torch.rand(1, 3, 224, 224)
model = model.to("xpu")
data = data.to("xpu")
with torch.no_grad():
  d = torch.rand(1, 3, 224, 224)
  d = d.to("xpu")
  # set dtype=torch.bfloat16 for BF16
  with torch.autocast(device_type="xpu", dtype=torch.float16, enabled=True):
    model(data)
print("Execution finished")

```
Copy to clipboard
#### Inference with `torch.compile`
```
import torch
import torchvision.models as models
import time
model = models.resnet50(weights="ResNet50_Weights.DEFAULT")
model.eval()
data = torch.rand(1, 3, 224, 224)
ITERS = 10
model = model.to("xpu")
data = data.to("xpu")
for i in range(ITERS):
  start = time.time()
  with torch.no_grad():
    model(data)
    torch.xpu.synchronize()
  end = time.time()
  print(f"Inference time before torch.compile for iteration {i}: {(end-start)*1000} ms")
model = torch.compile(model)
for i in range(ITERS):
  start = time.time()
  with torch.no_grad():
    model(data)
    torch.xpu.synchronize()
  end = time.time()
  print(f"Inference time after torch.compile for iteration {i}: {(end-start)*1000} ms")
print("Execution finished")

```
Copy to clipboard
### Training Examples
Here is a few training workflow examples.
#### Train with FP32
```
import torch
import torchvision
LR = 0.001
DOWNLOAD = True
DATA = "datasets/cifar10/"
transform = torchvision.transforms.Compose(
  [
    torchvision.transforms.Resize((224, 224)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
  ]
)
train_dataset = torchvision.datasets.CIFAR10(
  root=DATA,
  train=True,
  transform=transform,
  download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)
train_len = len(train_loader)
model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
model.train()
model = model.to("xpu")
criterion = criterion.to("xpu")
print(f"Initiating training")
for batch_idx, (data, target) in enumerate(train_loader):
  data = data.to("xpu")
  target = target.to("xpu")
  optimizer.zero_grad()
  output = model(data)
  loss = criterion(output, target)
  loss.backward()
  optimizer.step()
  if (batch_idx + 1) % 10 == 0:
     iteration_loss = loss.item()
     print(f"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}")
torch.save(
  {
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
  },
  "checkpoint.pth",
)
print("Execution finished")

```
Copy to clipboard
#### Train with AMP
Note: Training with `GradScaler` requires hardware support for `FP64`. `FP64` is not natively supported by the Intel® Arc™ A-Series Graphics. If you run your workloads on Intel® Arc™ A-Series Graphics, please disable `GradScaler`.
```
import torch
import torchvision
LR = 0.001
DOWNLOAD = True
DATA = "datasets/cifar10/"
use_amp=True
transform = torchvision.transforms.Compose(
  [
    torchvision.transforms.Resize((224, 224)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
  ]
)
train_dataset = torchvision.datasets.CIFAR10(
  root=DATA,
  train=True,
  transform=transform,
  download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)
train_len = len(train_loader)
model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
scaler = torch.amp.GradScaler(device="xpu", enabled=use_amp)
model.train()
model = model.to("xpu")
criterion = criterion.to("xpu")
print(f"Initiating training")
for batch_idx, (data, target) in enumerate(train_loader):
  data = data.to("xpu")
  target = target.to("xpu")
  # set dtype=torch.bfloat16 for BF16
  with torch.autocast(device_type="xpu", dtype=torch.float16, enabled=use_amp):
    output = model(data)
    loss = criterion(output, target)
  scaler.scale(loss).backward()
  scaler.step(optimizer)
  scaler.update()
  optimizer.zero_grad()
  if (batch_idx + 1) % 10 == 0:
     iteration_loss = loss.item()
     print(f"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}")
torch.save(
  {
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
  },
  "checkpoint.pth",
)
print("Execution finished")

```
Copy to clipboard
#### Train with `torch.compile`
```
import torch
import torchvision
LR = 0.001
DOWNLOAD = True
DATA = "datasets/cifar10/"
transform = torchvision.transforms.Compose(
  [
    torchvision.transforms.Resize((224, 224)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
  ]
)
train_dataset = torchvision.datasets.CIFAR10(
  root=DATA,
  train=True,
  transform=transform,
  download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)
train_len = len(train_loader)
model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
model.train()
model = model.to("xpu")
criterion = criterion.to("xpu")
model = torch.compile(model)
print(f"Initiating training with torch compile")
for batch_idx, (data, target) in enumerate(train_loader):
  data = data.to("xpu")
  target = target.to("xpu")
  optimizer.zero_grad()
  output = model(data)
  loss = criterion(output, target)
  loss.backward()
  optimizer.step()
  if (batch_idx + 1) % 10 == 0:
     iteration_loss = loss.item()
     print(f"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}")
torch.save(
  {
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
  },
  "checkpoint.pth",
)
print("Execution finished")

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Getting Started on Intel GPU
    * Hardware Prerequisite
    * Software Prerequisite
    * Installation
      * Binaries
      * From Source
    * Check availability for Intel GPU
    * Minimum Code Change
    * Examples
      * Inference Examples
        * Inference with FP32
        * Inference with AMP
        * Inference with `torch.compile`
      * Training Examples
        * Train with FP32
        * Train with AMP
        * Train with `torch.compile`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Gradcheck mechanics
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Gradcheck mechanics
This note presents an overview of how the `gradcheck()` and `gradgradcheck()` functions work.
It will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives. This note also covers both the default behavior of gradcheck as well as the case where `fast_mode=True` argument is passed (referred to as fast gradcheck below).
  * Notations and background information
  * Default backward mode gradcheck behavior
    * Real-to-real functions
    * Complex-to-real functions
    * Functions with complex outputs
  * Fast backward mode gradcheck
    * Fast gradcheck for real-to-real functions
    * Fast gradcheck for complex-to-real functions
    * Fast gradcheck for functions with complex outputs
  * Gradgradcheck implementation


## Notations and background information
Throughout this note, we will use the following convention:
  1. xxx, yyy, aaa, bbb, vvv, uuu, ururur and uiuiui are real-valued vectors and zzz is a complex-valued vector that can be rewritten in terms of two real-valued vectors as z=a+ibz = a + i bz=a+ib.
  2. NNN and MMM are two integers that we will use for the dimension of the input and output space respectively.
  3. f:RN→RMf: \mathcal{R}^N \to \mathcal{R}^Mf:RN→RM is our basic real-to-real function such that y=f(x)y = f(x)y=f(x).
  4. g:CN→RMg: \mathcal{C}^N \to \mathcal{R}^Mg:CN→RM is our basic complex-to-real function such that y=g(z)y = g(z)y=g(z).


For the simple real-to-real case, we write as JfJ_fJf​ the Jacobian matrix associated with fff of size M×NM \times NM×N. This matrix contains all the partial derivatives such that the entry at position (i,j)(i, j)(i,j) contains ∂yi∂xj\frac{\partial y_i}{\partial x_j}∂xj​∂yi​​. Backward mode AD is then computing, for a given vector vvv of size MMM, the quantity vTJfv^T J_fvTJf​. Forward mode AD on the other hand is computing, for a given vector uuu of size NNN, the quantity JfuJ_f uJf​u.
For functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found at Autograd for Complex Numbers.
The constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus. In a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (called WWW below) and the Conjugate Wirtinger derivative (called CWCWCW below). Both WWW and CWCWCW need to be propagated because in general, despite their name, one is not the complex conjugate of the other.
To avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions. In practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).
Under this assumption, using WWW and CWCWCW definitions, we can show that W=CW∗W = CW^*W=CW∗ (we use ∗*∗ to denote complex conjugation here) and so only one of the two values actually need to be “backwarded through the graph” as the other one can easily be recovered. To simplify internal computations, PyTorch uses 2∗CW2 * CW2∗CW as the value it backwards and returns when the user asks for gradients. Similarly to the real case, when the output is actually in RM\mathcal{R}^MRM, backward mode AD does not compute 2∗CW2 * CW2∗CW but only vT(2∗CW)v^T (2 * CW)vT(2∗CW) for a given vector v∈RMv \in \mathcal{R}^Mv∈RM.
For forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is in R\mathcal{R}R. Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in R\mathcal{R}R and in this case, using WWW and CWCWCW definitions, we can show that W=CWW = CWW=CW for the intermediary functions. To make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes 2∗CW2 * CW2∗CW. Similarly to the real case, when the input is actually in RN\mathcal{R}^NRN, forward mode AD does not compute 2∗CW2 * CW2∗CW but only (2∗CW)u(2 * CW) u(2∗CW)u for a given vector u∈RNu \in \mathcal{R}^Nu∈RN.
## Default backward mode gradcheck behavior
### Real-to-real functions
To test a function f:RN→RM,x→yf: \mathcal{R}^N \to \mathcal{R}^M, x \to yf:RN→RM,x→y, we reconstruct the full Jacobian matrix JfJ_fJf​ of size M×NM \times NM×N in two ways: analytically and numerically. The analytical version uses our backward mode AD while the numerical version uses finite difference. The two reconstructed Jacobian matrices are then compared elementwise for equality.
#### Default real input numerical evaluation
If we consider the elementary case of a one-dimensional function (N=M=1N = M = 1N=M=1), then we can use the basic finite difference formula from the wikipedia article. We use the “central difference” for better numerical properties:
∂y∂x≈f(x+eps)−f(x−eps)2∗eps\frac{\partial y}{\partial x} \approx \frac{f(x + eps) - f(x - eps)}{2 * eps} ∂x∂y​≈2∗epsf(x+eps)−f(x−eps)​
This formula easily generalizes for multiple outputs (M>1M \gt 1M>1) by having ∂y∂x\frac{\partial y}{\partial x}∂x∂y​ be a column vector of size M×1M \times 1M×1 like f(x+eps)f(x + eps)f(x+eps). In that case, the above formula can be re-used as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely f(x+eps)f(x + eps)f(x+eps) and f(x−eps)f(x - eps)f(x−eps)).
It is more computationally expensive to handle the case with multiple inputs (N>1N \gt 1N>1). In this scenario, we loop over all the inputs one after the other and apply the epsepseps perturbation for each element of xxx one after the other. This allows us to reconstruct the JfJ_fJf​ matrix column by column.
#### Default real input analytical evaluation
For the analytical evaluation, we use the fact, as described above, that backward mode AD computes vTJfv^T J_fvTJf​. For functions with a single output, we simply use v=1v = 1v=1 to recover the full Jacobian matrix with a single backward pass.
For functions with more than one output, we resort to a for-loop which iterates over the outputs where each vvv is a one-hot vector corresponding to each output one after the other. This allows to reconstruct the JfJ_fJf​ matrix row by row.
### Complex-to-real functions
To test a function g:CN→RM,z→yg: \mathcal{C}^N \to \mathcal{R}^M, z \to yg:CN→RM,z→y with z=a+ibz = a + i bz=a+ib, we reconstruct the (complex-valued) matrix that contains 2∗CW2 * CW2∗CW.
#### Default complex input numerical evaluation
Consider the elementary case where N=M=1N = M = 1N=M=1 first. We know from (chapter 3 of) this research paper that:
CW:=∂y∂z∗=12∗(∂y∂a+i∂y∂b)CW := \frac{\partial y}{\partial z^*} = \frac{1}{2} * (\frac{\partial y}{\partial a} + i \frac{\partial y}{\partial b}) CW:=∂z∗∂y​=21​∗(∂a∂y​+i∂b∂y​)
Note that ∂y∂a\frac{\partial y}{\partial a}∂a∂y​ and ∂y∂b\frac{\partial y}{\partial b}∂b∂y​, in the above equation, are R→R\mathcal{R} \to \mathcal{R}R→R derivatives. To evaluate these numerically, we use the method described above for the real-to-real case. This allows us to compute the CWCWCW matrix and then multiply it by 222.
Note that the code, as of time of writing, computes this value in a slightly convoluted way:
```
# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105
# Notation changes in this code block:
# s here is y above
# x, y here are a, b above
ds_dx = compute_gradient(eps)
ds_dy = compute_gradient(eps * 1j)
# conjugate wirtinger derivative
conj_w_d = 0.5 * (ds_dx + ds_dy * 1j)
# wirtinger derivative
w_d = 0.5 * (ds_dx - ds_dy * 1j)
d[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()
# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.

```
Copy to clipboard
#### Default complex input analytical evaluation
Since backward mode AD computes exactly twice the CWCWCW derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.
### Functions with complex outputs
In this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued. This means that using autograd directly on this function is not well defined. To solve this, we will replace the test of the function h:PN→CMh: \mathcal{P}^N \to \mathcal{C}^Mh:PN→CM (where P\mathcal{P}P can be either R\mathcal{R}R or C\mathcal{C}C), with two functions: hrhrhr and hihihi such that:
hr(q):=real(f(q))hi(q):=imag(f(q))\begin{aligned} hr(q) &:= real(f(q)) \\\ hi(q) &:= imag(f(q)) \end{aligned} hr(q)hi(q)​:=real(f(q)):=imag(f(q))​
where q∈Pq \in \mathcal{P}q∈P. We then do a basic gradcheck for both hrhrhr and hihihi using either the real-to-real or complex-to-real case described above, depending on P\mathcal{P}P.
Note that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the realrealreal or imagimagimag functions manually by passing the grad_out\text{grad\\_out}grad_out arguments to the different functions. When grad_out=1\text{grad\\_out} = 1grad_out=1, then we are considering hrhrhr. When grad_out=1j\text{grad\\_out} = 1jgrad_out=1j, then we are considering hihihi.
## Fast backward mode gradcheck
While the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices. This section presents a way to perform gradcheck in a faster way without affecting its correctness. The debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user.
The high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.
### Fast gradcheck for real-to-real functions
The scalar quantity that we want to compute here is vTJfuv^T J_f uvTJf​u for a given random vector v∈RMv \in \mathcal{R}^Mv∈RM and a random unit norm vector u∈RNu \in \mathcal{R}^Nu∈RN.
For the numerical evaluation, we can efficiently compute
Jfu≈f(x+u∗eps)−f(x−u∗eps)2∗eps.J_f u \approx \frac{f(x + u * eps) - f(x - u * eps)}{2 * eps}. Jf​u≈2∗epsf(x+u∗eps)−f(x−u∗eps)​.
We then perform the dot product between this vector and vvv to get the scalar value of interest.
For the analytical version, we can use backward mode AD to compute vTJfv^T J_fvTJf​ directly. We then perform the dot product with uuu to get the expected value.
### Fast gradcheck for complex-to-real functions
Similar to the real-to-real case, we want to perform a reduction of the full matrix. But the 2∗CW2 * CW2∗CW matrix is complex-valued and so in this case, we will compare to complex scalars.
Due to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:
s:=2∗vT(real(CW)ur+i∗imag(CW)ui)s := 2 * v^T (real(CW) ur + i * imag(CW) ui) s:=2∗vT(real(CW)ur+i∗imag(CW)ui)
where v∈RMv \in \mathcal{R}^Mv∈RM, ur∈RNur \in \mathcal{R}^Nur∈RN and ui∈RNui \in \mathcal{R}^Nui∈RN.
#### Fast complex input numerical evaluation
We first consider how to compute sss with a numerical method. To do so, keeping in mind that we’re considering g:CN→RM,z→yg: \mathcal{C}^N \to \mathcal{R}^M, z \to yg:CN→RM,z→y with z=a+ibz = a + i bz=a+ib, and that CW=12∗(∂y∂a+i∂y∂b)CW = \frac{1}{2} * (\frac{\partial y}{\partial a} + i \frac{\partial y}{\partial b})CW=21​∗(∂a∂y​+i∂b∂y​), we rewrite it as follows:
s=2∗vT(real(CW)ur+i∗imag(CW)ui)=2∗vT(12∗∂y∂aur+i∗12∗∂y∂bui)=vT(∂y∂aur+i∗∂y∂bui)=vT((∂y∂aur)+i∗(∂y∂bui))\begin{aligned} s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\ &= 2 * v^T (\frac{1}{2} * \frac{\partial y}{\partial a} ur + i * \frac{1}{2} * \frac{\partial y}{\partial b} ui) \\\ &= v^T (\frac{\partial y}{\partial a} ur + i * \frac{\partial y}{\partial b} ui) \\\ &= v^T ((\frac{\partial y}{\partial a} ur) + i * (\frac{\partial y}{\partial b} ui)) \end{aligned} s​=2∗vT(real(CW)ur+i∗imag(CW)ui)=2∗vT(21​∗∂a∂y​ur+i∗21​∗∂b∂y​ui)=vT(∂a∂y​ur+i∗∂b∂y​ui)=vT((∂a∂y​ur)+i∗(∂b∂y​ui))​
In this formula, we can see that ∂y∂aur\frac{\partial y}{\partial a} ur∂a∂y​ur and ∂y∂bui\frac{\partial y}{\partial b} ui∂b∂y​ui can be evaluated the same way as the fast version for the real-to-real case. Once these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued vvv vector.
#### Fast complex input analytical evaluation
For the analytical case, things are simpler and we rewrite the formula as:
s=2∗vT(real(CW)ur+i∗imag(CW)ui)=vTreal(2∗CW)ur+i∗vTimag(2∗CW)ui)=real(vT(2∗CW))ur+i∗imag(vT(2∗CW))ui\begin{aligned} s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\ &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\\ &= real(v^T (2 * CW)) ur + i * imag(v^T (2 * CW)) ui \end{aligned} s​=2∗vT(real(CW)ur+i∗imag(CW)ui)=vTreal(2∗CW)ur+i∗vTimag(2∗CW)ui)=real(vT(2∗CW))ur+i∗imag(vT(2∗CW))ui​
We can thus use the fact that the backward mode AD provides us with an efficient way to compute vT(2∗CW)v^T (2 * CW)vT(2∗CW) and then perform a dot product of the real part with ururur and the imaginary part with uiuiui before reconstructing the final complex scalar sss.
#### Why not use a complex uuu
At this point, you might be wondering why we did not select a complex uuu and just performed the reduction 2∗vTCWu′2 * v^T CW u'2∗vTCWu′. To dive into this, in this paragraph, we will use the complex version of uuu noted u′=ur′+iui′u' = ur' + i ui'u′=ur′+iui′. Using such complex u′u'u′, the problem is that when doing the numerical evaluation, we would need to compute:
2∗CWu′=(∂y∂a+i∂y∂b)(ur′+iui′)=∂y∂aur′+i∂y∂aui′+i∂y∂bur′−∂y∂bui′\begin{aligned} 2*CW u' &= (\frac{\partial y}{\partial a} + i \frac{\partial y}{\partial b})(ur' + i ui') \\\ &= \frac{\partial y}{\partial a} ur' + i \frac{\partial y}{\partial a} ui' + i \frac{\partial y}{\partial b} ur' - \frac{\partial y}{\partial b} ui' \end{aligned} 2∗CWu′​=(∂a∂y​+i∂b∂y​)(ur′+iui′)=∂a∂y​ur′+i∂a∂y​ui′+i∂b∂y​ur′−∂b∂y​ui′​
Which would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above). Since this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.
### Fast gradcheck for functions with complex outputs
Just like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function.
## Gradgradcheck implementation
PyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.
This feature is implemented by considering the function F:x,v→vTJfF: x, v \to v^T J_fF:x,v→vTJf​ and use the gradcheck defined above on this function. Note that vvv in this case is just a random vector with the same type as f(x)f(x)f(x).
The fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same function FFF.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Gradcheck mechanics
    * Notations and background information
    * Default backward mode gradcheck behavior
      * Real-to-real functions
        * Default real input numerical evaluation
        * Default real input analytical evaluation
      * Complex-to-real functions
        * Default complex input numerical evaluation
        * Default complex input analytical evaluation
      * Functions with complex outputs
    * Fast backward mode gradcheck
      * Fast gradcheck for real-to-real functions
      * Fast gradcheck for complex-to-real functions
        * Fast complex input numerical evaluation
        * Fast complex input analytical evaluation
        * Why not use a complex uuu
      * Fast gradcheck for functions with complex outputs
    * Gradgradcheck implementation


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Features for large-scale deployments
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Features for large-scale deployments
  * Fleet-wide operator profiling
  * API usage logging
  * Attaching metadata to saved TorchScript models
  * Build environment considerations
  * Common extension points


This note talks about several extension points and tricks that might be useful when running PyTorch within a larger system or operating multiple systems using PyTorch in a larger organization.
It doesn’t cover topics of deploying models to production. Check `torch.jit` or one of the corresponding tutorials.
The note assumes that you either build PyTorch from source in your organization or have an ability to statically link additional code to be loaded when PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that can be triggered once in a centralized place, e.g. in static initialization code.
## Fleet-wide operator profiling
PyTorch comes with `torch.autograd.profiler` capable of measuring time taken by individual operators on demand. One can use the same mechanism to do “always ON” measurements for any process running PyTorch. It might be useful for gathering information about PyTorch workloads running in a given process or across the entire set of machines.
New callbacks for any operator invocation can be added with `torch::addGlobalCallback`. Hooks will be called with `torch::RecordFunction` struct that describes invocation context (e.g. name). If enabled, `RecordFunction::inputs()` contains arguments of the function represented as `torch::IValue` variant type. Note, that inputs logging is relatively expensive and thus has to be enabled explicitly.
The operator callbacks also have access to `c10::ThreadLocalDebugInfo::get()` interface that returns a pointer to the struct holding the debug information. This debug information can be set earlier by using `at::DebugInfoGuard` object. Debug information is propagated through the forward (including async `fork` tasks) and backward passes and can be useful for passing some extra information about execution environment (e.g. model id) from the higher layers of the application down to the operator callbacks.
Invoking callbacks adds some overhead, so usually it’s useful to just randomly sample operator invocations. This can be enabled on per-callback basis with an optional sampling rate passed into `torch::addGlobalCallback`.
Note, that `addGlobalCallback` is not thread-safe and can be called only when no PyTorch operator is running. Usually, it’s a good idea to call them once during initialization.
Here’s an example:
```
// Called somewhere in the program beginning
voidinit(){
// Sample one in a hundred operator runs randomly
addGlobalCallback(
RecordFunctionCallback(
&onFunctionEnter,
&onFunctionExit)
.needsInputs(true)
.samplingProb(0.01)
);
// Note, to enable observers in the model calling thread,
// call enableRecordFunction() in the thread before running a model
}
voidonFunctionEnter(constRecordFunction&fn){
std::cerr<<"Before function "<<fn.name()
<<" with "<<fn.inputs().size()<<" inputs"<<std::endl;
}
voidonFunctionExit(constRecordFunction&fn){
std::cerr<<"After function "<<fn.name();
}

```
Copy to clipboard
## API usage logging
When running in a broader ecosystem, for example in managed job scheduler, it’s often useful to track which binaries invoke particular PyTorch APIs. There exists simple instrumentation injected at several important API points that triggers a given callback. Because usually PyTorch is invoked in one-off python scripts, the callback fires only once for a given process for each of the APIs.
`c10::SetAPIUsageHandler` can be used to register API usage instrumentation handler. Passed argument is going to be an “api key” identifying used point, for example `python.import` for PyTorch extension import or `torch.script.compile` if TorchScript compilation was triggered.
```
SetAPIUsageLogger([](conststd::string&event_name){
std::cerr<<"API was used: "<<event_name<<std::endl;
});

```
Copy to clipboard
Note for developers: new API trigger points can be added in code with `C10_LOG_API_USAGE_ONCE("my_api")` in C++ or `torch._C._log_api_usage_once("my.api")` in Python.
## Attaching metadata to saved TorchScript models
TorchScript modules can be saved as an archive file that bundles serialized parameters and module code as TorchScript (see `torch.jit.save()`). It’s often convenient to bundle additional information together with the model, for example, description of model producer or auxiliary artifacts.
It can be achieved by passing the `_extra_files` argument to `torch.jit.save()` and `torch::jit::load` to store and retrieve arbitrary binary blobs during saving process. Since TorchScript files are regular ZIP archives, extra information gets stored as regular files inside archive’s `extra/` directory.
There’s also a global hook allowing to attach extra files to any TorchScript archive produced in the current process. It might be useful to tag models with producer metadata, akin to JPEG metadata produced by digital cameras. Example usage might look like:
```
SetExportModuleExtraFilesHook([](constModule&){
ExtraFilesMapfiles;
files["producer_info.json"]="{\"user\": \""+getenv("USER")+"\"}";
returnfiles;
});

```
Copy to clipboard
## Build environment considerations
TorchScript’s compilation needs to have access to the original python files as it uses python’s `inspect.getsource` call. In certain production environments it might require explicitly deploying `.py` files along with precompiled `.pyc`.
## Common extension points
PyTorch APIs are generally loosely coupled and it’s easy to replace a component with specialized version. Common extension points include:
  * Custom operators implemented in C++ - see tutorial for more details.
  * Custom data reading can be often integrated directly by invoking corresponding python library. Existing functionality of `torch.utils.data` can be utilized by extending `Dataset` or `IterableDataset`.


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Features for large-scale deployments
    * Fleet-wide operator profiling
    * API usage logging
    * Attaching metadata to saved TorchScript models
    * Build environment considerations
    * Common extension points


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * LibTorch Stable ABI
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# LibTorch Stable ABI
This note will eventually contain more details on how to use the APIs in torch/csrc/stable. For the moment, it contains a table of internal representations:
  1. type in custom extension: type used within the end user custom library.
  2. StableIValue representation: a stable conversion of the type to liaison between the user model vs libtorch.so in an ABI-stable manner.
  3. type in libtorch: type used within libtorch.so (or any code binary locked with libtorch).
  4. Schema Type: type as described by the schema, which we hail as the source of truth for both ATen ops in native_functions.yaml and for user defined custom operators registered to the dispatcher via TORCH_LIBRARY or torch.library.


type in custom extension | StableIValue representation | type in libtorch | Schema Type  
---|---|---|---  
std::optional<S> | *reinterpret_cast<(StableIValue*)*>, pointer to a StableIValue recursively defined | std::optional<T> | Type?  
std::nullopt | *reinterpret_cast<nullptr_t*> | IValue() | None  
RAIIATH | *reinterpret_cast<uint64_t*> of AtenTensorHandle | at::Tensor | Tensor  
int32_t | *reinterpret_cast<uint64_t*> | at::ScalarType | ScalarType  
int32_t | *reinterpret_cast<uint64_t*> | at::Layout | Layout  
int32_t | *reinterpret_cast<uint64_t*> | at::MemoryFormat | MemoryFormat  
bool | *reinterpret_cast<uint64_t*> | bool | bool  
int64_t | *reinterpret_cast<uint64_t*> | int64_t | int  
double | *reinterpret_cast<uint64_t*> | double | float  
? | ? | c10::Device | Device  
? | ? | c10::Stream | Stream  
? | ? | c10::complex | complex  
? | ? | at::Scalar | Scalar  
? | ? | std::string/const char*/ivalue::ConstantString | str  
? | ? | at::Storage | Storage  
? | ? | at::Generator | Generator  
? | ? | c10::List<T> | Type[]  
? | ? | ivalue::Tuple<T> | (Type, …)  
? | ? | c10::SymInt | SymInt  
? | ? | c10::SymFloat | SymFloat  
? | ? | c10::SymBool | SymBool  
? | ? | at::QScheme | QScheme  
Our confidently supported types are the ones in the table that have completed rows. For a limited set of use cases, we also implicitly support any literal type that is representable within 64 bits as StableIValues, as the default reinterpret_cast will succeed. You can work with StableIValue abstractions in your custom kernel for types such as c10::Device even if there is no standard defined representation of device in custom extensions. For example, a custom operator can take as argument a StableIValue device and directly pass it through to an aten operator with aoti_torch_call_dispatcher.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * LibTorch Stable ABI


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Reproducibility
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Reproducibility
Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.
However, there are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release. First, you can control sources of randomness that can cause multiple executions of your application to behave differently. Second, you can configure PyTorch to avoid using nondeterministic algorithms for some operations, so that multiple calls to those operations, given the same inputs, will produce the same result.
Warning
Deterministic operations are often slower than nondeterministic operations, so single-run performance may decrease for your model. However, determinism may save time in development by facilitating experimentation, debugging, and regression testing.
## Controlling sources of randomness
### PyTorch random number generator
You can use `torch.manual_seed()` to seed the RNG for all devices (both CPU and CUDA):
```
import torch
torch.manual_seed(0)

```
Copy to clipboard
Some PyTorch operations may use random numbers internally. `torch.svd_lowrank()` does this, for instance. Consequently, calling it multiple times back-to-back with the same input arguments may give different results. However, as long as `torch.manual_seed()` is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment.
It is also possible to obtain identical results from an operation that uses random numbers by setting `torch.manual_seed()` to the same value between subsequent calls.
### Python
For custom operators, you might need to set python seed as well:
```
import random
random.seed(0)

```
Copy to clipboard
### Random number generators in other libraries
If you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG with:
```
import numpy as np
np.random.seed(0)

```
Copy to clipboard
However, some applications and libraries may use NumPy Random Generator objects, not the global RNG (https://numpy.org/doc/stable/reference/random/generator.html), and those will need to be seeded consistently as well.
If you are using any other libraries that use random number generators, refer to the documentation for those libraries to see how to set consistent seeds for them.
### CUDA convolution benchmarking
The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism across multiple executions of an application. When a cuDNN convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. Due to benchmarking noise and different hardware, the benchmark may select different algorithms on subsequent runs, even on the same machine.
Disabling the benchmarking feature with `torch.backends.cudnn.benchmark = False` causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.
However, if you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled with `torch.backends.cudnn.benchmark = True`.
Note that this setting is different from the `torch.backends.cudnn.deterministic` setting discussed below.
## Avoiding nondeterministic algorithms
`torch.use_deterministic_algorithms()` lets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative).
Please check the documentation for `torch.use_deterministic_algorithms()` for a full list of affected operations. If an operation does not act correctly according to the documentation, or if you need a deterministic implementation of an operation that does not have one, please submit an issue: https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22
For example, running the nondeterministic CUDA implementation of `torch.Tensor.index_add_()` will throw an error:
```
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
'torch.use_deterministic_algorithms(True)'. ...

```
Copy to clipboard
When `torch.bmm()` is called with sparse-dense CUDA tensors it typically uses a nondeterministic algorithm, but when the deterministic flag is turned on, its alternate deterministic implementation will be used:
```
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())
tensor([[[ 1.1900, -2.3409],
     [ 0.4796, 0.8003]],
    [[ 0.1509, 1.8027],
     [ 0.0333, -1.1444]]], device='cuda:0')

```
Copy to clipboard
Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you should set the environment variable CUBLAS_WORKSPACE_CONFIG according to CUDA documentation: https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility
### CUDA convolution determinism
While disabling CUDA convolution benchmarking (discussed above) ensures that CUDA selects the same algorithm each time an application is run, that algorithm itself may be nondeterministic, unless either `torch.use_deterministic_algorithms(True)` or `torch.backends.cudnn.deterministic = True` is set. The latter setting controls only this behavior, unlike `torch.use_deterministic_algorithms()` which will make other PyTorch operations behave deterministically, too.
### CUDA RNN and LSTM
In some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior. See `torch.nn.RNN()` and `torch.nn.LSTM()` for details and workarounds.
### Filling uninitialized memory
Operations like `torch.empty()` and `torch.Tensor.resize_()` can return tensors with uninitialized memory that contain undefined values. Using such a tensor as an input to another operation is invalid if determinism is required, because the output will be nondeterministic. But there is nothing to actually prevent such invalid code from being run. So for safety, `torch.utils.deterministic.fill_uninitialized_memory` is set to `True` by default, which will fill the uninitialized memory with a known value if `torch.use_deterministic_algorithms(True)` is set. This will prevent the possibility of this kind of nondeterministic behavior.
However, filling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance.
## DataLoader
DataLoader will reseed workers following Randomness in multi-process data loading algorithm. Use `worker_init_fn()` and generator to preserve reproducibility:
```
def seed_worker(worker_id):
  worker_seed = torch.initial_seed() % 2**32
  numpy.random.seed(worker_seed)
  random.seed(worker_seed)
g = torch.Generator()
g.manual_seed(0)
DataLoader(
  train_dataset,
  batch_size=batch_size,
  num_workers=num_workers,
  worker_init_fn=seed_worker,
  generator=g,
)

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Reproducibility
    * Controlling sources of randomness
      * PyTorch random number generator
      * Python
      * Random number generators in other libraries
      * CUDA convolution benchmarking
    * Avoiding nondeterministic algorithms
      * CUDA convolution determinism
      * CUDA RNN and LSTM
      * Filling uninitialized memory
    * DataLoader


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * HIP (ROCm) semantics
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# HIP (ROCm) semantics
ROCm™ is AMD’s open source software platform for GPU-accelerated high performance computing and machine learning. HIP is ROCm’s C++ dialect designed to ease conversion of CUDA applications to portable C++ code. HIP is used when converting existing CUDA applications like PyTorch to portable C++ and for new projects that require portability between AMD and NVIDIA.
## HIP Interfaces Reuse the CUDA Interfaces
PyTorch for HIP intentionally reuses the existing `torch.cuda` interfaces. This helps to accelerate the porting of existing PyTorch code and models because very few code changes are necessary, if any.
The example from CUDA semantics will work exactly the same for HIP:
```
cuda = torch.device('cuda')   # Default HIP device
cuda0 = torch.device('cuda:0') # 'rocm' or 'hip' are not valid, use 'cuda'
cuda2 = torch.device('cuda:2') # GPU 2 (these are 0-indexed)
x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)
with torch.cuda.device(1):
  # allocates a tensor on GPU 1
  a = torch.tensor([1., 2.], device=cuda)
  # transfers a tensor from CPU to GPU 1
  b = torch.tensor([1., 2.]).cuda()
  # a.device and b.device are device(type='cuda', index=1)
  # You can also use ``Tensor.to`` to transfer a tensor:
  b2 = torch.tensor([1., 2.]).to(device=cuda)
  # b.device and b2.device are device(type='cuda', index=1)
  c = a + b
  # c.device is device(type='cuda', index=1)
  z = x + y
  # z.device is device(type='cuda', index=0)
  # even within a context, you can specify the device
  # (or give a GPU index to the .cuda call)
  d = torch.randn(2, device=cuda2)
  e = torch.randn(2).to(cuda2)
  f = torch.randn(2).cuda(cuda2)
  # d.device, e.device, and f.device are all device(type='cuda', index=2)

```
Copy to clipboard
## Checking for HIP
Whether you are using PyTorch for CUDA or HIP, the result of calling `is_available()` will be the same. If you are using a PyTorch that has been built with GPU support, it will return True. If you must check which version of PyTorch you are using, refer to this example below:
```
if torch.cuda.is_available() and torch.version.hip:
  # do something specific for HIP
elif torch.cuda.is_available() and torch.version.cuda:
  # do something specific for CUDA

```
Copy to clipboard
## TensorFloat-32(TF32) on ROCm
TF32 is not supported on ROCm.
## Memory management
PyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in `rocm-smi`. You can use `memory_allocated()` and `max_memory_allocated()` to monitor memory occupied by tensors, and use `memory_reserved()` and `max_memory_reserved()` to monitor the total amount of memory managed by the caching allocator. Calling `empty_cache()` releases all **unused** cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.
For more advanced users, we offer more comprehensive memory benchmarking via `memory_stats()`. We also offer the capability to capture a complete snapshot of the memory allocator state via `memory_snapshot()`, which can help you understand the underlying allocation patterns produced by your code.
To debug memory errors, set `PYTORCH_NO_HIP_MEMORY_CACHING=1` in your environment to disable caching. `PYTORCH_NO_CUDA_MEMORY_CACHING=1` is also accepted for ease of porting.
## hipBLAS workspaces
For each combination of hipBLAS handle and HIP stream, a hipBLAS workspace will be allocated if that handle and stream combination executes a hipBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless `torch._C._cuda_clearCublasWorkspaces()` is called; note that it’s the same function for CUDA or HIP. The workspace size per allocation can be specified via the environment variable `HIPBLAS_WORKSPACE_CONFIG` with the format `:[SIZE]:[COUNT]`. As an example, the environment variable `HIPBLAS_WORKSPACE_CONFIG=:4096:2:16:8` specifies a total size of `2 * 4096 + 8 * 16 KiB` or 8 MIB. The default workspace size is 32 MiB; MI300 and newer defaults to 128 MiB. To force hipBLAS to avoid using workspaces, set `HIPBLAS_WORKSPACE_CONFIG=:0:0`. For convenience, `CUBLAS_WORKSPACE_CONFIG` is also accepted.
## hipFFT/rocFFT plan cache
Setting the size of the cache for hipFFT/rocFFT plans is not supported.
## torch.distributed backends
Currently, only the “nccl” and “gloo” backends for torch.distributed are supported on ROCm.
## CUDA API to HIP API mappings in C++
Please refer: https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP_API_Guide.html
NOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not semantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and hipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks.
For example: Instead of using
`#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000` to implicitly exclude ROCm/HIP,
use the following to not take the code path for ROCm/HIP:
`#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM)`
Alternatively, if it is desired to take the code path for ROCm/HIP:
`#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM)`
Or if it is desired to take the code path for ROCm/HIP only for specific HIP versions:
`#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || (defined(USE_ROCM) && ROCM_VERSION >= 40300)`
## Refer to CUDA Semantics doc
For any sections not listed here, please refer to the CUDA semantics doc: CUDA semantics
## Enabling kernel asserts
Kernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled by recompiling the PyTorch from source.
Please add below line as an argument to cmake command parameters:
```
-DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * HIP (ROCm) semantics
    * HIP Interfaces Reuse the CUDA Interfaces
    * Checking for HIP
    * TensorFloat-32(TF32) on ROCm
    * Memory management
    * hipBLAS workspaces
    * hipFFT/rocFFT plan cache
    * torch.distributed backends
    * CUDA API to HIP API mappings in C++
    * Refer to CUDA Semantics doc
    * Enabling kernel asserts


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Serialization semantics
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Serialization semantics
This note describes how you can save and load PyTorch tensors and module states in Python, and how to serialize Python modules so they can be loaded in C++.
Table of Contents
  * Serialization semantics
    * Saving and loading tensors
    * Saving and loading tensors preserves views
    * Saving and loading torch.nn.Modules
    * Serialized file format for `torch.save`
    * `torch.load` with `weights_only=True`
      * Troubleshooting `weights_only`
        * Getting unsafe globals
        * Environment Variables
    * Serializing torch.nn.Modules and loading them in C++
    * Saving and loading ScriptModules across PyTorch versions
      * torch.div performing integer division
      * torch.full always inferring a float dtype
    * Utility functions
    * Config


## Saving and loading tensors
`torch.save()` and `torch.load()` let you easily save and load tensors:
```
>>> t = torch.tensor([1., 2.])
>>> torch.save(t, 'tensor.pt')
>>> torch.load('tensor.pt')
tensor([1., 2.])

```
Copy to clipboard
By convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension.
`torch.save()` and `torch.load()` use Python’s pickle by default, so you can also save multiple tensors as part of Python objects like tuples, lists, and dicts:
```
>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}
>>> torch.save(d, 'tensor_dict.pt')
>>> torch.load('tensor_dict.pt')
{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}

```
Copy to clipboard
Custom data structures that include PyTorch tensors can also be saved if the data structure is pickle-able.
## Saving and loading tensors preserves views
Saving tensors preserves their view relationships:
```
>>> numbers = torch.arange(1, 10)
>>> evens = numbers[1::2]
>>> torch.save([numbers, evens], 'tensors.pt')
>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')
>>> loaded_evens *= 2
>>> loaded_numbers
tensor([ 1, 4, 3, 8, 5, 12, 7, 16, 9])

```
Copy to clipboard
Behind the scenes, these tensors share the same “storage.” See Tensor Views for more on views and storage.
When PyTorch saves tensors it saves their storage objects and tensor metadata separately. This is an implementation detail that may change in the future, but it typically saves space and lets PyTorch easily reconstruct the view relationships between the loaded tensors. In the above snippet, for example, only a single storage is written to ‘tensors.pt’.
In some cases, however, saving the current storage objects may be unnecessary and create prohibitively large files. In the following snippet a storage much larger than the saved tensor is written to a file:
```
>>> large = torch.arange(1, 1000)
>>> small = large[0:5]
>>> torch.save(small, 'small.pt')
>>> loaded_small = torch.load('small.pt')
>>> loaded_small.storage().size()
999

```
Copy to clipboard
Instead of saving only the five values in the small tensor to ‘small.pt,’ the 999 values in the storage it shares with large were saved and loaded.
When saving tensors with fewer elements than their storage objects, the size of the saved file can be reduced by first cloning the tensors. Cloning a tensor produces a new tensor with a new storage object containing only the values in the tensor:
```
>>> large = torch.arange(1, 1000)
>>> small = large[0:5]
>>> torch.save(small.clone(), 'small.pt') # saves a clone of small
>>> loaded_small = torch.load('small.pt')
>>> loaded_small.storage().size()
5

```
Copy to clipboard
Since the cloned tensors are independent of each other, however, they have none of the view relationships the original tensors did. If both file size and view relationships are important when saving tensors smaller than their storage objects, then care must be taken to construct new tensors that minimize the size of their storage objects but still have the desired view relationships before saving.
## Saving and loading torch.nn.Modules
See also: Tutorial: Saving and loading modules
In PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers:
```
>>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)
>>> list(bn.named_parameters())
[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),
 ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]
>>> list(bn.named_buffers())
[('running_mean', tensor([0., 0., 0.])),
 ('running_var', tensor([1., 1., 1.])),
 ('num_batches_tracked', tensor(0))]
>>> bn.state_dict()
OrderedDict([('weight', tensor([1., 1., 1.])),
       ('bias', tensor([0., 0., 0.])),
       ('running_mean', tensor([0., 0., 0.])),
       ('running_var', tensor([1., 1., 1.])),
       ('num_batches_tracked', tensor(0))])

```
Copy to clipboard
Instead of saving a module directly, for compatibility reasons it is recommended to instead save only its state dict. Python modules even have a function, `load_state_dict()`, to restore their states from a state dict:
```
>>> torch.save(bn.state_dict(), 'bn.pt')
>>> bn_state_dict = torch.load('bn.pt')
>>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)
>>> new_bn.load_state_dict(bn_state_dict)
<All keys matched successfully>

```
Copy to clipboard
Note that the state dict is first loaded from its file with `torch.load()` and the state then restored with `load_state_dict()`.
Even custom modules and modules containing other modules have state dicts and can use this pattern:
```
# A module with two linear layers
>>> class MyModule(torch.nn.Module):
   def __init__(self):
    super().__init__()
    self.l0 = torch.nn.Linear(4, 2)
    self.l1 = torch.nn.Linear(2, 1)
   def forward(self, input):
    out0 = self.l0(input)
    out0_relu = torch.nn.functional.relu(out0)
    return self.l1(out0_relu)
>>> m = MyModule()
>>> m.state_dict()
OrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],
                  [-0.3289, 0.2827, 0.4588, 0.2031]])),
       ('l0.bias', tensor([ 0.0300, -0.1316])),
       ('l1.weight', tensor([[0.6533, 0.3413]])),
       ('l1.bias', tensor([-0.1112]))])
>>> torch.save(m.state_dict(), 'mymodule.pt')
>>> m_state_dict = torch.load('mymodule.pt')
>>> new_m = MyModule()
>>> new_m.load_state_dict(m_state_dict)
<All keys matched successfully>

```
Copy to clipboard
## Serialized file format for `torch.save`
Since PyTorch 1.6.0, `torch.save` defaults to returning an uncompressed ZIP64 archive unless the user sets `_use_new_zipfile_serialization=False`.
In this archive, the files are ordered as such
```
checkpoint.pth
├── data.pkl
├── byteorder # added in PyTorch 2.1.0
├── data/
│  ├── 0
│  ├── 1
│  ├── 2
│  └── …
└── version

```
Copy to clipboard 

The entries are as follows:
    
  * `data.pkl` is the result of pickling the object passed to `torch.save` excluding `torch.Storage` objects that it contains
  * `byteorder` contains a string with the `sys.byteorder` when saving (“little” or “big”)
  * `data/` contains all the storages in the object, where each storage is a separate file
  * `version` contains a version number at save time that can be used at load time


When saving, PyTorch will ensure that the local file header of each file is padded to an offset that is a multiple of 64 bytes, ensuring that the offset of each file is 64-byte aligned.
Note
Tensors on certain devices such as XLA are serialized as pickled numpy arrays. As such, their storages are not serialized. In these cases `data/` might not exist in the checkpoint.
## `torch.load` with `weights_only=True`
Starting in version 2.6, `torch.load` will use `weights_only=True` if the `pickle_module` argument is not passed.
As discussed in the documentation for `torch.load()`, `weights_only=True` restricts the unpickler used in `torch.load` to only executing functions/building classes required for `state_dicts` of plain `torch.Tensors` as well as some other primitive types. Further, unlike the default `Unpickler` provided by the `pickle` module, the `weights_only` Unpickler is not allowed to dynamically import anything during unpickling.
As mentioned above, saving a module’s `state_dict` is a best practice when using `torch.save`. If loading an old checkpoint that contains an `nn.Module`, we recommend `weights_only=False`. When loading a checkpoint that contains tensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details.
If the `weights_only` Unpickler encounters a function or class that is not allowlisted by default within the pickle file, you should see an actionable error like such
```
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded,
to do so you have two options, do those steps only if you trust the source of the checkpoint.
  1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed,
    but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
  2. Alternatively, to load with `weights_only=True` please check the recommended
    steps in the following error message.
    WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by
    default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the
    `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global
    if you trust this class/function.

```
Copy to clipboard
Please follow the steps in the error message and allowlist the functions or classes only if you trust them.
To get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can use `torch.serialization.get_unsafe_globals_in_checkpoint()` which will return a list of strings of the form `{__module__}.{__name__}`. If you trust these functions/classes, you can import them and allowlist them per the error message either via `torch.serialization.add_safe_globals()` or the context manager `torch.serialization.safe_globals`.
To access the list of user-allowlisted functions/classes you can use `torch.serialization.get_safe_globals()` and to clear the current list see `torch.serialization.clear_safe_globals()`.
### Troubleshooting `weights_only`
#### Getting unsafe globals
A caveat is that `torch.serialization.get_unsafe_globals_in_checkpoint()` analyzes the checkpoint statically, some types might be built dynamically during the unpickling process and hence will not be reported by `torch.serialization.get_unsafe_globals_in_checkpoint()`. One such example is `dtypes` in numpy. In `numpy < 1.25` after allowlisting all the functions/classes reported by `torch.serialization.get_unsafe_globals_in_checkpoint()` you might see an error like
```
WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got <class 'numpy.dtype[float32]'>

```
Copy to clipboard
This can be allowlisted via `{add_}safe_globals([type(np.dtype(np.float32))])`.
In `numpy >=1.25` you would see
```
WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got <class 'numpy.dtypes.Float32DType'>

```
Copy to clipboard
This can be allowlisted via `{add_}safe_globals([np.dtypes.Float32DType])`.
#### Environment Variables
There are two environment variables that will influence the behavior of `torch.load`. These can be helpful if one does not have access to the `torch.load` callsites.
  * `TORCH_FORCE_WEIGHTS_ONLY_LOAD=1` will override all `torch.load` callsites to use `weights_only=True`.
  * `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1` will make `torch.load` callsites use `weights_only=False` **only** if `weights_only` was not passed as an argument.


## Serializing torch.nn.Modules and loading them in C++
See also: Tutorial: Loading a TorchScript Model in C++
ScriptModules can be serialized as a TorchScript program and loaded using `torch.jit.load()`. This serialization encodes all the modules’ methods, submodules, parameters, and attributes, and it allows the serialized program to be loaded in C++ (i.e. without Python).
The distinction between `torch.jit.save()` and `torch.save()` may not be immediately clear. `torch.save()` saves Python objects with pickle. This is especially useful for prototyping, researching, and training. `torch.jit.save()`, on the other hand, serializes ScriptModules to a format that can be loaded in Python or C++. This is useful when saving and loading C++ modules or for running modules trained in Python with C++, a common practice when deploying PyTorch models.
To script, serialize and load a module in Python:
```
>>> scripted_module = torch.jit.script(MyModule())
>>> torch.jit.save(scripted_module, 'mymodule.pt')
>>> torch.jit.load('mymodule.pt')
RecursiveScriptModule( original_name=MyModule
           (l0): RecursiveScriptModule(original_name=Linear)
           (l1): RecursiveScriptModule(original_name=Linear) )

```
Copy to clipboard
Traced modules can also be saved with `torch.jit.save()`, with the caveat that only the traced code path is serialized. The following example demonstrates this:
```
# A module with control flow
>>> class ControlFlowModule(torch.nn.Module):
   def __init__(self):
    super().__init__()
    self.l0 = torch.nn.Linear(4, 2)
    self.l1 = torch.nn.Linear(2, 1)
   def forward(self, input):
    if input.dim() > 1:
      return torch.tensor(0)
    out0 = self.l0(input)
    out0_relu = torch.nn.functional.relu(out0)
    return self.l1(out0_relu)
>>> traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4))
>>> torch.jit.save(traced_module, 'controlflowmodule_traced.pt')
>>> loaded = torch.jit.load('controlflowmodule_traced.pt')
>>> loaded(torch.randn(2, 4)))
tensor([[-0.1571], [-0.3793]], grad_fn=<AddBackward0>)
>>> scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4))
>>> torch.jit.save(scripted_module, 'controlflowmodule_scripted.pt')
>>> loaded = torch.jit.load('controlflowmodule_scripted.pt')
>> loaded(torch.randn(2, 4))
tensor(0)

```
Copy to clipboard
The above module has an if statement that is not triggered by the traced inputs, and so is not part of the traced module and not serialized with it. The scripted module, however, contains the if statement and is serialized with it. See the TorchScript documentation for more on scripting and tracing.
Finally, to load the module in C++:
```
>>> torch::jit::script::Module module;
>>> module = torch::jit::load('controlflowmodule_scripted.pt');

```
Copy to clipboard
See the PyTorch C++ API documentation for details about how to use PyTorch modules in C++.
## Saving and loading ScriptModules across PyTorch versions
The PyTorch Team recommends saving and loading modules with the same version of PyTorch. Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch’s release notes, and modules relying on functionality that has changed may need to be updated to continue working properly. In limited cases, detailed below, PyTorch will preserve the historic behavior of serialized ScriptModules so they do not require an update.
### torch.div performing integer division
In PyTorch 1.5 and earlier `torch.div()` would perform floor division when given two integer inputs:
```
# PyTorch 1.5 (and earlier)
>>> a = torch.tensor(5)
>>> b = torch.tensor(3)
>>> a / b
tensor(1)

```
Copy to clipboard
In PyTorch 1.7, however, `torch.div()` will always perform a true division of its inputs, just like division in Python 3:
```
# PyTorch 1.7
>>> a = torch.tensor(5)
>>> b = torch.tensor(3)
>>> a / b
tensor(1.6667)

```
Copy to clipboard
The behavior of `torch.div()` is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see `torch.div()` perform floor division when given two integer inputs even when loaded with newer versions of PyTorch. ScriptModules using `torch.div()` and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.
### torch.full always inferring a float dtype
In PyTorch 1.5 and earlier `torch.full()` always returned a float tensor, regardless of the fill value it’s given:
```
# PyTorch 1.5 and earlier
>>> torch.full((3,), 1) # Note the integer fill value...
tensor([1., 1., 1.])   # ...but float tensor!

```
Copy to clipboard
In PyTorch 1.7, however, `torch.full()` will infer the returned tensor’s dtype from the fill value:
```
# PyTorch 1.7
>>> torch.full((3,), 1)
tensor([1, 1, 1])
>>> torch.full((3,), True)
tensor([True, True, True])
>>> torch.full((3,), 1.)
tensor([1., 1., 1.])
>>> torch.full((3,), 1 + 1j)
tensor([1.+1.j, 1.+1.j, 1.+1.j])

```
Copy to clipboard
The behavior of `torch.full()` is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see torch.full return float tensors by default, even when given bool or integer fill values. ScriptModules using `torch.full()` and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.
## Utility functions
The following utility functions are related to serialization: 

torch.serialization.register_package(_priority_ , _tagger_ , _deserializer_)[source][source]
    
Registers callables for tagging and deserializing storage objects with an associated priority. Tagging associates a device with a storage object at save time while deserializing moves a storage object to an appropriate device at load time. `tagger` and `deserializer` are run in the order given by their `priority` until a tagger/deserializer returns a value that is not None.
To override the deserialization behavior for a device in the global registry, one can register a tagger with a higher priority than the existing tagger.
This function can also be used to register a tagger and deserializer for new devices. 

Parameters
    
  * **priority** (_int_) – Indicates the priority associated with the tagger and deserializer, where a lower value indicates higher priority.
  * **tagger** (_Callable_ _[__[__Union_ _[__Storage_ _,__TypedStorage_ _,__UntypedStorage_ _]__]__,__Optional_ _[__str_ _]__]_) – Callable that takes in a storage object and returns its tagged device as a string or None.
  * **deserializer** (_Callable_ _[__[__Union_ _[__Storage_ _,__TypedStorage_ _,__UntypedStorage_ _]__,__str_ _]__,__Optional_ _[__Union_ _[__Storage_ _,__TypedStorage_ _,__UntypedStorage_ _]__]__]_) – Callable that takes in storage object and a device string and returns a storage object on the appropriate device or None.



Returns
    
None
Example
```
>>> def ipu_tag(obj):
>>>   if obj.device.type == 'ipu':
>>>     return 'ipu'
>>> def ipu_deserialize(obj, location):
>>>   if location.startswith('ipu'):
>>>     ipu = getattr(torch, "ipu", None)
>>>     assert ipu is not None, "IPU device module is not loaded"
>>>     assert torch.ipu.is_available(), "ipu is not available"
>>>     return obj.ipu(location)
>>> torch.serialization.register_package(11, ipu_tag, ipu_deserialize)

```
Copy to clipboard 

torch.serialization.get_crc32_options()[source][source]
    
Get whether `torch.save()` computes and writes crc32 for each record.
Defaults to `True`. 

Return type
    
bool 

torch.serialization.set_crc32_options(_compute_crc32_)[source][source]
    
Set whether `torch.save()` computes and writes crc32 for each record.
Note
Setting this to `False` may make unzipping of the `torch.save` output fail or warn due to corrupted CRC32. However `torch.load` will be able to load the file. 

Parameters
    
**compute_crc32** (_bool_) – set crc32 compuation flag 

torch.serialization.get_default_load_endianness()[source][source]
    
Get fallback byte order for loading files
If byteorder mark is not present in saved checkpoint, this byte order is used as fallback. By default, it’s “native” byte order. 

Returns
    
Optional[LoadEndianness] 

Return type
    
default_load_endian 

torch.serialization.set_default_load_endianness(_endianness_)[source][source]
    
Set fallback byte order for loading files
If byteorder mark is not present in saved checkpoint, this byte order is used as fallback. By default, it’s “native” byte order. 

Parameters
    
**endianness** – the new fallback byte order 

torch.serialization.get_default_mmap_options()[source][source]
    
Get default mmap options for `torch.load()` with `mmap=True`.
Defaults to `mmap.MAP_PRIVATE`. 

Returns
    
int 

Return type
    
default_mmap_options 

torch.serialization.set_default_mmap_options(_flags_)[source][source]
    
Context manager or function to set default mmap options for `torch.load()` with `mmap=True` to flags.
For now, only either `mmap.MAP_PRIVATE` or `mmap.MAP_SHARED` are supported. Please open an issue if you need any other option to be added here.
Note
This feature is currently not supported for Windows. 

Parameters
    
**flags** (_int_) – `mmap.MAP_PRIVATE` or `mmap.MAP_SHARED` 

torch.serialization.add_safe_globals(_safe_globals_)[source][source]
    
Marks the given globals as safe for `weights_only` load. For example, functions added to this list can be called during unpickling, classes could be instantiated and have state set.
Each item in the list can either be a function/class or a tuple of the form (function/class, string) where string is the full path of the function/class.
Within the serialized format, each function is identified with its full path as `{__module__}.{__qualname__}`. When calling this API, you can provide this full path that should match the one in the checkpoint otherwise the default `{fn.__module__}.{fn.__qualname__}` will be used. 

Parameters
    
**safe_globals** (_List_ _[__Union_ _[__Callable_ _,__Tuple_ _[__Callable_ _,__str_ _]__]__]_) – list of globals to mark as safe
Example
```
>>> import tempfile
>>> class MyTensor(torch.Tensor):
...   pass
>>> t = MyTensor(torch.randn(2, 3))
>>> with tempfile.NamedTemporaryFile() as f:
...   torch.save(t, f.name)
# Running `torch.load(f.name, weights_only=True)` will fail with
# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.
# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.
...   torch.serialization.add_safe_globals([MyTensor])
...   torch.load(f.name, weights_only=True)
# MyTensor([[-0.5024, -1.8152, -0.5455],
#     [-0.8234, 2.0500, -0.3657]])

```
Copy to clipboard 

torch.serialization.clear_safe_globals()[source][source]
    
Clears the list of globals that are safe for `weights_only` load. 

torch.serialization.get_safe_globals()[source][source]
    
Returns the list of user-added globals that are safe for `weights_only` load. 

Return type
    
list[_Union_[_Callable_, tuple[_Callable_, str]]] 

torch.serialization.get_unsafe_globals_in_checkpoint(_f_)[source][source]
    
Returns a list of strings of functions/classes in a `torch.save` object that are not safe for `weights_only`.
For a given function or class `f`, the corresponding string will be of the form `{f.__module__}.{f.__name__}`.
This function will return any GLOBALs in the checkpoint that are not in the set marked safe for `weights_only` (either via `add_safe_globals()` or `safe_globals` context or allowlisted by `torch` by default).
Note
This function will statically disassemble the pickle file in the checkpoint. The implication is any classes dynamically pushed onto the stack during unpickling will not be included in the output. 

Parameters
    
**f** (_Union_ _[__str_ _,__PathLike_ _[__str_ _]__,__IO_ _[__bytes_ _]__]_) – File-like object or string containing the checkpoint object saved via `torch.save` 

Returns
    
A list of strings of pickle GLOBALs in the checkpoint that are not allowlisted for `weights_only`. 

Return type
    
list[str] 

_class_ torch.serialization.safe_globals(_safe_globals_)[source][source]
    
Context-manager that adds certain globals as safe for `weights_only` load. 

Parameters
    
**safe_globals** (_list_ _[__Union_ _[__Callable_ _,__tuple_ _[__Callable_ _,__str_ _]__]__]_) – List of globals for weights_only load.
Example
```
>>> import tempfile
>>> class MyTensor(torch.Tensor):
...   pass
>>> t = MyTensor(torch.randn(2, 3))
>>> with tempfile.NamedTemporaryFile() as f:
...   torch.save(t, f.name)
# Running `torch.load(f.name, weights_only=True)` will fail with
# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.
# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.
...   with torch.serialization.safe_globals([MyTensor]):
...     torch.load(f.name, weights_only=True)
# MyTensor([[-0.5024, -1.8152, -0.5455],
#     [-0.8234, 2.0500, -0.3657]])
>>> assert torch.serialization.get_safe_globals() == []

```
Copy to clipboard 

_class_ torch.serialization.skip_data(_materialize_fake_tensors =False_)[source][source]
    
Context-manager that skips writing/reading storage bytes for `torch.save` / `torch.load` calls.
For the save path, storages will still be saved, but the space that their bytes would usually be written to will be empty space. The storage bytes can then be populated in a separate pass.
For the load path, tensors will be loaded per the checkpoint but their storages will not be populated with data.
Warning
The `skip_data` context manager is an early prototype and is subject to change. 

Parameters
    
**materialize_fake_tensors** (_bool_) – Whether to materialize FakeTensors during save. This is a no-op for the load path.
Example
```
>>> import tempfile
>>> t = torch.randn(2, 3)
>>> with tempfile.NamedTemporaryFile() as f:
...   with torch.serialization.skip_data():
...     torch.save(t, f.name)
...   torch.load(f.name, weights_only=True)
tensor([[0., 0., 0.],
    [0., 0., 0.]])

```
Copy to clipboard
## Config
`torch.utils.serialization.config` provides a global config that can control the behavior of `torch.save` and `torch.load`.
`torch.utils.serialization.config.save` contains options that control the behavior of `torch.save`.
>   * `compute_crc32`: whether to compute and write the zip file checksum (Default : `True`). See `set_crc32_options()`.
>   * `use_pinned_memory_for_d2h`: for storages that are on an accelerator when passed to `torch.save`, whether to move storage to pinned memory or pageable memory on CPU within `torch.save`. (Default: `False` (i.e. pageable))
>   * `storage_alignment`: alignment of storages in the checkpoint during `torch.save` in bytes. (Default `64`)
> 

`torch.utils.serialization.config.load` contains options that control the behavior of `torch.load`.
>   * `mmap`: See the documentation for `mmap` argument in `torch.load()`. This config will set the behavior of `mmap` for `torch.load` if it is not already explicitly passed to the `torch.load` call (Default : `False`).
>   * `endianness`: See `set_default_load_endianness()`. (Default : `torch.serialization.LoadEndianness.NATIVE`)
>   * `mmap_flags`: See `set_default_mmap_options`. (Default : `MAP_PRIVATE`)
>   * `calculate_storage_offsets`: If this config is set to `True`, offsets for storages will be calculated rather than read via random reads when using `torch.load(mmap=True)`. This minimizes random reads, which can be helpful when the file is being loaded over a network. (Default : `False`)
> 

Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Serialization semantics
    * Saving and loading tensors
    * Saving and loading tensors preserves views
    * Saving and loading torch.nn.Modules
    * Serialized file format for `torch.save`
    * `torch.load` with `weights_only=True`
      * Troubleshooting `weights_only`
        * Getting unsafe globals
        * Environment Variables
    * Serializing torch.nn.Modules and loading them in C++
    * Saving and loading ScriptModules across PyTorch versions
      * torch.div performing integer division
      * torch.full always inferring a float dtype
    * Utility functions
      * `register_package()`
      * `get_crc32_options()`
      * `set_crc32_options()`
      * `get_default_load_endianness()`
      * `set_default_load_endianness()`
      * `get_default_mmap_options()`
      * `set_default_mmap_options()`
      * `add_safe_globals()`
      * `clear_safe_globals()`
      * `get_safe_globals()`
      * `get_unsafe_globals_in_checkpoint()`
      * `safe_globals`
      * `skip_data`
    * Config


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Numerical accuracy
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Numerical accuracy
In modern computers, floating point numbers are represented using IEEE 754 standard. For more details on floating point arithmetic and IEEE 754 standard, please see Floating point arithmetic In particular, note that floating point provides limited accuracy (about 7 decimal digits for single precision floating point numbers, about 16 decimal digits for double precision floating point numbers) and that floating point addition and multiplication are not associative, so the order of the operations affects the results. Because of this, PyTorch is not guaranteed to produce bitwise identical results for floating point computations that are mathematically identical. Similarly, bitwise identical results are not guaranteed across PyTorch releases, individual commits, or different platforms. In particular, CPU and GPU results can be different even for bitwise-identical inputs and even after controlling for the sources of randomness.
## Batched computations or slice computations
Many operations in PyTorch support batched computation, where the same operation is performed for the elements of the batches of inputs. An example of this is `torch.mm()` and `torch.bmm()`. It is possible to implement batched computation as a loop over batch elements, and apply the necessary math operations to the individual batch elements, for efficiency reasons we are not doing that, and typically perform computation for the whole batch. The mathematical libraries that we are calling, and PyTorch internal implementations of operations can produces slightly different results in this case, compared to non-batched computations. In particular, let `A` and `B` be 3D tensors with the dimensions suitable for batched matrix multiplication. Then `(A@B)[0]` (the first element of the batched result) is not guaranteed to be bitwise identical to `A[0]@B[0]` (the matrix product of the first elements of the input batches) even though mathematically it’s an identical computation.
Similarly, an operation applied to a tensor slice is not guaranteed to produce results that are identical to the slice of the result of the same operation applied to the full tensor. E.g. let `A` be a 2-dimensional tensor. `A.sum(-1)[0]` is not guaranteed to be bitwise equal to `A[:,0].sum()`.
## Extremal values
When inputs contain large values such that intermediate results may overflow the range of the used datatype, the end result may overflow too, even though it is representable in the original datatype. E.g.:
```
import torch
a=torch.tensor([1e20, 1e20]) # fp32 type by default
a.norm() # produces tensor(inf)
a.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32

```
Copy to clipboard
## Linear algebra (`torch.linalg`)
### Non-finite values
The external libraries (backends) that `torch.linalg` uses provide no guarantees on their behaviour when the inputs have non-finite values like `inf` or `NaN`. As such, neither does PyTorch. The operations may return a tensor with non-finite values, or raise an exception, or even segfault.
Consider using `torch.isfinite()` before calling these functions to detect this situation.
### Extremal values in linalg
Functions within `torch.linalg` have more Extremal Values than other PyTorch functions.
Solvers and Inverses assume that the input matrix `A` is invertible. If it is close to being non-invertible (for example, if it has a very small singular value), then these algorithms may silently return incorrect results. These matrices are said to be ill-conditioned. If provided with ill-conditioned inputs, the result of these functions they may vary when using the same inputs on different devices or when using different backends via the keyword `driver`.
Spectral operations like `svd`, `eig`, and `eigh` may also return incorrect results (and their gradients may be infinite) when their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions struggle to converge for these inputs.
Running the computation in `float64` (as NumPy does by default) often helps, but it does not solve these issues in all cases. Analyzing the spectrum of the inputs via `torch.linalg.svdvals()` or their condition number via `torch.linalg.cond()` may help to detect these issues.
## TensorFloat-32(TF32) on Nvidia Ampere (and later) devices
On Ampere (and later) Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions. When an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read. This may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input). By default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32. We recommend enabling TF32 tensor cores for matrix multiplications with `torch.backends.cuda.matmul.allow_tf32 = True` if your network does not need full float32 precision. If your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions with `torch.backends.cudnn.allow_tf32 = False`.
For more information see TensorFloat32.
## Reduced Precision Reduction for FP16 and BF16 GEMMs
Half-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., `inf` values when the final result should be be representable in half-precision). If reduced-precision reductions are problematic, they can be turned off with `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False`
A similar flag exists for BF16 GEMM operations and is turned on by default. If BF16 reduced-precision reductions are problematic, they can be turned off with `torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False`
For more information see allow_fp16_reduced_precision_reduction and allow_bf16_reduced_precision_reduction
## Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA)
A naive SDPA math backend, when using FP16/BF16 inputs, can accumulate significant numerical errors due to the usage of low-precision intermediate buffers. To mitigate this issue, the default behavior now involves upcasting FP16/BF16 inputs to FP32. Computations are performed in FP32/TF32, and the final FP32 results are then downcasted back to FP16/BF16. This will improve numerical accuracy of the final output for the math backend with FP16/BF16 inputs, but increases memory usages and may cause the performance regressions in the math backend as computations shift from FP16/BF16 BMM to FP32/TF32 BMM/Matmul.
For scenarios where reduced-precision reductions are preferred for speed, they can be enabled with the following setting: `torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)`
## Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices
On AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior.
rocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged.
When training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows:
| forward | backward  
---|---|---  
Env unset | original | alternate  
Env set to 1 | alternate | alternate  
Env set to 0 | original | original  
The following is the list of operations where rocBLAS may be used:
  * torch.addbmm
  * torch.addmm
  * torch.baddbmm
  * torch.bmm
  * torch.mm
  * torch.nn.GRUCell
  * torch.nn.LSTMCell
  * torch.nn.Linear
  * torch.sparse.addmm
  * the following torch._C._ConvBackend implementations:
    * slowNd
    * slowNd_transposed
    * slowNd_dilated
    * slowNd_dilated_transposed


The following is the list of operations where MIOpen may be used:
  * torch.nn.Conv[Transpose]Nd
  * the following torch._C._ConvBackend implementations:
    * ConvBackend::Miopen
    * ConvBackend::MiopenDepthwise
    * ConvBackend::MiopenTranspose


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Numerical accuracy
    * Batched computations or slice computations
    * Extremal values
    * Linear algebra (`torch.linalg`)
      * Non-finite values
      * Extremal values in linalg
    * TensorFloat-32(TF32) on Nvidia Ampere (and later) devices
    * Reduced Precision Reduction for FP16 and BF16 GEMMs
    * Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA)
    * Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Modules
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Modules
PyTorch uses modules to represent neural networks. Modules are:
  * **Building blocks of stateful computation.** PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks.
  * **Tightly integrated with PyTorch’s** autograd **system.** Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.
  * **Easy to work with and transform.** Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.


This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch, many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents are provided here as well.
  * A Simple Custom Module
  * Modules as Building Blocks
  * Neural Network Training with Modules
  * Module State
  * Module Initialization
  * Module Hooks
  * Advanced Features
    * Distributed Training
    * Profiling Performance
    * Improving Performance with Quantization
    * Improving Memory Usage with Pruning
    * Parametrizations
    * Transforming Modules with FX


## A Simple Custom Module
To get started, let’s look at a simpler, custom version of PyTorch’s `Linear` module. This module applies an affine transformation to its input.
```
import torch
from torch import nn
class MyLinear(nn.Module):
 def __init__(self, in_features, out_features):
  super().__init__()
  self.weight = nn.Parameter(torch.randn(in_features, out_features))
  self.bias = nn.Parameter(torch.randn(out_features))
 def forward(self, input):
  return (input @ self.weight) + self.bias

```
Copy to clipboard
This simple module has the following fundamental characteristics of modules:
  * **It inherits from the base Module class.** All modules should subclass `Module` for composability with other modules.
  * **It defines some “state” that is used in computation.** Here, the state consists of randomly-initialized `weight` and `bias` tensors that define the affine transformation. Because each of these is defined as a `Parameter`, they are _registered_ for the module and will automatically be tracked and returned from calls to `parameters()`. Parameters can be considered the “learnable” aspects of the module’s computation (more on this later). Note that modules are not required to have state, and can also be stateless.
  * **It defines a forward() function that performs the computation.** For this affine transformation module, the input is matrix-multiplied with the `weight` parameter (using the `@` short-hand notation) and added to the `bias` parameter to produce the output. More generally, the `forward()` implementation for a module can perform arbitrary computation involving any number of inputs and outputs.


This simple module demonstrates how modules package state and computation together. Instances of this module can be constructed and called:
```
m = MyLinear(4, 3)
sample_input = torch.randn(4)
m(sample_input)
: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)

```
Copy to clipboard
Note that the module itself is callable, and that calling it invokes its `forward()` function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module. The “forward pass” is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it is not required to manually implement a `backward()` function for each module. The process of training module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules.
The full set of parameters registered by the module can be iterated through via a call to `parameters()` or `named_parameters()`, where the latter includes each parameter’s name:
```
for parameter in m.named_parameters():
 print(parameter)
: ('weight', Parameter containing:
tensor([[ 1.0597, 1.1796, 0.8247],
    [-0.5080, -1.2635, -1.1045],
    [ 0.0593, 0.2469, -1.4299],
    [-0.4926, -0.5457, 0.4793]], requires_grad=True))
('bias', Parameter containing:
tensor([ 0.3634, 0.2015, -0.8525], requires_grad=True))

```
Copy to clipboard
In general, the parameters registered by a module are aspects of the module’s computation that should be “learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers. Before we get to that, however, let’s first examine how modules can be composed with one another.
## Modules as Building Blocks
Modules can contain other modules, making them useful building blocks for developing more elaborate functionality. The simplest way to do this is using the `Sequential` module. It allows us to chain together multiple modules:
```
net = nn.Sequential(
 MyLinear(4, 3),
 nn.ReLU(),
 MyLinear(3, 1)
)
sample_input = torch.randn(4)
net(sample_input)
: tensor([-0.6749], grad_fn=<AddBackward0>)

```
Copy to clipboard
Note that `Sequential` automatically feeds the output of the first `MyLinear` module as input into the `ReLU`, and the output of that as input into the second `MyLinear` module. As shown, it is limited to in-order chaining of modules with a single input and output.
In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives full flexibility on how submodules are used for a module’s computation.
For example, here’s a simple neural network implemented as a custom module:
```
import torch.nn.functional as F
class Net(nn.Module):
 def __init__(self):
  super().__init__()
  self.l0 = MyLinear(4, 3)
  self.l1 = MyLinear(3, 1)
 def forward(self, x):
  x = self.l0(x)
  x = F.relu(x)
  x = self.l1(x)
  return x

```
Copy to clipboard
This module is composed of two “children” or “submodules” (`l0` and `l1`) that define the layers of the neural network and are utilized for computation within the module’s `forward()` method. Immediate children of a module can be iterated through via a call to `children()` or `named_children()`:
```
net = Net()
for child in net.named_children():
 print(child)
: ('l0', MyLinear())
('l1', MyLinear())

```
Copy to clipboard
To go deeper than just the immediate children, `modules()` and `named_modules()` _recursively_ iterate through a module and its child modules:
```
class BigNet(nn.Module):
 def __init__(self):
  super().__init__()
  self.l1 = MyLinear(5, 4)
  self.net = Net()
 def forward(self, x):
  return self.net(self.l1(x))
big_net = BigNet()
for module in big_net.named_modules():
 print(module)
: ('', BigNet(
 (l1): MyLinear()
 (net): Net(
  (l0): MyLinear()
  (l1): MyLinear()
 )
))
('l1', MyLinear())
('net', Net(
 (l0): MyLinear()
 (l1): MyLinear()
))
('net.l0', MyLinear())
('net.l1', MyLinear())

```
Copy to clipboard
Sometimes, it’s necessary for a module to dynamically define submodules. The `ModuleList` and `ModuleDict` modules are useful here; they register submodules from a list or dict:
```
class DynamicNet(nn.Module):
 def __init__(self, num_layers):
  super().__init__()
  self.linears = nn.ModuleList(
   [MyLinear(4, 4) for _ in range(num_layers)])
  self.activations = nn.ModuleDict({
   'relu': nn.ReLU(),
   'lrelu': nn.LeakyReLU()
  })
  self.final = MyLinear(4, 1)
 def forward(self, x, act):
  for linear in self.linears:
   x = linear(x)
   x = self.activations[act](x)
  x = self.final(x)
  return x
dynamic_net = DynamicNet(3)
sample_input = torch.randn(4)
output = dynamic_net(sample_input, 'relu')

```
Copy to clipboard
For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules. This means that calls to `parameters()` and `named_parameters()` will recursively include child parameters, allowing for convenient optimization of all parameters within the network:
```
for parameter in dynamic_net.named_parameters():
 print(parameter)
: ('linears.0.weight', Parameter containing:
tensor([[-1.2051, 0.7601, 1.1065, 0.1963],
    [ 3.0592, 0.4354, 1.6598, 0.9828],
    [-0.4446, 0.4628, 0.8774, 1.6848],
    [-0.1222, 1.5458, 1.1729, 1.4647]], requires_grad=True))
('linears.0.bias', Parameter containing:
tensor([ 1.5310, 1.0609, -2.0940, 1.1266], requires_grad=True))
('linears.1.weight', Parameter containing:
tensor([[ 2.1113, -0.0623, -1.0806, 0.3508],
    [-0.0550, 1.5317, 1.1064, -0.5562],
    [-0.4028, -0.6942, 1.5793, -1.0140],
    [-0.0329, 0.1160, -1.7183, -1.0434]], requires_grad=True))
('linears.1.bias', Parameter containing:
tensor([ 0.0361, -0.9768, -0.3889, 1.1613], requires_grad=True))
('linears.2.weight', Parameter containing:
tensor([[-2.6340, -0.3887, -0.9979, 0.0767],
    [-0.3526, 0.8756, -1.5847, -0.6016],
    [-0.3269, -0.1608, 0.2897, -2.0829],
    [ 2.6338, 0.9239, 0.6943, -1.5034]], requires_grad=True))
('linears.2.bias', Parameter containing:
tensor([ 1.0268, 0.4489, -0.9403, 0.1571], requires_grad=True))
('final.weight', Parameter containing:
tensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))
('final.bias', Parameter containing:
tensor([0.3381], requires_grad=True))

```
Copy to clipboard
It’s also easy to move all parameters to a different device or change their precision using `to()`:
```
# Move all parameters to a CUDA device
dynamic_net.to(device='cuda')
# Change precision of all parameters
dynamic_net.to(dtype=torch.float64)
dynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))
: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)

```
Copy to clipboard
More generally, an arbitrary function can be applied to a module and its submodules recursively by using the `apply()` function. For example, to apply custom initialization to parameters of a module and its submodules:
```
# Define a function to initialize Linear weights.
# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.
@torch.no_grad()
def init_weights(m):
 if isinstance(m, nn.Linear):
  nn.init.xavier_normal_(m.weight)
  m.bias.fill_(0.0)
# Apply the function recursively on the module and its submodules.
dynamic_net.apply(init_weights)

```
Copy to clipboard
These examples show how elaborate neural networks can be formed through module composition and conveniently manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of performant modules within the `torch.nn` namespace that perform common neural network operations like pooling, convolutions, loss functions, etc.
In the next section, we give a full example of training a neural network.
For more information, check out:
  * Library of PyTorch-provided modules: torch.nn
  * Defining neural net modules: https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html


## Neural Network Training with Modules
Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s Optimizers from `torch.optim`:
```
# Create the network (from previous section) and optimizer
net = Net()
optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)
# Run a sample training loop that "teaches" the network
# to output the constant zero function
for _ in range(10000):
 input = torch.randn(4)
 output = net(input)
 loss = torch.abs(output)
 net.zero_grad()
 loss.backward()
 optimizer.step()
# After training, switch the module to eval mode to do inference, compute performance metrics, etc.
# (see discussion below for a description of training and evaluation modes)
...
net.eval()
...

```
Copy to clipboard
In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according to its absolute value by employing `torch.abs()` as a loss function. While this is not a very interesting task, the key parts of training are present:
  * A network is created.
  * An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s parameters are associated with it.
  * 

A training loop…
    
    * acquires an input,
    * runs the network,
    * computes a loss,
    * zeros the network’s parameters’ gradients,
    * calls loss.backward() to update the parameters’ gradients,
    * calls optimizer.step() to apply the gradients to the parameters.


After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the value of `l1`‘s `weight` parameter shows that its values are now much closer to 0 (as may be expected):
```
print(net.l1.weight)
: Parameter containing:
tensor([[-0.0013],
    [ 0.0030],
    [-0.0008]], requires_grad=True)

```
Copy to clipboard
Note that the above process is done entirely while the network module is in “training mode”. Modules default to training mode and can be switched between training and evaluation modes using `train()` and `eval()`. They can behave differently depending on which mode they are in. For example, the `BatchNorm` module maintains a running mean and variance during training that are not updated when the module is in evaluation mode. In general, modules should be in training mode during training and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module that behaves differently between the two modes:
```
class ModalModule(nn.Module):
 def __init__(self):
  super().__init__()
 def forward(self, x):
  if self.training:
   # Add a constant only in training mode.
   return x + 1.
  else:
   return x

m = ModalModule()
x = torch.randn(4)
print('training mode output: {}'.format(m(x)))
: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])
m.eval()
print('evaluation mode output: {}'.format(m(x)))
: tensor([ 0.6614, 0.2669, 0.0617, 0.6213, -0.4519])

```
Copy to clipboard
Training neural networks can often be tricky. For more information, check out:
  * Using Optimizers: https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.
  * Neural network training: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html
  * Introduction to autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html


## Module State
In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation. Now, if we want to save the trained model to disk, we can do so by saving its `state_dict` (i.e. “state dictionary”):
```
# Save the module
torch.save(net.state_dict(), 'net.pt')
...
# Load the module later on
new_net = Net()
new_net.load_state_dict(torch.load('net.pt'))
: <All keys matched successfully>

```
Copy to clipboard
A module’s `state_dict` contains state that affects its computation. This includes, but is not limited to, the module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent” and “non-persistent”. Following is an overview of the various types of state a module can have:
  * **Parameters** : learnable aspects of computation; contained within the `state_dict`
  * **Buffers** : non-learnable aspects of computation
    * **Persistent** buffers: contained within the `state_dict` (i.e. serialized when saving & loading)
    * **Non-persistent** buffers: not contained within the `state_dict` (i.e. left out of serialization)


As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want the current value of the running mean to be considered part of the module’s `state_dict` so that it will be restored when loading a serialized form of the module, but we don’t want it to be learnable. This snippet shows how to use `register_buffer()` to accomplish this:
```
class RunningMean(nn.Module):
 def __init__(self, num_features, momentum=0.9):
  super().__init__()
  self.momentum = momentum
  self.register_buffer('mean', torch.zeros(num_features))
 def forward(self, x):
  self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x
  return self.mean

```
Copy to clipboard
Now, the current value of the running mean is considered part of the module’s `state_dict` and will be properly restored when loading the module from disk:
```
m = RunningMean(4)
for _ in range(10):
 input = torch.randn(4)
 m(input)
print(m.state_dict())
: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647, 0.1515]))]))
# Serialized form will contain the 'mean' tensor
torch.save(m.state_dict(), 'mean.pt')
m_loaded = RunningMean(4)
m_loaded.load_state_dict(torch.load('mean.pt'))
assert(torch.all(m.mean == m_loaded.mean))

```
Copy to clipboard
As mentioned previously, buffers can be left out of the module’s `state_dict` by marking them as non-persistent:
```
self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)

```
Copy to clipboard
Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with `to()`:
```
# Moves all module parameters and buffers to the specified device / dtype
m.to(device='cuda', dtype=torch.float64)

```
Copy to clipboard
Buffers of a module can be iterated over using `buffers()` or `named_buffers()`.
```
for buffer in m.named_buffers():
 print(buffer)

```
Copy to clipboard
The following class demonstrates the various ways of registering parameters and buffers within a module:
```
class StatefulModule(nn.Module):
 def __init__(self):
  super().__init__()
  # Setting a nn.Parameter as an attribute of the module automatically registers the tensor
  # as a parameter of the module.
  self.param1 = nn.Parameter(torch.randn(2))
  # Alternative string-based way to register a parameter.
  self.register_parameter('param2', nn.Parameter(torch.randn(3)))
  # Reserves the "param3" attribute as a parameter, preventing it from being set to anything
  # except a parameter. "None" entries like this will not be present in the module's state_dict.
  self.register_parameter('param3', None)
  # Registers a list of parameters.
  self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])
  # Registers a dictionary of parameters.
  self.param_dict = nn.ParameterDict({
   'foo': nn.Parameter(torch.randn(3)),
   'bar': nn.Parameter(torch.randn(4))
  })
  # Registers a persistent buffer (one that appears in the module's state_dict).
  self.register_buffer('buffer1', torch.randn(4), persistent=True)
  # Registers a non-persistent buffer (one that does not appear in the module's state_dict).
  self.register_buffer('buffer2', torch.randn(5), persistent=False)
  # Reserves the "buffer3" attribute as a buffer, preventing it from being set to anything
  # except a buffer. "None" entries like this will not be present in the module's state_dict.
  self.register_buffer('buffer3', None)
  # Adding a submodule registers its parameters as parameters of the module.
  self.linear = nn.Linear(2, 3)
m = StatefulModule()
# Save and load state_dict.
torch.save(m.state_dict(), 'state.pt')
m_loaded = StatefulModule()
m_loaded.load_state_dict(torch.load('state.pt'))
# Note that non-persistent buffer "buffer2" and reserved attributes "param3" and "buffer3" do
# not appear in the state_dict.
print(m_loaded.state_dict())
: OrderedDict([('param1', tensor([-0.0322, 0.9066])),
        ('param2', tensor([-0.4472, 0.1409, 0.4852])),
        ('buffer1', tensor([ 0.6949, -0.1944, 1.2911, -2.1044])),
        ('param_list.0', tensor([ 0.4202, -0.1953])),
        ('param_list.1', tensor([ 1.5299, -0.8747])),
        ('param_list.2', tensor([-1.6289, 1.4898])),
        ('param_dict.bar', tensor([-0.6434, 1.5187, 0.0346, -0.4077])),
        ('param_dict.foo', tensor([-0.0845, -1.4324, 0.7022])),
        ('linear.weight', tensor([[-0.3915, -0.6176],
                     [ 0.6062, -0.5992],
                     [ 0.4452, -0.2843]])),
        ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))])

```
Copy to clipboard
For more information, check out:
  * Saving and loading: https://pytorch.org/tutorials/beginner/saving_loading_models.html
  * Serialization semantics: https://pytorch.org/docs/main/notes/serialization.html
  * What is a state dict? https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html


## Module Initialization
By default, parameters and floating-point buffers for modules provided by `torch.nn` are initialized during module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to perform well historically for the module type. For certain use cases, it may be desired to initialize with a different dtype, device (e.g. GPU), or initialization technique.
Examples:
```
# Initialize module directly onto GPU.
m = nn.Linear(5, 3, device='cuda')
# Initialize module with 16-bit floating point parameters.
m = nn.Linear(5, 3, dtype=torch.half)
# Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.
m = torch.nn.utils.skip_init(nn.Linear, 5, 3)
nn.init.orthogonal_(m.weight)

```
Copy to clipboard
Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered for the module:
```
m = nn.BatchNorm2d(3, dtype=torch.half)
print(m.running_mean)
: tensor([0., 0., 0.], dtype=torch.float16)

```
Copy to clipboard
While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is to use `dtype=torch.float` and `device='cpu'` by default as well. Optionally, you can provide full flexibility in these areas for your custom module by conforming to the convention demonstrated above that all `torch.nn` modules follow:
  * Provide a `device` constructor kwarg that applies to any parameters / buffers registered by the module.
  * Provide a `dtype` constructor kwarg that applies to any parameters / floating-point buffers registered by the module.
  * Only use initialization functions (i.e. functions from `torch.nn.init`) on parameters and buffers within the module’s constructor. Note that this is only required to use `skip_init()`; see this page for an explanation.


For more information, check out:
  * Skipping module parameter initialization: https://pytorch.org/tutorials/prototype/skip_param_init.html


## Module Hooks
In Neural Network Training with Modules, we demonstrated the training process for a module, which iteratively performs forward and backward passes, updating module parameters each iteration. For more control over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward pass, even modifying how the pass is done if desired. Some useful examples for this functionality include debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules.
PyTorch provides two types of hooks for modules:
  * **Forward hooks** are called during the forward pass. They can be installed for a given module with `register_forward_pre_hook()` and `register_forward_hook()`. These hooks will be called respectively just before the forward function is called and just after it is called. Alternatively, these hooks can be installed globally for all modules with the analogous `register_module_forward_pre_hook()` and `register_module_forward_hook()` functions.
  * **Backward hooks** are called during the backward pass. They can be installed with `register_full_backward_pre_hook()` and `register_full_backward_hook()`. These hooks will be called when the backward for this Module has been computed. `register_full_backward_pre_hook()` will allow the user to access the gradients for outputs while `register_full_backward_hook()` will allow the user to access the gradients both the inputs and outputs. Alternatively, they can be installed globally for all modules with `register_module_full_backward_hook()` and `register_module_full_backward_pre_hook()`.


All hooks allow the user to return an updated value that will be used throughout the remaining computation. Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or modify some inputs/outputs without having to change the module’s `forward()` function.
Below is an example demonstrating usage of forward and backward hooks:
```
torch.manual_seed(1)
def forward_pre_hook(m, inputs):
 # Allows for examination and modification of the input before the forward pass.
 # Note that inputs are always wrapped in a tuple.
 input = inputs[0]
 return input + 1.
def forward_hook(m, inputs, output):
 # Allows for examination of inputs / outputs and modification of the outputs
 # after the forward pass. Note that inputs are always wrapped in a tuple while outputs
 # are passed as-is.
 # Residual computation a la ResNet.
 return output + inputs[0]
def backward_hook(m, grad_inputs, grad_outputs):
 # Allows for examination of grad_inputs / grad_outputs and modification of
 # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and
 # grad_outputs are always wrapped in tuples.
 new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]
 return new_grad_inputs
# Create sample module & input.
m = nn.Linear(3, 3)
x = torch.randn(2, 3, requires_grad=True)
# ==== Demonstrate forward hooks. ====
# Run input through module before and after adding hooks.
print('output with no forward hooks: {}'.format(m(x)))
: output with no forward hooks: tensor([[-0.5059, -0.8158, 0.2390],
                    [-0.0043, 0.4724, -0.1714]], grad_fn=<AddmmBackward>)
# Note that the modified input results in a different output.
forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)
print('output with forward pre hook: {}'.format(m(x)))
: output with forward pre hook: tensor([[-0.5752, -0.7421, 0.4942],
                    [-0.0736, 0.5461, 0.0838]], grad_fn=<AddmmBackward>)
# Note the modified output.
forward_hook_handle = m.register_forward_hook(forward_hook)
print('output with both forward hooks: {}'.format(m(x)))
: output with both forward hooks: tensor([[-1.0980, 0.6396, 0.4666],
                     [ 0.3634, 0.6538, 1.0256]], grad_fn=<AddBackward0>)
# Remove hooks; note that the output here matches the output before adding hooks.
forward_pre_hook_handle.remove()
forward_hook_handle.remove()
print('output after removing forward hooks: {}'.format(m(x)))
: output after removing forward hooks: tensor([[-0.5059, -0.8158, 0.2390],
                        [-0.0043, 0.4724, -0.1714]], grad_fn=<AddmmBackward>)
# ==== Demonstrate backward hooks. ====
m(x).sum().backward()
print('x.grad with no backwards hook: {}'.format(x.grad))
: x.grad with no backwards hook: tensor([[ 0.4497, -0.5046, 0.3146],
                     [ 0.4497, -0.5046, 0.3146]])
# Clear gradients before running backward pass again.
m.zero_grad()
x.grad.zero_()
m.register_full_backward_hook(backward_hook)
m(x).sum().backward()
print('x.grad with backwards hook: {}'.format(x.grad))
: x.grad with backwards hook: tensor([[42., 42., 42.],
                   [42., 42., 42.]])

```
Copy to clipboard
## Advanced Features
PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities are available for custom-written modules, with the small caveat that certain features may require modules to conform to particular constraints in order to be supported. In-depth discussion of these features and the corresponding requirements can be found in the links below.
### Distributed Training
Various methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs as well as training across multiple machines. Check out the distributed training overview page for detailed information on how to utilize these.
### Profiling Performance
The PyTorch Profiler can be useful for identifying performance bottlenecks within your models. It measures and outputs performance characteristics for both memory usage and time spent.
### Improving Performance with Quantization
Applying quantization techniques to modules can improve performance and memory usage by utilizing lower bitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantization here.
### Improving Memory Usage with Pruning
Large deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch provides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. The Pruning tutorial describes how to utilize the pruning techniques PyTorch provides or define custom pruning techniques as necessary.
### Parametrizations
For certain applications, it can be beneficial to constrain the parameter space during model training. For example, enforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for applying parametrizations such as this, and further allows for custom constraints to be defined.
### Transforming Modules with FX
The FX component of PyTorch provides a flexible way to transform modules by operating directly on module computation graphs. This can be used to programmatically generate or manipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX for convolution + batch norm fusion and CPU performance analysis.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Modules
    * A Simple Custom Module
    * Modules as Building Blocks
    * Neural Network Training with Modules
    * Module State
    * Module Initialization
    * Module Hooks
    * Advanced Features
      * Distributed Training
      * Profiling Performance
      * Improving Performance with Quantization
      * Improving Memory Usage with Pruning
      * Parametrizations
      * Transforming Modules with FX


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * MPS backend
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# MPS backend
`mps` device enables high-performance training on GPU for MacOS devices with Metal programming framework. It introduces a new device to map Machine Learning computational graphs and primitives on highly efficient Metal Performance Shaders Graph framework and tuned kernels provided by Metal Performance Shaders framework respectively.
The new MPS backend extends the PyTorch ecosystem and provides existing scripts capabilities to setup and run operations on GPU.
To get started, simply move your Tensor and Module to the `mps` device:
```
# Check that MPS is available
if not torch.backends.mps.is_available():
  if not torch.backends.mps.is_built():
    print("MPS not available because the current PyTorch install was not "
       "built with MPS enabled.")
  else:
    print("MPS not available because the current MacOS version is not 12.3+ "
       "and/or you do not have an MPS-enabled device on this machine.")
else:
  mps_device = torch.device("mps")
  # Create a Tensor directly on the mps device
  x = torch.ones(5, device=mps_device)
  # Or
  x = torch.ones(5, device="mps")
  # Any operation happens on the GPU
  y = x * 2
  # Move your model to mps just like any other device
  model = YourFavoriteNet()
  model.to(mps_device)
  # Now every call runs on the GPU
  pred = model(x)

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * MPS backend


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Multiprocessing best practices
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Multiprocessing best practices
`torch.multiprocessing` is a drop in replacement for Python’s `multiprocessing` module. It supports the exact same operations, but extends it, so that all tensors sent through a `multiprocessing.Queue`, will have their data moved into shared memory and will only send a handle to another process.
Note
When a `Tensor` is sent to another process, the `Tensor` data is shared. If `torch.Tensor.grad` is not `None`, it is also shared. After a `Tensor` without a `torch.Tensor.grad` field is sent to the other process, it creates a standard process-specific `.grad` `Tensor` that is not automatically shared across all processes, unlike how the `Tensor`’s data has been shared.
This allows to implement various training methods, like Hogwild, A3C, or any others that require asynchronous operation.
## CUDA in multiprocessing
The CUDA runtime does not support the `fork` start method; either the `spawn` or `forkserver` start method are required to use CUDA in subprocesses.
Note
The start method can be set via either creating a context with `multiprocessing.get_context(...)` or directly using `multiprocessing.set_start_method(...)`.
Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. It is implemented under the hood but requires users to follow the best practices for the program to run correctly. For example, the sending process must stay alive as long as the consumer process has references to the tensor, and the refcounting can not save you if the consumer process exits abnormally via a fatal signal. See this section.
See also: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel
## Best practices and tips
### Avoiding and fighting deadlocks
There are a lot of things that can go wrong when a new process is spawned, with the most common cause of deadlocks being background threads. If there’s any thread that holds a lock or imports a module, and `fork` is called, it’s very likely that the subprocess will be in a corrupted state and will deadlock or fail in a different way. Note that even if you don’t, Python built in libraries do - no need to look further than `multiprocessing`. `multiprocessing.Queue` is actually a very complex class, that spawns multiple threads used to serialize, send and receive objects, and they can cause aforementioned problems too. If you find yourself in such situation try using a `SimpleQueue`, that doesn’t use any additional threads.
We’re trying our best to make it easy for you and ensure these deadlocks don’t happen but some things are out of our control. If you have any issues you can’t cope with for a while, try reaching out on forums, and we’ll see if it’s an issue we can fix.
### Reuse buffers passed through a Queue
Remember that each time you put a `Tensor` into a `multiprocessing.Queue`, it has to be moved into shared memory. If it’s already shared, it is a no-op, otherwise it will incur an additional memory copy that can slow down the whole process. Even if you have a pool of processes sending data to a single one, make it send the buffers back - this is nearly free and will let you avoid a copy when sending next batch.
### Asynchronous multiprocess training (e.g. Hogwild)
Using `torch.multiprocessing`, it is possible to train a model asynchronously, with parameters either shared all the time, or being periodically synchronized. In the first case, we recommend sending over the whole model object, while in the latter, we advise to only send the `state_dict()`.
We recommend using `multiprocessing.Queue` for passing all kinds of PyTorch objects between processes. It is possible to e.g. inherit the tensors and storages already in shared memory, when using the `fork` start method, however it is very bug prone and should be used with care, and only by advanced users. Queues, even though they’re sometimes a less elegant solution, will work properly in all cases.
Warning
You should be careful about having global statements, that are not guarded with an `if __name__ == '__main__'`. If a different start method than `fork` is used, they will be executed in all subprocesses.
#### Hogwild
A concrete Hogwild implementation can be found in the examples repository, but to showcase the overall structure of the code, there’s also a minimal example below as well:
```
import torch.multiprocessing as mp
from model import MyModel
def train(model):
  # Construct data_loader, optimizer, etc.
  for data, labels in data_loader:
    optimizer.zero_grad()
    loss_fn(model(data), labels).backward()
    optimizer.step() # This will update the shared parameters
if __name__ == '__main__':
  num_processes = 4
  model = MyModel()
  # NOTE: this is required for the ``fork`` method to work
  model.share_memory()
  processes = []
  for rank in range(num_processes):
    p = mp.Process(target=train, args=(model,))
    p.start()
    processes.append(p)
  for p in processes:
    p.join()

```
Copy to clipboard
## CPU in multiprocessing
Inappropriate multiprocessing can lead to CPU oversubscription, causing different processes to compete for CPU resources, resulting in low efficiency.
This tutorial will explain what CPU oversubscription is and how to avoid it.
### CPU oversubscription
CPU oversubscription is a technical term that refers to a situation where the total number of vCPUs allocated to a system exceeds the total number of vCPUs available on the hardware.
This leads to severe contention for CPU resources. In such cases, there is frequent switching between processes, which increases processes switching overhead and decreases overall system efficiency.
See CPU oversubscription with the code examples in the Hogwild implementation found in the example repository.
When running the training example with the following command on CPU using 4 processes:
```
pythonmain.py--num-processes4

```
Copy to clipboard
Assuming there are N vCPUs available on the machine, executing the above command will generate 4 subprocesses. Each subprocess will allocate N vCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the machine only has N vCPUs available. Consequently, the different processes will compete for resources, leading to frequent process switching.
The following observations indicate the presence of CPU over subscription:
  1. High CPU Utilization: By using the `htop` command, you can observe that the CPU utilization is consistently high, often reaching or exceeding its maximum capacity. This indicates that the demand for CPU resources exceeds the available physical cores, causing contention and competition among processes for CPU time.
  2. Frequent Context Switching with Low System Efficiency: In an oversubscribed CPU scenario, processes compete for CPU time, and the operating system needs to rapidly switch between different processes to allocate resources fairly. This frequent context switching adds overhead and reduces the overall system efficiency.


### Avoid CPU oversubscription
A good way to avoid CPU oversubscription is proper resource allocation. Ensure that the number of processes or threads running concurrently does not exceed the available CPU resources.
In this case, a solution would be to specify the appropriate number of threads in the subprocesses. This can be achieved by setting the number of threads for each process using the `torch.set_num_threads(int)` function in subprocess.
Assuming there are N vCPUs on the machine and M processes will be generated, the maximum `num_threads` value used by each process would be `floor(N/M)`. To avoid CPU oversubscription in the mnist_hogwild example, the following changes are needed for the file `train.py` in example repository.
```
def train(rank, args, model, device, dataset, dataloader_kwargs):
  torch.manual_seed(args.seed + rank)
  #### define the num threads used in current sub-processes
  torch.set_num_threads(floor(N/M))
  train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)
  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)
  for epoch in range(1, args.epochs + 1):
    train_epoch(epoch, args, model, device, train_loader, optimizer)

```
Copy to clipboard
Set `num_thread` for each process using `torch.set_num_threads(floor(N/M))`. where you replace N with the number of vCPUs available and M with the chosen number of processes. The appropriate `num_thread` value will vary depending on the specific task at hand. However, as a general guideline, the maximum value for the `num_thread` should be `floor(N/M)` to avoid CPU oversubscription. In the mnist_hogwild training example, after avoiding CPU over subscription, you can achieve a 30x performance boost.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Multiprocessing best practices
    * CUDA in multiprocessing
    * Best practices and tips
      * Avoiding and fighting deadlocks
      * Reuse buffers passed through a Queue
      * Asynchronous multiprocess training (e.g. Hogwild)
        * Hogwild
    * CPU in multiprocessing
      * CPU oversubscription
      * Avoid CPU oversubscription


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Windows FAQ
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Windows FAQ
## Building from source
### Include optional components
There are two supported components for Windows PyTorch: MKL and MAGMA. Here are the steps to build with them.
```
REM Make sure you have 7z and curl installed.
REM Download MKL files
curl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O
7z x -aoa mkl_2020.2.254.7z -omkl
REM Download MAGMA files
REM version available:
REM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)
REM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)
REM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)
REM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)
set CUDA_PREFIX=cuda102
set CONFIG=release
curl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z -o magma.7z
7z x -aoa magma.7z -omagma
REM Setting essential environment variables
set "CMAKE_INCLUDE_PATH=%cd%\mkl\include"
set "LIB=%cd%\mkl\lib;%LIB%"
set "MAGMA_HOME=%cd%\magma"

```
Copy to clipboard
### Speeding CUDA build for Windows
Visual Studio doesn’t support parallel custom task currently. As an alternative, we can use `Ninja` to parallelize CUDA build tasks. It can be used by typing only a few lines of code.
```
REM Let's install ninja first.
pip install ninja
REM Set it as the cmake generator
set CMAKE_GENERATOR=Ninja

```
Copy to clipboard
### One key install script
You can take a look at this set of scripts. It will lead the way for you.
## Extension
### CFFI Extension
The support for CFFI Extension is very experimental. You must specify additional `libraries` in `Extension` object to make it build on Windows.
```
ffi = create_extension(
  '_ext.my_lib',
  headers=headers,
  sources=sources,
  define_macros=defines,
  relative_to=__file__,
  with_cuda=with_cuda,
  extra_compile_args=["-std=c99"],
  libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart
)

```
Copy to clipboard
### Cpp Extension
This type of extension has better support compared with the previous one. However, it still needs some manual configuration. First, you should open the **x86_x64 Cross Tools Command Prompt for VS 2017**. And then, you can start your compiling process.
## Installation
### Package not found in win-32 channel.
```
Solving environment: failed
PackagesNotFoundError: The following packages are not available from current channels:
- pytorch
Current channels:
- https://conda.anaconda.org/pytorch/win-32
- https://conda.anaconda.org/pytorch/noarch
- https://repo.continuum.io/pkgs/main/win-32
- https://repo.continuum.io/pkgs/main/noarch
- https://repo.continuum.io/pkgs/free/win-32
- https://repo.continuum.io/pkgs/free/noarch
- https://repo.continuum.io/pkgs/r/win-32
- https://repo.continuum.io/pkgs/r/noarch
- https://repo.continuum.io/pkgs/pro/win-32
- https://repo.continuum.io/pkgs/pro/noarch
- https://repo.continuum.io/pkgs/msys2/win-32
- https://repo.continuum.io/pkgs/msys2/noarch

```
Copy to clipboard
PyTorch doesn’t work on 32-bit system. Please use Windows and Python 64-bit version.
### Import error
```
from torch._C import *
ImportError: DLL load failed: The specified module could not be found.

```
Copy to clipboard
The problem is caused by the missing of the essential files. Actually, we include almost all the essential files that PyTorch need for the conda package except VC2017 redistributable and some mkl libraries. You can resolve this by typing the following command.
```
conda install -c peterjc123 vc vs2017_runtime
conda install mkl_fft intel_openmp numpy mkl

```
Copy to clipboard
As for the wheels package, since we didn’t pack some libraries and VS2017 redistributable files in, please make sure you install them manually. The VS 2017 redistributable installer can be downloaded. And you should also pay attention to your installation of Numpy. Make sure it uses MKL instead of OpenBLAS. You may type in the following command.
```
pip install numpy mkl intel-openmp mkl_fft

```
Copy to clipboard
Another possible cause may be you are using GPU version without NVIDIA graphics cards. Please replace your GPU package with the CPU one.
```
from torch._C import *
ImportError: DLL load failed: The operating system cannot run %1.

```
Copy to clipboard
This is actually an upstream issue of Anaconda. When you initialize your environment with conda-forge channel, this issue will emerge. You may fix the intel-openmp libraries through this command.
```
conda install -c defaults intel-openmp -f

```
Copy to clipboard
## Usage (multiprocessing)
### Multiprocessing error without if-clause protection
```
RuntimeError:
    An attempt has been made to start a new process before the
    current process has finished its bootstrapping phase.
  This probably means that you are not using fork to start your
  child processes and you have forgotten to use the proper idiom
  in the main module:
    if __name__ == '__main__':
      freeze_support()
      ...
  The "freeze_support()" line can be omitted if the program
  is not going to be frozen to produce an executable.

```
Copy to clipboard
The implementation of `multiprocessing` is different on Windows, which uses `spawn` instead of `fork`. So we have to wrap the code with an if-clause to protect the code from executing multiple times. Refactor your code into the following structure.
```
import torch
def main()
  for i, data in enumerate(dataloader):
    # do something here
if __name__ == '__main__':
  main()

```
Copy to clipboard
### Multiprocessing error “Broken pipe”
```
ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe

```
Copy to clipboard
This issue happens when the child process ends before the parent process finishes sending data. There may be something wrong with your code. You can debug your code by reducing the `num_worker` of `DataLoader` to zero and see if the issue persists.
### Multiprocessing error “driver shut down”
```
Couldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\lib\TH\THAllocator.c:154
[windows] driver shut down

```
Copy to clipboard
Please update your graphics driver. If this persists, this may be that your graphics card is too old or the calculation is too heavy for your card. Please update the TDR settings according to this post.
### CUDA IPC operations
```
THCudaCheck FAIL file=torch\csrc\generic\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS

```
Copy to clipboard
They are not supported on Windows. Something like doing multiprocessing on CUDA tensors cannot succeed, there are two alternatives for this.
1. Don’t use `multiprocessing`. Set the `num_worker` of `DataLoader` to zero.
2. Share CPU tensors instead. Make sure your custom `DataSet` returns CPU tensors.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Windows FAQ
    * Building from source
      * Include optional components
      * Speeding CUDA build for Windows
      * One key install script
    * Extension
      * CFFI Extension
      * Cpp Extension
    * Installation
      * Package not found in win-32 channel.
      * Import error
    * Usage (multiprocessing)
      * Multiprocessing error without if-clause protection
      * Multiprocessing error “Broken pipe”
      * Multiprocessing error “driver shut down”
      * CUDA IPC operations


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/py-modindex.html) ![](https://pytorch.org/docs/stable/py-modindex.html)


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.onnx
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.onnx
## Overview
Open Neural Network eXchange (ONNX) is an open standard format for representing machine learning models. The `torch.onnx` module captures the computation graph from a native PyTorch `torch.nn.Module` model and converts it into an ONNX graph.
The exported model can be consumed by any of the many runtimes that support ONNX, including Microsoft’s ONNX Runtime.
**There are two flavors of ONNX exporter API that you can use, as listed below.** Both can be called through function `torch.onnx.export()`. Next example shows how to export a simple model.
```
import torch
class MyModel(torch.nn.Module):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = torch.nn.Conv2d(1, 128, 5)
  def forward(self, x):
    return torch.relu(self.conv1(x))
input_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32)
model = MyModel()
torch.onnx.export(
  model,         # model to export
  (input_tensor,),    # inputs of the model,
  "my_model.onnx",    # filename of the ONNX model
  input_names=["input"], # Rename inputs for the ONNX model
  dynamo=True       # True or False to select the exporter to use
)

```
Copy to clipboard
Next sections introduces the two versions of the exporter.
## TorchDynamo-based ONNX Exporter
_The TorchDynamo-based ONNX exporter is the newest (and Beta) exporter for PyTorch 2.1 and newer_
TorchDynamo engine is leveraged to hook into Python’s frame evaluation API and dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then polished before it is finally translated into an ONNX graph.
The main advantage of this approach is that the FX graph is captured using bytecode analysis that preserves the dynamic nature of the model instead of using traditional static tracing techniques.
Learn more about the TorchDynamo-based ONNX Exporter
## TorchScript-based ONNX Exporter
_The TorchScript-based ONNX exporter is available since PyTorch 1.2.0_
TorchScript is leveraged to trace (through `torch.jit.trace()`) the model and capture a static computation graph.
As a consequence, the resulting graph has a couple limitations:
  * It does not record any control-flow, like if-statements or loops;
  * Does not handle nuances between `training` and `eval` mode;
  * Does not truly handle dynamic inputs


As an attempt to support the static tracing limitations, the exporter also supports TorchScript scripting (through `torch.jit.script()`), which adds support for data-dependent control-flow, for example. However, TorchScript itself is a subset of the Python language, so not all features in Python are supported, such as in-place operations.
Learn more about the TorchScript-based ONNX Exporter
## Contributing / Developing
The ONNX exporter is a community project and we welcome contributions. We follow the PyTorch guidelines for contributions, but you might also be interested in reading our development wiki.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.onnx
    * Overview
    * TorchDynamo-based ONNX Exporter
    * TorchScript-based ONNX Exporter
    * Contributing / Developing


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.optim
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.optim
`torch.optim` is a package implementing various optimization algorithms.
Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.
## How to use an optimizer
To use `torch.optim` you have to construct an optimizer object that will hold the current state and will update the parameters based on the computed gradients.
### Constructing it
To construct an `Optimizer` you have to give it an iterable containing the parameters (all should be `Parameter` s) or named parameters (tuples of (str, `Parameter`)) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.
Example:
```
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)

```
Copy to clipboard
Named parameters example:
```
optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)

```
Copy to clipboard
### Per-parameter options
`Optimizer` s also support specifying per-parameter options. To do this, instead of passing an iterable of `Variable` s, pass in an iterable of `dict` s. Each of them will define a separate parameter group, and should contain a `params` key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.
For example, this is very useful when one wants to specify per-layer learning rates:
```
optim.SGD([
        {'params': model.base.parameters(), 'lr': 1e-2},
        {'params': model.classifier.parameters()}
      ], lr=1e-3, momentum=0.9)
optim.SGD([
        {'params': model.base.named_parameters(), 'lr': 1e-2},
        {'params': model.classifier.named_parameters()}
      ], lr=1e-3, momentum=0.9)

```
Copy to clipboard
This means that `model.base`’s parameters will use a learning rate of `1e-2`, whereas `model.classifier`’s parameters will stick to the default learning rate of `1e-3`. Finally a momentum of `0.9` will be used for all parameters.
Note
You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn’t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.
Also consider the following example related to the distinct penalization of parameters. Remember that `parameters()` returns an iterable that contains all learnable parameters, including biases and other parameters that may prefer distinct penalization. To address this, one can specify individual penalization weights for each parameter group:
```
bias_params = [p for name, p in self.named_parameters() if 'bias' in name]
others = [p for name, p in self.named_parameters() if 'bias' not in name]
optim.SGD([
        {'params': others},
        {'params': bias_params, 'weight_decay': 0}
      ], weight_decay=1e-2, lr=1e-2)

```
Copy to clipboard
In this manner, bias terms are isolated from non-bias terms, and a `weight_decay` of `0` is set specifically for the bias terms, as to avoid any penalization for this group.
### Taking an optimization step
All optimizers implement a `step()` method, that updates the parameters. It can be used in two ways:
#### `optimizer.step()`
This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. `backward()`.
Example:
```
for input, target in dataset:
  optimizer.zero_grad()
  output = model(input)
  loss = loss_fn(output, target)
  loss.backward()
  optimizer.step()

```
Copy to clipboard
#### `optimizer.step(closure)`
Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.
Example:
```
for input, target in dataset:
  def closure():
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    return loss
  optimizer.step(closure)

```
Copy to clipboard
## Base class 

_class_ torch.optim.Optimizer(_params_ , _defaults_)[source][source]
    
Base class for all optimizers.
Warning
Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don’t satisfy those properties are sets and iterators over values of dictionaries. 

Parameters
    
  * **params** (_iterable_) – an iterable of `torch.Tensor` s or `dict` s. Specifies what Tensors should be optimized.
  * **defaults** (_dict_ _[__str_ _,__Any_ _]_) – (dict): a dict containing default values of optimization options (used when a parameter group doesn’t specify them).


`Optimizer.add_param_group` | Add a param group to the `Optimizer` s param_groups.  
---|---  
`Optimizer.load_state_dict` | Load the optimizer state.  
`Optimizer.register_load_state_dict_pre_hook` | Register a load_state_dict pre-hook which will be called before `load_state_dict()` is called. It should have the following signature::.  
`Optimizer.register_load_state_dict_post_hook` | Register a load_state_dict post-hook which will be called after `load_state_dict()` is called. It should have the following signature::.  
`Optimizer.state_dict` | Return the state of the optimizer as a `dict`.  
`Optimizer.register_state_dict_pre_hook` | Register a state dict pre-hook which will be called before `state_dict()` is called.  
`Optimizer.register_state_dict_post_hook` | Register a state dict post-hook which will be called after `state_dict()` is called.  
`Optimizer.step` | Perform a single optimization step to update parameter.  
`Optimizer.register_step_pre_hook` | Register an optimizer step pre hook which will be called before optimizer step.  
`Optimizer.register_step_post_hook` | Register an optimizer step post hook which will be called after optimizer step.  
`Optimizer.zero_grad` | Reset the gradients of all optimized `torch.Tensor` s.  
## Algorithms
`Adadelta` | Implements Adadelta algorithm.  
---|---  
`Adafactor` | Implements Adafactor algorithm.  
`Adagrad` | Implements Adagrad algorithm.  
`Adam` | Implements Adam algorithm.  
`AdamW` | Implements AdamW algorithm, where weight decay does not accumulate in the momentum nor variance.  
`SparseAdam` | SparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients.  
`Adamax` | Implements Adamax algorithm (a variant of Adam based on infinity norm).  
`ASGD` | Implements Averaged Stochastic Gradient Descent.  
`LBFGS` | Implements L-BFGS algorithm.  
`NAdam` | Implements NAdam algorithm.  
`RAdam` | Implements RAdam algorithm.  
`RMSprop` | Implements RMSprop algorithm.  
`Rprop` | Implements the resilient backpropagation algorithm.  
`SGD` | Implements stochastic gradient descent (optionally with momentum).  
Many of our algorithms have various implementations optimized for performance, readability and/or generality, so we attempt to default to the generally fastest implementation for the current device if no particular implementation has been specified by the user.
We have 3 major categories of implementations: for-loop, foreach (multi-tensor), and fused. The most straightforward implementations are for-loops over the parameters with big chunks of computation. For-looping is usually slower than our foreach implementations, which combine parameters into a multi-tensor and run the big chunks of computation all at once, thereby saving many sequential kernel calls. A few of our optimizers have even faster fused implementations, which fuse the big chunks of computation into one kernel. We can think of foreach implementations as fusing horizontally and fused implementations as fusing vertically on top of that.
In general, the performance ordering of the 3 implementations is fused > foreach > for-loop. So when applicable, we default to foreach over for-loop. Applicable means the foreach implementation is available, the user has not specified any implementation-specific kwargs (e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused should be even faster than foreach, the implementations are newer and we would like to give them more bake-in time before flipping the switch everywhere. We summarize the stability status for each implementation on the second table below, you are welcome to try them out though!
Below is a table showing the available and default implementations of each algorithm:
Algorithm | Default | Has foreach? | Has fused?  
---|---|---|---  
`Adadelta` | foreach | yes | no  
`Adafactor` | for-loop | no | no  
`Adagrad` | foreach | yes | yes (cpu only)  
`Adam` | foreach | yes | yes  
`AdamW` | foreach | yes | yes  
`SparseAdam` | for-loop | no | no  
`Adamax` | foreach | yes | no  
`ASGD` | foreach | yes | no  
`LBFGS` | for-loop | no | no  
`NAdam` | foreach | yes | no  
`RAdam` | foreach | yes | no  
`RMSprop` | foreach | yes | no  
`Rprop` | foreach | yes | no  
`SGD` | foreach | yes | yes  
Below table is showing the stability status for fused implementations:
Algorithm | CPU | CUDA | MPS  
---|---|---|---  
`Adadelta` | unsupported | unsupported | unsupported  
`Adafactor` | unsupported | unsupported | unsupported  
`Adagrad` | beta | unsupported | unsupported  
`Adam` | beta | stable | beta  
`AdamW` | beta | stable | beta  
`SparseAdam` | unsupported | unsupported | unsupported  
`Adamax` | unsupported | unsupported | unsupported  
`ASGD` | unsupported | unsupported | unsupported  
`LBFGS` | unsupported | unsupported | unsupported  
`NAdam` | unsupported | unsupported | unsupported  
`RAdam` | unsupported | unsupported | unsupported  
`RMSprop` | unsupported | unsupported | unsupported  
`Rprop` | unsupported | unsupported | unsupported  
`SGD` | beta | beta | beta  
## How to adjust learning rate
`torch.optim.lr_scheduler.LRScheduler` provides several methods to adjust the learning rate based on the number of epochs. `torch.optim.lr_scheduler.ReduceLROnPlateau` allows dynamic learning rate reducing based on some validation measurements.
Learning rate scheduling should be applied after optimizer’s update; e.g., you should write your code this way:
Example:
```
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = ExponentialLR(optimizer, gamma=0.9)
for epoch in range(20):
  for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
  scheduler.step()

```
Copy to clipboard
Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.
Example:
```
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)
for epoch in range(20):
  for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
  scheduler1.step()
  scheduler2.step()

```
Copy to clipboard
In many places in the documentation, we will use the following template to refer to schedulers algorithms.
```
>>> scheduler = ...
>>> for epoch in range(100):
>>>   train(...)
>>>   validate(...)
>>>   scheduler.step()

```
Copy to clipboard
Warning
Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling `scheduler.step()`) before the optimizer’s update (calling `optimizer.step()`), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling `scheduler.step()` at the wrong time.
`lr_scheduler.LRScheduler` | Adjusts the learning rate during optimization.  
---|---  
`lr_scheduler.LambdaLR` | Sets the initial learning rate.  
`lr_scheduler.MultiplicativeLR` | Multiply the learning rate of each parameter group by the factor given in the specified function.  
`lr_scheduler.StepLR` | Decays the learning rate of each parameter group by gamma every step_size epochs.  
`lr_scheduler.MultiStepLR` | Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.  
`lr_scheduler.ConstantLR` | Multiply the learning rate of each parameter group by a small constant factor.  
`lr_scheduler.LinearLR` | Decays the learning rate of each parameter group by linearly changing small multiplicative factor.  
`lr_scheduler.ExponentialLR` | Decays the learning rate of each parameter group by gamma every epoch.  
`lr_scheduler.PolynomialLR` | Decays the learning rate of each parameter group using a polynomial function in the given total_iters.  
`lr_scheduler.CosineAnnealingLR` | Set the learning rate of each parameter group using a cosine annealing schedule.  
`lr_scheduler.ChainedScheduler` | Chains a list of learning rate schedulers.  
`lr_scheduler.SequentialLR` | Contains a list of schedulers expected to be called sequentially during the optimization process.  
`lr_scheduler.ReduceLROnPlateau` | Reduce learning rate when a metric has stopped improving.  
`lr_scheduler.CyclicLR` | Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).  
`lr_scheduler.OneCycleLR` | Sets the learning rate of each parameter group according to the 1cycle learning rate policy.  
`lr_scheduler.CosineAnnealingWarmRestarts` | Set the learning rate of each parameter group using a cosine annealing schedule.  
## How to utilize named parameters to load optimizer state dict
The function `load_state_dict()` stores the optional `param_names` content from the loaded state dict if present. However, the process of loading the optimizer state is not affected, as the order of the parameters matters to maintain compatibility (in case of different ordering). To utilize the loaded parameters names from the loaded state dict, a custom `register_load_state_dict_pre_hook` needs to be implemented according to the desired behavior.
This can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to remain unchanged. The following example demonstrates how to implement this customization.
Example:
```
class OneLayerModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.fc = nn.Linear(3, 4)
  def forward(self, x):
    return self.fc(x)
model = OneLayerModel()
optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)
# training..
torch.save(optimizer.state_dict(), PATH)

```
Copy to clipboard
Let’s say that `model` implements an expert (MoE), and we want to duplicate it and resume training for two experts, both initialized the same way as the `fc` layer. For the following `model2` we create two layers identical to `fc` and resume training by loading the model weights and optimizer states from `model` into both `fc1` and `fc2` of `model2` (and adjust them accordingly):
```
class TwoLayerModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.fc1 = nn.Linear(3, 4)
    self.fc2 = nn.Linear(3, 4)
  def forward(self, x):
    return (self.fc1(x) + self.fc2(x)) / 2
model2 = TwoLayerModel()
# adapt and load model weights..
optimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)

```
Copy to clipboard
To load the state dict for `optimizer2` with the state dict of the previous optimizer such that both `fc1` and `fc2` will be initialized with a copy of `fc` optimizer states (to resume training for each layer from `fc`), we can use the following hook:
```
def adapt_state_dict_ids(optimizer, state_dict):
  adapted_state_dict = deepcopy(optimizer.state_dict())
  # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.
  for k, v in state_dict['param_groups'][0].items():
    if k not in ['params', 'param_names']:
      adapted_state_dict['param_groups'][0][k] = v
  lookup_dict = {
    'fc1.weight': 'fc.weight',
    'fc1.bias': 'fc.bias',
    'fc2.weight': 'fc.weight',
    'fc2.bias': 'fc.bias'
  }
  clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}
  for param_id, param_name in zip(
      optimizer.state_dict()['param_groups'][0]['params'],
      optimizer.state_dict()['param_groups'][0]['param_names']):
    name_in_loaded = lookup_dict[param_name]
    index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)
    id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]
    # Copy the state of the corresponding parameter
    if id_in_loaded in state_dict['state']:
      adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])
  return adapted_state_dict
optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)
optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict

```
Copy to clipboard
This ensures that the adapted state_dict with the correct states for the layers of `model2` will be used during model loading. Note that this code is designed specifically for this example (e.g., assuming a single parameter group), and other cases might require different adaptations.
The following example shows how to handle missing parameters in a loaded `state dict` when the model structure changes. The `Model_bypass` adds a new `bypass` layer, which is not present in the original `Model1`. To resume training, a custom `adapt_state_dict_missing_param` hook is used to adapt the optimizer’s `state_dict`, ensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged (as initialized in this example). This approach enables smooth loading and resuming of the optimizer state despite model changes. The new bypass layer will be trained from scratch:
```
class Model1(nn.Module):
  def __init__(self):
    super().__init__()
    self.fc = nn.Linear(5, 5)
  def forward(self, x):
    return self.fc(x) + x

model = Model1()
optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)
# training..
torch.save(optimizer.state_dict(), PATH)
class Model_bypass(nn.Module):
  def __init__(self):
    super().__init__()
    self.fc = nn.Linear(5, 5)
    self.bypass = nn.Linear(5, 5, bias=False)
    torch.nn.init.eye_(self.bypass.weight)
  def forward(self, x):
    return self.fc(x) + self.bypass(x)
model2 = Model_bypass()
optimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)
def adapt_state_dict_missing_param(optimizer, state_dict):
  adapted_state_dict = deepcopy(optimizer.state_dict())
  # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.
  for k, v in state_dict['param_groups'][0].items():
    if k not in ['params', 'param_names']:
      adapted_state_dict['param_groups'][0][k] = v
  lookup_dict = {
    'fc.weight': 'fc.weight',
    'fc.bias': 'fc.bias',
    'bypass.weight': None,
  }
  clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}
  for param_id, param_name in zip(
      optimizer.state_dict()['param_groups'][0]['params'],
      optimizer.state_dict()['param_groups'][0]['param_names']):
    name_in_loaded = lookup_dict[param_name]
    if name_in_loaded in state_dict['param_groups'][0]['param_names']:
      index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)
      id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]
      # Copy the state of the corresponding parameter
      if id_in_loaded in state_dict['state']:
        adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])
  return adapted_state_dict
optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)
optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict

```
Copy to clipboard
As a third example, instead of loading a state according to the order of parameters (the default approach), this hook can be used to load according to the parameters’ names:
```
def names_matching(optimizer, state_dict):
  assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups'])
  adapted_state_dict = deepcopy(optimizer.state_dict())
  for g_ind in range(len(state_dict['param_groups'])):
    assert len(state_dict['param_groups'][g_ind]['params']) == len(
      optimizer.state_dict()['param_groups'][g_ind]['params'])
    for k, v in state_dict['param_groups'][g_ind].items():
      if k not in ['params', 'param_names']:
        adapted_state_dict['param_groups'][g_ind][k] = v
    for param_id, param_name in zip(
        optimizer.state_dict()['param_groups'][g_ind]['params'],
        optimizer.state_dict()['param_groups'][g_ind]['param_names']):
      index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name)
      id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list]
      # Copy the state of the corresponding parameter
      if id_in_loaded in state_dict['state']:
        adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded])
  return adapted_state_dict

```
Copy to clipboard
## Weight Averaging (SWA and EMA)
`torch.optim.swa_utils.AveragedModel` implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA), `torch.optim.swa_utils.SWALR` implements the SWA learning rate scheduler and `torch.optim.swa_utils.update_bn()` is a utility function used to update SWA/EMA batch normalization statistics at the end of training.
SWA has been proposed in Averaging Weights Leads to Wider Optima and Better Generalization.
EMA is a widely known technique to reduce the training time by reducing the number of weight updates needed. It is a variation of Polyak averaging, but using exponential weights instead of equal weights across iterations.
### Constructing averaged models
The AveragedModel class serves to compute the weights of the SWA or EMA model.
You can create an SWA averaged model by running:
```
>>> averaged_model = AveragedModel(model)

```
Copy to clipboard
EMA models are constructed by specifying the `multi_avg_fn` argument as follows:
```
>>> decay = 0.999
>>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))

```
Copy to clipboard
Decay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to `torch.optim.swa_utils.get_ema_multi_avg_fn()`, the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues.
`torch.optim.swa_utils.get_ema_multi_avg_fn()` returns a function that applies the following EMA equation to the weights:
Wt+1EMA=αWtEMA+(1−α)WtmodelW^\textrm{EMA}_{t+1} = \alpha W^\textrm{EMA}_{t} + (1 - \alpha) W^\textrm{model}_t Wt+1EMA​=αWtEMA​+(1−α)Wtmodel​
where alpha is the EMA decay.
Here the model `model` can be an arbitrary `torch.nn.Module` object. `averaged_model` will keep track of the running averages of the parameters of the `model`. To update these averages, you should use the `update_parameters()` function after the optimizer.step():
```
>>> averaged_model.update_parameters(model)

```
Copy to clipboard
For SWA and EMA, this call is usually done right after the optimizer `step()`. In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.
### Custom averaging strategies
By default, `torch.optim.swa_utils.AveragedModel` computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the `avg_fn` or `multi_avg_fn` parameters:
  * `avg_fn` allows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter.
  * `multi_avg_fn` allows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using the `torch._foreach*` functions. This function must update the averaged parameters in-place.


In the following example `ema_model` computes an exponential moving average using the `avg_fn` parameter:
```
>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\
>>>     0.9 * averaged_model_parameter + 0.1 * model_parameter
>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)

```
Copy to clipboard
In the following example `ema_model` computes an exponential moving average using the more efficient `multi_avg_fn` parameter:
```
>>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))

```
Copy to clipboard
### SWA learning rate schedules
Typically, in SWA the learning rate is set to a high constant value. `SWALR` is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:
```
>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \
>>>     anneal_strategy="linear", anneal_epochs=5, swa_lr=0.05)

```
Copy to clipboard
You can also use cosine annealing to a fixed value instead of linear annealing by setting `anneal_strategy="cos"`.
### Taking care of batch normalization
`update_bn()` is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader `loader` at the end of training:
```
>>> torch.optim.swa_utils.update_bn(loader, swa_model)

```
Copy to clipboard
`update_bn()` applies the `swa_model` to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.
Warning
`update_bn()` assumes that each batch in the dataloader `loader` is either a tensors or a list of tensors where the first element is the tensor that the network `swa_model` should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the `swa_model` by doing a forward pass with the `swa_model` on each element of the dataset.
### Putting it all together: SWA
In the example below, `swa_model` is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:
```
>>> loader, optimizer, model, loss_fn = ...
>>> swa_model = torch.optim.swa_utils.AveragedModel(model)
>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)
>>> swa_start = 160
>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)
>>>
>>> for epoch in range(300):
>>>    for input, target in loader:
>>>      optimizer.zero_grad()
>>>      loss_fn(model(input), target).backward()
>>>      optimizer.step()
>>>    if epoch > swa_start:
>>>      swa_model.update_parameters(model)
>>>      swa_scheduler.step()
>>>    else:
>>>      scheduler.step()
>>>
>>> # Update bn statistics for the swa_model at the end
>>> torch.optim.swa_utils.update_bn(loader, swa_model)
>>> # Use swa_model to make predictions on test data
>>> preds = swa_model(test_input)

```
Copy to clipboard
### Putting it all together: EMA
In the example below, `ema_model` is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999. We train the model for a total of 300 epochs and start to collect EMA averages immediately.
```
>>> loader, optimizer, model, loss_fn = ...
>>> ema_model = torch.optim.swa_utils.AveragedModel(model, \
>>>       multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))
>>>
>>> for epoch in range(300):
>>>    for input, target in loader:
>>>      optimizer.zero_grad()
>>>      loss_fn(model(input), target).backward()
>>>      optimizer.step()
>>>      ema_model.update_parameters(model)
>>>
>>> # Update bn statistics for the ema_model at the end
>>> torch.optim.swa_utils.update_bn(loader, ema_model)
>>> # Use ema_model to make predictions on test data
>>> preds = ema_model(test_input)

```
Copy to clipboard
`swa_utils.AveragedModel` | Implements averaged model for Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA).  
---|---  
`swa_utils.SWALR` | Anneals the learning rate in each parameter group to a fixed value. 

torch.optim.swa_utils.get_ema_multi_avg_fn(_decay =0.999_)[source][source]
      
Get the function applying exponential moving average (EMA) across multiple params. 

torch.optim.swa_utils.update_bn(_loader_ , _model_ , _device =None_)[source][source]
    
Update BatchNorm running_mean, running_var buffers in the model.
It performs one pass over data in loader to estimate the activation statistics for BatchNorm layers in the model. 

Parameters
    
  * **loader** (_torch.utils.data.DataLoader_) – dataset loader to compute the activation statistics on. Each data batch should be either a tensor, or a list/tuple whose first element is a tensor containing data.
  * **model** (_torch.nn.Module_) – model for which we seek to update BatchNorm statistics.
  * **device** (_torch.device_ _,__optional_) – If set, data will be transferred to `device` before being passed into `model`.


Example
```
>>> loader, model = ...
>>> torch.optim.swa_utils.update_bn(loader, model)

```
Copy to clipboard
Note
The update_bn utility assumes that each data batch in `loader` is either a tensor or a list or tuple of tensors; in the latter case it is assumed that `model.forward()` should be called on the first element of the list or tuple corresponding to the data batch.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.optim
    * How to use an optimizer
      * Constructing it
      * Per-parameter options
      * Taking an optimization step
        * `optimizer.step()`
        * `optimizer.step(closure)`
    * Base class
      * `Optimizer`
    * Algorithms
    * How to adjust learning rate
    * How to utilize named parameters to load optimizer state dict
    * Weight Averaging (SWA and EMA)
      * Constructing averaged models
      * Custom averaging strategies
      * SWA learning rate schedules
      * Taking care of batch normalization
      * Putting it all together: SWA
      * Putting it all together: EMA
        * `get_ema_multi_avg_fn()`
        * `update_bn()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.package
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.package
`torch.package` adds support for creating packages containing both artifacts and arbitrary PyTorch code. These packages can be saved, shared, used to load and execute models at a later date or on a different machine, and can even be deployed to production using `torch::deploy`.
This document contains tutorials, how-to guides, explanations, and an API reference that will help you learn more about `torch.package` and how to use it.
Warning
This module depends on the `pickle` module which is not secure. Only unpackage data you trust.
It is possible to construct malicious pickle data which will **execute arbitrary code during unpickling**. Never unpackage data that could have come from an untrusted source, or that could have been tampered with.
For more information, review the documentation for the `pickle` module.
  * Tutorials
    * Packaging your first model
  * How do I…
    * See what is inside a package?
    * See why a given module was included as a dependency?
    * Include arbitrary resources with my package and access them later?
    * Customize how a class is packaged?
    * Test in my source code whether or not it is executing inside a package?
    * Patch code into a package?
    * Access package contents from packaged code?
    * Distinguish between packaged code and non-packaged code?
    * Re-export an imported object?
    * Package a TorchScript module?
  * Explanation
    * `torch.package` Format Overview
    * How `torch.package` finds your code’s dependencies
    * Dependency Management
    * `torch.package` sharp edges
    * How `torch.package` keeps packages isolated from each other
  * API Reference


## Tutorials
### Packaging your first model
A tutorial that guides you through packaging and unpackaging a simple model is available on Colab. After completing this exercise, you will be familiar with the basic API for creating and using Torch packages.
## How do I…
### See what is inside a package?
#### Treat the package like a ZIP archive
The container format for a `torch.package` is ZIP, so any tools that work with standard ZIP files should work for exploring the contents. Some common ways to interact with ZIP files:
  * `unzip my_package.pt` will unzip the `torch.package` archive to disk, where you can freely inspect its contents.


```
$ unzip my_package.pt && tree my_package
my_package
├── .data
│  ├── 94304870911616.storage
│  ├── 94304900784016.storage
│  ├── extern_modules
│  └── version
├── models
│  └── model_1.pkl
└── torchvision
  └── models
    ├── resnet.py
    └── utils.py
~ cd my_package && cat torchvision/models/resnet.py
...

```
Copy to clipboard
  * The Python `zipfile` module provides a standard way to read and write ZIP archive contents.


```
from zipfile import ZipFile
with ZipFile("my_package.pt") as myzip:
  file_bytes = myzip.read("torchvision/models/resnet.py")
  # edit file_bytes in some way
  myzip.writestr("torchvision/models/resnet.py", new_file_bytes)

```
Copy to clipboard
  * vim has the ability to natively read ZIP archives. You can even edit files and :`write` them back into the archive!


```
# add this to your .vimrc to treat `*.pt` files as zip files
au BufReadCmd *.pt call zip#Browse(expand("<amatch>"))
~ vi my_package.pt

```
Copy to clipboard
#### Use the `file_structure()` API
`PackageImporter` provides a `file_structure()` method, which will return a printable and queryable `Directory` object. The `Directory` object is a simple directory structure that you can use to explore the current contents of a `torch.package`.
The `Directory` object itself is directly printable and will print out a file tree representation. To filter what is returned, use the glob-style `include` and `exclude` filtering arguments.
```
with PackageExporter('my_package.pt') as pe:
  pe.save_pickle('models', 'model_1.pkl', mod)
importer = PackageImporter('my_package.pt')
# can limit printed items with include/exclude args
print(importer.file_structure(include=["**/utils.py", "**/*.pkl"], exclude="**/*.storage"))
print(importer.file_structure()) # will print out all files

```
Copy to clipboard
Output:
```
# filtered with glob pattern:
#  include=["**/utils.py", "**/*.pkl"], exclude="**/*.storage"
─── my_package.pt
  ├── models
  │  └── model_1.pkl
  └── torchvision
    └── models
      └── utils.py
# all files
─── my_package.pt
  ├── .data
  │  ├── 94304870911616.storage
  │  ├── 94304900784016.storage
  │  ├── extern_modules
  │  └── version
  ├── models
  │  └── model_1.pkl
  └── torchvision
    └── models
      ├── resnet.py
      └── utils.py

```
Copy to clipboard
You can also query `Directory` objects with the `has_file()` method.
```
importer_file_structure = importer.file_structure()
found: bool = importer_file_structure.has_file("package_a/subpackage.py")

```
Copy to clipboard
### See why a given module was included as a dependency?
Say there is a given module `foo`, and you want to know why your `PackageExporter` is pulling in `foo` as a dependency.
`PackageExporter.get_rdeps()` will return all modules that directly depend on `foo`.
If you would like to see how a given module `src` depends on `foo`, the `PackageExporter.all_paths()` method will return a DOT-formatted graph showing all the dependency paths between `src` and `foo`.
If you would just like to see the whole dependency graph of your `PackageExporter`, you can use `PackageExporter.dependency_graph_string()`.
### Include arbitrary resources with my package and access them later?
`PackageExporter` exposes three methods, `save_pickle`, `save_text` and `save_binary` that allow you to save Python objects, text, and binary data to a package.
```
with torch.PackageExporter("package.pt") as exporter:
  # Pickles the object and saves to `my_resources/tensor.pkl` in the archive.
  exporter.save_pickle("my_resources", "tensor.pkl", torch.randn(4))
  exporter.save_text("config_stuff", "words.txt", "a sample string")
  exporter.save_binary("raw_data", "binary", my_bytes)

```
Copy to clipboard
`PackageImporter` exposes complementary methods named `load_pickle`, `load_text` and `load_binary` that allow you to load Python objects, text and binary data from a package.
```
importer = torch.PackageImporter("package.pt")
my_tensor = importer.load_pickle("my_resources", "tensor.pkl")
text = importer.load_text("config_stuff", "words.txt")
binary = importer.load_binary("raw_data", "binary")

```
Copy to clipboard
### Customize how a class is packaged?
`torch.package` allows for the customization of how classes are packaged. This behavior is accessed through defining the method `__reduce_package__` on a class and by defining a corresponding de-packaging function. This is similar to defining `__reduce__` for Python’s normal pickling process.
Steps:
  1. Define the method `__reduce_package__(self, exporter: PackageExporter)` on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the `PackageExporter` when it encounters an instance of the target class.
  2. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature’s first parameter should be a `PackageImporter` instance, and the rest of the parameters are user defined.


```
# foo.py [Example of customizing how class Foo is packaged]
from torch.package import PackageExporter, PackageImporter
import time

class Foo:
  def __init__(self, my_string: str):
    super().__init__()
    self.my_string = my_string
    self.time_imported = 0
    self.time_exported = 0
  def __reduce_package__(self, exporter: PackageExporter):
"""
    Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when
    saving an instance of this object. This method should do the work to save this
    object inside of the ``torch.package`` archive.
    Returns function w/ arguments to load the object from a
    ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.
    """
    # use this pattern to ensure no naming conflicts with normal dependencies,
    # anything saved under this module name shouldn't conflict with other
    # items in the package
    generated_module_name = f"foo-generated._{exporter.get_unique_id()}"
    exporter.save_text(
      generated_module_name,
      "foo.txt",
      self.my_string + ", with exporter modification!",
    )
    time_exported = time.clock_gettime(1)
    # returns de-packaging function w/ arguments to invoke with
    return (unpackage_foo, (generated_module_name, time_exported,))

def unpackage_foo(
  importer: PackageImporter, generated_module_name: str, time_exported: float
) -> Foo:
"""
  Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function
  when depickling a Foo object.
  Performs work of loading and returning a Foo instance from a ``torch.package`` archive.
  """
  time_imported = time.clock_gettime(1)
  foo = Foo(importer.load_text(generated_module_name, "foo.txt"))
  foo.time_imported = time_imported
  foo.time_exported = time_exported
  return foo

```
Copy to clipboard
```
# example of saving instances of class Foo
import torch
from torch.package import PackageImporter, PackageExporter
import foo
foo_1 = foo.Foo("foo_1 initial string")
foo_2 = foo.Foo("foo_2 initial string")
with PackageExporter('foo_package.pt') as pe:
  # save as normal, no extra work necessary
  pe.save_pickle('foo_collection', 'foo1.pkl', foo_1)
  pe.save_pickle('foo_collection', 'foo2.pkl', foo_2)
pi = PackageImporter('foo_package.pt')
print(pi.file_structure())
imported_foo = pi.load_pickle('foo_collection', 'foo1.pkl')
print(f"foo_1 string: '{imported_foo.my_string}'")
print(f"foo_1 export time: {imported_foo.time_exported}")
print(f"foo_1 import time: {imported_foo.time_imported}")

```
Copy to clipboard
```
# output of running above script
─── foo_package
  ├── foo-generated
  │  ├── _0
  │  │  └── foo.txt
  │  └── _1
  │    └── foo.txt
  ├── foo_collection
  │  ├── foo1.pkl
  │  └── foo2.pkl
  └── foo.py
foo_1 string: 'foo_1 initial string, with reduction modification!'
foo_1 export time: 9857706.650140837
foo_1 import time: 9857706.652698385

```
Copy to clipboard
### Test in my source code whether or not it is executing inside a package?
A `PackageImporter` will add the attribute `__torch_package__` to every module that it initializes. Your code can check for the presence of this attribute to determine whether it is executing in a packaged context or not.
```
# In foo/bar.py:
if "__torch_package__" in dir(): # true if the code is being loaded from a package
  def is_in_package():
    return True
  UserException = Exception
else:
  def is_in_package():
    return False
  UserException = UnpackageableException

```
Copy to clipboard
Now, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from a `torch.package`.
```
from foo.bar import is_in_package
print(is_in_package()) # False
loaded_module = PackageImporter(my_package).import_module("foo.bar")
loaded_module.is_in_package() # True

```
Copy to clipboard
**Warning** : in general, it’s bad practice to have code that behaves differently depending on whether it’s packaged or not. This can lead to hard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring your code so that it behaves the same way no matter how it was loaded.
### Patch code into a package?
`PackageExporter` offers a `save_source_string()` method that allows one to save arbitrary Python source code to a module of your choosing.
```
with PackageExporter(f) as exporter:
  # Save the my_module.foo available in your current Python environment.
  exporter.save_module("my_module.foo")
  # This saves the provided string to my_module/foo.py in the package archive.
  # It will override the my_module.foo that was previously saved.
  exporter.save_source_string("my_module.foo", textwrap.dedent(
"""\
    def my_function():
      print('hello world')
    """
  ))
  # If you want to treat my_module.bar as a package
  # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)
  # pass is_package=True,
  exporter.save_source_string("my_module.bar",
                "def foo(): print('hello')\n",
                is_package=True)
importer = PackageImporter(f)
importer.import_module("my_module.foo").my_function() # prints 'hello world'

```
Copy to clipboard
### Access package contents from packaged code?
`PackageImporter` implements the importlib.resources API for accessing resources from inside a package.
```
with PackageExporter(f) as exporter:
  # saves text to my_resource/a.txt in the archive
  exporter.save_text("my_resource", "a.txt", "hello world!")
  # saves the tensor to my_pickle/obj.pkl
  exporter.save_pickle("my_pickle", "obj.pkl", torch.ones(2, 2))
  # see below for module contents
  exporter.save_module("foo")
  exporter.save_module("bar")

```
Copy to clipboard
The `importlib.resources` API allows access to resources from within packaged code.
```
# foo.py:
import importlib.resources
import my_resource
# returns "hello world!"
def get_my_resource():
  return importlib.resources.read_text(my_resource, "a.txt")

```
Copy to clipboard
Using `importlib.resources` is the recommended way to access package contents from within packaged code, since it complies with the Python standard. However, it is also possible to access the parent `PackageImporter` instance itself from within packaged code.
```
# bar.py:
import torch_package_importer # this is the PackageImporter that imported this module.
# Prints "hello world!", equivalent to importlib.resources.read_text
def get_my_resource():
  return torch_package_importer.load_text("my_resource", "a.txt")
# You also do things that the importlib.resources API does not support, like loading
# a pickled object from the package.
def get_my_pickle():
  return torch_package_importer.load_pickle("my_pickle", "obj.pkl")

```
Copy to clipboard
### Distinguish between packaged code and non-packaged code?
To tell if an object’s code is from a `torch.package`, use the `torch.package.is_from_package()` function. Note: if an object is from a package but its definition is from a module marked `extern` or from `stdlib`, this check will return `False`.
```
importer = PackageImporter(f)
mod = importer.import_module('foo')
obj = importer.load_pickle('model', 'model.pkl')
txt = importer.load_text('text', 'my_test.txt')
assert is_from_package(mod)
assert is_from_package(obj)
assert not is_from_package(txt) # str is from stdlib, so this will return False

```
Copy to clipboard
### Re-export an imported object?
To re-export an object that was previously imported by a `PackageImporter`, you must make the new `PackageExporter` aware of the original `PackageImporter` so that it can find source code for your object’s dependencies.
```
importer = PackageImporter(f)
obj = importer.load_pickle("model", "model.pkl")
# re-export obj in a new package
with PackageExporter(f2, importer=(importer, sys_importer)) as exporter:
  exporter.save_pickle("model", "model.pkl", obj)

```
Copy to clipboard
### Package a TorchScript module?
To package a TorchScript model, use the same `save_pickle` and `load_pickle` APIs as you would with any other object. Saving TorchScript objects that are attributes or submodules is supported as well with no extra work.
```
# save TorchScript just like any other object
with PackageExporter(file_name) as e:
  e.save_pickle("res", "script_model.pkl", scripted_model)
  e.save_pickle("res", "mixed_model.pkl", python_model_with_scripted_submodule)
# load as normal
importer = PackageImporter(file_name)
loaded_script = importer.load_pickle("res", "script_model.pkl")
loaded_mixed = importer.load_pickle("res", "mixed_model.pkl"

```
Copy to clipboard
## Explanation
### `torch.package` Format Overview
A `torch.package` file is a ZIP archive which conventionally uses the `.pt` extension. Inside the ZIP archive, there are two kinds of files:
  * Framework files, which are placed in the `.data/`.
  * User files, which is everything else.


As an example, this is what a fully packaged ResNet model from `torchvision` looks like:
```
resnet
├── .data # All framework-specific data is stored here.
│  │   # It's named to avoid conflicts with user-serialized code.
│  ├── 94286146172688.storage # tensor data
│  ├── 94286146172784.storage
│  ├── extern_modules # text file with names of extern modules (e.g. 'torch')
│  ├── version     # version metadata
│  ├── ...
├── model # the pickled model
│  └── model.pkl
└── torchvision # all code dependencies are captured as source files
  └── models
    ├── resnet.py
    └── utils.py

```
Copy to clipboard
#### Framework files
The `.data/` directory is owned by torch.package, and its contents are considered to be a private implementation detail. The `torch.package` format makes no guarantees about the contents of `.data/`, but any changes made will be backward compatible (that is, newer version of PyTorch will always be able to load older `torch.packages`).
Currently, the `.data/` directory contains the following items:
  * `version`: a version number for the serialized format, so that the `torch.package` import infrastructures knows how to load this package.
  * `extern_modules`: a list of modules that are considered `extern`. `extern` modules will be imported using the loading environment’s system importer.
  * `*.storage`: serialized tensor data.


```
.data
├── 94286146172688.storage
├── 94286146172784.storage
├── extern_modules
├── version
├── ...

```
Copy to clipboard
#### User files
All other files in the archive were put there by a user. The layout is identical to a Python regular package. For a deeper dive in how Python packaging works, please consult this essay (it’s slightly out of date, so double-check implementation details with the Python reference documentation).
```
<package root>
├── model # the pickled model
│  └── model.pkl
├── another_package
│  ├── __init__.py
│  ├── foo.txt     # a resource file , see importlib.resources
│  └── ...
└── torchvision
  └── models
    ├── resnet.py  # torchvision.models.resnet
    └── utils.py  # torchvision.models.utils

```
Copy to clipboard
### How `torch.package` finds your code’s dependencies
#### Analyzing an object’s dependencies
When you issue a `save_pickle(obj, ...)` call, `PackageExporter` will pickle the object normally. Then, it uses the `pickletools` standard library module to parse the pickle bytecode.
In a pickle, an object is saved along with a `GLOBAL` opcode that describes where to find the implementation of the object’s type, like:
```
GLOBAL 'torchvision.models.resnet Resnet`

```
Copy to clipboard
The dependency resolver will gather up all `GLOBAL` ops and mark them as dependencies of your pickled object. For more information about pickling and the pickle format, please consult the Python docs.
#### Analyzing a module’s dependencies
When a Python module is identified as a dependency, `torch.package` walks the module’s python AST representation and looks for import statements with full support for the standard forms: `from x import y`, `import z`, `from w import v as u`, etc. When one of these import statements are encountered, `torch.package` registers the imported modules as dependencies that are then themselves parsed in the same AST walking way.
**Note** : AST parsing has limited support for the `__import__(...)` syntax and does not support `importlib.import_module` calls. In general, you should not expect dynamic imports to be detected by `torch.package`.
### Dependency Management
`torch.package` automatically finds the Python modules that your code and objects depend on. This process is called dependency resolution. For each module that the dependency resolver finds, you must specify an _action_ to take.
The allowed actions are:
  * `intern`: put this module into the package.
  * `extern`: declare this module as an external dependency of the package.
  * `mock`: stub out this module.
  * `deny`: depending on this module will raise an error during package export.


Finally, there is one more important action that is not technically part of `torch.package`:
  * Refactoring: remove or change the dependencies in your code.


Note that actions are only defined on entire Python modules. There is no way to package “just” a function or class from a module and leave the rest out. This is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a module, so that’s what `torch.package` uses.
Actions are applied to modules using patterns. Patterns can either be module names (`"foo.bar"`) or globs (like `"foo.**"`). You associate a pattern with an action using methods on `PackageExporter`, e.g.
```
my_exporter.intern("torchvision.**")
my_exporter.extern("numpy")

```
Copy to clipboard
If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined, and the first action will be taken.
#### `intern`
If a module is `intern`-ed, it will be placed into the package.
This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet from `torchvision`, you will need to `intern` the module torchvision.models.resnet.
On package import, when your packaged code tries to import an `intern`-ed module, PackageImporter will look inside your package for that module. If it can’t find that module, an error will be raised. This ensures that each `PackageImporter` is isolated from the loading environment—even if you have `my_interned_module` available in both your package and the loading environment, `PackageImporter` will only use the version in your package.
**Note** : Only Python source modules can be `intern`-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if you attempt to `intern` them. These kinds of modules need to be `mock`-ed or `extern`-ed.
#### `extern`
If a module is `extern`-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this list on `package_exporter.extern_modules`.
On package import, when the packaged code tries to import an `extern`-ed module, `PackageImporter` will use the default Python importer to find that module, as if you did `importlib.import_module("my_externed_module")`. If it can’t find that module, an error will be raised.
In this way, you can depend on third-party libraries like `numpy` and `scipy` from within your package without having to package them too.
**Warning** : If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility for your package, try to limit your use of `extern`.
#### `mock`
If a module is `mock`-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve objects from it (so that `from my_mocked_module import foo` will not error), but any use of that object will raise a `NotImplementedError`.
`mock` should be used for code that you “know” will not be needed in the loaded package, but you still want available for use in non-packaged contents. For example, initialization/configuration code, or code only used for debugging/training.
**Warning** : In general, `mock` should be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code, which may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.
#### Refactoring
The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some guidelines for writing code with clean dependencies (which are also generally good practices!):
**Include only what you use**. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused, and will try to process them.
**Qualify your imports**. For example, instead of writing import foo and later using `foo.bar.baz`, prefer to write `from foo.bar import baz`. This more precisely specifies your real dependency (`foo.bar`) and lets the dependency resolver know you don’t need all of `foo`.
**Split up large files with unrelated functionality into smaller ones**. If your `utils` module contains a hodge-podge of unrelated functionality, any module that depends on `utils` will need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define single-purpose modules that can be packaged independently of one another.
#### Patterns
Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buck glob().
A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a separator string, e.g. `foo.bar.baz`.
A pattern contains one or more segments. Segments can be:
  * A literal string (e.g. `foo`), which matches exactly.
  * A string containing a wildcard (e.g. `torch`, or `foo*baz*`). The wildcard matches any string, including the empty string.
  * A double wildcard (`**`). This matches against zero or more complete segments.


Examples:
  * `torch.**`: matches `torch` and all its submodules, e.g. `torch.nn` and `torch.nn.functional`.
  * `torch.*`: matches `torch.nn` or `torch.functional`, but not `torch.nn.functional` or `torch`
  * `torch*.**`: matches `torch`, `torchvision`, and all of their submodules


When specifying actions, you can pass multiple patterns, e.g.
```
exporter.intern(["torchvision.models.**", "torchvision.utils.**"])

```
Copy to clipboard
A module will match against this action if it matches any of the patterns.
You can also specify patterns to exclude, e.g.
```
exporter.mock("**", exclude=["torchvision.**"])

```
Copy to clipboard
A module will not match against this action if it matches any of the exclude patterns. In this example, we are mocking all modules except `torchvision` and its submodules.
When a module could potentially match against multiple actions, the first action defined will be taken.
### `torch.package` sharp edges
#### Avoid global state in your modules
Python makes it really easy to bind objects and run code at module-level scope. This is generally fine—after all, functions and classes are bound to names this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable global state.
Mutable global state is quite useful—it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can cause complications when used with `torch.package`.
Every `PackageImporter` creates an independent environment for its contents. This is nice because it means we load multiple packages and ensure they are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug errors.
#### Types are not shared between packages and the loading environment
Any class that you import from a `PackageImporter` will be a version of the class specific to that importer. For example:
```
from foo import MyClass
my_class_instance = MyClass()
with PackageExporter(f) as exporter:
  exporter.save_module("foo")
importer = PackageImporter(f)
imported_MyClass = importer.import_module("foo").MyClass
assert isinstance(my_class_instance, MyClass) # works
assert isinstance(my_class_instance, imported_MyClass) # ERROR!

```
Copy to clipboard
In this example, `MyClass` and `imported_MyClass` are _not the same type_. In this specific example, `MyClass` and `imported_MyClass` have exactly the same implementation, so you might think it’s okay to consider them the same class. But consider the situation where `imported_MyClass` is coming from an older package with an entirely different implementation of `MyClass` — in that case, it’s unsafe to consider them the same class.
Under the hood, each importer has a prefix that allows it to uniquely identify classes:
```
print(MyClass.__name__) # prints "foo.MyClass"
print(imported_MyClass.__name__) # prints <torch_package_0>.foo.MyClass

```
Copy to clipboard
That means you should not expect `isinstance` checks to work when one of the arguments is from a package and the other is not. If you need this functionality, consider the following options:
  * Doing duck typing (just using the class instead of explicitly checking that it is of a given type).
  * Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tag `self.handler = "handle_me_this_way"` and have client code check for the value of `handler` instead of checking the type directly.


### How `torch.package` keeps packages isolated from each other
Each `PackageImporter` instance creates an independent, isolated environment for its modules and objects. Modules in a package can only import other packaged modules, or modules marked `extern`. If you use multiple `PackageImporter` instances to load a single package, you will get multiple independent environments that do not interact.
This is achieved by extending Python’s import infrastructure with a custom importer. `PackageImporter` provides the same core API as the `importlib` importer; namely, it implements the `import_module` and `__import__` methods.
When you invoke `PackageImporter.import_module()`, `PackageImporter` will construct and return a new module, much as the system importer does. However, `PackageImporter` patches the returned module to use `self` (i.e. that `PackageImporter` instance) to fulfill future import requests by looking in the package rather than searching the user’s Python environment.
#### Mangling
To avoid confusion (“is this `foo.bar` object the one from my package, or the one from my Python environment?”), `PackageImporter` mangles the `__name__` and `__file__` of all imported modules, by adding a _mangle prefix_ to them.
For `__name__`, a name like `torchvision.models.resnet18` becomes `<torch_package_0>.torchvision.models.resnet18`.
For `__file__`, a name like `torchvision/models/resnet18.py` becomes `<torch_package_0>.torchvision/modules/resnet18.py`.
Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print statements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consult `mangling.md` in `torch/package/`.
## API Reference 

_class_ torch.package.PackagingError(_dependency_graph_ , _debug =False_)[source][source]
    
This exception is raised when there is an issue with exporting a package. `PackageExporter` will attempt to gather up all the errors and present them to you at once. 

_class_ torch.package.EmptyMatchError[source][source]
    
This is an exception that is thrown when a mock or extern is marked as `allow_empty=False`, and is not matched with any module during packaging. 

_class_ torch.package.PackageExporter(_f_ , _importer= <torch.package.importer._SysImporter object>_, _debug=False_)[source][source]
    
Exporters allow you to write packages of code, pickled Python data, and arbitrary binary and text resources into a self-contained package.
Imports can load this code in a hermetic way, such that code is loaded from the package rather than the normal Python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.
The code contained in packages is copied file-by-file from the original source when it is created, and the file format is a specially organized zip file. Future users of the package can unzip the package, and edit the code in order to perform custom modifications to it.
The importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external using `extern()`. The file `extern_modules` in the zip archive lists all the modules that a package externally depends on. This prevents “implicit” dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.
When source code is added to the package, the exporter can optionally scan it for further code dependencies (`dependencies=True`). It looks for import statements, resolves relative references to qualified module names, and performs an action specified by the user (See: `extern()`, `mock()`, and `intern()`). 

__init__(_f_ , _importer= <torch.package.importer._SysImporter object>_, _debug=False_)[source][source]
    
Create an exporter. 

Parameters
    
  * **f** (_Union_ _[__str_ _,__PathLike_ _[__str_ _]__,__IO_ _[__bytes_ _]__]_) – The location to export to. Can be a `string`/`Path` object containing a filename or a binary I/O object.
  * **importer** (_Union_ _[__Importer_ _,__Sequence_ _[__Importer_ _]__]_) – If a single Importer is passed, use that to search for modules. If a sequence of importers are passed, an `OrderedImporter` will be constructed out of them.
  * **debug** (_bool_) – If set to True, add path of broken modules to PackagingErrors.



add_dependency(_module_name_ , _dependencies =True_)[source][source]
    
Given a module, add it to the dependency graph according to patterns specified by the user. 

all_paths(_src_ , _dst_)[source][source]
     

Return a dot representation of the subgraph
    
that has all paths from src to dst. 

Returns
    
A dot representation containing all paths from src to dst. (https://graphviz.org/doc/info/lang.html) 

Return type
    
str 

close()[source][source]
    
Write the package to the filesystem. Any calls after `close()` are now invalid. It is preferable to use resource guard syntax instead:
```
with PackageExporter("file.zip") as e:
  ...

```
Copy to clipboard 

denied_modules()[source][source]
    
Return all modules that are currently denied. 

Returns
    
A list containing the names of modules which will be denied in this package. 

Return type
    
list[str] 

deny(_include_ , _*_ , _exclude =()_)[source][source]
    
Blocklist modules who names match the given glob patterns from the list of modules the package can import. If a dependency on any matching packages is found, a `PackagingError` is raised. 

Parameters
    
  * **include** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – A string e.g. `"my_package.my_subpackage"`, or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in `mock()`.
  * **exclude** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – An optional pattern that excludes some patterns that match the include string.



dependency_graph_string()[source][source]
    
Returns digraph string representation of dependencies in package. 

Returns
    
A string representation of dependencies in package. 

Return type
    
str 

extern(_include_ , _*_ , _exclude =()_, _allow_empty =True_)[source][source]
    
Include `module` in the list of external modules the package can import. This will prevent dependency discovery from saving it in the package. The importer will load an external module directly from the standard import system. Code for extern modules must also exist in the process loading the package. 

Parameters
    
  * **include** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – A string e.g. `"my_package.my_subpackage"`, or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in `mock()`.
  * **exclude** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – An optional pattern that excludes some patterns that match the include string.
  * **allow_empty** (_bool_) – An optional flag that specifies whether the extern modules specified by this call to the `extern` method must be matched to some module during packaging. If an extern module glob pattern is added with `allow_empty=False`, and `close()` is called (either explicitly or via `__exit__`) before any modules match that pattern, an exception is thrown. If `allow_empty=True`, no such exception is thrown.



externed_modules()[source][source]
    
Return all modules that are currently externed. 

Returns
    
A list containing the names of modules which will be externed in this package. 

Return type
    
list[str] 

get_rdeps(_module_name_)[source][source]
    
Return a list of all modules which depend on the module `module_name`. 

Returns
    
A list containing the names of modules which depend on `module_name`. 

Return type
    
list[str] 

get_unique_id()[source][source]
    
Get an id. This id is guaranteed to only be handed out once for this package. 

Return type
    
str 

intern(_include_ , _*_ , _exclude =()_, _allow_empty =True_)[source][source]
    
Specify modules that should be packaged. A module must match some `intern` pattern in order to be included in the package and have its dependencies processed recursively. 

Parameters
    
  * **include** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – A string e.g. “my_package.my_subpackage”, or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in `mock()`.
  * **exclude** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – An optional pattern that excludes some patterns that match the include string.
  * **allow_empty** (_bool_) – An optional flag that specifies whether the intern modules specified by this call to the `intern` method must be matched to some module during packaging. If an `intern` module glob pattern is added with `allow_empty=False`, and `close()` is called (either explicitly or via `__exit__`) before any modules match that pattern, an exception is thrown. If `allow_empty=True`, no such exception is thrown.



interned_modules()[source][source]
    
Return all modules that are currently interned. 

Returns
    
A list containing the names of modules which will be interned in this package. 

Return type
    
list[str] 

mock(_include_ , _*_ , _exclude =()_, _allow_empty =True_)[source][source]
    
Replace some required modules with a mock implementation. Mocked modules will return a fake object for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used (e.g. custom serialization code or training helpers). Use this function to mock this functionality out without having to modify the original code. 

Parameters
    
  * **include** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – 
A string e.g. `"my_package.my_subpackage"`, or list of strings for the names of the modules to be mocked out. Strings can also be a glob-style pattern string that may match multiple modules. Any required dependencies that match this pattern string will be mocked out automatically. 

Examples :
    
`'torch.**'` – matches `torch` and all submodules of torch, e.g. `'torch.nn'` and `'torch.nn.functional'`
`'torch.*'` – matches `'torch.nn'` or `'torch.functional'`, but not `'torch.nn.functional'`
  * **exclude** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – An optional pattern that excludes some patterns that match the include string. e.g. `include='torch.**', exclude='torch.foo'` will mock all torch packages except `'torch.foo'`, Default: is `[]`.
  * **allow_empty** (_bool_) – An optional flag that specifies whether the mock implementation(s) specified by this call to the `mock()` method must be matched to some module during packaging. If a mock is added with `allow_empty=False`, and `close()` is called (either explicitly or via `__exit__`) and the mock has not been matched to a module used by the package being exported, an exception is thrown. If `allow_empty=True`, no such exception is thrown.



mocked_modules()[source][source]
    
Return all modules that are currently mocked. 

Returns
    
A list containing the names of modules which will be mocked in this package. 

Return type
    
list[str] 

register_extern_hook(_hook_)[source][source]
    
Registers an extern hook on the exporter.
The hook will be called each time a module matches against an `extern()` pattern. It should have the following signature:
```
hook(exporter: PackageExporter, module_name: str) -> None

```
Copy to clipboard
Hooks will be called in order of registration. 

Returns
    
A handle that can be used to remove the added hook by calling `handle.remove()`. 

Return type
    
`torch.utils.hooks.RemovableHandle` 

register_intern_hook(_hook_)[source][source]
    
Registers an intern hook on the exporter.
The hook will be called each time a module matches against an `intern()` pattern. It should have the following signature:
```
hook(exporter: PackageExporter, module_name: str) -> None

```
Copy to clipboard
Hooks will be called in order of registration. 

Returns
    
A handle that can be used to remove the added hook by calling `handle.remove()`. 

Return type
    
`torch.utils.hooks.RemovableHandle` 

register_mock_hook(_hook_)[source][source]
    
Registers a mock hook on the exporter.
The hook will be called each time a module matches against a `mock()` pattern. It should have the following signature:
```
hook(exporter: PackageExporter, module_name: str) -> None

```
Copy to clipboard
Hooks will be called in order of registration. 

Returns
    
A handle that can be used to remove the added hook by calling `handle.remove()`. 

Return type
    
`torch.utils.hooks.RemovableHandle` 

save_binary(_package_ , _resource_ , _binary_)[source][source]
    
Save raw bytes to the package. 

Parameters
    
  * **package** (_str_) – The name of module package this resource should go it (e.g. `"my_package.my_subpackage"`).
  * **resource** (_str_) – A unique name for the resource, used to identify it to load.
  * **binary** (_str_) – The data to save.



save_module(_module_name_ , _dependencies =True_)[source][source]
    
Save the code for `module` into the package. Code for the module is resolved using the `importers` path to find the module object, and then using its `__file__` attribute to find the source code. 

Parameters
    
  * **module_name** (_str_) – e.g. `my_package.my_subpackage`, code will be saved to provide code for this package.
  * **dependencies** (_bool_ _,__optional_) – If `True`, we scan the source for dependencies.



save_pickle(_package_ , _resource_ , _obj_ , _dependencies =True_, _pickle_protocol =3_)[source][source]
    
Save a python object to the archive using pickle. Equivalent to `torch.save()` but saving into the archive rather than a stand-alone file. Standard pickle does not save the code, only the objects. If `dependencies` is true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.
To be able to save an object where `type(obj).__name__` is `my_module.MyObject`, `my_module.MyObject` must resolve to the class of the object according to the `importer` order. When saving objects that have previously been packaged, the importer’s `import_module` method will need to be present in the `importer` list for this to work. 

Parameters
    
  * **package** (_str_) – The name of module package this resource should go in (e.g. `"my_package.my_subpackage"`).
  * **resource** (_str_) – A unique name for the resource, used to identify it to load.
  * **obj** (_Any_) – The object to save, must be picklable.
  * **dependencies** (_bool_ _,__optional_) – If `True`, we scan the source for dependencies.



save_source_file(_module_name_ , _file_or_directory_ , _dependencies =True_)[source][source]
    
Adds the local file system `file_or_directory` to the source package to provide the code for `module_name`. 

Parameters
    
  * **module_name** (_str_) – e.g. `"my_package.my_subpackage"`, code will be saved to provide code for this package.
  * **file_or_directory** (_str_) – the path to a file or directory of code. When a directory, all python files in the directory are recursively copied using `save_source_file()`. If a file is named `"/__init__.py"` the code is treated as a package.
  * **dependencies** (_bool_ _,__optional_) – If `True`, we scan the source for dependencies.



save_source_string(_module_name_ , _src_ , _is_package =False_, _dependencies =True_)[source][source]
    
Adds `src` as the source code for `module_name` in the exported package. 

Parameters
    
  * **module_name** (_str_) – e.g. `my_package.my_subpackage`, code will be saved to provide code for this package.
  * **src** (_str_) – The Python source code to save for this package.
  * **is_package** (_bool_ _,__optional_) – If `True`, this module is treated as a package. Packages are allowed to have submodules (e.g. `my_package.my_subpackage.my_subsubpackage`), and resources can be saved inside them. Defaults to `False`.
  * **dependencies** (_bool_ _,__optional_) – If `True`, we scan the source for dependencies.



save_text(_package_ , _resource_ , _text_)[source][source]
    
Save text data to the package. 

Parameters
    
  * **package** (_str_) – The name of module package this resource should go it (e.g. `"my_package.my_subpackage"`).
  * **resource** (_str_) – A unique name for the resource, used to identify it to load.
  * **text** (_str_) – The contents to save.



_class_ torch.package.PackageImporter(_file_or_buffer_ , _module_allowed= <function PackageImporter.<lambda>>_)[source][source]
    
Importers allow you to load code written to packages by `PackageExporter`. Code is loaded in a hermetic way, using files from the package rather than the normal python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.
The importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external during export. The file `extern_modules` in the zip archive lists all the modules that a package externally depends on. This prevents “implicit” dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine. 

__init__(_file_or_buffer_ , _module_allowed= <function PackageImporter.<lambda>>_)[source][source]
    
Open `file_or_buffer` for importing. This checks that the imported package only requires modules allowed by `module_allowed` 

Parameters
    
  * **file_or_buffer** (_Union_ _[__str_ _,__PathLike_ _[__str_ _]__,__IO_ _[__bytes_ _]__,__PyTorchFileReader_ _]_) – a file-like object (has to implement `read()`, `readline()`, `tell()`, and `seek()`), a string, or an `os.PathLike` object containing a filename.
  * **module_allowed** (_Callable_ _[__[__str_ _]__,__bool_ _]__,__optional_) – A method to determine if a externally provided module should be allowed. Can be used to ensure packages loaded do not depend on modules that the server does not support. Defaults to allowing anything.



Raises
    
**ImportError** – If the package will use a disallowed module. 

file_structure(_*_ , _include ='**'_, _exclude =()_)[source][source]
    
Returns a file structure representation of package’s zipfile. 

Parameters
    
  * **include** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – An optional string e.g. `"my_package.my_subpackage"`, or optional list of strings for the names of the files to be included in the zipfile representation. This can also be a glob-style pattern, as described in `PackageExporter.mock()`
  * **exclude** (_Union_ _[__List_ _[__str_ _]__,__str_ _]_) – An optional pattern that excludes files whose name match the pattern.



Returns
    
`Directory` 

Return type
    
_Directory_ 

id()[source][source]
    
Returns internal identifier that torch.package uses to distinguish `PackageImporter` instances. Looks like:
```
<torch_package_0>

```
Copy to clipboard 

import_module(_name_ , _package =None_)[source][source]
    
Load a module from the package if it hasn’t already been loaded, and then return the module. Modules are loaded locally to the importer and will appear in `self.modules` rather than `sys.modules`. 

Parameters
    
  * **name** (_str_) – Fully qualified name of the module to load.
  * **package** (_[__type_ _]__,__optional_) – Unused, but present to match the signature of importlib.import_module. Defaults to `None`.



Returns
    
The (possibly already) loaded module. 

Return type
    
types.ModuleType 

load_binary(_package_ , _resource_)[source][source]
    
Load raw bytes. 

Parameters
    
  * **package** (_str_) – The name of module package (e.g. `"my_package.my_subpackage"`).
  * **resource** (_str_) – The unique name for the resource.



Returns
    
The loaded data. 

Return type
    
bytes 

load_pickle(_package_ , _resource_ , _map_location =None_)[source][source]
    
Unpickles the resource from the package, loading any modules that are needed to construct the objects using `import_module()`. 

Parameters
    
  * **package** (_str_) – The name of module package (e.g. `"my_package.my_subpackage"`).
  * **resource** (_str_) – The unique name for the resource.
  * **map_location** – Passed to torch.load to determine how tensors are mapped to devices. Defaults to `None`.



Returns
    
The unpickled object. 

Return type
    
Any 

load_text(_package_ , _resource_ , _encoding ='utf-8'_, _errors ='strict'_)[source][source]
    
Load a string. 

Parameters
    
  * **package** (_str_) – The name of module package (e.g. `"my_package.my_subpackage"`).
  * **resource** (_str_) – The unique name for the resource.
  * **encoding** (_str_ _,__optional_) – Passed to `decode`. Defaults to `'utf-8'`.
  * **errors** (_str_ _,__optional_) – Passed to `decode`. Defaults to `'strict'`.



Returns
    
The loaded text. 

Return type
    
str 

python_version()[source][source]
    
Returns the version of python that was used to create this package.
Note: this function is experimental and not Forward Compatible. The plan is to move this into a lock file later on. 

Returns
    
`Optional[str]` a python version e.g. 3.8.9 or None if no version was stored with this package 

_class_ torch.package.Directory(_name_ , _is_dir_)[source][source]
    
A file structure representation. Organized as Directory nodes that have lists of their Directory children. Directories for a package are created by calling `PackageImporter.file_structure()`. 

has_file(_filename_)[source][source]
    
Checks if a file is present in a `Directory`. 

Parameters
    
**filename** (_str_) – Path of file to search for. 

Returns
    
If a `Directory` contains the specified file. 

Return type
    
bool
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.package
    * Tutorials
      * Packaging your first model
    * How do I…
      * See what is inside a package?
        * Treat the package like a ZIP archive
        * Use the `file_structure()` API
      * See why a given module was included as a dependency?
      * Include arbitrary resources with my package and access them later?
      * Customize how a class is packaged?
      * Test in my source code whether or not it is executing inside a package?
      * Patch code into a package?
      * Access package contents from packaged code?
      * Distinguish between packaged code and non-packaged code?
      * Re-export an imported object?
      * Package a TorchScript module?
    * Explanation
      * `torch.package` Format Overview
        * Framework files
        * User files
      * How `torch.package` finds your code’s dependencies
        * Analyzing an object’s dependencies
        * Analyzing a module’s dependencies
      * Dependency Management
        * `intern`
        * `extern`
        * `mock`
        * Refactoring
        * Patterns
      * `torch.package` sharp edges
        * Avoid global state in your modules
        * Types are not shared between packages and the loading environment
      * How `torch.package` keeps packages isolated from each other
        * Mangling
    * API Reference
      * `PackagingError`
      * `EmptyMatchError`
      * `PackageExporter`
        * `PackageExporter.__init__()`
        * `PackageExporter.add_dependency()`
        * `PackageExporter.all_paths()`
        * `PackageExporter.close()`
        * `PackageExporter.denied_modules()`
        * `PackageExporter.deny()`
        * `PackageExporter.dependency_graph_string()`
        * `PackageExporter.extern()`
        * `PackageExporter.externed_modules()`
        * `PackageExporter.get_rdeps()`
        * `PackageExporter.get_unique_id()`
        * `PackageExporter.intern()`
        * `PackageExporter.interned_modules()`
        * `PackageExporter.mock()`
        * `PackageExporter.mocked_modules()`
        * `PackageExporter.register_extern_hook()`
        * `PackageExporter.register_intern_hook()`
        * `PackageExporter.register_mock_hook()`
        * `PackageExporter.save_binary()`
        * `PackageExporter.save_module()`
        * `PackageExporter.save_pickle()`
        * `PackageExporter.save_source_file()`
        * `PackageExporter.save_source_string()`
        * `PackageExporter.save_text()`
      * `PackageImporter`
        * `PackageImporter.__init__()`
        * `PackageImporter.file_structure()`
        * `PackageImporter.id()`
        * `PackageImporter.import_module()`
        * `PackageImporter.load_binary()`
        * `PackageImporter.load_pickle()`
        * `PackageImporter.load_text()`
        * `PackageImporter.python_version()`
      * `Directory`
        * `Directory.has_file()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Quantization
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Quantization
Warning
Quantization is in beta and subject to change.
## Introduction to Quantization
Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. A quantized model executes some or all of the operations on tensors with reduced precision rather than full precision (floating point) values. This allows for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. PyTorch supports INT8 quantization compared to typical FP32 models allowing for a 4x reduction in the model size and a 4x reduction in memory bandwidth requirements. Hardware support for INT8 computations is typically 2 to 4 times faster compared to FP32 compute. Quantization is primarily a technique to speed up inference and only the forward pass is supported for quantized operators.
PyTorch supports multiple approaches to quantizing a deep learning model. In most cases the model is trained in FP32 and then the model is converted to INT8. In addition, PyTorch also supports quantization aware training, which models quantization errors in both the forward and backward passes using fake-quantization modules. Note that the entire computation is carried out in floating point. At the end of quantization aware training, PyTorch provides conversion functions to convert the trained model into lower precision.
At lower level, PyTorch provides a way to represent quantized tensors and perform operations with them. They can be used to directly construct models that perform all or part of the computation in lower precision. Higher-level APIs are provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss.
## Quantization API Summary
PyTorch provides three different modes of quantization: Eager Mode Quantization, FX Graph Mode Quantization (maintenance) and PyTorch 2 Export Quantization.
Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.
FX Graph Mode Quantization is an automated quantization workflow in PyTorch, and currently it’s a prototype feature, it is in maintenance mode since we have PyTorch 2 Export Quantization. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with `torch.fx`). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we’ll provide general guidelines, but to actually make it work, users might need to be familiar with `torch.fx`, especially on how to make a model symbolically traceable.
PyTorch 2 Export Quantization is the new full graph mode quantization workflow, released as prototype feature in PyTorch 2.1. With PyTorch 2, we are moving to a better solution for full program capture (torch.export) since it can capture a higher percentage (88.8% on 14K models) of models compared to torch.fx.symbolic_trace (72.7% on 14K models), the program capture solution used by FX Graph Mode Quantization. torch.export still has limitations around some python constructs and requires user involvement to support dynamism in the exported model, but overall it is an improvement over the previous program capture solution. PyTorch 2 Export Quantization is built for models captured by torch.export, with flexibility and productivity of both modeling users and backend developers in mind. The main features are (1). Programmable API for configuring how a model is quantized that can scale to many more use cases (2). Simplified UX for modeling users and backend developers since they only need to interact with a single object (Quantizer) for expressing user’s intention about how to quantize a model and what the backend support. (3). Optional reference quantized model representation that can represent quantized computation with integer operations that maps closer to actual quantized computations that happens in hardware.
New users of quantization are encouraged to try out PyTorch 2 Export Quantization first, if it does not work well, user can try eager mode quantization.
The following table compares the differences between Eager Mode Quantization, FX Graph Mode Quantization and PyTorch 2 Export Quantization:
| Eager Mode Quantization | FX Graph Mode Quantization | PyTorch 2 Export Quantization  
---|---|---|---  
Release Status | beta | prototype (maintenance) | prototype  
Operator Fusion | Manual | Automatic | Automatic  
Quant/DeQuant Placement | Manual | Automatic | Automatic  
Quantizing Modules | Supported | Supported | Supported  
Quantizing Functionals/Torch Ops | Manual | Automatic | Supported  
Support for Customization | Limited Support | Fully Supported | Fully Supported  
Quantization Mode Support | Post Training Quantization: Static, Dynamic, Weight Only Quantization Aware Training: Static | Post Training Quantization: Static, Dynamic, Weight Only Quantization Aware Training: Static | Defined by Backend Specific Quantizer  
Input/Output Model Type | `torch.nn.Module` | `torch.nn.Module` (May need some refactors to make the model compatible with FX Graph Mode Quantization) | `torch.fx.GraphModule` (captured by `torch.export`  
There are three types of quantization supported:
  1. dynamic quantization (weights quantized with activations read/stored in floating point and quantized for compute)
  2. static quantization (weights quantized, activations quantized, calibration required post training)
  3. static quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training)


Please see our Introduction to Quantization on PyTorch blog post for a more comprehensive overview of the tradeoffs between these quantization types.
Operator coverage varies between dynamic and static quantization and is captured in the table below.
| Static Quantization | Dynamic Quantization  
---|---|---  
nn.Linear nn.Conv1d/2d/3d |  Y Y |  Y N  
nn.LSTM nn.GRU |  Y (through custom modules) N |  Y Y  
nn.RNNCell nn.GRUCell nn.LSTMCell |  N N N |  Y Y Y  
nn.EmbeddingBag | Y (activations are in fp32) | Y  
nn.Embedding | Y | Y  
nn.MultiheadAttention | Y (through custom modules) | Not supported  
Activations | Broadly supported | Un-changed, computations stay in fp32  
### Eager Mode Quantization
For a general introduction to the quantization flow, including different types of quantization, please take a look at General Quantization Flow.
#### Post Training Dynamic Quantization
This is the simplest to apply form of quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference. This is used for situations where the model execution time is dominated by loading weights from memory rather than computing the matrix multiplications. This is true for LSTM and Transformer type models with small batch size.
Diagram:
```
# original model
# all tensors and computations are in floating point
previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
         /
linear_weight_fp32
# dynamically quantized model
# linear and LSTM weights are in int8
previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32
           /
  linear_weight_int8

```
Copy to clipboard
PTDQ API Example:
```
import torch
# define a floating point model
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.fc = torch.nn.Linear(4, 4)
  def forward(self, x):
    x = self.fc(x)
    return x
# create a model instance
model_fp32 = M()
# create a quantized model instance
model_int8 = torch.ao.quantization.quantize_dynamic(
  model_fp32, # the original model
  {torch.nn.Linear}, # a set of layers to dynamically quantize
  dtype=torch.qint8) # the target dtype for quantized weights
# run the model
input_fp32 = torch.randn(4, 4, 4, 4)
res = model_int8(input_fp32)

```
Copy to clipboard
To learn more about dynamic quantization please see our dynamic quantization tutorial.
#### Post Training Static Quantization
Post Training Static Quantization (PTQ static) quantizes the weights and activations of the model. It fuses activations into preceding layers where possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations. Post Training Static Quantization is typically used when both memory bandwidth and compute savings are important with CNNs being a typical use case.
We may need to modify the model before applying post training static quantization. Please see Model Preparation for Eager Mode Static Quantization.
Diagram:
```
# original model
# all tensors and computations are in floating point
previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
          /
  linear_weight_fp32
# statically quantized model
# weights and activations are in int8
previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8
          /
 linear_weight_int8

```
Copy to clipboard
PTSQ API Example:
```
import torch
# define a floating point model where some layers could be statically quantized
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # QuantStub converts tensors from floating point to quantized
    self.quant = torch.ao.quantization.QuantStub()
    self.conv = torch.nn.Conv2d(1, 1, 1)
    self.relu = torch.nn.ReLU()
    # DeQuantStub converts tensors from quantized to floating point
    self.dequant = torch.ao.quantization.DeQuantStub()
  def forward(self, x):
    # manually specify where tensors will be converted from floating
    # point to quantized in the quantized model
    x = self.quant(x)
    x = self.conv(x)
    x = self.relu(x)
    # manually specify where tensors will be converted from quantized
    # to floating point in the quantized model
    x = self.dequant(x)
    return x
# create a model instance
model_fp32 = M()
# model must be set to eval mode for static quantization logic to work
model_fp32.eval()
# attach a global qconfig, which contains information about what kind
# of observers to attach. Use 'x86' for server inference and 'qnnpack'
# for mobile inference. Other quantization configurations such as selecting
# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques
# can be specified here.
# Note: the old 'fbgemm' is still available but 'x86' is the recommended default
# for server inference.
# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')
# Fuse the activations to preceding layers, where applicable.
# This needs to be done manually depending on the model architecture.
# Common fusions include `conv + relu` and `conv + batchnorm + relu`
model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])
# Prepare the model for static quantization. This inserts observers in
# the model that will observe activation tensors during calibration.
model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)
# calibrate the prepared model to determine quantization parameters for activations
# in a real world setting, the calibration would be done with a representative dataset
input_fp32 = torch.randn(4, 1, 4, 4)
model_fp32_prepared(input_fp32)
# Convert the observed model to a quantized model. This does several things:
# quantizes the weights, computes and stores the scale and bias value to be
# used with each activation tensor, and replaces key operators with quantized
# implementations.
model_int8 = torch.ao.quantization.convert(model_fp32_prepared)
# run the model, relevant calculations will happen in int8
res = model_int8(input_fp32)

```
Copy to clipboard
To learn more about static quantization, please see the static quantization tutorial.
#### Quantization Aware Training for Static Quantization
Quantization Aware Training (QAT) models the effects of quantization during training allowing for higher accuracy compared to other quantization methods. We can do QAT for static, dynamic or weight only quantization. During training, all calculations are done in floating point, with fake_quant modules modeling the effects of quantization by clamping and rounding to simulate the effects of INT8. After model conversion, weights and activations are quantized, and activations are fused into the preceding layer where possible. It is commonly used with CNNs and yields a higher accuracy compared to static quantization.
We may need to modify the model before applying post training static quantization. Please see Model Preparation for Eager Mode Static Quantization.
Diagram:
```
# original model
# all tensors and computations are in floating point
previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
           /
  linear_weight_fp32
# model with fake_quants for modeling quantization numerics during training
previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32
              /
  linear_weight_fp32 -- fq
# quantized model
# weights and activations are in int8
previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8
           /
  linear_weight_int8

```
Copy to clipboard
QAT API Example:
```
import torch
# define a floating point model where some layers could benefit from QAT
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # QuantStub converts tensors from floating point to quantized
    self.quant = torch.ao.quantization.QuantStub()
    self.conv = torch.nn.Conv2d(1, 1, 1)
    self.bn = torch.nn.BatchNorm2d(1)
    self.relu = torch.nn.ReLU()
    # DeQuantStub converts tensors from quantized to floating point
    self.dequant = torch.ao.quantization.DeQuantStub()
  def forward(self, x):
    x = self.quant(x)
    x = self.conv(x)
    x = self.bn(x)
    x = self.relu(x)
    x = self.dequant(x)
    return x
# create a model instance
model_fp32 = M()
# model must be set to eval for fusion to work
model_fp32.eval()
# attach a global qconfig, which contains information about what kind
# of observers to attach. Use 'x86' for server inference and 'qnnpack'
# for mobile inference. Other quantization configurations such as selecting
# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques
# can be specified here.
# Note: the old 'fbgemm' is still available but 'x86' is the recommended default
# for server inference.
# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')
# fuse the activations to preceding layers, where applicable
# this needs to be done manually depending on the model architecture
model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,
  [['conv', 'bn', 'relu']])
# Prepare the model for QAT. This inserts observers and fake_quants in
# the model needs to be set to train for QAT logic to work
# the model that will observe weight and activation tensors during calibration.
model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())
# run the training loop (not shown)
training_loop(model_fp32_prepared)
# Convert the observed model to a quantized model. This does several things:
# quantizes the weights, computes and stores the scale and bias value to be
# used with each activation tensor, fuses modules where appropriate,
# and replaces key operators with quantized implementations.
model_fp32_prepared.eval()
model_int8 = torch.ao.quantization.convert(model_fp32_prepared)
# run the model, relevant calculations will happen in int8
res = model_int8(input_fp32)

```
Copy to clipboard
To learn more about quantization aware training, please see the QAT tutorial.
#### Model Preparation for Eager Mode Static Quantization
It is necessary to currently make some modifications to the model definition prior to Eager mode quantization. This is because currently quantization works on a module by module basis. Specifically, for all quantization techniques, the user needs to:
  1. Convert any operations that require output requantization (and thus have additional parameters) from functionals to module form (for example, using `torch.nn.ReLU` instead of `torch.nn.functional.relu`).
  2. Specify which parts of the model need to be quantized either by assigning `.qconfig` attributes on submodules or by specifying `qconfig_mapping`. For example, setting `model.conv1.qconfig = None` means that the `model.conv` layer will not be quantized, and setting `model.linear1.qconfig = custom_qconfig` means that the quantization settings for `model.linear1` will be using `custom_qconfig` instead of the global qconfig.


For static quantization techniques which quantize activations, the user needs to do the following in addition:
  1. Specify where activations are quantized and de-quantized. This is done using `QuantStub` and `DeQuantStub` modules.
  2. Use `FloatFunctional` to wrap tensor operations that require special handling for quantization into modules. Examples are operations like `add` and `cat` which require special handling to determine output quantization parameters.
  3. Fuse modules: combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the `fuse_modules()` API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]


### (Prototype - maintenance mode) FX Graph Mode Quantization
There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_mapping (an argument of the prepare_fx function).
FXPTQ API Example:
```
import torch
from torch.ao.quantization import (
 get_default_qconfig_mapping,
 get_default_qat_qconfig_mapping,
 QConfigMapping,
)
import torch.ao.quantization.quantize_fx as quantize_fx
import copy
model_fp = UserModel()
#
# post training dynamic/weight_only quantization
#
# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model
model_to_quantize = copy.deepcopy(model_fp)
model_to_quantize.eval()
qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)
# a tuple of one or more example inputs are needed to trace the model
example_inputs = (input_fp32)
# prepare
model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)
# no calibration needed when we only have dynamic/weight_only quantization
# quantize
model_quantized = quantize_fx.convert_fx(model_prepared)
#
# post training static quantization
#
model_to_quantize = copy.deepcopy(model_fp)
qconfig_mapping = get_default_qconfig_mapping("qnnpack")
model_to_quantize.eval()
# prepare
model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)
# calibrate (not shown)
# quantize
model_quantized = quantize_fx.convert_fx(model_prepared)
#
# quantization aware training for static quantization
#
model_to_quantize = copy.deepcopy(model_fp)
qconfig_mapping = get_default_qat_qconfig_mapping("qnnpack")
model_to_quantize.train()
# prepare
model_prepared = quantize_fx.prepare_qat_fx(model_to_quantize, qconfig_mapping, example_inputs)
# training loop (not shown)
# quantize
model_quantized = quantize_fx.convert_fx(model_prepared)
#
# fusion
#
model_to_quantize = copy.deepcopy(model_fp)
model_fused = quantize_fx.fuse_fx(model_to_quantize)

```
Copy to clipboard
Please follow the tutorials below to learn more about FX Graph Mode Quantization:
  * User Guide on Using FX Graph Mode Quantization
  * FX Graph Mode Post Training Static Quantization
  * FX Graph Mode Post Training Dynamic Quantization


### (Prototype) PyTorch 2 Export Quantization
API Example:
```
import torch
from torch.ao.quantization.quantize_pt2e import prepare_pt2e
from torch.export import export_for_training
from torch.ao.quantization.quantizer import (
  XNNPACKQuantizer,
  get_symmetric_quantization_config,
)
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = torch.nn.Linear(5, 10)
  def forward(self, x):
    return self.linear(x)
# initialize a floating point model
float_model = M().eval()
# define calibration function
def calibrate(model, data_loader):
  model.eval()
  with torch.no_grad():
    for image, target in data_loader:
      model(image)
# Step 1. program capture
# NOTE: this API will be updated to torch.export API in the future, but the captured
# result should mostly stay the same
m = export_for_training(m, *example_inputs).module()
# we get a model with aten ops
# Step 2. quantization
# backend developer will write their own Quantizer and expose methods to allow
# users to express how they
# want the model to be quantized
quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())
# or prepare_qat_pt2e for Quantization Aware Training
m = prepare_pt2e(m, quantizer)
# run calibration
# calibrate(m, sample_inference_data)
m = convert_pt2e(m)
# Step 3. lowering
# lower to target backend

```
Copy to clipboard
Please follow these tutorials to get started on PyTorch 2 Export Quantization:
Modeling Users:
  * PyTorch 2 Export Post Training Quantization
  * PyTorch 2 Export Post Training Quantization with X86 Backend through Inductor
  * PyTorch 2 Export Quantization Aware Training


Backend Developers (please check out all Modeling Users docs as well):
  * How to Write a Quantizer for PyTorch 2 Export Quantization


## Quantization Stack
Quantization is the process to convert a floating point model to a quantized model. So at high level the quantization stack can be split into two parts: 1). The building blocks or abstractions for a quantized model 2). The building blocks or abstractions for the quantization flow that converts a floating point model to a quantized model
### Quantized Model
#### Quantized Tensor
In order to do quantization in PyTorch, we need to be able to represent quantized data in Tensors. A Quantized Tensor allows for storing quantized data (represented as int8/uint8/int32) along with quantization parameters like scale and zero_point. Quantized Tensors allow for many useful operations making quantized arithmetic easy, in addition to allowing for serialization of data in a quantized format.
PyTorch supports both per tensor and per channel symmetric and asymmetric quantization. Per tensor means that all the values within the tensor are quantized the same way with the same quantization parameters. Per channel means that for each dimension, typically the channel dimension of a tensor, the values in the tensor are quantized with different quantization parameters. This allows for less error in converting tensors to quantized values since outlier values would only impact the channel it was in, instead of the entire Tensor.
The mapping is performed by converting the floating point tensors using
![_images/math-quantizer-equation.png](https://pytorch.org/docs/stable/_images/math-quantizer-equation.png)
Note that, we ensure that zero in floating point is represented with no error after quantization, thereby ensuring that operations like padding do not cause additional quantization error.
Here are a few key attributes for quantized Tensor:
  * QScheme (torch.qscheme): a enum that specifies the way we quantize the Tensor
    * torch.per_tensor_affine
    * torch.per_tensor_symmetric
    * torch.per_channel_affine
    * torch.per_channel_symmetric
  * dtype (torch.dtype): data type of the quantized Tensor
    * torch.quint8
    * torch.qint8
    * torch.qint32
    * torch.float16
  * quantization parameters (varies based on QScheme): parameters for the chosen way of quantization
    * torch.per_tensor_affine would have quantization parameters of
      * scale (float)
      * zero_point (int)
    * torch.per_channel_affine would have quantization parameters of
      * per_channel_scales (list of float)
      * per_channel_zero_points (list of int)
      * axis (int)


#### Quantize and Dequantize
The input and output of a model are floating point Tensors, but activations in the quantized model are quantized, so we need operators to convert between floating point and quantized Tensors.
  * Quantize (float -> quantized)
    * torch.quantize_per_tensor(x, scale, zero_point, dtype)
    * torch.quantize_per_channel(x, scales, zero_points, axis, dtype)
    * torch.quantize_per_tensor_dynamic(x, dtype, reduce_range)
    * to(torch.float16)
  * Dequantize (quantized -> float)
    * quantized_tensor.dequantize() - calling dequantize on a torch.float16 Tensor will convert the Tensor back to torch.float
    * torch.dequantize(x)


#### Quantized Operators/Modules
  * Quantized Operator are the operators that takes quantized Tensor as inputs, and outputs a quantized Tensor.
  * Quantized Modules are PyTorch Modules that performs quantized operations. They are typically defined for weighted operations like linear and conv.


#### Quantized Engine
When a quantized model is executed, the qengine (torch.backends.quantized.engine) specifies which backend is to be used for execution. It is important to ensure that the qengine is compatible with the quantized model in terms of value range of quantized activation and weights.
### Quantization Flow
#### Observer and FakeQuantize
  * Observer are PyTorch Modules used to:
    * collect tensor statistics like min value and max value of the Tensor passing through the observer
    * and calculate quantization parameters based on the collected tensor statistics
  * FakeQuantize are PyTorch Modules used to:
    * simulate quantization (performing quantize/dequantize) for a Tensor in the network
    * it can calculate quantization parameters based on the collected statistics from observer, or it can learn the quantization parameters as well


#### QConfig
  * QConfig is a namedtuple of Observer or FakeQuantize Module class that can are configurable with qscheme, dtype etc. it is used to configure how an operator should be observed
    * Quantization configuration for an operator/module
      * different types of Observer/FakeQuantize
      * dtype
      * qscheme
      * quant_min/quant_max: can be used to simulate lower precision Tensors
    * Currently supports configuration for activation and weight
    * We insert input/weight/output observer based on the qconfig that is configured for a given operator or module


#### General Quantization Flow
In general, the flow is the following
  * prepare
    * insert Observer/FakeQuantize modules based on user specified qconfig
  * calibrate/train (depending on post training quantization or quantization aware training)
    * allow Observers to collect statistics or FakeQuantize modules to learn the quantization parameters
  * convert
    * convert a calibrated/trained model to a quantized model


There are different modes of quantization, they can be classified in two ways:
In terms of where we apply the quantization flow, we have:
  1. Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data)
  2. Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data)


And in terms of how we quantize the operators, we can have:
  * Weight Only Quantization (only weight is statically quantized)
  * Dynamic Quantization (weight is statically quantized, activation is dynamically quantized)
  * Static Quantization (both weight and activations are statically quantized)


We can mix different ways of quantizing operators in the same quantization flow. For example, we can have post training quantization that has both statically and dynamically quantized operators.
## Quantization Support Matrix
### Quantization Mode Support
| Quantization Mode | Dataset Requirement | Works Best For | Accuracy | Notes  
---|---|---|---|---|---  
Post Training Quantization | Dynamic/Weight Only Quantization | activation dynamically quantized (fp16, int8) or not quantized, weight statically quantized (fp16, int8, in4) | None | LSTM, MLP, Embedding, Transformer | good | Easy to use, close to static quantization when performance is compute or memory bound due to weights  
Static Quantization | activation and weights statically quantized (int8) | calibration dataset | CNN | good | Provides best perf, may have big impact on accuracy, good for hardwares that only support int8 computation  
Quantization Aware Training | Dynamic Quantization | activation and weight are fake quantized | fine-tuning dataset | MLP, Embedding | best | Limited support for now  
Static Quantization | activation and weight are fake quantized | fine-tuning dataset | CNN, MLP, Embedding | best | Typically used when static quantization leads to bad accuracy, and used to close the accuracy gap  
Please see our Introduction to Quantization on Pytorch blog post for a more comprehensive overview of the tradeoffs between these quantization types.
### Quantization Flow Support
PyTorch provides two modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.
Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.
FX Graph Mode Quantization is an automated quantization framework in PyTorch, and currently it’s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with `torch.fx`). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we’ll provide general guidelines, but to actually make it work, users might need to be familiar with `torch.fx`, especially on how to make a model symbolically traceable.
New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization.
The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization:
| Eager Mode Quantization | FX Graph Mode Quantization  
---|---|---  
Release Status | beta | prototype  
Operator Fusion | Manual | Automatic  
Quant/DeQuant Placement | Manual | Automatic  
Quantizing Modules | Supported | Supported  
Quantizing Functionals/Torch Ops | Manual | Automatic  
Support for Customization | Limited Support | Fully Supported  
Quantization Mode Support | Post Training Quantization: Static, Dynamic, Weight Only Quantization Aware Training: Static | Post Training Quantization: Static, Dynamic, Weight Only Quantization Aware Training: Static  
Input/Output Model Type | `torch.nn.Module` | `torch.nn.Module` (May need some refactors to make the model compatible with FX Graph Mode Quantization)  
### Backend/Hardware Support
Hardware | Kernel Library | Eager Mode Quantization | FX Graph Mode Quantization | Quantization Mode Support  
---|---|---|---|---  
server CPU | fbgemm/onednn | Supported | All Supported  
mobile CPU | qnnpack/xnnpack  
server GPU | TensorRT (early prototype) | Not support this it requires a graph | Supported | Static Quantization  
Today, PyTorch supports the following backends for running quantized operators efficiently:
  * x86 CPUs with AVX2 support or higher (without AVX2 some operations have inefficient implementations), via x86 optimized by fbgemm and onednn (see the details at RFC)
  * ARM CPUs (typically found in mobile/embedded devices), via qnnpack
  * (early prototype) support for NVidia GPU via TensorRT through fx2trt (to be open sourced)


#### Note for native CPU backends
We expose both x86 and qnnpack with the same native pytorch quantized operators, so we need additional flag to distinguish between them. The corresponding implementation of x86 and qnnpack is chosen automatically based on the PyTorch build mode, though users have the option to override this by setting torch.backends.quantization.engine to x86 or qnnpack.
When preparing a quantized model, it is necessary to ensure that qconfig and the engine used for quantized computations match the backend on which the model will be executed. The qconfig controls the type of observers used during the quantization passes. The qengine controls whether x86 or qnnpack specific packing function is used when packing weights for linear and convolution functions and modules. For example:
Default settings for x86:
```
# set the qconfig for PTQ
# Note: the old 'fbgemm' is still available but 'x86' is the recommended default on x86 CPUs
qconfig = torch.ao.quantization.get_default_qconfig('x86')
# or, set the qconfig for QAT
qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')
# set the qengine to control weight packing
torch.backends.quantized.engine = 'x86'

```
Copy to clipboard
Default settings for qnnpack:
```
# set the qconfig for PTQ
qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')
# or, set the qconfig for QAT
qconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack')
# set the qengine to control weight packing
torch.backends.quantized.engine = 'qnnpack'

```
Copy to clipboard
### Operator Support
Operator coverage varies between dynamic and static quantization and is captured in the table below. Note that for FX Graph Mode Quantization, the corresponding functionals are also supported.
| Static Quantization | Dynamic Quantization  
---|---|---  
nn.Linear nn.Conv1d/2d/3d |  Y Y |  Y N  
nn.LSTM nn.GRU |  N N |  Y Y  
nn.RNNCell nn.GRUCell nn.LSTMCell |  N N N |  Y Y Y  
nn.EmbeddingBag | Y (activations are in fp32) | Y  
nn.Embedding | Y | Y  
nn.MultiheadAttention | Not Supported | Not supported  
Activations | Broadly supported | Un-changed, computations stay in fp32  
Note: this will be updated with some information generated from native backend_config_dict soon.
## Quantization API Reference
The Quantization API Reference contains documentation of quantization APIs, such as quantization passes, quantized tensor operations, and supported quantized modules and functions.
## Quantization Backend Configuration
The Quantization Backend Configuration contains documentation on how to configure the quantization workflows for various backends.
## Quantization Accuracy Debugging
The Quantization Accuracy Debugging contains documentation on how to debug quantization accuracy.
## Quantization Customizations
While default implementations of observers to select the scale factor and bias based on observed tensor data are provided, developers can provide their own quantization functions. Quantization can be applied selectively to different parts of the model or configured differently for different parts of the model.
We also provide support for per channel quantization for **conv1d()** , **conv2d()** , **conv3d()** and **linear()**.
Quantization workflows work by adding (e.g. adding observers as `.observer` submodule) or replacing (e.g. converting `nn.Conv2d` to `nn.quantized.Conv2d`) submodules in the model’s module hierarchy. It means that the model stays a regular `nn.Module`-based instance throughout the process and thus can work with the rest of PyTorch APIs.
### Quantization Custom Module API
Both Eager mode and FX graph mode quantization APIs provide a hook for the user to specify module quantized in a custom way, with user defined logic for observation and quantization. The user needs to specify:
  1. The Python type of the source fp32 module (existing in the model)
  2. The Python type of the observed module (provided by user). This module needs to define a from_float function which defines how the observed module is created from the original fp32 module.
  3. The Python type of the quantized module (provided by user). This module needs to define a from_observed function which defines how the quantized module is created from the observed module.
  4. A configuration describing (1), (2), (3) above, passed to the quantization APIs.


The framework will then do the following:
  1. during the prepare module swaps, it will convert every module of type specified in (1) to the type specified in (2), using the from_float function of the class in (2).
  2. during the convert module swaps, it will convert every module of type specified in (2) to the type specified in (3), using the from_observed function of the class in (3).


Currently, there is a requirement that ObservedCustomModule will have a single Tensor output, and an observer will be added by the framework (not by the user) on that output. The observer will be stored under the activation_post_process key as an attribute of the custom module instance. Relaxing these restrictions may be done at a future time.
Custom API Example:
```
import torch
import torch.ao.nn.quantized as nnq
from torch.ao.quantization import QConfigMapping
import torch.ao.quantization.quantize_fx
# original fp32 module to replace
class CustomModule(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = torch.nn.Linear(3, 3)
  def forward(self, x):
    return self.linear(x)
# custom observed module, provided by user
class ObservedCustomModule(torch.nn.Module):
  def __init__(self, linear):
    super().__init__()
    self.linear = linear
  def forward(self, x):
    return self.linear(x)
  @classmethod
  def from_float(cls, float_module):
    assert hasattr(float_module, 'qconfig')
    observed = cls(float_module.linear)
    observed.qconfig = float_module.qconfig
    return observed
# custom quantized module, provided by user
class StaticQuantCustomModule(torch.nn.Module):
  def __init__(self, linear):
    super().__init__()
    self.linear = linear
  def forward(self, x):
    return self.linear(x)
  @classmethod
  def from_observed(cls, observed_module):
    assert hasattr(observed_module, 'qconfig')
    assert hasattr(observed_module, 'activation_post_process')
    observed_module.linear.activation_post_process = \
      observed_module.activation_post_process
    quantized = cls(nnq.Linear.from_float(observed_module.linear))
    return quantized
#
# example API call (Eager mode quantization)
#
m = torch.nn.Sequential(CustomModule()).eval()
prepare_custom_config_dict = {
  "float_to_observed_custom_module_class": {
    CustomModule: ObservedCustomModule
  }
}
convert_custom_config_dict = {
  "observed_to_quantized_custom_module_class": {
    ObservedCustomModule: StaticQuantCustomModule
  }
}
m.qconfig = torch.ao.quantization.default_qconfig
mp = torch.ao.quantization.prepare(
  m, prepare_custom_config_dict=prepare_custom_config_dict)
# calibration (not shown)
mq = torch.ao.quantization.convert(
  mp, convert_custom_config_dict=convert_custom_config_dict)
#
# example API call (FX graph mode quantization)
#
m = torch.nn.Sequential(CustomModule()).eval()
qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_qconfig)
prepare_custom_config_dict = {
  "float_to_observed_custom_module_class": {
    "static": {
      CustomModule: ObservedCustomModule,
    }
  }
}
convert_custom_config_dict = {
  "observed_to_quantized_custom_module_class": {
    "static": {
      ObservedCustomModule: StaticQuantCustomModule,
    }
  }
}
mp = torch.ao.quantization.quantize_fx.prepare_fx(
  m, qconfig_mapping, torch.randn(3,3), prepare_custom_config=prepare_custom_config_dict)
# calibration (not shown)
mq = torch.ao.quantization.quantize_fx.convert_fx(
  mp, convert_custom_config=convert_custom_config_dict)

```
Copy to clipboard
## Best Practices
1. If you are using the `x86` backend, we need to use 7 bits instead of 8 bits. Make sure you reduce the range for the `quant\_min`, `quant\_max`, e.g. if `dtype` is `torch.quint8`, make sure to set a custom `quant_min` to be `0` and `quant_max` to be `127` (`255` / `2`) if `dtype` is `torch.qint8`, make sure to set a custom `quant_min` to be `-64` (`-128` / `2`) and `quant_max` to be `63` (`127` / `2`), we already set this correctly if you call the torch.ao.quantization.get_default_qconfig(backend) or torch.ao.quantization.get_default_qat_qconfig(backend) function to get the default `qconfig` for `x86` or `qnnpack` backend
2. If `onednn` backend is selected, 8 bits for activation will be used in the default qconfig mapping `torch.ao.quantization.get_default_qconfig_mapping('onednn')` and default qconfig `torch.ao.quantization.get_default_qconfig('onednn')`. It is recommended to be used on CPUs with Vector Neural Network Instruction (VNNI) support. Otherwise, setting `reduce_range` to True of the activation’s observer to get better accuracy on CPUs without VNNI support.
## Frequently Asked Questions
  1. How can I do quantized inference on GPU?:
We don’t have official GPU support yet, but this is an area of active development, you can find more information here
  2. Where can I get ONNX support for my quantized model?
If you get errors exporting the model (using APIs under `torch.onnx`), you may open an issue in the PyTorch repository. Prefix the issue title with `[ONNX]` and tag the issue as `module: onnx`.
If you encounter issues with ONNX Runtime, open an issue at GitHub - microsoft/onnxruntime.
  3. How can I use quantization with LSTM’s?:
LSTM is supported through our custom module api in both eager mode and fx graph mode quantization. Examples can be found at Eager Mode: pytorch/test_quantized_op.py TestQuantizedOps.test_custom_module_lstm FX Graph Mode: pytorch/test_quantize_fx.py TestQuantizeFx.test_static_lstm


## Common Errors
### Passing a non-quantized Tensor into a quantized kernel
If you see an error similar to:
```
RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...

```
Copy to clipboard
This means that you are trying to pass a non-quantized Tensor to a quantized kernel. A common workaround is to use `torch.ao.quantization.QuantStub` to quantize the tensor. This needs to be done manually in Eager mode quantization. An e2e example:
```
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.quant = torch.ao.quantization.QuantStub()
    self.conv = torch.nn.Conv2d(1, 1, 1)
  def forward(self, x):
    # during the convert step, this will be replaced with a
    # `quantize_per_tensor` call
    x = self.quant(x)
    x = self.conv(x)
    return x

```
Copy to clipboard
### Passing a quantized Tensor into a non-quantized kernel
If you see an error similar to:
```
RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.

```
Copy to clipboard
This means that you are trying to pass a quantized Tensor to a non-quantized kernel. A common workaround is to use `torch.ao.quantization.DeQuantStub` to dequantize the tensor. This needs to be done manually in Eager mode quantization. An e2e example:
```
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.quant = torch.ao.quantization.QuantStub()
    self.conv1 = torch.nn.Conv2d(1, 1, 1)
    # this module will not be quantized (see `qconfig = None` logic below)
    self.conv2 = torch.nn.Conv2d(1, 1, 1)
    self.dequant = torch.ao.quantization.DeQuantStub()
  def forward(self, x):
    # during the convert step, this will be replaced with a
    # `quantize_per_tensor` call
    x = self.quant(x)
    x = self.conv1(x)
    # during the convert step, this will be replaced with a
    # `dequantize` call
    x = self.dequant(x)
    x = self.conv2(x)
    return x
m = M()
m.qconfig = some_qconfig
# turn off quantization for conv2
m.conv2.qconfig = None

```
Copy to clipboard
### Saving and Loading Quantized models
When calling `torch.load` on a quantized model, if you see an error like:
```
AttributeError: 'LinearPackedParams' object has no attribute '_modules'

```
Copy to clipboard
This is because directly saving and loading a quantized model using `torch.save` and `torch.load` is not supported. To save/load quantized models, the following ways can be used:
  1. Saving/Loading the quantized model state_dict


An example:
```
class M(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(5, 5)
    self.relu = nn.ReLU()
  def forward(self, x):
    x = self.linear(x)
    x = self.relu(x)
    return x
m = M().eval()
prepare_orig = prepare_fx(m, {'' : default_qconfig})
prepare_orig(torch.rand(5, 5))
quantized_orig = convert_fx(prepare_orig)
# Save/load using state_dict
b = io.BytesIO()
torch.save(quantized_orig.state_dict(), b)
m2 = M().eval()
prepared = prepare_fx(m2, {'' : default_qconfig})
quantized = convert_fx(prepared)
b.seek(0)
quantized.load_state_dict(torch.load(b))

```
Copy to clipboard
  1. Saving/Loading scripted quantized models using `torch.jit.save` and `torch.jit.load`


An example:
```
# Note: using the same model M from previous example
m = M().eval()
prepare_orig = prepare_fx(m, {'' : default_qconfig})
prepare_orig(torch.rand(5, 5))
quantized_orig = convert_fx(prepare_orig)
# save/load using scripted model
scripted = torch.jit.script(quantized_orig)
b = io.BytesIO()
torch.jit.save(scripted, b)
b.seek(0)
scripted_quantized = torch.jit.load(b)

```
Copy to clipboard
### Symbolic Trace Error when using FX Graph Mode Quantization
Symbolic traceability is a requirement for (Prototype - maintenance mode) FX Graph Mode Quantization, so if you pass a PyTorch Model that is not symbolically traceable to torch.ao.quantization.prepare_fx or torch.ao.quantization.prepare_qat_fx, we might see an error like the following:
```
torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow

```
Copy to clipboard
Please take a look at Limitations of Symbolic Tracing and use - User Guide on Using FX Graph Mode Quantization to workaround the problem.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Quantization
    * Introduction to Quantization
    * Quantization API Summary
      * Eager Mode Quantization
        * Post Training Dynamic Quantization
        * Post Training Static Quantization
        * Quantization Aware Training for Static Quantization
        * Model Preparation for Eager Mode Static Quantization
      * (Prototype - maintenance mode) FX Graph Mode Quantization
      * (Prototype) PyTorch 2 Export Quantization
    * Quantization Stack
      * Quantized Model
        * Quantized Tensor
        * Quantize and Dequantize
        * Quantized Operators/Modules
        * Quantized Engine
      * Quantization Flow
        * Observer and FakeQuantize
        * QConfig
        * General Quantization Flow
    * Quantization Support Matrix
      * Quantization Mode Support
      * Quantization Flow Support
      * Backend/Hardware Support
        * Note for native CPU backends
      * Operator Support
    * Quantization API Reference
    * Quantization Backend Configuration
    * Quantization Accuracy Debugging
    * Quantization Customizations
      * Quantization Custom Module API
    * Best Practices
    * Frequently Asked Questions
    * Common Errors
      * Passing a non-quantized Tensor into a quantized kernel
      * Passing a quantized Tensor into a non-quantized kernel
      * Saving and Loading Quantized models
      * Symbolic Trace Error when using FX Graph Mode Quantization


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.random
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.random 

torch.random.fork_rng(_devices =None_, _enabled =True_, __caller ='fork_rng'_, __devices_kw ='devices'_, _device_type ='cuda'_)[source][source]
    
Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in. 

Parameters
    
  * **devices** (_iterable_ _of_ _Device IDs_) – devices for which to fork the RNG. CPU RNG state is always forked. By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed
  * **enabled** (_bool_) – if `False`, the RNG is not forked. This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.
  * **device_type** (_str_) – device type str, default is cuda. As for custom device, see details in [Note: support the custom device with privateuse1]



Return type
    
_Generator_ 

torch.random.get_rng_state()[source][source]
    
Returns the random number generator state as a torch.ByteTensor.
Note
The returned state is for the default generator on CPU only.
See also: `torch.random.fork_rng()`. 

Return type
    
_Tensor_ 

torch.random.initial_seed()[source][source]
    
Returns the initial seed for generating random numbers as a Python long.
Note
The returned seed is for the default generator on CPU only. 

Return type
    
int 

torch.random.manual_seed(_seed_)[source][source]
    
Sets the seed for generating random numbers on all devices. Returns a torch.Generator object. 

Parameters
    
**seed** (_int_) – The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed. 

Return type
    
_Generator_ 

torch.random.seed()[source][source]
    
Sets the seed for generating random numbers to a non-deterministic random number on all devices. Returns a 64 bit number used to seed the RNG. 

Return type
    
int 

torch.random.set_rng_state(_new_state_)[source][source]
    
Sets the random number generator state.
Note
This function only works for CPU. For CUDA, please use `torch.manual_seed()`, which works for both CPU and CUDA. 

Parameters
    
**new_state** (_torch.ByteTensor_) – The desired state
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.random
    * `fork_rng()`
    * `get_rng_state()`
    * `initial_seed()`
    * `manual_seed()`
    * `seed()`
    * `set_rng_state()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.signal
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.signal
The torch.signal module, modeled after SciPy’s signal module.
## torch.signal.windows
`bartlett` | Computes the Bartlett window.  
---|---  
`blackman` | Computes the Blackman window.  
`cosine` | Computes a window with a simple cosine waveform, following the same implementation as SciPy.  
`exponential` | Computes a window with an exponential waveform.  
`gaussian` | Computes a window with a gaussian waveform.  
`general_cosine` | Computes the general cosine window.  
`general_hamming` | Computes the general Hamming window.  
`hamming` | Computes the Hamming window.  
`hann` | Computes the Hann window.  
`kaiser` | Computes the Kaiser window.  
`nuttall` | Computes the minimum 4-term Blackman-Harris window according to Nuttall.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.signal
    * torch.signal.windows


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.profiler
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.profiler
## Overview
PyTorch Profiler is a tool that allows the collection of performance metrics during training and inference. Profiler’s context manager API can be used to better understand what model operators are the most expensive, examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
Note
An earlier version of the API in `torch.autograd` module is considered legacy and will be deprecated.
## API Reference 

_class_ torch.profiler._KinetoProfile(_*_ , _activities =None_, _record_shapes =False_, _profile_memory =False_, _with_stack =False_, _with_flops =False_, _with_modules =False_, _experimental_config =None_, _execution_trace_observer =None_, _acc_events =False_, _custom_trace_id_callback =None_)[source][source]
    
Low-level profiler wrap the autograd profile 

Parameters
    
  * **activities** (_iterable_) – list of activity groups (CPU, CUDA) to use in profiling, supported values: `torch.profiler.ProfilerActivity.CPU`, `torch.profiler.ProfilerActivity.CUDA`, `torch.profiler.ProfilerActivity.XPU`. Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA or (when available) ProfilerActivity.XPU.
  * **record_shapes** (_bool_) – save information about operator’s input shapes.
  * **profile_memory** (_bool_) – track tensor memory allocation/deallocation (see `export_memory_timeline` for more details).
  * **with_stack** (_bool_) – record source information (file and line number) for the ops.
  * **with_flops** (_bool_) – use formula to estimate the FLOPS of specific operators (matrix multiplication and 2D convolution).
  * **with_modules** (_bool_) – record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module A’s forward call’s module B’s forward which contains an aten::add op, then aten::add’s module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models.
  * **experimental_config** (__ExperimentalConfig_) – A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.
  * **execution_trace_observer** (_ExecutionTraceObserver_) – A PyTorch Execution Trace Observer object. PyTorch Execution Traces offer a graph based representation of AI/ML workloads and enable replay benchmarks, simulators, and emulators. When this argument is included the observer start() and stop() will be called for the same time window as PyTorch profiler.
  * **acc_events** (_bool_) – Enable the accumulation of FunctionEvents across multiple profiling cycles


Note
This API is experimental and subject to change in the future.
Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies. 

add_metadata(_key_ , _value_)[source][source]
    
Adds a user defined metadata with a string key and a string value into the trace file 

add_metadata_json(_key_ , _value_)[source][source]
    
Adds a user defined metadata with a string key and a valid json value into the trace file 

events()[source][source]
    
Returns the list of unaggregated profiler events, to be used in the trace callback or after the profiling is finished 

export_chrome_trace(_path_)[source][source]
    
Exports the collected trace in Chrome JSON format. If kineto is enabled, only last cycle in schedule is exported. 

export_memory_timeline(_path_ , _device =None_)[source][source]
    
Export memory event information from the profiler collected tree for a given device, and export a timeline plot. There are 3 exportable files using `export_memory_timeline`, each controlled by the `path`’s suffix.
  * For an HTML compatible plot, use the suffix `.html`, and a memory timeline plot will be embedded as a PNG file in the HTML file.
  * For plot points consisting of `[times, [sizes by category]]`, where `times` are timestamps and `sizes` are memory usage for each category. The memory timeline plot will be saved a JSON (`.json`) or gzipped JSON (`.json.gz`) depending on the suffix.
  * For raw memory points, use the suffix `.raw.json.gz`. Each raw memory event will consist of `(timestamp, action, numbytes, category)`, where `action` is one of `[PREEXISTING, CREATE, INCREMENT_VERSION, DESTROY]`, and `category` is one of the enums from `torch.profiler._memory_profiler.Category`.


Output: Memory timeline written as gzipped JSON, JSON, or HTML. 

export_stacks(_path_ , _metric ='self_cpu_time_total'_)[source][source]
    
Save stack traces to a file 

Parameters
    
  * **path** (_str_) – save stacks file to this location;
  * **metric** (_str_) – metric to use: “self_cpu_time_total” or “self_cuda_time_total”



key_averages(_group_by_input_shape =False_, _group_by_stack_n =0_, _group_by_overload_name =False_)[source][source]
    
Averages events, grouping them by operator name and (optionally) input shapes, stack and overload name.
Note
To use shape/stack functionality make sure to set record_shapes/with_stack when creating profiler context manager. 

preset_metadata_json(_key_ , _value_)[source][source]
    
Preset a user defined metadata when the profiler is not started and added into the trace file later. Metadata is in the format of a string key and a valid json value 

toggle_collection_dynamic(_enable_ , _activities_)[source][source]
    
Toggle collection of activities on/off at any point of collection. Currently supports toggling Torch Ops (CPU) and CUDA activity supported in Kineto 

Parameters
    
**activities** (_iterable_) – list of activity groups to use in profiling, supported values: `torch.profiler.ProfilerActivity.CPU`, `torch.profiler.ProfilerActivity.CUDA`
Examples:
```
with torch.profiler.profile(
  activities=[
    torch.profiler.ProfilerActivity.CPU,
    torch.profiler.ProfilerActivity.CUDA,
  ]
) as p:
  code_to_profile_0()
  // turn off collection of all CUDA activity
  p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CUDA])
  code_to_profile_1()
  // turn on collection of all CUDA activity
  p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CUDA])
  code_to_profile_2()
print(p.key_averages().table(
  sort_by="self_cuda_time_total", row_limit=-1))

```
Copy to clipboard 

_class_ torch.profiler.profile(_*_ , _activities =None_, _schedule =None_, _on_trace_ready =None_, _record_shapes =False_, _profile_memory =False_, _with_stack =False_, _with_flops =False_, _with_modules =False_, _experimental_config =None_, _execution_trace_observer =None_, _acc_events =False_, _use_cuda =None_, _custom_trace_id_callback =None_)[source][source]
    
Profiler context manager. 

Parameters
    
  * **activities** (_iterable_) – list of activity groups (CPU, CUDA) to use in profiling, supported values: `torch.profiler.ProfilerActivity.CPU`, `torch.profiler.ProfilerActivity.CUDA`, `torch.profiler.ProfilerActivity.XPU`. Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA or (when available) ProfilerActivity.XPU.
  * **schedule** (_Callable_) – callable that takes step (int) as a single parameter and returns `ProfilerAction` value that specifies the profiler action to perform at each step.
  * **on_trace_ready** (_Callable_) – callable that is called at each step when `schedule` returns `ProfilerAction.RECORD_AND_SAVE` during the profiling.
  * **record_shapes** (_bool_) – save information about operator’s input shapes.
  * **profile_memory** (_bool_) – track tensor memory allocation/deallocation.
  * **with_stack** (_bool_) – record source information (file and line number) for the ops.
  * **with_flops** (_bool_) – use formula to estimate the FLOPs (floating point operations) of specific operators (matrix multiplication and 2D convolution).
  * **with_modules** (_bool_) – record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module A’s forward call’s module B’s forward which contains an aten::add op, then aten::add’s module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models.
  * **experimental_config** (__ExperimentalConfig_) – A set of experimental options used for Kineto library features. Note, backward compatibility is not guaranteed.
  * **execution_trace_observer** (_ExecutionTraceObserver_) – A PyTorch Execution Trace Observer object. PyTorch Execution Traces offer a graph based representation of AI/ML workloads and enable replay benchmarks, simulators, and emulators. When this argument is included the observer start() and stop() will be called for the same time window as PyTorch profiler. See the examples section below for a code sample.
  * **acc_events** (_bool_) – Enable the accumulation of FunctionEvents across multiple profiling cycles
  * **use_cuda** (_bool_) – 
Deprecated since version 1.8.1: use `activities` instead.


Note
Use `schedule()` to generate the callable schedule. Non-default schedules are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process. The default schedule simply records all the events continuously for the duration of the context manager.
Note
Use `tensorboard_trace_handler()` to generate result files for TensorBoard:
`on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)`
After profiling, result files can be found in the specified directory. Use the command:
`tensorboard --logdir dir_name`
to see the results in TensorBoard. For more information, see PyTorch Profiler TensorBoard Plugin
Note
Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.
Examples:
```
with torch.profiler.profile(
  activities=[
    torch.profiler.ProfilerActivity.CPU,
    torch.profiler.ProfilerActivity.CUDA,
  ]
) as p:
  code_to_profile()
print(p.key_averages().table(
  sort_by="self_cuda_time_total", row_limit=-1))

```
Copy to clipboard
Using the profiler’s `schedule`, `on_trace_ready` and `step` functions:
```
# Non-default profiler schedule allows user to turn profiler on and off
# on different iterations of the training loop;
# trace_handler is called every time a new trace becomes available
def trace_handler(prof):
  print(prof.key_averages().table(
    sort_by="self_cuda_time_total", row_limit=-1))
  # prof.export_chrome_trace("/tmp/test_trace_" + str(prof.step_num) + ".json")
with torch.profiler.profile(
  activities=[
    torch.profiler.ProfilerActivity.CPU,
    torch.profiler.ProfilerActivity.CUDA,
  ],
  # In this example with wait=1, warmup=1, active=2, repeat=1,
  # profiler will skip the first step/iteration,
  # start warming up on the second, record
  # the third and the forth iterations,
  # after which the trace will become available
  # and on_trace_ready (when set) is called;
  # the cycle repeats starting with the next step
  schedule=torch.profiler.schedule(
    wait=1,
    warmup=1,
    active=2,
    repeat=1),
  on_trace_ready=trace_handler
  # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')
  # used when outputting for tensorboard
  ) as p:
    for iter in range(N):
      code_iteration_to_profile(iter)
      # send a signal to the profiler that the next iteration has started
      p.step()

```
Copy to clipboard
The following sample shows how to setup up an Execution Trace Observer (execution_trace_observer)
```
with torch.profiler.profile(
  ...
  execution_trace_observer=(
    ExecutionTraceObserver().register_callback("./execution_trace.json")
  ),
) as p:
  for iter in range(N):
    code_iteration_to_profile(iter)
    p.step()

```
Copy to clipboard
You can also refer to test_execution_trace_with_kineto() in tests/profiler/test_profiler.py. Note: One can also pass any object satisfying the _ITraceObserver interface. 

get_trace_id()[source][source]
    
Returns the current trace ID. 

set_custom_trace_id_callback(_callback_)[source][source]
    
Sets a callback to be called when a new trace ID is generated. 

step()[source][source]
    
Signals the profiler that the next profiling step has started. 

_class_ torch.profiler.ProfilerAction(_value_)[source][source]
    
Profiler actions that can be taken at the specified intervals 

_class_ torch.profiler.ProfilerActivity
    
Members:
CPU
XPU
MTIA
CUDA
HPU
PrivateUse1 

_property_ name


torch.profiler.schedule(_*_ , _wait_ , _warmup_ , _active_ , _repeat =0_, _skip_first =0_, _skip_first_wait =0_)[source][source]
    
Returns a callable that can be used as profiler `schedule` argument. The profiler will skip the first `skip_first` steps, then wait for `wait` steps, then do the warmup for the next `warmup` steps, then do the active recording for the next `active` steps and then repeat the cycle starting with `wait` steps. The optional number of cycles is specified with the `repeat` parameter, the zero value means that the cycles will continue until the profiling is finished.
The `skip_first_wait` parameter controls whether the first `wait` stage should be skipped. This can be useful if a user wants to wait longer than `skip_first` between cycles, but not for the first profile. For example, if `skip_first` is 10 and `wait` is 20, the first cycle will wait 10 + 20 = 30 steps before warmup if `skip_first_wait` is zero, but will wait only 10 steps if `skip_first_wait` is non-zero. All subsequent cycles will then wait 20 steps between the last active and warmup. 

Return type
    
_Callable_ 

torch.profiler.tensorboard_trace_handler(_dir_name_ , _worker_name =None_, _use_gzip =False_)[source][source]
    
Outputs tracing files to directory of `dir_name`, then that directory can be directly delivered to tensorboard as logdir. `worker_name` should be unique for each worker in distributed scenario, it will be set to ‘[hostname]_[pid]’ by default.
## Intel Instrumentation and Tracing Technology APIs 

torch.profiler.itt.is_available()[source][source]
    
Check if ITT feature is available or not 

torch.profiler.itt.mark(_msg_)[source][source]
    
Describe an instantaneous event that occurred at some point. 

Parameters
    
**msg** (_str_) – ASCII message to associate with the event. 

torch.profiler.itt.range_push(_msg_)[source][source]
    
Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started. 

Parameters
    
**msg** (_str_) – ASCII message to associate with range 

torch.profiler.itt.range_pop()[source][source]
    
Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.profiler
    * Overview
    * API Reference
      * `_KinetoProfile`
        * `_KinetoProfile.add_metadata()`
        * `_KinetoProfile.add_metadata_json()`
        * `_KinetoProfile.events()`
        * `_KinetoProfile.export_chrome_trace()`
        * `_KinetoProfile.export_memory_timeline()`
        * `_KinetoProfile.export_stacks()`
        * `_KinetoProfile.key_averages()`
        * `_KinetoProfile.preset_metadata_json()`
        * `_KinetoProfile.toggle_collection_dynamic()`
      * `profile`
        * `profile.get_trace_id()`
        * `profile.set_custom_trace_id_callback()`
        * `profile.step()`
      * `ProfilerAction`
      * `ProfilerActivity`
        * `ProfilerActivity.name`
      * `schedule()`
      * `tensorboard_trace_handler()`
    * Intel Instrumentation and Tracing Technology APIs
      * `is_available()`
      * `mark()`
      * `range_push()`
      * `range_pop()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Distributed RPC Framework
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Distributed RPC Framework
The distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.
Warning
APIs in the RPC package are stable. There are multiple ongoing work items to improve performance and error handling, which will ship in future releases.
Warning
CUDA support was introduced in PyTorch 1.9 and is still a **beta** feature. Not all features of the RPC package are yet compatible with CUDA support and thus their use is discouraged. These unsupported features include: RRefs, JIT compatibility, dist autograd and dist optimizer, and profiling. These shortcomings will be addressed in future releases.
Note
Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.
## Basics
The distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.
  1. **Remote Procedure Call (RPC)** supports running a function on the specified destination worker with the given arguments and getting the return value back or creating a reference to the return value. There are three main RPC APIs: `rpc_sync()` (synchronous), `rpc_async()` (asynchronous), and `remote()` (asynchronous and returns a reference to the remote return value). Use the synchronous API if the user code cannot proceed without the return value. Otherwise, use the asynchronous API to get a future, and wait on the future when the return value is needed on the caller. The `remote()` API is useful when the requirement is to create something remotely but never need to fetch it to the caller. Imagine the case that a driver process is setting up a parameter server and a trainer. The driver can create an embedding table on the parameter server and then share the reference to the embedding table with the trainer, but itself will never use the embedding table locally. In this case, `rpc_sync()` and `rpc_async()` are no longer appropriate, as they always imply that the return value will be returned to the caller immediately or in the future.
  2. **Remote Reference (RRef)** serves as a distributed shared pointer to a local or remote object. It can be shared with other workers and reference counting will be handled transparently. Each RRef only has one owner and the object only lives on that owner. Non-owner workers holding RRefs can get copies of the object from the owner by explicitly requesting it. This is useful when a worker needs to access some data object, but itself is neither the creator (the caller of `remote()`) or the owner of the object. The distributed optimizer, as we will discuss below, is one example of such use cases.
  3. **Distributed Autograd** stitches together local autograd engines on all the workers involved in the forward pass, and automatically reach out to them during the backward pass to compute gradients. This is especially helpful if the forward pass needs to span multiple machines when conducting, e.g., distributed model parallel training, parameter-server training, etc. With this feature, user code no longer needs to worry about how to send gradients across RPC boundaries and in which order should the local autograd engines be launched, which can become quite complicated where there are nested and inter-dependent RPC calls in the forward pass.
  4. **Distributed Optimizer** ’s constructor takes a `Optimizer()` (e.g., `SGD()`, `Adagrad()`, etc.) and a list of parameter RRefs, creates an `Optimizer()` instance on each distinct RRef owner, and updates parameters accordingly when running `step()`. When you have distributed forward and backward passes, parameters and gradients will be scattered across multiple workers, and hence it requires an optimizer on each of the involved workers. Distributed Optimizer wraps all those local optimizers into one, and provides a concise constructor and `step()` API.


## RPC
Before using RPC and distributed autograd primitives, initialization must take place. To initialize the RPC framework we need to use `init_rpc()` which would initialize the RPC framework, RRef framework and distributed autograd. 

torch.distributed.rpc.init_rpc(_name_ , _backend =None_, _rank =-1_, _world_size =None_, _rpc_backend_options =None_)[source][source]
    
Initializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs. 

Parameters
    
  * **name** (_str_) – a globally unique name of this node. (e.g., `Trainer3`, `ParameterServer2`, `Master`, `Worker1`) Name can only contain number, alphabet, underscore, colon, and/or dash, and must be shorter than 128 characters.
  * **backend** (_BackendType_ _,__optional_) – The type of RPC backend implementation. Supported values is `BackendType.TENSORPIPE` (the default). See Backends for more information.
  * **rank** (_int_) – a globally unique id/rank of this node.
  * **world_size** (_int_) – The number of workers in the group.
  * **rpc_backend_options** (_RpcBackendOptions_ _,__optional_) – The options passed to the RpcAgent constructor. It must be an agent-specific subclass of `RpcBackendOptions` and contains agent-specific initialization configurations. By default, for all agents, it sets the default timeout to 60 seconds and performs the rendezvous with an underlying process group initialized using `init_method = "env://"`, meaning that environment variables `MASTER_ADDR` and `MASTER_PORT` need to be set properly. See Backends for more information and find which options are available.


The following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a `Tensor` as an argument or a return value, the destination worker will try to create a `Tensor` with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.
Warning
TorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, `torch.distributed.rpc` supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL. 

torch.distributed.rpc.rpc_sync(_to_ , _func_ , _args =None_, _kwargs =None_, _timeout =-1.0_)[source][source]
    
Make a blocking RPC call to run function `func` on worker `to`. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. 

Parameters
    
  * **to** (_str_ _or_ _WorkerInfo_ _or_ _int_) – name/rank/`WorkerInfo` of the destination worker.
  * **func** (_Callable_) – a callable function, such as Python callables, builtin operators (e.g. `add()`) and annotated TorchScript functions.
  * **args** (_tuple_) – the argument tuple for the `func` invocation.
  * **kwargs** (_dict_) – is a dictionary of keyword arguments for the `func` invocation.
  * **timeout** (_float_ _,__optional_) – timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with `_set_rpc_timeout` is used.



Returns
    
Returns the result of running `func` with `args` and `kwargs`. 

Example::
    
Make sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both workers. Refer to `init_process_group()` API for more details. For example,
export MASTER_ADDR=localhost export MASTER_PORT=5678
Then run the following code in two different processes:
```
>>> # On worker 0:
>>> import torch
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> ret = rpc.rpc_sync("worker1", torch.add, args=(torch.ones(2), 3))
>>> rpc.shutdown()

```
Copy to clipboard
```
>>> # On worker 1:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> rpc.shutdown()

```
Copy to clipboard
Below is an example of running a TorchScript function using RPC.
```
>>> # On both workers:
>>> @torch.jit.script
>>> def my_script_add(tensor: torch.Tensor, scalar: int):
>>>   return torch.add(tensor, scalar)

```
Copy to clipboard
```
>>> # On worker 0:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> ret = rpc.rpc_sync("worker1", my_script_add, args=(torch.ones(2), 3))
>>> rpc.shutdown()

```
Copy to clipboard
```
>>> # On worker 1:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> rpc.shutdown()

```
Copy to clipboard 

torch.distributed.rpc.rpc_async(_to_ , _func_ , _args =None_, _kwargs =None_, _timeout =-1.0_)[source][source]
    
Make a non-blocking RPC call to run function `func` on worker `to`. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a `Future` that can be awaited on. 

Parameters
    
  * **to** (_str_ _or_ _WorkerInfo_ _or_ _int_) – name/rank/`WorkerInfo` of the destination worker.
  * **func** (_Callable_) – a callable function, such as Python callables, builtin operators (e.g. `add()`) and annotated TorchScript functions.
  * **args** (_tuple_) – the argument tuple for the `func` invocation.
  * **kwargs** (_dict_) – is a dictionary of keyword arguments for the `func` invocation.
  * **timeout** (_float_ _,__optional_) – timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with `_set_rpc_timeout` is used.



Returns
    
Returns a `Future` object that can be waited on. When completed, the return value of `func` on `args` and `kwargs` can be retrieved from the `Future` object.
Warning
Using GPU tensors as arguments or return values of `func` is not supported since we don’t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of `func`.
Warning
The `rpc_async` API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned `Future` completes. 

Example::
    
Make sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both workers. Refer to `init_process_group()` API for more details. For example,
export MASTER_ADDR=localhost export MASTER_PORT=5678
Then run the following code in two different processes:
```
>>> # On worker 0:
>>> import torch
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> fut1 = rpc.rpc_async("worker1", torch.add, args=(torch.ones(2), 3))
>>> fut2 = rpc.rpc_async("worker1", min, args=(1, 2))
>>> result = fut1.wait() + fut2.wait()
>>> rpc.shutdown()

```
Copy to clipboard
```
>>> # On worker 1:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> rpc.shutdown()

```
Copy to clipboard
Below is an example of running a TorchScript function using RPC.
```
>>> # On both workers:
>>> @torch.jit.script
>>> def my_script_add(tensor: torch.Tensor, scalar: int):
>>>   return torch.add(tensor, scalar)

```
Copy to clipboard
```
>>> # On worker 0:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> fut = rpc.rpc_async("worker1", my_script_add, args=(torch.ones(2), 3))
>>> ret = fut.wait()
>>> rpc.shutdown()

```
Copy to clipboard
```
>>> # On worker 1:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> rpc.shutdown()

```
Copy to clipboard 

torch.distributed.rpc.remote(_to_ , _func_ , _args =None_, _kwargs =None_, _timeout =-1.0_)[source][source]
    
Make a remote call to run `func` on worker `to` and return an `RRef` to the result value immediately. Worker `to` will be the owner of the returned `RRef`, and the worker calling `remote` is a user. The owner manages the global reference count of its `RRef`, and the owner `RRef` is only destructed when globally there are no living references to it. 

Parameters
    
  * **to** (_str_ _or_ _WorkerInfo_ _or_ _int_) – name/rank/`WorkerInfo` of the destination worker.
  * **func** (_Callable_) – a callable function, such as Python callables, builtin operators (e.g. `add()`) and annotated TorchScript functions.
  * **args** (_tuple_) – the argument tuple for the `func` invocation.
  * **kwargs** (_dict_) – is a dictionary of keyword arguments for the `func` invocation.
  * **timeout** (_float_ _,__optional_) – timeout in seconds for this remote call. If the creation of this `RRef` on worker `to` is not successfully processed on this worker within this timeout, then the next time there is an attempt to use the RRef (such as `to_here()`), a timeout will be raised indicating this failure. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with `_set_rpc_timeout` is used.



Returns
    
A user `RRef` instance to the result value. Use the blocking API `torch.distributed.rpc.RRef.to_here()` to retrieve the result value locally.
Warning
The `remote` API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the `torch.distributed.rpc.RRef.confirmed_by_owner()` API.
Warning
Errors such as timeouts for the `remote` API are handled on a best-effort basis. This means that when remote calls initiated by `remote` fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as `to_here` or fork call), then future uses of the `RRef` will appropriately raise errors. However, it is possible that the user application will use the `RRef` before the errors are handled. In this case, errors may not be raised as they have not yet been handled.
Example:
```
Make sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly
on both workers. Refer to :meth:`~torch.distributed.init_process_group`
API for more details. For example,
export MASTER_ADDR=localhost
export MASTER_PORT=5678
Then run the following code in two different processes:
>>> # On worker 0:
>>> import torch
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> rref1 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 3))
>>> rref2 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 1))
>>> x = rref1.to_here() + rref2.to_here()
>>> rpc.shutdown()
>>> # On worker 1:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> rpc.shutdown()
Below is an example of running a TorchScript function using RPC.
>>> # On both workers:
>>> @torch.jit.script
>>> def my_script_add(tensor: torch.Tensor, scalar: int):
>>>  return torch.add(tensor, scalar)
>>> # On worker 0:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> rref = rpc.remote("worker1", my_script_add, args=(torch.ones(2), 3))
>>> rref.to_here()
>>> rpc.shutdown()
>>> # On worker 1:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> rpc.shutdown()

```
Copy to clipboard 

torch.distributed.rpc.get_worker_info(_worker_name =None_)[source][source]
    
Get `WorkerInfo` of a given worker name. Use this `WorkerInfo` to avoid passing an expensive string on every invocation. 

Parameters
    
**worker_name** (_str_) – the string name of a worker. If `None`, return the the id of the current worker. (default `None`) 

Returns
    
`WorkerInfo` instance for the given `worker_name` or `WorkerInfo` of the current worker if `worker_name` is `None`. 

torch.distributed.rpc.shutdown(_graceful =True_, _timeout =0_)[source][source]
    
Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If `graceful=True`, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if `graceful=False`, this is a local shutdown, and it does not wait for other RPC processes to reach this method.
Warning
For `Future` objects returned by `rpc_async()`, `future.wait()` should not be called after `shutdown()`. 

Parameters
    
**graceful** (_bool_) – Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for `UserRRefs` and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete. 

Example::
    
Make sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both workers. Refer to `init_process_group()` API for more details. For example,
export MASTER_ADDR=localhost export MASTER_PORT=5678
Then run the following code in two different processes:
```
>>> # On worker 0:
>>> import torch
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> # do some work
>>> result = rpc.rpc_sync("worker1", torch.add, args=(torch.ones(1), 1))
>>> # ready to shutdown
>>> rpc.shutdown()

```
Copy to clipboard
```
>>> # On worker 1:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> # wait for worker 0 to finish work, and then shutdown.
>>> rpc.shutdown()

```
Copy to clipboard 

_class_ torch.distributed.rpc.WorkerInfo
    
A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through `get_worker_info()` and the result can be passed in to functions such as `rpc_sync()`, `rpc_async()`, `remote()` to avoid copying a string on every invocation. 

_property_ id
    
Globally unique id to identify the worker. 

_property_ name
    
The name of the worker.
The RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side. 

torch.distributed.rpc.functions.async_execution(_fn_)[source][source]
    
A decorator for a function indicating that the return value of the function is guaranteed to be a `Future` object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the `Future` returned by the wrapped function and installs subsequent processing steps as a callback to that `Future`. The installed callback will read the value from the `Future` when completed and send the value back as the RPC response. That also means the returned `Future` only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function’s (`fn`) execution needs to pause and resume due to, e.g., containing `rpc_async()` or waiting for other signals.
Note
To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a `Future` object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with `@staticmethod` or `@classmethod`, `@rpc.functions.async_execution` needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by `@rpc.functions.async_execution`. 

Example::
    
The returned `Future` object can come from `rpc_async()`, `then()`, or `Future` constructor. The example below shows directly using the `Future` returned by `then()`.
```
>>> from torch.distributed import rpc
>>>
>>> # omitting setup and shutdown RPC
>>>
>>> # On all workers
>>> @rpc.functions.async_execution
>>> def async_add_chained(to, x, y, z):
>>>   # This function runs on "worker1" and returns immediately when
>>>   # the callback is installed through the `then(cb)` API. In the
>>>   # mean time, the `rpc_async` to "worker2" can run concurrently.
>>>   # When the return value of that `rpc_async` arrives at
>>>   # "worker1", "worker1" will run the lambda function accordingly
>>>   # and set the value for the previously returned `Future`, which
>>>   # will then trigger RPC to send the result back to "worker0".
>>>   return rpc.rpc_async(to, torch.add, args=(x, y)).then(
>>>     lambda fut: fut.wait() + z
>>>   )
>>>
>>> # On worker0
>>> ret = rpc.rpc_sync(
>>>   "worker1",
>>>   async_add_chained,
>>>   args=("worker2", torch.ones(2), 1, 1)
>>> )
>>> print(ret) # prints tensor([3., 3.])

```
Copy to clipboard
When combined with TorchScript decorators, this decorator must be the outmost one.
```
>>> from torch import Tensor
>>> from torch.futures import Future
>>> from torch.distributed import rpc
>>>
>>> # omitting setup and shutdown RPC
>>>
>>> # On all workers
>>> @torch.jit.script
>>> def script_add(x: Tensor, y: Tensor) -> Tensor:
>>>   return x + y
>>>
>>> @rpc.functions.async_execution
>>> @torch.jit.script
>>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]:
>>>   return rpc.rpc_async(to, script_add, (x, y))
>>>
>>> # On worker0
>>> ret = rpc.rpc_sync(
>>>   "worker1",
>>>   async_add,
>>>   args=("worker2", torch.ones(2), 1)
>>> )
>>> print(ret) # prints tensor([2., 2.])

```
Copy to clipboard
When combined with static or class method, this decorator must be the inner one.
```
>>> from torch.distributed import rpc
>>>
>>> # omitting setup and shutdown RPC
>>>
>>> # On all workers
>>> class AsyncExecutionClass:
>>>
>>>   @staticmethod
>>>   @rpc.functions.async_execution
>>>   def static_async_add(to, x, y, z):
>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(
>>>       lambda fut: fut.wait() + z
>>>     )
>>>
>>>   @classmethod
>>>   @rpc.functions.async_execution
>>>   def class_async_add(cls, to, x, y, z):
>>>     ret_fut = torch.futures.Future()
>>>     rpc.rpc_async(to, torch.add, args=(x, y)).then(
>>>       lambda fut: ret_fut.set_result(fut.wait() + z)
>>>     )
>>>     return ret_fut
>>>
>>>   @rpc.functions.async_execution
>>>   def bound_async_add(self, to, x, y, z):
>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(
>>>       lambda fut: fut.wait() + z
>>>     )
>>>
>>> # On worker0
>>> ret = rpc.rpc_sync(
>>>   "worker1",
>>>   AsyncExecutionClass.static_async_add,
>>>   args=("worker2", torch.ones(2), 1, 2)
>>> )
>>> print(ret) # prints tensor([4., 4.])
>>>
>>> ret = rpc.rpc_sync(
>>>   "worker1",
>>>   AsyncExecutionClass.class_async_add,
>>>   args=("worker2", torch.ones(2), 1, 2)
>>> )
>>> print(ret) # prints tensor([4., 4.])

```
Copy to clipboard
This decorator also works with RRef helpers, i.e., . `torch.distributed.rpc.RRef.rpc_sync()`, `torch.distributed.rpc.RRef.rpc_async()`, and `torch.distributed.rpc.RRef.remote()`.
```
>>> from torch.distributed import rpc
>>>
>>> # reuse the AsyncExecutionClass class above
>>> rref = rpc.remote("worker1", AsyncExecutionClass)
>>> ret = rref.rpc_sync().static_async_add("worker2", torch.ones(2), 1, 2)
>>> print(ret) # prints tensor([4., 4.])
>>>
>>> rref = rpc.remote("worker1", AsyncExecutionClass)
>>> ret = rref.rpc_async().static_async_add("worker2", torch.ones(2), 1, 2).wait()
>>> print(ret) # prints tensor([4., 4.])
>>>
>>> rref = rpc.remote("worker1", AsyncExecutionClass)
>>> ret = rref.remote().static_async_add("worker2", torch.ones(2), 1, 2).to_here()
>>> print(ret) # prints tensor([4., 4.])

```
Copy to clipboard
### Backends
The RPC module can leverage different backends to perform the communication between the nodes. The backend to be used can be specified in the `init_rpc()` function, by passing a certain value of the `BackendType` enum. Regardless of what backend is used, the rest of the RPC API won’t change. Each backend also defines its own subclass of the `RpcBackendOptions` class, an instance of which can also be passed to `init_rpc()` to configure the backend’s behavior. 

_class_ torch.distributed.rpc.BackendType(_value_)
    
An enum class of available backends.
PyTorch ships with a builtin `BackendType.TENSORPIPE` backend. Additional ones can be registered using the `register_backend()` function. 

_class_ torch.distributed.rpc.RpcBackendOptions
    
An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to `init_rpc()` in order to initialize RPC with specific configurations, such as the RPC timeout and `init_method` to be used. 

_property_ init_method
    
URL specifying how to initialize the process group. Default is `env://` 

_property_ rpc_timeout
    
A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out.
#### TensorPipe Backend
The TensorPipe agent, which is the default, leverages the TensorPipe library, which provides a natively point-to-point communication primitive specifically suited for machine learning that fundamentally addresses some of the limitations of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows a large number of transfers to occur simultaneously, each at their own speed, without blocking each other. It will only open pipes between pairs of nodes when needed, on demand, and when one node fails only its incident pipes will be closed, while all other ones will keep working as normal. In addition, it is able to support multiple different transports (TCP, of course, but also shared memory, NVLink, InfiniBand, …) and can automatically detect their availability and negotiate the best transport to use for each pipe.
The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.
Example:
```
>>> import os
>>> from torch.distributed import rpc
>>> os.environ['MASTER_ADDR'] = 'localhost'
>>> os.environ['MASTER_PORT'] = '29500'
>>>
>>> rpc.init_rpc(
>>>   "worker1",
>>>   rank=0,
>>>   world_size=2,
>>>   rpc_backend_options=rpc.TensorPipeRpcBackendOptions(
>>>     num_worker_threads=8,
>>>     rpc_timeout=20 # 20 second timeout
>>>   )
>>> )
>>>
>>> # omitting init_rpc invocation on worker2

```
Copy to clipboard 

_class_ torch.distributed.rpc.TensorPipeRpcBackendOptions(_*_ , _num_worker_threads =16_, _rpc_timeout =60.0_, _init_method ='env://'_, _device_maps =None_, _devices =None_, __transports =None_, __channels =None_)[source][source]
    
The backend options for `TensorPipeAgent`, derived from `RpcBackendOptions`. 

Parameters
    
  * **num_worker_threads** (_int_ _,__optional_) – The number of threads in the thread-pool used by `TensorPipeAgent` to execute requests (default: 16).
  * **rpc_timeout** (_float_ _,__optional_) – The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in `rpc_sync()` and `rpc_async()` if necessary.
  * **init_method** (_str_ _,__optional_) – The URL to initialize the distributed store used for rendezvous. It takes any value accepted for the same argument of `init_process_group()` (default: `env://`).
  * **device_maps** (_Dict_ _[__str_ _,__Dict_ _]__,__optional_) – Device placement mappings from this worker to the callee. Key is the callee worker name and value the dictionary (`Dict` of `int`, `str`, or `torch.device`) that maps this worker’s devices to the callee worker’s devices. (default: `None`)
  * **devices** (List[int, str, or `torch.device`], optional) – all local CUDA devices used by RPC agent. By Default, it will be initialized to all local devices from its own `device_maps` and corresponding devices from its peers’ `device_maps`. When processing CUDA RPC requests, the agent will properly synchronize CUDA streams for all devices in this `List`.



_property_ device_maps
    
The device map locations. 

_property_ devices
    
All devices used by the local agent. 

_property_ init_method
    
URL specifying how to initialize the process group. Default is `env://` 

_property_ num_worker_threads
    
The number of threads in the thread-pool used by `TensorPipeAgent` to execute requests. 

_property_ rpc_timeout
    
A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. 

set_device_map(_to_ , _device_map_)[source][source]
    
Set device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations. 

Parameters
    
  * **to** (_str_) – Callee name.
  * **device_map** (_Dict_ _of_ _int_ _,__str_ _, or_ _torch.device_) – Device placement mappings from this worker to the callee. This map must be invertible.


Example
```
>>> # both workers
>>> def add(x, y):
>>>   print(x) # tensor([1., 1.], device='cuda:1')
>>>   return x + y, (x + y).to(2)
>>>
>>> # on worker 0
>>> options = TensorPipeRpcBackendOptions(
>>>   num_worker_threads=8,
>>>   device_maps={"worker1": {0: 1}}
>>> # maps worker0's cuda:0 to worker1's cuda:1
>>> )
>>> options.set_device_map("worker1", {1: 2})
>>> # maps worker0's cuda:1 to worker1's cuda:2
>>>
>>> rpc.init_rpc(
>>>   "worker0",
>>>   rank=0,
>>>   world_size=2,
>>>   backend=rpc.BackendType.TENSORPIPE,
>>>   rpc_backend_options=options
>>> )
>>>
>>> x = torch.ones(2)
>>> rets = rpc.rpc_sync("worker1", add, args=(x.to(0), 1))
>>> # The first argument will be moved to cuda:1 on worker1. When
>>> # sending the return value back, it will follow the invert of
>>> # the device map, and hence will be moved back to cuda:0 and
>>> # cuda:1 on worker0
>>> print(rets[0]) # tensor([2., 2.], device='cuda:0')
>>> print(rets[1]) # tensor([2., 2.], device='cuda:1')

```
Copy to clipboard 

set_devices(_devices_)[source][source]
    
Set local devices used by the TensorPipe RPC agent. When processing CUDA RPC requests, the TensorPipe RPC agent will properly synchronize CUDA streams for all devices in this `List`. 

Parameters
    
**devices** (_List_ _of_ _int_ _,__str_ _, or_ _torch.device_) – local devices used by the TensorPipe RPC agent.
Note
The RPC framework does not automatically retry any `rpc_sync()`, `rpc_async()` and `remote()` calls. The reason being that there is no way the RPC framework can determine whether an operation is idempotent or not and whether it is safe to retry. As a result, it is the application’s responsibility to deal with failures and retry if necessary. RPC communication is based on TCP and as a result failures could happen due to network failures or intermittent network connectivity issues. In such scenarios, the application needs to retry appropriately with reasonable backoffs to ensure the network isn’t overwhelmed by aggressive retries.
## RRef
Warning
RRefs are not currently supported when using CUDA tensors
An `RRef` (Remote REFerence) is a reference to a value of some type `T` (e.g. `Tensor`) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to nn.Modules that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See Remote Reference Protocol for more details. 

_class_ torch.distributed.rpc.PyRRef(_RRef_)
    
A class encapsulating a reference to a value of some type on a remote worker. This handle will keep the referenced remote value alive on the worker. A `UserRRef` will be deleted when 1) no references to it in both the application code and in the local RRef context, or 2) the application has called a graceful shutdown. Invoking methods on a deleted RRef leads to undefined behaviors. RRef implementation only offers best-effort error detection, and applications should not use `UserRRefs` after `rpc.shutdown()`.
Warning
RRefs can only be serialized and deserialized by the RPC module. Serializing and deserializing RRefs without RPC (e.g., Python pickle, torch `save()` / `load()`, JIT `save()` / `load()`, etc.) will lead to errors. 

Parameters
    
  * **value** (_object_) – The value to be wrapped by this RRef.
  * **type_hint** (_Type_ _,__optional_) – Python type that should be passed to `TorchScript` compiler as type hint for `value`.



Example::
    
Following examples skip RPC initialization and shutdown code for simplicity. Refer to RPC docs for those details.
  1. Create an RRef using rpc.remote


```
>>> import torch
>>> import torch.distributed.rpc as rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.ones(2), 3))
>>> # get a copy of value from the RRef
>>> x = rref.to_here()

```
Copy to clipboard
  1. Create an RRef from a local object


```
>>> import torch
>>> from torch.distributed.rpc import RRef
>>> x = torch.zeros(2, 2)
>>> rref = RRef(x)

```
Copy to clipboard
  1. Share an RRef with other workers


```
>>> # On both worker0 and worker1:
>>> def f(rref):
>>>  return rref.to_here() + 1

```
Copy to clipboard
```
>>> # On worker0:
>>> import torch
>>> import torch.distributed.rpc as rpc
>>> from torch.distributed.rpc import RRef
>>> rref = RRef(torch.zeros(2, 2))
>>> # the following RPC shares the rref with worker1, reference
>>> # count is automatically updated.
>>> rpc.rpc_sync("worker1", f, args=(rref,))

```
Copy to clipboard 

backward(_self :torch._C._distributed_rpc.PyRRef_, _dist_autograd_ctx_id :int=-1_, _retain_graph :bool=False_) → None
    
> Runs the backward pass using the RRef as the root of the backward pass. If `dist_autograd_ctx_id` is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, `get_gradients()` should be used to retrieve the gradients. If `dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor. 

Parameters
    
  * **dist_autograd_ctx_id** (_int_ _,__optional_) – The distributed autograd context id for which we should retrieve the gradients (default: -1).
  * **retain_graph** (_bool_ _,__optional_) – If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to `True` to run backward multiple times (default: False).



Example::
    
```
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
>>>   rref.backward(context_id)

```
Copy to clipboard 

confirmed_by_owner(_self :torch._C._distributed_rpc.PyRRef_) → bool
    
Returns whether this `RRef` has been confirmed by the owner. `OwnerRRef` always returns true, while `UserRRef` only returns true when the owner knowns about this `UserRRef`. 

is_owner(_self :torch._C._distributed_rpc.PyRRef_) → bool
    
Returns whether or not the current node is the owner of this `RRef`. 

local_value(_self :torch._C._distributed_rpc.PyRRef_) → object
    
If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. 

owner(_self :torch._C._distributed_rpc.PyRRef_) → torch._C._distributed_rpc.WorkerInfo
    
Returns worker information of the node that owns this `RRef`. 

owner_name(_self :torch._C._distributed_rpc.PyRRef_) → str
    
Returns worker name of the node that owns this `RRef`. 

remote(_self :torch._C._distributed_rpc.PyRRef_, _timeout :float=-1.0_) → object
    
Create a helper proxy to easily launch a `remote` using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, `rref.remote().func_name(*args, **kwargs)` is the same as the following:
```
>>> def run(rref, func_name, args, kwargs):
>>>  return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))

```
Copy to clipboard 

Parameters
    
**timeout** (_float_ _,__optional_) – Timeout for `rref.remote()`. If the creation of this `RRef` is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as `to_here`), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see `rpc.remote()` for specific timeout semantics for `RRef`. 

Example::
    
```
>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.remote().size().to_here() # returns torch.Size([2, 2])
>>> rref.remote().view(1, 4).to_here() # returns tensor([[1., 1., 1., 1.]])

```
Copy to clipboard 

rpc_async(_self :torch._C._distributed_rpc.PyRRef_, _timeout :float=-1.0_) → object
    
Create a helper proxy to easily launch an `rpc_async` using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, `rref.rpc_async().func_name(*args, **kwargs)` is the same as the following:
```
>>> def run(rref, func_name, args, kwargs):
>>>  return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))

```
Copy to clipboard 

Parameters
    
**timeout** (_float_ _,__optional_) – Timeout for `rref.rpc_async()`. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used. 

Example::
    
```
>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.rpc_async().size().wait() # returns torch.Size([2, 2])
>>> rref.rpc_async().view(1, 4).wait() # returns tensor([[1., 1., 1., 1.]])

```
Copy to clipboard 

rpc_sync(_self :torch._C._distributed_rpc.PyRRef_, _timeout :float=-1.0_) → object
    
Create a helper proxy to easily launch an `rpc_sync` using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, `rref.rpc_sync().func_name(*args, **kwargs)` is the same as the following:
```
>>> def run(rref, func_name, args, kwargs):
>>>  return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))

```
Copy to clipboard 

Parameters
    
**timeout** (_float_ _,__optional_) – Timeout for `rref.rpc_sync()`. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used. 

Example::
    
```
>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.rpc_sync().size() # returns torch.Size([2, 2])
>>> rref.rpc_sync().view(1, 4) # returns tensor([[1., 1., 1., 1.]])

```
Copy to clipboard 

to_here(_self :torch._C._distributed_rpc.PyRRef_, _timeout :float=-1.0_) → object
    
Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value. 

Parameters
    
**timeout** (_float_ _,__optional_) – Timeout for `to_here`. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.
More Information about RRef
  * Remote Reference Protocol
    * Background
    * Assumptions
    * RRef Lifetime
      * Design Reasoning
      * Implementation
    * Protocol Scenarios
      * User Share RRef with Owner as Return Value
      * User Share RRef with Owner as Argument
      * Owner Share RRef with User
      * User Share RRef with User


## RemoteModule
Warning
RemoteModule is not currently supported when using CUDA tensors
`RemoteModule` is an easy way to create an nn.Module remotely on a different process. The actual module resides on a remote host, but the local host has a handle to this module and invoke this module similar to a regular nn.Module. The invocation however incurs RPC calls to the remote end and can be performed asynchronously if needed via additional APIs supported by RemoteModule. 

_class_ torch.distributed.nn.api.remote_module.RemoteModule(_* args_, _** kwargs_)[source][source]
    
> A RemoteModule instance can only be created after RPC initialization.
> It creates a user-specified module on a specified remote node. It behaves like a regular `nn.Module` except that the `forward` method is executed on the remote node. It takes care of autograd recording to ensure the backward pass propagates gradients back to the corresponding remote module.
> It generates two methods `forward_async` and `forward` based on the signature of the `forward` method of `module_cls`. `forward_async` runs asynchronously and returns a Future. The arguments of `forward_async` and `forward` are the same as the `forward` method of the module returned by the `module_cls`.
> For example, if `module_cls` returns an instance of `nn.Linear`, that has `forward` method signature: `def forward(input: Tensor) -> Tensor:`, the generated `RemoteModule` will have 2 methods with the signatures:
> `def forward(input: Tensor) -> Tensor:`
> `def forward_async(input: Tensor) -> Future[Tensor]:` 

Parameters
    
  * **remote_device** (_str_) – Device on the destination worker where we’d like to place this module. The format should be “<workername>/<device>”, where the device field can be parsed as torch.device type. E.g., “trainer0/cpu”, “trainer0”, “ps0/cuda:0”. In addition, the device field can be optional and the default value is “cpu”.
  * **module_cls** (_nn.Module_) – 
Class for the module to be created remotely. For example,
```
>>> class MyModule(nn.Module):
>>>   def forward(input):
>>>     return input + 1
>>>
>>> module_cls = MyModule

```
Copy to clipboard
  * **args** (_Sequence_ _,__optional_) – args to be passed to `module_cls`.
  * **kwargs** (_Dict_ _,__optional_) – kwargs to be passed to `module_cls`.



Returns
    
A remote module instance which wraps the `Module` created by the user-provided `module_cls`, it has a blocking `forward` method and an asynchronous `forward_async` method that returns a future of the `forward` call on the user-provided module on the remote side. 

Example::
    
Run the following code in two different processes:
```
>>> # On worker 0:
>>> import torch
>>> import torch.distributed.rpc as rpc
>>> from torch import nn, Tensor
>>> from torch.distributed.nn.api.remote_module import RemoteModule
>>>
>>> rpc.init_rpc("worker0", rank=0, world_size=2)
>>> remote_linear_module = RemoteModule(
>>>   "worker1/cpu", nn.Linear, args=(20, 30),
>>> )
>>> input = torch.randn(128, 20)
>>> ret_fut = remote_linear_module.forward_async(input)
>>> ret = ret_fut.wait()
>>> rpc.shutdown()

```
Copy to clipboard
```
>>> # On worker 1:
>>> import torch
>>> import torch.distributed.rpc as rpc
>>>
>>> rpc.init_rpc("worker1", rank=1, world_size=2)
>>> rpc.shutdown()

```
Copy to clipboard
Furthermore, a more practical example that is combined with DistributedDataParallel (DDP) can be found in this tutorial. 

get_module_rref()[source]
    
Return an `RRef` (`RRef[nn.Module]`) pointing to the remote module. 

Return type
    
_RRef_[_Module_] 

remote_parameters(_recurse =True_)[source]
    
Return a list of `RRef` pointing to the remote module’s parameters.
This can typically be used in conjunction with `DistributedOptimizer`. 

Parameters
    
**recurse** (_bool_) – if True, then returns parameters of the remote module and all submodules of the remote module. Otherwise, returns only parameters that are direct members of the remote module. 

Returns
    
A list of `RRef` (`List[RRef[nn.Parameter]]`) to remote module’s parameters. 

Return type
    
list[torch.distributed.rpc.api.RRef[torch.nn.parameter.Parameter]]
## Distributed Autograd Framework
Warning
Distributed autograd is not currently supported when using CUDA tensors
This module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see Distributed Autograd Design. 

torch.distributed.autograd.backward(_context_id :int_, _roots :List[Tensor]_, _retain_graph =False_) → None
    
Kicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.
We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.
We accumulate the gradients in the appropriate `torch.distributed.autograd.context` on each of the nodes. The autograd context to be used is looked up given the `context_id` that is passed in when `torch.distributed.autograd.backward()` is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the `get_gradients()` API. 

Parameters
    
  * **context_id** (_int_) – The autograd context id for which we should retrieve the gradients.
  * **roots** (_list_) – Tensors which represent the roots of the autograd computation. All the tensors should be scalars.
  * **retain_graph** (_bool_ _,__optional_) – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times.



Example::
    
```
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
>>>   pred = model.forward()
>>>   loss = loss_func(pred, loss)
>>>   dist_autograd.backward(context_id, loss)

```
Copy to clipboard 

_class_ torch.distributed.autograd.context[source][source]
    
Context object to wrap forward and backward passes when using distributed autograd. The `context_id` generated in the `with` statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this `context_id`, which is required to correctly execute a distributed autograd pass. 

Example::
    
```
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
>>>   t1 = torch.rand((3, 3), requires_grad=True)
>>>   t2 = torch.rand((3, 3), requires_grad=True)
>>>   loss = rpc.rpc_sync("worker1", torch.add, args=(t1, t2)).sum()
>>>   dist_autograd.backward(context_id, [loss])

```
Copy to clipboard 

torch.distributed.autograd.get_gradients(_context_id :int_) → Dict[Tensor,Tensor]
    
Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given `context_id` as part of the distributed autograd backward pass. 

Parameters
    
**context_id** (_int_) – The autograd context id for which we should retrieve the gradients. 

Returns
    
A map where the key is the Tensor and the value is the associated gradient for that Tensor. 

Example::
    
```
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
>>>   t1 = torch.rand((3, 3), requires_grad=True)
>>>   t2 = torch.rand((3, 3), requires_grad=True)
>>>   loss = t1 + t2
>>>   dist_autograd.backward(context_id, [loss.sum()])
>>>   grads = dist_autograd.get_gradients(context_id)
>>>   print(grads[t1])
>>>   print(grads[t2])

```
Copy to clipboard
More Information about RPC Autograd
  * Distributed Autograd Design
    * Background
    * Autograd recording during the forward pass
    * Distributed Autograd Context
    * Distributed Backward Pass
      * Computing dependencies
      * FAST mode algorithm
      * SMART mode algorithm
    * Distributed Optimizer
    * Simple end to end example


## Distributed Optimizer
See the torch.distributed.optim page for documentation on distributed optimizers.
## Design Notes
The distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.
  * Distributed Autograd Design


The RRef design note covers the design of the RRef (Remote REFerence) protocol used to refer to values on remote workers by the framework.
  * Remote Reference Protocol


## Tutorials
The RPC tutorials introduce users to the RPC framework, provide several example applications using torch.distributed.rpc APIs, and demonstrate how to use the profiler to profile RPC-based workloads.
  * Getting started with Distributed RPC Framework
  * Implementing a Parameter Server using Distributed RPC Framework
  * Combining Distributed DataParallel with Distributed RPC Framework (covers **RemoteModule** as well)
  * Profiling RPC-based Workloads
  * Implementing batch RPC processing
  * Distributed Pipeline Parallel


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Distributed RPC Framework
    * Basics
    * RPC
      * `init_rpc()`
      * `rpc_sync()`
      * `rpc_async()`
      * `remote()`
      * `get_worker_info()`
      * `shutdown()`
      * `WorkerInfo`
        * `WorkerInfo.id`
        * `WorkerInfo.name`
      * `async_execution()`
      * Backends
        * `BackendType`
        * `RpcBackendOptions`
          * `RpcBackendOptions.init_method`
          * `RpcBackendOptions.rpc_timeout`
        * TensorPipe Backend
          * `TensorPipeRpcBackendOptions`
            * `TensorPipeRpcBackendOptions.device_maps`
            * `TensorPipeRpcBackendOptions.devices`
            * `TensorPipeRpcBackendOptions.init_method`
            * `TensorPipeRpcBackendOptions.num_worker_threads`
            * `TensorPipeRpcBackendOptions.rpc_timeout`
            * `TensorPipeRpcBackendOptions.set_device_map()`
            * `TensorPipeRpcBackendOptions.set_devices()`
    * RRef
      * `PyRRef`
        * `PyRRef.backward()`
        * `PyRRef.confirmed_by_owner()`
        * `PyRRef.is_owner()`
        * `PyRRef.local_value()`
        * `PyRRef.owner()`
        * `PyRRef.owner_name()`
        * `PyRRef.remote()`
        * `PyRRef.rpc_async()`
        * `PyRRef.rpc_sync()`
        * `PyRRef.to_here()`
    * RemoteModule
      * `RemoteModule`
        * `RemoteModule.get_module_rref()`
        * `RemoteModule.remote_parameters()`
    * Distributed Autograd Framework
      * `backward()`
      * `context`
      * `get_gradients()`
    * Distributed Optimizer
    * Design Notes
    * Tutorials


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.Size
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.Size
`torch.Size` is the result type of a call to `torch.Tensor.size()`. It describes the size of all dimensions of the original tensor. As a subclass of `tuple`, it supports common sequence operations like indexing and length.
Example:
```
>>> x = torch.ones(10, 20, 30)
>>> s = x.size()
>>> s
torch.Size([10, 20, 30])
>>> s[1]
20
>>> len(s)
3

```
Copy to clipboard 

_class_ torch.Size(_iterable =()_, _/_)
     

count(_value_ , _/_)
    
Return number of occurrences of value. 

index(_value_ , _start =0_, _stop =9223372036854775807_, _/_)
    
Return first index of value.
Raises ValueError if the value is not present. 

numel() → int
    
Returns the number of elements a `torch.Tensor` with the given size would contain.
More formally, for a tensor `x = tensor.ones(10, 10)` with size `s = torch.Size([10, 10])`, `x.numel() == x.size().numel() == s.numel() == 100` holds true. 

Example::
    
```
>>> x=torch.ones(10, 10)
>>> s=x.size()
>>> s
torch.Size([10, 10])
>>> s.numel()
100
>>> x.numel() == s.numel()
True

```
Copy to clipboard
Warning
This function does not return the number of dimensions described by `torch.Size`, but instead the number of elements a `torch.Tensor` with that size would contain.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.Size
    * `Size`
      * `Size.count()`
      * `Size.index()`
      * `Size.numel()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.sparse
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.sparse
Warning
The PyTorch API of sparse tensors is in beta and may change in the near future. We highly welcome feature requests, bug reports and general suggestions as GitHub issues.
## Why and when to use sparsity
By default, PyTorch stores `torch.Tensor` elements contiguously in physical memory. This leads to efficient implementations of various array processing algorithms that require fast access to elements.
Now, some users might decide to represent data such as graph adjacency matrices, pruned weights or points clouds by Tensors whose _elements are mostly zero valued_. We recognize these are important applications and aim to provide performance optimizations for these use cases via sparse storage formats.
Various sparse storage formats such as COO, CSR/CSC, semi-structured, LIL, etc. have been developed over the years. While they differ in exact layouts, they all compress data through efficient representation of zero valued elements. We call the uncompressed values _specified_ in contrast to _unspecified_ , compressed elements.
By compressing repeat zeros sparse storage formats aim to save memory and computational resources on various CPUs and GPUs. Especially for high degrees of sparsity or highly structured sparsity this can have significant performance implications. As such sparse storage formats can be seen as a performance optimization.
Like many other performance optimization sparse storage formats are not always advantageous. When trying sparse formats for your use case you might find your execution time to increase rather than decrease.
Please feel encouraged to open a GitHub issue if you analytically expected to see a stark increase in performance but measured a degradation instead. This helps us prioritize the implementation of efficient kernels and wider performance optimizations.
We make it easy to try different sparsity layouts, and convert between them, without being opinionated on what’s best for your particular application.
## Functionality overview
We want it to be straightforward to construct a sparse Tensor from a given dense Tensor by providing conversion routines for each layout.
In the next example we convert a 2D Tensor with default dense (strided) layout to a 2D Tensor backed by the COO memory layout. Only values and indices of non-zero elements are stored in this case.
```
>>> a = torch.tensor([[0, 2.], [3, 0]])
>>> a.to_sparse()
tensor(indices=tensor([[0, 1],
            [1, 0]]),
    values=tensor([2., 3.]),
    size=(2, 2), nnz=2, layout=torch.sparse_coo)

```
Copy to clipboard
PyTorch currently supports COO, CSR, CSC, BSR, and BSC.
We also have a prototype implementation to support :ref: semi-structured sparsity<sparse-semi-structured-docs>. Please see the references for more details.
Note that we provide slight generalizations of these formats.
Batching: Devices such as GPUs require batching for optimal performance and thus we support batch dimensions.
We currently offer a very simple version of batching where each component of a sparse format itself is batched. This also requires the same number of specified elements per batch entry. In this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor.
```
>>> t = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]])
>>> t.dim()
3
>>> t.to_sparse_csr()
tensor(crow_indices=tensor([[0, 1, 3],
              [0, 1, 3]]),
    col_indices=tensor([[0, 0, 1],
              [0, 0, 1]]),
    values=tensor([[1., 2., 3.],
           [4., 5., 6.]]), size=(2, 2, 2), nnz=3,
    layout=torch.sparse_csr)

```
Copy to clipboard
Dense dimensions: On the other hand, some data such as Graph embeddings might be better viewed as sparse collections of vectors instead of scalars.
In this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension from a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is not stored. If however any of the values in the row are non-zero, they are stored entirely. This reduces the number of indices since we need one index one per row instead of one per element. But it also increases the amount of storage for the values. Since only rows that are _entirely_ zero can be emitted and the presence of any non-zero valued elements cause the entire row to be stored.
```
>>> t = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]])
>>> t.to_sparse(sparse_dim=2)
tensor(indices=tensor([[0, 1],
            [1, 1]]),
    values=tensor([[1., 2.],
           [3., 4.]]),
    size=(2, 2, 2), nnz=2, layout=torch.sparse_coo)

```
Copy to clipboard
## Operator overview
Fundamentally, operations on Tensor with sparse storage formats behave the same as operations on Tensor with strided (or other) storage formats. The particularities of storage, that is the physical layout of the data, influences the performance of an operation but should not influence the semantics.
We are actively increasing operator coverage for sparse tensors. Users should not expect support same level of support as for dense Tensors yet. See our operator documentation for a list.
```
>>> b = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]])
>>> b_s = b.to_sparse_csr()
>>> b_s.cos()
Traceback (most recent call last):
 File "<stdin>", line 1, in <module>
RuntimeError: unsupported tensor layout: SparseCsr
>>> b_s.sin()
tensor(crow_indices=tensor([0, 3, 6]),
    col_indices=tensor([2, 3, 4, 0, 1, 3]),
    values=tensor([ 0.8415, 0.9093, 0.1411, -0.7568, -0.9589, -0.2794]),
    size=(2, 6), nnz=6, layout=torch.sparse_csr)

```
Copy to clipboard
As shown in the example above, we don’t support non-zero preserving unary operators such as cos. The output of a non-zero preserving unary operation will not be able to take advantage of sparse storage formats to the same extent as the input and potentially result in a catastrophic increase in memory. We instead rely on the user to explicitly convert to a dense Tensor first and then run the operation.
```
>>> b_s.to_dense().cos()
tensor([[ 1.0000, -0.4161],
    [-0.9900, 1.0000]])

```
Copy to clipboard
We are aware that some users want to ignore compressed zeros for operations such as cos instead of preserving the exact semantics of the operation. For this we can point to torch.masked and its MaskedTensor, which is in turn also backed and powered by sparse storage formats and kernels.
Also note that, for now, the user doesn’t have a choice of the output layout. For example, adding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some users might prefer for this to stay a sparse layout, because they know the result will still be sufficiently sparse.
```
>>> a + b.to_sparse()
tensor([[0., 3.],
    [3., 0.]])

```
Copy to clipboard
We acknowledge that access to kernels that can efficiently produce different output layouts can be very useful. A subsequent operation might significantly benefit from receiving a particular layout. We are working on an API to control the result layout and recognize it is an important feature to plan a more optimal path of execution for any given model.
## Sparse Semi-Structured Tensors
Warning
Sparse semi-structured tensors are currently a prototype feature and subject to change. Please feel free to open an issue to report a bug or if you have feedback to share.
Semi-Structured sparsity is a sparse data layout that was first introduced in NVIDIA’s Ampere architecture. It is also referred to as **fine-grained structured sparsity** or **2:4 structured sparsity**.
This sparse layout stores n elements out of every 2n elements, with n being determined by the width of the Tensor’s data type (dtype). The most frequently used dtype is float16, where n=2, thus the term “2:4 structured sparsity.”
Semi-structured sparsity is explained in greater detail in this NVIDIA blog post.
In PyTorch, semi-structured sparsity is implemented via a Tensor subclass. By subclassing, we can override `__torch_dispatch__` , allowing us to use faster sparse kernels when performing matrix multiplication. We can also store the tensor in it’s compressed form inside the subclass to reduce memory overhead.
In this compressed form, the sparse tensor is stored by retaining only the _specified_ elements and some metadata, which encodes the mask.
Note
The specified elements and metadata mask of a semi-structured sparse tensor are stored together in a single flat compressed tensor. They are appended to each other to form a contiguous chunk of memory.
compressed tensor = [ specified elements of original tensor | metadata_mask ]
For an original tensor of size (r, c) we expect the first m * k // 2 elements to be the kept elements and the rest of the tensor is metadata.
In order to make it easier for the user to view the specified elements and mask, one can use `.indices()` and `.values()` to access the mask and specified elements respectively.
  * `.values()` returns the specified elements in a tensor of size (r, c//2) and with the same dtype as the dense matrix.
  * `.indices()` returns the metadata_mask in a tensor of size (r, c//2 ) and with element type `torch.int16` if dtype is torch.float16 or torch.bfloat16, and element type `torch.int32` if dtype is torch.int8.


For 2:4 sparse tensors, the metadata overhead is minor - just 2 bits per specified element.
Note
It’s important to note that `torch.float32` is only supported for 1:2 sparsity. Therefore, it does not follow the same formula as above.
Here, we break down how to calculate the compression ratio ( size dense / size sparse) of a 2:4 sparse tensor.
Let (r, c) = tensor.shape and e = bitwidth(tensor.dtype), so e = 16 for `torch.float16` and `torch.bfloat16` and e = 8 for `torch.int8`.
Mdense=r×c×eMsparse=Mspecified+Mmetadata=r×c2×e+r×c2×2=rce2+rc=rce(12+1e)M_{dense} = r \times c \times e \\\ M_{sparse} = M_{specified} + M_{metadata} = r \times \frac{c}{2} \times e + r \times \frac{c}{2} \times 2 = \frac{rce}{2} + rc =rce(\frac{1}{2} +\frac{1}{e}) Mdense​=r×c×eMsparse​=Mspecified​+Mmetadata​=r×2c​×e+r×2c​×2=2rce​+rc=rce(21​+e1​)
Using these calculations, we can determine the total memory footprint for both the original dense and the new sparse representation.
This gives us a simple formula for the compression ratio, which is dependent only on the bitwidth of the tensor datatype.
C=MsparseMdense=12+1eC = \frac{M_{sparse}}{M_{dense}} = \frac{1}{2} + \frac{1}{e} C=Mdense​Msparse​​=21​+e1​
By using this formula, we find that the compression ratio is 56.25% for `torch.float16` or `torch.bfloat16`, and 62.5% for `torch.int8`.
### Constructing Sparse Semi-Structured Tensors
You can transform a dense tensor into a sparse semi-structured tensor by simply using the `torch.to_sparse_semi_structured` function.
Please also note that we only support CUDA tensors since hardware compatibility for semi-structured sparsity is limited to NVIDIA GPUs.
The following datatypes are supported for semi-structured sparsity. Note that each datatype has its own shape constraints and compression factor.
PyTorch dtype | Shape Constraints | Compression Factor | Sparsity Pattern  
---|---|---|---  
`torch.float16` | Tensor must be 2D and (r, c) must both be a positive multiple of 64 | 9/16 | 2:4  
`torch.bfloat16` | Tensor must be 2D and (r, c) must both be a positive multiple of 64 | 9/16 | 2:4  
`torch.int8` | Tensor must be 2D and (r, c) must both be a positive multiple of 128 | 10/16 | 2:4  
To construct a semi-structured sparse tensor, start by creating a regular dense tensor that adheres to a 2:4 (or semi-structured) sparse format. To do this we tile a small 1x4 strip to create a 16x16 dense float16 tensor. Afterwards, we can call `to_sparse_semi_structured` function to compress it for accelerated inference.
```
>>> from torch.sparse import to_sparse_semi_structured
>>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()
tensor([[0., 0., 1., ..., 0., 1., 1.],
    [0., 0., 1., ..., 0., 1., 1.],
    [0., 0., 1., ..., 0., 1., 1.],
    ...,
    [0., 0., 1., ..., 0., 1., 1.],
    [0., 0., 1., ..., 0., 1., 1.],
    [0., 0., 1., ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)
>>> A_sparse = to_sparse_semi_structured(A)
SparseSemiStructuredTensor(shape=torch.Size([128, 128]), transposed=False, values=tensor([[1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    ...,
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.],
    [1., 1., 1., ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), metadata=tensor([[-4370, -4370, -4370, ..., -4370, -4370, -4370],
    [-4370, -4370, -4370, ..., -4370, -4370, -4370],
    [-4370, -4370, -4370, ..., -4370, -4370, -4370],
    ...,
    [-4370, -4370, -4370, ..., -4370, -4370, -4370],
    [-4370, -4370, -4370, ..., -4370, -4370, -4370],
    [-4370, -4370, -4370, ..., -4370, -4370, -4370]], device='cuda:0',
dtype=torch.int16))

```
Copy to clipboard
### Sparse Semi-Structured Tensor Operations
Currently, the following operations are supported for semi-structured sparse tensors:
  * torch.addmm(bias, dense, sparse.t())
  * torch.mm(dense, sparse)
  * torch.mm(sparse, dense)
  * aten.linear.default(dense, sparse, bias)
  * aten.t.default(sparse)
  * aten.t.detach(sparse)


To use these ops, simply pass the output of `to_sparse_semi_structured(tensor)` instead of using `tensor` once your tensor has 0s in a semi-structured sparse format, like this:
```
>>> a = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).half().cuda()
>>> b = torch.rand(64, 64).half().cuda()
>>> c = torch.mm(a, b)
>>> a_sparse = to_sparse_semi_structured(a)
>>> torch.allclose(c, torch.mm(a_sparse, b))
True

```
Copy to clipboard
### Accelerating nn.Linear with semi-structured sparsity
You can accelerate the linear layers in your model if the weights are already semi-structured sparse with just a few lines of code:
```
>>> input = torch.rand(64, 64).half().cuda()
>>> mask = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()
>>> linear = nn.Linear(64, 64).half().cuda()
>>> linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))

```
Copy to clipboard
## Sparse COO tensors
PyTorch implements the so-called Coordinate format, or COO format, as one of the storage formats for implementing sparse tensors. In COO format, the specified elements are stored as tuples of element indices and the corresponding values. In particular,
>   * the indices of specified elements are collected in `indices` tensor of size `(ndim, nse)` and with element type `torch.int64`,
>   * the corresponding values are collected in `values` tensor of size `(nse,)` and with an arbitrary integer or floating point number element type,
> 

where `ndim` is the dimensionality of the tensor and `nse` is the number of specified elements.
Note
The memory consumption of a sparse COO tensor is at least `(ndim * 8 + <size of element type in bytes>) * nse` bytes (plus a constant overhead from storing other tensor data).
The memory consumption of a strided tensor is at least `product(<tensor shape>) * <size of element type in bytes>`.
For example, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least `(2 * 8 + 4) * 100 000 = 2 000 000` bytes when using COO tensor layout and `10 000 * 10 000 * 4 = 400 000 000` bytes when using the default strided tensor layout. Notice the 200 fold memory saving from using the COO storage format.
### Construction
A sparse COO tensor can be constructed by providing the two tensors of indices and values, as well as the size of the sparse tensor (when it cannot be inferred from the indices and values tensors) to a function `torch.sparse_coo_tensor()`.
Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified elements are assumed to have the same value, fill value, which is zero by default. We would then write:
```
>>> i = [[0, 1, 1],
     [2, 0, 2]]
>>> v = [3, 4, 5]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3))
>>> s
tensor(indices=tensor([[0, 1, 1],
            [2, 0, 2]]),
    values=tensor([3, 4, 5]),
    size=(2, 3), nnz=3, layout=torch.sparse_coo)
>>> s.to_dense()
tensor([[0, 0, 3],
    [4, 0, 5]])

```
Copy to clipboard
Note that the input `i` is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:
```
>>> i = [[0, 2], [1, 0], [1, 2]]
>>> v = [3,   4,   5  ]
>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))
>>> # Or another equivalent formulation to get s
>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))
>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()
tensor([[0, 0, 3],
    [4, 0, 5]])

```
Copy to clipboard
An empty sparse COO tensor can be constructed by specifying its size only:
```
>>> torch.sparse_coo_tensor(size=(2, 3))
tensor(indices=tensor([], size=(2, 0)),
    values=tensor([], size=(0,)),
    size=(2, 3), nnz=0, layout=torch.sparse_coo)

```
Copy to clipboard
### Sparse hybrid COO tensors
PyTorch implements an extension of sparse tensors with scalar values to sparse tensors with (contiguous) tensor values. Such tensors are called hybrid tensors.
PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the `values` tensor to be a multi-dimensional tensor so that we have:
>   * the indices of specified elements are collected in `indices` tensor of size `(sparse_dims, nse)` and with element type `torch.int64`,
>   * the corresponding (tensor) values are collected in `values` tensor of size `(nse, dense_dims)` and with an arbitrary integer or floating point number element type.
> 

Note
We use (M + K)-dimensional tensor to denote a N-dimensional sparse hybrid tensor, where M and K are the numbers of sparse and dense dimensions, respectively, such that M + K == N holds.
Suppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at location (1, 2). We would write
```
>>> i = [[0, 1, 1],
     [2, 0, 2]]
>>> v = [[3, 4], [5, 6], [7, 8]]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))
>>> s
tensor(indices=tensor([[0, 1, 1],
            [2, 0, 2]]),
    values=tensor([[3, 4],
           [5, 6],
           [7, 8]]),
    size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)

```
Copy to clipboard
```
>>> s.to_dense()
tensor([[[0, 0],
     [0, 0],
     [3, 4]],
    [[5, 6],
     [0, 0],
     [7, 8]]])

```
Copy to clipboard
In general, if `s` is a sparse COO tensor and `M = s.sparse_dim()`, `K = s.dense_dim()`, then we have the following invariants:
>   * `M + K == len(s.shape) == s.ndim` - dimensionality of a tensor is the sum of the number of sparse and dense dimensions,
>   * `s.indices().shape == (M, nse)` - sparse indices are stored explicitly,
>   * `s.values().shape == (nse,) + s.shape[M : M + K]` - the values of a hybrid tensor are K-dimensional tensors,
>   * `s.values().layout == torch.strided` - values are stored as strided tensors.
> 

Note
Dense dimensions always follow sparse dimensions, that is, mixing of dense and sparse dimensions is not supported.
Note
To be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via `check_invariants=True` keyword argument, or globally using `torch.sparse.check_sparse_tensor_invariants` context manager instance. By default, the sparse tensor invariants checks are disabled.
### Uncoalesced sparse COO tensors
PyTorch sparse COO tensor format permits sparse _uncoalesced_ tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. For example, one can specify multiple values, `3` and `4`, for the same index `1`, that leads to an 1-D uncoalesced tensor:
```
>>> i = [[1, 1]]
>>> v = [3, 4]
>>> s=torch.sparse_coo_tensor(i, v, (3,))
>>> s
tensor(indices=tensor([[1, 1]]),
    values=tensor( [3, 4]),
    size=(3,), nnz=2, layout=torch.sparse_coo)

```
Copy to clipboard
while the coalescing process will accumulate the multi-valued elements into a single value using summation:
```
>>> s.coalesce()
tensor(indices=tensor([[1]]),
    values=tensor([7]),
    size=(3,), nnz=1, layout=torch.sparse_coo)

```
Copy to clipboard
In general, the output of `torch.Tensor.coalesce()` method is a sparse tensor with the following properties:
  * the indices of specified tensor elements are unique,
  * the indices are sorted in lexicographical order,
  * `torch.Tensor.is_coalesced()` returns `True`.


Note
For the most part, you shouldn’t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a sparse coalesced or uncoalesced tensor.
However, some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors.
For instance, addition of sparse COO tensors is implemented by simply concatenating the indices and values tensors:
```
>>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))
>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))
>>> a + b
tensor(indices=tensor([[0, 0, 1, 1]]),
    values=tensor([7, 8, 5, 6]),
    size=(2,), nnz=4, layout=torch.sparse_coo)

```
Copy to clipboard
If you repeatedly perform an operation that can produce duplicate entries (e.g., `torch.Tensor.add()`), you should occasionally coalesce your sparse tensors to prevent them from growing too large.
On the other hand, the lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products.
### Working with sparse COO tensors
Let’s consider the following example:
```
>>> i = [[0, 1, 1],
     [2, 0, 2]]
>>> v = [[3, 4], [5, 6], [7, 8]]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))

```
Copy to clipboard
As mentioned above, a sparse COO tensor is a `torch.Tensor` instance and to distinguish it from the Tensor instances that use some other layout, one can use `torch.Tensor.is_sparse` or `torch.Tensor.layout` properties:
```
>>> isinstance(s, torch.Tensor)
True
>>> s.is_sparse
True
>>> s.layout == torch.sparse_coo
True

```
Copy to clipboard
The number of sparse and dense dimensions can be acquired using methods `torch.Tensor.sparse_dim()` and `torch.Tensor.dense_dim()`, respectively. For instance:
```
>>> s.sparse_dim(), s.dense_dim()
(2, 1)

```
Copy to clipboard
If `s` is a sparse COO tensor then its COO format data can be acquired using methods `torch.Tensor.indices()` and `torch.Tensor.values()`.
Note
Currently, one can acquire the COO format data only when the tensor instance is coalesced:
```
>>> s.indices()
RuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first

```
Copy to clipboard
For acquiring the COO format data of an uncoalesced tensor, use `torch.Tensor._values()` and `torch.Tensor._indices()`:
```
>>> s._indices()
tensor([[0, 1, 1],
    [2, 0, 2]])

```
Copy to clipboard
Warning
Calling `torch.Tensor._values()` will return a _detached_ tensor. To track gradients, `torch.Tensor.coalesce().values()` must be used instead.
Constructing a new sparse COO tensor results a tensor that is not coalesced:
```
>>> s.is_coalesced()
False

```
Copy to clipboard
but one can construct a coalesced copy of a sparse COO tensor using the `torch.Tensor.coalesce()` method:
```
>>> s2 = s.coalesce()
>>> s2.indices()
tensor([[0, 1, 1],
    [2, 0, 2]])

```
Copy to clipboard
When working with uncoalesced sparse COO tensors, one must take into an account the additive nature of uncoalesced data: the values of the same indices are the terms of a sum that evaluation gives the value of the corresponding tensor element. For example, the scalar multiplication on a sparse uncoalesced tensor could be implemented by multiplying all the uncoalesced values with the scalar because `c * (a + b) == c * a + c * b` holds. However, any nonlinear operation, say, a square root, cannot be implemented by applying the operation to uncoalesced data because `sqrt(a + b) == sqrt(a) + sqrt(b)` does not hold in general.
Slicing (with positive step) of a sparse COO tensor is supported only for dense dimensions. Indexing is supported for both sparse and dense dimensions:
```
>>> s[1]
tensor(indices=tensor([[0, 2]]),
    values=tensor([[5, 6],
           [7, 8]]),
    size=(3, 2), nnz=2, layout=torch.sparse_coo)
>>> s[1, 0, 1]
tensor(6)
>>> s[1, 0, 1:]
tensor([6])

```
Copy to clipboard
In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be zero in general. However, there exists operations that may interpret the fill value differently. For instance, `torch.sparse.softmax()` computes the softmax with the assumption that the fill value is negative infinity.
## Sparse Compressed Tensors
Sparse Compressed Tensors represents a class of sparse tensors that have a common feature of compressing the indices of a certain dimension using an encoding that enables certain optimizations on linear algebra kernels of sparse compressed tensors. This encoding is based on the Compressed Sparse Row (CSR) format that PyTorch sparse compressed tensors extend with the support of sparse tensor batches, allowing multi-dimensional tensor values, and storing sparse tensor values in dense blocks.
Note
We use (B + M + K)-dimensional tensor to denote a N-dimensional sparse compressed hybrid tensor, where B, M, and K are the numbers of batch, sparse, and dense dimensions, respectively, such that `B + M + K == N` holds. The number of sparse dimensions for sparse compressed tensors is always two, `M == 2`.
Note
We say that an indices tensor `compressed_indices` uses CSR encoding if the following invariants are satisfied:
  * `compressed_indices` is a contiguous strided 32 or 64 bit integer tensor
  * `compressed_indices` shape is `(*batchsize, compressed_dim_size + 1)` where `compressed_dim_size` is the number of compressed dimensions (e.g. rows or columns)
  * `compressed_indices[..., 0] == 0` where `...` denotes batch indices
  * `compressed_indices[..., compressed_dim_size] == nse` where `nse` is the number of specified elements
  * `0 <= compressed_indices[..., i] - compressed_indices[..., i - 1] <= plain_dim_size` for `i=1, ..., compressed_dim_size`, where `plain_dim_size` is the number of plain dimensions (orthogonal to compressed dimensions, e.g. columns or rows).


To be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via `check_invariants=True` keyword argument, or globally using `torch.sparse.check_sparse_tensor_invariants` context manager instance. By default, the sparse tensor invariants checks are disabled.
Note
The generalization of sparse compressed layouts to N-dimensional tensors can lead to some confusion regarding the count of specified elements. When a sparse compressed tensor contains batch dimensions the number of specified elements will correspond to the number of such elements per-batch. When a sparse compressed tensor has dense dimensions the element considered is now the K-dimensional array. Also for block sparse compressed layouts the 2-D block is considered as the element being specified. Take as an example a 3-dimensional block sparse tensor, with one batch dimension of length `b`, and a block shape of `p, q`. If this tensor has `n` specified elements, then in fact we have `n` blocks specified per batch. This tensor would have `values` with shape `(b, n, p, q)`. This interpretation of the number of specified elements comes from all sparse compressed layouts being derived from the compression of a 2-dimensional matrix. Batch dimensions are treated as stacking of sparse matrices, dense dimensions change the meaning of the element from a simple scalar value to an array with its own dimensions.
### Sparse CSR Tensor
The primary advantage of the CSR format over the COO format is better use of storage and much faster computation operations such as sparse matrix-vector multiplication using MKL and MAGMA backends.
In the simplest case, a (0 + 2 + 0)-dimensional sparse CSR tensor consists of three 1-D tensors: `crow_indices`, `col_indices` and `values`:
>   * The `crow_indices` tensor consists of compressed row indices. This is a 1-D tensor of size `nrows + 1` (the number of rows plus 1). The last element of `crow_indices` is the number of specified elements, `nse`. This tensor encodes the index in `values` and `col_indices` depending on where the given row starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given row.
>   * The `col_indices` tensor contains the column indices of each element. This is a 1-D tensor of size `nse`.
>   * The `values` tensor contains the values of the CSR tensor elements. This is a 1-D tensor of size `nse`.
> 

Note
The index tensors `crow_indices` and `col_indices` should have element type either `torch.int64` (default) or `torch.int32`. If you want to use MKL-enabled matrix operations, use `torch.int32`. This is as a result of the default linking of pytorch being with MKL LP64, which uses 32 bit integer indexing.
In the general case, the (B + 2 + K)-dimensional sparse CSR tensor consists of two (B + 1)-dimensional index tensors `crow_indices` and `col_indices`, and of (1 + K)-dimensional `values` tensor such that
>   * `crow_indices.shape == (*batchsize, nrows + 1)`
>   * `col_indices.shape == (*batchsize, nse)`
>   * `values.shape == (nse, *densesize)`
> 

while the shape of the sparse CSR tensor is `(*batchsize, nrows, ncols, *densesize)` where `len(batchsize) == B` and `len(densesize) == K`.
Note
The batches of sparse CSR tensors are dependent: the number of specified elements in all batches must be the same. This somewhat artificial constraint allows efficient storage of the indices of different CSR batches.
Note
The number of sparse and dense dimensions can be acquired using `torch.Tensor.sparse_dim()` and `torch.Tensor.dense_dim()` methods. The batch dimensions can be computed from the tensor shape: `batchsize = tensor.shape[:-tensor.sparse_dim() - tensor.dense_dim()]`.
Note
The memory consumption of a sparse CSR tensor is at least `(nrows * 8 + (8 + <size of element type in bytes> * prod(densesize)) * nse) * prod(batchsize)` bytes (plus a constant overhead from storing other tensor data).
With the same example data of the note in sparse COO format introduction, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least `(10000 * 8 + (8 + 4 * 1) * 100 000) * 1 = 1 280 000` bytes when using CSR tensor layout. Notice the 1.6 and 310 fold savings from using CSR storage format compared to using the COO and strided formats, respectively.
#### Construction of CSR tensors
Sparse CSR tensors can be directly constructed by using the `torch.sparse_csr_tensor()` function. The user must supply the row and column indices and values tensors separately where the row indices must be specified using the CSR compression encoding. The `size` argument is optional and will be deduced from the `crow_indices` and `col_indices` if it is not present.
```
>>> crow_indices = torch.tensor([0, 2, 4])
>>> col_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([1, 2, 3, 4])
>>> csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)
>>> csr
tensor(crow_indices=tensor([0, 2, 4]),
    col_indices=tensor([0, 1, 0, 1]),
    values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
    dtype=torch.float64)
>>> csr.to_dense()
tensor([[1., 2.],
    [3., 4.]], dtype=torch.float64)

```
Copy to clipboard
Note
The values of sparse dimensions in deduced `size` is computed from the size of `crow_indices` and the maximal index value in `col_indices`. If the number of columns needs to be larger than in the deduced `size` then the `size` argument must be specified explicitly.
The simplest way of constructing a 2-D sparse CSR tensor from a strided or sparse COO tensor is to use `torch.Tensor.to_sparse_csr()` method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:
```
>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)
>>> sp = a.to_sparse_csr()
>>> sp
tensor(crow_indices=tensor([0, 1, 3, 3]),
   col_indices=tensor([2, 0, 1]),
   values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)

```
Copy to clipboard
#### CSR Tensor Operations
The sparse matrix-vector multiplication can be performed with the `tensor.matmul()` method. This is currently the only math operation supported on CSR tensors.
```
>>> vec = torch.randn(4, 1, dtype=torch.float64)
>>> sp.matmul(vec)
tensor([[0.9078],
    [1.3180],
    [0.0000]], dtype=torch.float64)

```
Copy to clipboard
### Sparse CSC Tensor
The sparse CSC (Compressed Sparse Column) tensor format implements the CSC format for storage of 2 dimensional tensors with an extension to supporting batches of sparse CSC tensors and values being multi-dimensional tensors.
Note
Sparse CSC tensor is essentially a transpose of the sparse CSR tensor when the transposition is about swapping the sparse dimensions.
Similarly to sparse CSR tensors, a sparse CSC tensor consists of three tensors: `ccol_indices`, `row_indices` and `values`:
>   * The `ccol_indices` tensor consists of compressed column indices. This is a (B + 1)-D tensor of shape `(*batchsize, ncols + 1)`. The last element is the number of specified elements, `nse`. This tensor encodes the index in `values` and `row_indices` depending on where the given column starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given column.
>   * The `row_indices` tensor contains the row indices of each element. This is a (B + 1)-D tensor of shape `(*batchsize, nse)`.
>   * The `values` tensor contains the values of the CSC tensor elements. This is a (1 + K)-D tensor of shape `(nse, *densesize)`.
> 

#### Construction of CSC tensors
Sparse CSC tensors can be directly constructed by using the `torch.sparse_csc_tensor()` function. The user must supply the row and column indices and values tensors separately where the column indices must be specified using the CSR compression encoding. The `size` argument is optional and will be deduced from the `row_indices` and `ccol_indices` tensors if it is not present.
```
>>> ccol_indices = torch.tensor([0, 2, 4])
>>> row_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([1, 2, 3, 4])
>>> csc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)
>>> csc
tensor(ccol_indices=tensor([0, 2, 4]),
    row_indices=tensor([0, 1, 0, 1]),
    values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
    dtype=torch.float64, layout=torch.sparse_csc)
>>> csc.to_dense()
tensor([[1., 3.],
    [2., 4.]], dtype=torch.float64)

```
Copy to clipboard
Note
The sparse CSC tensor constructor function has the compressed column indices argument before the row indices argument.
The (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from any two-dimensional tensor using `torch.Tensor.to_sparse_csc()` method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:
```
>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)
>>> sp = a.to_sparse_csc()
>>> sp
tensor(ccol_indices=tensor([0, 1, 2, 3, 3]),
    row_indices=tensor([1, 1, 0]),
    values=tensor([1., 2., 1.]), size=(3, 4), nnz=3, dtype=torch.float64,
    layout=torch.sparse_csc)

```
Copy to clipboard
### Sparse BSR Tensor
The sparse BSR (Block compressed Sparse Row) tensor format implements the BSR format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSR tensors and values being blocks of multi-dimensional tensors.
A sparse BSR tensor consists of three tensors: `crow_indices`, `col_indices` and `values`:
>   * The `crow_indices` tensor consists of compressed row indices. This is a (B + 1)-D tensor of shape `(*batchsize, nrowblocks + 1)`. The last element is the number of specified blocks, `nse`. This tensor encodes the index in `values` and `col_indices` depending on where the given column block starts. Each successive number in the tensor subtracted by the number before it denotes the number of blocks in a given row.
>   * The `col_indices` tensor contains the column block indices of each element. This is a (B + 1)-D tensor of shape `(*batchsize, nse)`.
>   * The `values` tensor contains the values of the sparse BSR tensor elements collected into two-dimensional blocks. This is a (1 + 2 + K)-D tensor of shape `(nse, nrowblocks, ncolblocks, *densesize)`.
> 

#### Construction of BSR tensors
Sparse BSR tensors can be directly constructed by using the `torch.sparse_bsr_tensor()` function. The user must supply the row and column block indices and values tensors separately where the row block indices must be specified using the CSR compression encoding. The `size` argument is optional and will be deduced from the `crow_indices` and `col_indices` tensors if it is not present.
```
>>> crow_indices = torch.tensor([0, 2, 4])
>>> col_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],
...             [[3, 4, 5], [9, 10, 11]],
...             [[12, 13, 14], [18, 19, 20]],
...             [[15, 16, 17], [21, 22, 23]]])
>>> bsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64)
>>> bsr
tensor(crow_indices=tensor([0, 2, 4]),
    col_indices=tensor([0, 1, 0, 1]),
    values=tensor([[[ 0., 1., 2.],
            [ 6., 7., 8.]],
           [[ 3., 4., 5.],
            [ 9., 10., 11.]],
           [[12., 13., 14.],
            [18., 19., 20.]],
           [[15., 16., 17.],
            [21., 22., 23.]]]),
    size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr)
>>> bsr.to_dense()
tensor([[ 0., 1., 2., 3., 4., 5.],
    [ 6., 7., 8., 9., 10., 11.],
    [12., 13., 14., 15., 16., 17.],
    [18., 19., 20., 21., 22., 23.]], dtype=torch.float64)

```
Copy to clipboard
The (0 + 2 + 0)-dimensional sparse BSR tensors can be constructed from any two-dimensional tensor using `torch.Tensor.to_sparse_bsr()` method that also requires the specification of the values block size:
```
>>> dense = torch.tensor([[0, 1, 2, 3, 4, 5],
...            [6, 7, 8, 9, 10, 11],
...            [12, 13, 14, 15, 16, 17],
...            [18, 19, 20, 21, 22, 23]])
>>> bsr = dense.to_sparse_bsr(blocksize=(2, 3))
>>> bsr
tensor(crow_indices=tensor([0, 2, 4]),
    col_indices=tensor([0, 1, 0, 1]),
    values=tensor([[[ 0, 1, 2],
            [ 6, 7, 8]],
           [[ 3, 4, 5],
            [ 9, 10, 11]],
           [[12, 13, 14],
            [18, 19, 20]],
           [[15, 16, 17],
            [21, 22, 23]]]), size=(4, 6), nnz=4,
    layout=torch.sparse_bsr)

```
Copy to clipboard
### Sparse BSC Tensor
The sparse BSC (Block compressed Sparse Column) tensor format implements the BSC format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSC tensors and values being blocks of multi-dimensional tensors.
A sparse BSC tensor consists of three tensors: `ccol_indices`, `row_indices` and `values`:
>   * The `ccol_indices` tensor consists of compressed column indices. This is a (B + 1)-D tensor of shape `(*batchsize, ncolblocks + 1)`. The last element is the number of specified blocks, `nse`. This tensor encodes the index in `values` and `row_indices` depending on where the given row block starts. Each successive number in the tensor subtracted by the number before it denotes the number of blocks in a given column.
>   * The `row_indices` tensor contains the row block indices of each element. This is a (B + 1)-D tensor of shape `(*batchsize, nse)`.
>   * The `values` tensor contains the values of the sparse BSC tensor elements collected into two-dimensional blocks. This is a (1 + 2 + K)-D tensor of shape `(nse, nrowblocks, ncolblocks, *densesize)`.
> 

#### Construction of BSC tensors
Sparse BSC tensors can be directly constructed by using the `torch.sparse_bsc_tensor()` function. The user must supply the row and column block indices and values tensors separately where the column block indices must be specified using the CSR compression encoding. The `size` argument is optional and will be deduced from the `ccol_indices` and `row_indices` tensors if it is not present.
```
>>> ccol_indices = torch.tensor([0, 2, 4])
>>> row_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],
...             [[3, 4, 5], [9, 10, 11]],
...             [[12, 13, 14], [18, 19, 20]],
...             [[15, 16, 17], [21, 22, 23]]])
>>> bsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)
>>> bsc
tensor(ccol_indices=tensor([0, 2, 4]),
    row_indices=tensor([0, 1, 0, 1]),
    values=tensor([[[ 0., 1., 2.],
            [ 6., 7., 8.]],
           [[ 3., 4., 5.],
            [ 9., 10., 11.]],
           [[12., 13., 14.],
            [18., 19., 20.]],
           [[15., 16., 17.],
            [21., 22., 23.]]]), size=(4, 6), nnz=4,
    dtype=torch.float64, layout=torch.sparse_bsc)

```
Copy to clipboard
### Tools for working with sparse compressed tensors
All sparse compressed tensors — CSR, CSC, BSR, and BSC tensors — are conceptionally very similar in that their indices data is split into two parts: so-called compressed indices that use the CSR encoding, and so-called plain indices that are orthogonal to the compressed indices. This allows various tools on these tensors to share the same implementations that are parameterized by tensor layout.
#### Construction of sparse compressed tensors
Sparse CSR, CSC, BSR, and CSC tensors can be constructed by using `torch.sparse_compressed_tensor()` function that have the same interface as the above discussed constructor functions `torch.sparse_csr_tensor()`, `torch.sparse_csc_tensor()`, `torch.sparse_bsr_tensor()`, and `torch.sparse_bsc_tensor()`, respectively, but with an extra required `layout` argument. The following example illustrates a method of constructing CSR and CSC tensors using the same input data by specifying the corresponding layout parameter to the `torch.sparse_compressed_tensor()` function:
```
>>> compressed_indices = torch.tensor([0, 2, 4])
>>> plain_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([1, 2, 3, 4])
>>> csr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr)
>>> csr
tensor(crow_indices=tensor([0, 2, 4]),
    col_indices=tensor([0, 1, 0, 1]),
    values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,
    layout=torch.sparse_csr)
>>> csc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc)
>>> csc
tensor(ccol_indices=tensor([0, 2, 4]),
    row_indices=tensor([0, 1, 0, 1]),
    values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,
    layout=torch.sparse_csc)
>>> (csr.transpose(0, 1).to_dense() == csc.to_dense()).all()
tensor(True)

```
Copy to clipboard
## Supported operations
### Linear Algebra operations
The following table summarizes supported Linear Algebra operations on sparse matrices where the operands layouts may vary. Here `T[layout]` denotes a tensor with a given layout. Similarly, `M[layout]` denotes a matrix (2-D PyTorch tensor), and `V[layout]` denotes a vector (1-D PyTorch tensor). In addition, `f` denotes a scalar (float or 0-D PyTorch tensor), `*` is element-wise multiplication, and `@` is matrix multiplication.
PyTorch operation | Sparse grad? | Layout signature  
---|---|---  
`torch.mv()` | no | `M[sparse_coo] @ V[strided] -> V[strided]`  
`torch.mv()` | no | `M[sparse_csr] @ V[strided] -> V[strided]`  
`torch.matmul()` | no | `M[sparse_coo] @ M[strided] -> M[strided]`  
`torch.matmul()` | no | `M[sparse_csr] @ M[strided] -> M[strided]`  
`torch.matmul()` | no | `M[SparseSemiStructured] @ M[strided] -> M[strided]`  
`torch.matmul()` | no | `M[strided] @ M[SparseSemiStructured] -> M[strided]`  
`torch.mm()` | no | `M[sparse_coo] @ M[strided] -> M[strided]`  
`torch.mm()` | no | `M[SparseSemiStructured] @ M[strided] -> M[strided]`  
`torch.mm()` | no | `M[strided] @ M[SparseSemiStructured] -> M[strided]`  
`torch.sparse.mm()` | yes | `M[sparse_coo] @ M[strided] -> M[strided]`  
`torch.smm()` | no | `M[sparse_coo] @ M[strided] -> M[sparse_coo]`  
`torch.hspmm()` | no | `M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]`  
`torch.bmm()` | no | `T[sparse_coo] @ T[strided] -> T[strided]`  
`torch.addmm()` | no | `f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]`  
`torch.addmm()` | no | `f * M[strided] + f * (M[SparseSemiStructured] @ M[strided]) -> M[strided]`  
`torch.addmm()` | no | `f * M[strided] + f * (M[strided] @ M[SparseSemiStructured]) -> M[strided]`  
`torch.sparse.addmm()` | yes | `f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]`  
`torch.sparse.spsolve()` | no | `SOLVE(M[sparse_csr], V[strided]) -> V[strided]`  
`torch.sspaddmm()` | no | `f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]`  
`torch.lobpcg()` | no | `GENEIG(M[sparse_coo]) -> M[strided], M[strided]`  
`torch.pca_lowrank()` | yes | `PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided]`  
`torch.svd_lowrank()` | yes | `SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]`  
where “Sparse grad?” column indicates if the PyTorch operation supports backward with respect to sparse matrix argument. All PyTorch operations, except `torch.smm()`, support backward with respect to strided matrix arguments.
Note
Currently, PyTorch does not support matrix multiplication with the layout signature `M[strided] @ M[sparse_coo]`. However, applications can still compute this using the matrix relation `D @ S == (S.t() @ D.t()).t()`.
### Tensor methods and sparse
The following Tensor methods are related to sparse tensors:
`Tensor.is_sparse` | Is `True` if the Tensor uses sparse COO storage layout, `False` otherwise.  
---|---  
`Tensor.is_sparse_csr` | Is `True` if the Tensor uses sparse CSR storage layout, `False` otherwise.  
`Tensor.dense_dim` | Return the number of dense dimensions in a sparse tensor `self`.  
`Tensor.sparse_dim` | Return the number of sparse dimensions in a sparse tensor `self`.  
`Tensor.sparse_mask` | Returns a new sparse tensor with values from a strided tensor `self` filtered by the indices of the sparse tensor `mask`.  
`Tensor.to_sparse` | Returns a sparse copy of the tensor.  
`Tensor.to_sparse_coo` | Convert a tensor to coordinate format.  
`Tensor.to_sparse_csr` | Convert a tensor to compressed row storage format (CSR).  
`Tensor.to_sparse_csc` | Convert a tensor to compressed column storage (CSC) format.  
`Tensor.to_sparse_bsr` | Convert a tensor to a block sparse row (BSR) storage format of given blocksize.  
`Tensor.to_sparse_bsc` | Convert a tensor to a block sparse column (BSC) storage format of given blocksize.  
`Tensor.to_dense` | Creates a strided copy of `self` if `self` is not a strided tensor, otherwise returns `self`.  
`Tensor.values` | Return the values tensor of a sparse COO tensor.  
The following Tensor methods are specific to sparse COO tensors:
`Tensor.coalesce` | Returns a coalesced copy of `self` if `self` is an uncoalesced tensor.  
---|---  
`Tensor.sparse_resize_` | Resizes `self` sparse tensor to the desired size and the number of sparse and dense dimensions.  
`Tensor.sparse_resize_and_clear_` | Removes all specified elements from a sparse tensor `self` and resizes `self` to the desired size and the number of sparse and dense dimensions.  
`Tensor.is_coalesced` | Returns `True` if `self` is a sparse COO tensor that is coalesced, `False` otherwise.  
`Tensor.indices` | Return the indices tensor of a sparse COO tensor.  
The following methods are specific to sparse CSR tensors and sparse BSR tensors:
`Tensor.crow_indices` | Returns the tensor containing the compressed row indices of the `self` tensor when `self` is a sparse CSR tensor of layout `sparse_csr`.  
---|---  
`Tensor.col_indices` | Returns the tensor containing the column indices of the `self` tensor when `self` is a sparse CSR tensor of layout `sparse_csr`.  
The following methods are specific to sparse CSC tensors and sparse BSC tensors:
`Tensor.row_indices` |   
---|---  
`Tensor.ccol_indices` |   
The following Tensor methods support sparse COO tensors:
`add()` `add_()` `addmm()` `addmm_()` `any()` `asin()` `asin_()` `arcsin()` `arcsin_()` `bmm()` `clone()` `deg2rad()` `deg2rad_()` `detach()` `detach_()` `dim()` `div()` `div_()` `floor_divide()` `floor_divide_()` `get_device()` `index_select()` `isnan()` `log1p()` `log1p_()` `mm()` `mul()` `mul_()` `mv()` `narrow_copy()` `neg()` `neg_()` `negative()` `negative_()` `numel()` `rad2deg()` `rad2deg_()` `resize_as_()` `size()` `pow()` `sqrt()` `square()` `smm()` `sspaddmm()` `sub()` `sub_()` `t()` `t_()` `transpose()` `transpose_()` `zero_()`
### Torch functions specific to sparse Tensors
`sparse_coo_tensor` | Constructs a sparse tensor in COO(rdinate) format with specified values at the given `indices`.  
---|---  
`sparse_csr_tensor` | Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given `crow_indices` and `col_indices`.  
`sparse_csc_tensor` | Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given `ccol_indices` and `row_indices`.  
`sparse_bsr_tensor` | Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given `crow_indices` and `col_indices`.  
`sparse_bsc_tensor` | Constructs a sparse tensor in BSC (Block Compressed Sparse Column)) with specified 2-dimensional blocks at the given `ccol_indices` and `row_indices`.  
`sparse_compressed_tensor` | Constructs a sparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC - with specified values at the given `compressed_indices` and `plain_indices`.  
`sparse.sum` | Return the sum of each row of the given sparse tensor.  
`sparse.addmm` | This function does exact same thing as `torch.addmm()` in the forward, except that it supports backward for sparse COO matrix `mat1`.  
`sparse.sampled_addmm` | Performs a matrix multiplication of the dense matrices `mat1` and `mat2` at the locations specified by the sparsity pattern of `input`.  
`sparse.mm` | Performs a matrix multiplication of the sparse matrix `mat1`  
`sspaddmm` | Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2`, then adds the sparse tensor `input` to the result.  
`hspmm` | Performs a matrix multiplication of a sparse COO matrix `mat1` and a strided matrix `mat2`.  
`smm` | Performs a matrix multiplication of the sparse matrix `input` with the dense matrix `mat`.  
`sparse.softmax` | Applies a softmax function.  
`sparse.spsolve` | Computes the solution of a square system of linear equations with a unique solution.  
`sparse.log_softmax` | Applies a softmax function followed by logarithm.  
`sparse.spdiags` | Creates a sparse 2D tensor by placing the values from rows of `diagonals` along specified diagonals of the output  
### Other functions
The following `torch` functions support sparse tensors:
`cat()` `dstack()` `empty()` `empty_like()` `hstack()` `index_select()` `is_complex()` `is_floating_point()` `is_nonzero()` `is_same_size()` `is_signed()` `is_tensor()` `lobpcg()` `mm()` `native_norm()` `pca_lowrank()` `select()` `stack()` `svd_lowrank()` `unsqueeze()` `vstack()` `zeros()` `zeros_like()`
To manage checking sparse tensor invariants, see:
`sparse.check_sparse_tensor_invariants` | A tool to control checking sparse tensor invariants.  
---|---  
To use sparse tensors with `gradcheck()` function, see:
`sparse.as_sparse_gradcheck` | Decorate function, to extend gradcheck for sparse tensors.  
---|---  
### Zero-preserving unary functions
We aim to support all ‘zero-preserving unary functions’: functions of one argument that map zero to zero.
If you find that we are missing a zero-preserving unary function that you need, please feel encouraged to open an issue for a feature request. As always please kindly try the search function first before opening an issue.
The following operators currently support sparse COO/CSR/CSC/BSR/CSR tensor inputs.
`abs()` `asin()` `asinh()` `atan()` `atanh()` `ceil()` `conj_physical()` `floor()` `log1p()` `neg()` `round()` `sin()` `sinh()` `sign()` `sgn()` `signbit()` `tan()` `tanh()` `trunc()` `expm1()` `sqrt()` `angle()` `isinf()` `isposinf()` `isneginf()` `isnan()` `erf()` `erfinv()`
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.sparse
    * Why and when to use sparsity
    * Functionality overview
    * Operator overview
    * Sparse Semi-Structured Tensors
      * Constructing Sparse Semi-Structured Tensors
      * Sparse Semi-Structured Tensor Operations
      * Accelerating nn.Linear with semi-structured sparsity
    * Sparse COO tensors
      * Construction
      * Sparse hybrid COO tensors
      * Uncoalesced sparse COO tensors
      * Working with sparse COO tensors
    * Sparse Compressed Tensors
      * Sparse CSR Tensor
        * Construction of CSR tensors
        * CSR Tensor Operations
      * Sparse CSC Tensor
        * Construction of CSC tensors
      * Sparse BSR Tensor
        * Construction of BSR tensors
      * Sparse BSC Tensor
        * Construction of BSC tensors
      * Tools for working with sparse compressed tensors
        * Construction of sparse compressed tensors
    * Supported operations
      * Linear Algebra operations
      * Tensor methods and sparse
      * Torch functions specific to sparse Tensors
      * Other functions
      * Zero-preserving unary functions


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.special
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.special
The torch.special module, modeled after SciPy’s special module.
## Functions 

torch.special.airy_ai(_input_ , _*_ , _out =None_) → Tensor
    
Airy function Ai(input)\text{Ai}\left(\text{input}\right)Ai(input). 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

torch.special.bessel_j0(_input_ , _*_ , _out =None_) → Tensor
    
Bessel function of the first kind of order 000. 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

torch.special.bessel_j1(_input_ , _*_ , _out =None_) → Tensor
    
Bessel function of the first kind of order 111. 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

torch.special.digamma(_input_ , _*_ , _out =None_) → Tensor
    
Computes the logarithmic derivative of the gamma function on input.
ϝ(x)=ddxln⁡(Γ(x))=Γ′(x)Γ(x)\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)} ϝ(x)=dxd​ln(Γ(x))=Γ(x)Γ′(x)​ 

Parameters
    
**input** (_Tensor_) – the tensor to compute the digamma function on 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Note
This function is similar to SciPy’s scipy.special.digamma.
Note
From PyTorch 1.8 onwards, the digamma function returns -Inf for 0. Previously it returned NaN for 0.
Example:
```
>>> a = torch.tensor([1, 0.5])
>>> torch.special.digamma(a)
tensor([-0.5772, -1.9635])

```
Copy to clipboard 

torch.special.entr(_input_ , _*_ , _out =None_) → Tensor
    
Computes the entropy on `input` (as defined below), elementwise.
entr(x)={−x∗ln⁡(x)x>00x=0.0−∞x<0\begin{align} \text{entr(x)} = \begin{cases} -x * \ln(x) & x > 0 \\\ 0 & x = 0.0 \\\ -\infty & x < 0 \end{cases} \end{align} entr(x)=⎩⎨⎧​−x∗ln(x)0−∞​x>0x=0.0x<0​​​ 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> a = torch.arange(-0.5, 1, 0.5)
>>> a
tensor([-0.5000, 0.0000, 0.5000])
>>> torch.special.entr(a)
tensor([ -inf, 0.0000, 0.3466])

```
Copy to clipboard 

torch.special.erf(_input_ , _*_ , _out =None_) → Tensor
    
Computes the error function of `input`. The error function is defined as follows:
erf(x)=2π∫0xe−t2dt\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt erf(x)=π​2​∫0x​e−t2dt 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> torch.special.erf(torch.tensor([0, -1., 10.]))
tensor([ 0.0000, -0.8427, 1.0000])

```
Copy to clipboard 

torch.special.erfc(_input_ , _*_ , _out =None_) → Tensor
    
Computes the complementary error function of `input`. The complementary error function is defined as follows:
erfc(x)=1−2π∫0xe−t2dt\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt erfc(x)=1−π​2​∫0x​e−t2dt 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> torch.special.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427, 0.0000])

```
Copy to clipboard 

torch.special.erfcx(_input_ , _*_ , _out =None_) → Tensor
    
Computes the scaled complementary error function for each element of `input`. The scaled complementary error function is defined as follows:
erfcx(x)=ex2erfc(x)\mathrm{erfcx}(x) = e^{x^2} \mathrm{erfc}(x) erfcx(x)=ex2erfc(x) 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> torch.special.erfcx(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 5.0090, 0.0561])

```
Copy to clipboard 

torch.special.erfinv(_input_ , _*_ , _out =None_) → Tensor
    
Computes the inverse error function of `input`. The inverse error function is defined in the range (−1,1)(-1, 1)(−1,1) as:
erfinv(erf(x))=x\mathrm{erfinv}(\mathrm{erf}(x)) = x erfinv(erf(x))=x 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))
tensor([ 0.0000, 0.4769,  -inf])

```
Copy to clipboard 

torch.special.exp2(_input_ , _*_ , _out =None_) → Tensor
    
Computes the base two exponential function of `input`.
yi=2xiy_{i} = 2^{x_{i}} yi​=2xi​ 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))
tensor([ 1., 2., 8., 16.])

```
Copy to clipboard 

torch.special.expit(_input_ , _*_ , _out =None_) → Tensor
    
Computes the expit (also known as the logistic sigmoid function) of the elements of `input`.
outi=11+e−inputi\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}} outi​=1+e−inputi​1​ 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> t = torch.randn(4)
>>> t
tensor([ 0.9213, 1.0887, -0.8858, -1.7683])
>>> torch.special.expit(t)
tensor([ 0.7153, 0.7481, 0.2920, 0.1458])

```
Copy to clipboard 

torch.special.expm1(_input_ , _*_ , _out =None_) → Tensor
    
Computes the exponential of the elements minus 1 of `input`.
yi=exi−1y_{i} = e^{x_{i}} - 1 yi​=exi​−1
Note
This function provides greater precision than exp(x) - 1 for small values of x. 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0., 1.])

```
Copy to clipboard 

torch.special.gammainc(_input_ , _other_ , _*_ , _out =None_) → Tensor
    
Computes the regularized lower incomplete gamma function:
outi=1Γ(inputi)∫0otheritinputi−1e−tdt\text{out}_{i} = \frac{1}{\Gamma(\text{input}_i)} \int_0^{\text{other}_i} t^{\text{input}_i-1} e^{-t} dt outi​=Γ(inputi​)1​∫0otheri​​tinputi​−1e−tdt
where both inputi\text{input}_iinputi​ and otheri\text{other}_iotheri​ are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\text{out}_i=\text{nan}outi​=nan. Γ(⋅)\Gamma(\cdot)Γ(⋅) in the equation above is the gamma function,
Γ(inputi)=∫0∞t(inputi−1)e−tdt.\Gamma(\text{input}_i) = \int_0^\infty t^{(\text{input}_i-1)} e^{-t} dt. Γ(inputi​)=∫0∞​t(inputi​−1)e−tdt.
See `torch.special.gammaincc()` and `torch.special.gammaln()` for related functions.
Supports broadcasting to a common shape and float inputs.
Note
The backward pass with respect to `input` is not yet supported. Please open an issue on PyTorch’s Github to request it. 

Parameters
    
  * **input** (_Tensor_) – the first non-negative input tensor
  * **other** (_Tensor_) – the second non-negative input tensor



Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> a1 = torch.tensor([4.0])
>>> a2 = torch.tensor([3.0, 4.0, 5.0])
>>> a = torch.special.gammaincc(a1, a2)
tensor([0.3528, 0.5665, 0.7350])
tensor([0.3528, 0.5665, 0.7350])
>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)
tensor([1., 1., 1.])

```
Copy to clipboard 

torch.special.gammaincc(_input_ , _other_ , _*_ , _out =None_) → Tensor
    
Computes the regularized upper incomplete gamma function:
outi=1Γ(inputi)∫otheri∞tinputi−1e−tdt\text{out}_{i} = \frac{1}{\Gamma(\text{input}_i)} \int_{\text{other}_i}^{\infty} t^{\text{input}_i-1} e^{-t} dt outi​=Γ(inputi​)1​∫otheri​∞​tinputi​−1e−tdt
where both inputi\text{input}_iinputi​ and otheri\text{other}_iotheri​ are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\text{out}_i=\text{nan}outi​=nan. Γ(⋅)\Gamma(\cdot)Γ(⋅) in the equation above is the gamma function,
Γ(inputi)=∫0∞t(inputi−1)e−tdt.\Gamma(\text{input}_i) = \int_0^\infty t^{(\text{input}_i-1)} e^{-t} dt. Γ(inputi​)=∫0∞​t(inputi​−1)e−tdt.
See `torch.special.gammainc()` and `torch.special.gammaln()` for related functions.
Supports broadcasting to a common shape and float inputs.
Note
The backward pass with respect to `input` is not yet supported. Please open an issue on PyTorch’s Github to request it. 

Parameters
    
  * **input** (_Tensor_) – the first non-negative input tensor
  * **other** (_Tensor_) – the second non-negative input tensor



Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> a1 = torch.tensor([4.0])
>>> a2 = torch.tensor([3.0, 4.0, 5.0])
>>> a = torch.special.gammaincc(a1, a2)
tensor([0.6472, 0.4335, 0.2650])
>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)
tensor([1., 1., 1.])

```
Copy to clipboard 

torch.special.gammaln(_input_ , _*_ , _out =None_) → Tensor
    
Computes the natural logarithm of the absolute value of the gamma function on `input`.
outi=ln⁡Γ(∣inputi∣)\text{out}_{i} = \ln \Gamma(|\text{input}_{i}|) outi​=lnΓ(∣inputi​∣) 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> a = torch.arange(0.5, 2, 0.5)
>>> torch.special.gammaln(a)
tensor([ 0.5724, 0.0000, -0.1208])

```
Copy to clipboard 

torch.special.i0(_input_ , _*_ , _out =None_) → Tensor
    
Computes the zeroth order modified Bessel function of the first kind for each element of `input`.
outi=I0(inputi)=∑k=0∞(inputi2/4)k(k!)2\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2} outi​=I0​(inputi​)=k=0∑∞​(k!)2(inputi2​/4)k​ 

Parameters
    
**input** (_Tensor_) – the input tensor 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000, 1.2661, 2.2796, 4.8808, 11.3019])

```
Copy to clipboard 

torch.special.i0e(_input_ , _*_ , _out =None_) → Tensor
    
Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below) for each element of `input`.
outi=exp⁡(−∣x∣)∗i0(x)=exp⁡(−∣x∣)∗∑k=0∞(inputi2/4)k(k!)2\text{out}_{i} = \exp(-|x|) * i0(x) = \exp(-|x|) * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2} outi​=exp(−∣x∣)∗i0(x)=exp(−∣x∣)∗k=0∑∞​(k!)2(inputi2​/4)k​ 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))
tensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])

```
Copy to clipboard 

torch.special.i1(_input_ , _*_ , _out =None_) → Tensor
    
Computes the first order modified Bessel function of the first kind (as defined below) for each element of `input`.
outi=(inputi)2∗∑k=0∞(inputi2/4)k(k!)∗(k+1)!\text{out}_{i} = \frac{(\text{input}_{i})}{2} * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!) * (k+1)!} outi​=2(inputi​)​∗k=0∑∞​(k!)∗(k+1)!(inputi2​/4)k​ 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> torch.special.i1(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595])

```
Copy to clipboard 

torch.special.i1e(_input_ , _*_ , _out =None_) → Tensor
    
Computes the exponentially scaled first order modified Bessel function of the first kind (as defined below) for each element of `input`.
outi=exp⁡(−∣x∣)∗i1(x)=exp⁡(−∣x∣)∗(inputi)2∗∑k=0∞(inputi2/4)k(k!)∗(k+1)!\text{out}_{i} = \exp(-|x|) * i1(x) = \exp(-|x|) * \frac{(\text{input}_{i})}{2} * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!) * (k+1)!} outi​=exp(−∣x∣)∗i1(x)=exp(−∣x∣)∗2(inputi​)​∗k=0∑∞​(k!)∗(k+1)!(inputi2​/4)k​ 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])

```
Copy to clipboard 

torch.special.log1p(_input_ , _*_ , _out =None_) → Tensor
    
Alias for `torch.log1p()`. 

torch.special.log_ndtr(_input_ , _*_ , _out =None_) → Tensor
    
Computes the log of the area under the standard Gaussian probability density function, integrated from minus infinity to `input`, elementwise.
log_ndtr(x)=log⁡(12π∫−∞xe−12t2dt)\text{log\\_ndtr}(x) = \log\left(\frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{x} e^{-\frac{1}{2}t^2} dt \right) log_ndtr(x)=log(2π​1​∫−∞x​e−21​t2dt) 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> torch.special.log_ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))
tensor([-6.6077 -3.7832 -1.841 -0.6931 -0.1728 -0.023 -0.0014])

```
Copy to clipboard 

torch.special.log_softmax(_input_ , _dim_ , _*_ , _dtype =None_) → Tensor
    
Computes softmax followed by a logarithm.
While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function is computed as:
log_softmax(xi)=log⁡(exp⁡(xi)∑jexp⁡(xj))\text{log\\_softmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right) log_softmax(xi​)=log(∑j​exp(xj​)exp(xi​)​) 

Parameters
    
  * **input** (_Tensor_) – input
  * **dim** (_int_) – A dimension along which log_softmax will be computed.
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned tensor. If specified, the input tensor is cast to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.



Example::
    
```
>>> t = torch.ones(2, 2)
>>> torch.special.log_softmax(t, 0)
tensor([[-0.6931, -0.6931],
    [-0.6931, -0.6931]])

```
Copy to clipboard 

torch.special.logit(_input_ , _eps =None_, _*_ , _out =None_) → Tensor
    
Returns a new tensor with the logit of the elements of `input`. `input` is clamped to [eps, 1 - eps] when eps is not None. When eps is None and `input` < 0 or `input` > 1, the function will yields NaN.
yi=ln⁡(zi1−zi)zi={xiif eps is Noneepsif xi<epsxiif eps≤xi≤1−eps1−epsif xi>1−eps\begin{align} y_{i} &= \ln(\frac{z_{i}}{1 - z_{i}}) \\\ z_{i} &= \begin{cases} x_{i} & \text{if eps is None} \\\ \text{eps} & \text{if } x_{i} < \text{eps} \\\ x_{i} & \text{if } \text{eps} \leq x_{i} \leq 1 - \text{eps} \\\ 1 - \text{eps} & \text{if } x_{i} > 1 - \text{eps} \end{cases} \end{align} yi​zi​​=ln(1−zi​zi​​)=⎩⎨⎧​xi​epsxi​1−eps​if eps is Noneif xi​<epsif eps≤xi​≤1−epsif xi​>1−eps​​​ 

Parameters
    
  * **input** (_Tensor_) – the input tensor.
  * **eps** (_float_ _,__optional_) – the epsilon for input clamp bound. Default: `None`



Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> a = torch.rand(5)
>>> a
tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])
>>> torch.special.logit(a, eps=1e-6)
tensor([-0.9466, 2.6352, 0.6131, -1.7169, 0.6261])

```
Copy to clipboard 

torch.special.logsumexp(_input_ , _dim_ , _keepdim =False_, _*_ , _out =None_)
    
Alias for `torch.logsumexp()`. 

torch.special.multigammaln(_input_ , _p_ , _*_ , _out =None_) → Tensor
    
Computes the multivariate log-gamma function with dimension ppp element-wise, given by
log⁡(Γp(a))=C+∑i=1plog⁡(Γ(a−i−12))\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right) log(Γp​(a))=C+i=1∑p​log(Γ(a−2i−1​))
where C=log⁡(π)⋅p(p−1)4C = \log(\pi) \cdot \frac{p (p - 1)}{4}C=log(π)⋅4p(p−1)​ and Γ(−)\Gamma(-)Γ(−) is the Gamma function.
All elements must be greater than p−12\frac{p - 1}{2}2p−1​, otherwise the behavior is undefiend. 

Parameters
    
  * **input** (_Tensor_) – the tensor to compute the multivariate log-gamma function
  * **p** (_int_) – the number of dimensions



Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> a = torch.empty(2, 3).uniform_(1, 2)
>>> a
tensor([[1.6835, 1.8474, 1.1929],
    [1.0475, 1.7162, 1.4180]])
>>> torch.special.multigammaln(a, 2)
tensor([[0.3928, 0.4007, 0.7586],
    [1.0311, 0.3901, 0.5049]])

```
Copy to clipboard 

torch.special.ndtr(_input_ , _*_ , _out =None_) → Tensor
    
Computes the area under the standard Gaussian probability density function, integrated from minus infinity to `input`, elementwise.
ndtr(x)=12π∫−∞xe−12t2dt\text{ndtr}(x) = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{x} e^{-\frac{1}{2}t^2} dt ndtr(x)=2π​1​∫−∞x​e−21​t2dt 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> torch.special.ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))
tensor([0.0013, 0.0228, 0.1587, 0.5000, 0.8413, 0.9772, 0.9987])

```
Copy to clipboard 

torch.special.ndtri(_input_ , _*_ , _out =None_) → Tensor
    
Computes the argument, x, for which the area under the Gaussian probability density function (integrated from minus infinity to x) is equal to `input`, elementwise.
ndtri(p)=2erf−1(2p−1)\text{ndtri}(p) = \sqrt{2}\text{erf}^{-1}(2p - 1) ndtri(p)=2​erf−1(2p−1)
Note
Also known as quantile function for Normal Distribution. 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> torch.special.ndtri(torch.tensor([0, 0.25, 0.5, 0.75, 1]))
tensor([  -inf, -0.6745, 0.0000, 0.6745,   inf])

```
Copy to clipboard 

torch.special.polygamma(_n_ , _input_ , _*_ , _out =None_) → Tensor
    
Computes the nthn^{th}nth derivative of the digamma function on `input`. n≥0n \geq 0n≥0 is called the order of the polygamma function.
ψ(n)(x)=d(n)dx(n)ψ(x)\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x) ψ(n)(x)=dx(n)d(n)​ψ(x)
Note
This function is implemented only for nonnegative integers n≥0n \geq 0n≥0. 

Parameters
    
  * **n** (_int_) – the order of the polygamma function
  * **input** (_Tensor_) – the input tensor.



Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> a = torch.tensor([1, 0.5])
>>> torch.special.polygamma(1, a)
tensor([1.64493, 4.9348])
>>> torch.special.polygamma(2, a)
tensor([ -2.4041, -16.8288])
>>> torch.special.polygamma(3, a)
tensor([ 6.4939, 97.4091])
>>> torch.special.polygamma(4, a)
tensor([ -24.8863, -771.4742])

```
Copy to clipboard 

torch.special.psi(_input_ , _*_ , _out =None_) → Tensor
    
Alias for `torch.special.digamma()`. 

torch.special.round(_input_ , _*_ , _out =None_) → Tensor
    
Alias for `torch.round()`. 

torch.special.scaled_modified_bessel_k0(_input_ , _*_ , _out =None_) → Tensor
    
Scaled modified Bessel function of the second kind of order 000. 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

torch.special.scaled_modified_bessel_k1(_input_ , _*_ , _out =None_) → Tensor
    
Scaled modified Bessel function of the second kind of order 111. 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

torch.special.sinc(_input_ , _*_ , _out =None_) → Tensor
    
Computes the normalized sinc of `input.`
outi={1,if inputi=0sin⁡(πinputi)/(πinputi),otherwise\text{out}_{i} = \begin{cases} 1, & \text{if}\ \text{input}_{i}=0 \\\ \sin(\pi \text{input}_{i}) / (\pi \text{input}_{i}), & \text{otherwise} \end{cases} outi​={1,sin(πinputi​)/(πinputi​),​ifinputi​=0otherwise​ 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> t = torch.randn(4)
>>> t
tensor([ 0.2252, -0.2948, 1.0267, -1.1566])
>>> torch.special.sinc(t)
tensor([ 0.9186, 0.8631, -0.0259, -0.1300])

```
Copy to clipboard 

torch.special.softmax(_input_ , _dim_ , _*_ , _dtype =None_) → Tensor
    
Computes the softmax function.
Softmax is defined as:
Softmax(xi)=exp⁡(xi)∑jexp⁡(xj)\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}Softmax(xi​)=∑j​exp(xj​)exp(xi​)​
It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1. 

Parameters
    
  * **input** (_Tensor_) – input
  * **dim** (_int_) – A dimension along which softmax will be computed.
  * **dtype** (`torch.dtype`, optional) – the desired data type of returned tensor. If specified, the input tensor is cast to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.



Examples::
    
```
>>> t = torch.ones(2, 2)
>>> torch.special.softmax(t, 0)
tensor([[0.5000, 0.5000],
    [0.5000, 0.5000]])

```
Copy to clipboard 

torch.special.spherical_bessel_j0(_input_ , _*_ , _out =None_) → Tensor
    
Spherical Bessel function of the first kind of order 000. 

Parameters
    
**input** (_Tensor_) – the input tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

torch.special.xlog1py(_input_ , _other_ , _*_ , _out =None_) → Tensor
    
Computes `input * log1p(other)` with the following cases.
outi={NaNif otheri=NaN0if inputi=0.0 and otheri!=NaNinputi∗log1p(otheri)otherwise\text{out}_{i} = \begin{cases} \text{NaN} & \text{if } \text{other}_{i} = \text{NaN} \\\ 0 & \text{if } \text{input}_{i} = 0.0 \text{ and } \text{other}_{i} != \text{NaN} \\\ \text{input}_{i} * \text{log1p}(\text{other}_{i})& \text{otherwise} \end{cases} outi​=⎩⎨⎧​NaN0inputi​∗log1p(otheri​)​if otheri​=NaNif inputi​=0.0 and otheri​!=NaNotherwise​
Similar to SciPy’s scipy.special.xlog1py. 

Parameters
    
  * **input** (_Number_ _or_ _Tensor_) – Multiplier
  * **other** (_Number_ _or_ _Tensor_) – Argument


Note
At least one of `input` or `other` must be a tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> x = torch.zeros(5,)
>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])
>>> torch.special.xlog1py(x, y)
tensor([0., 0., 0., 0., nan])
>>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([3, 2, 1])
>>> torch.special.xlog1py(x, y)
tensor([1.3863, 2.1972, 2.0794])
>>> torch.special.xlog1py(x, 4)
tensor([1.6094, 3.2189, 4.8283])
>>> torch.special.xlog1py(2, y)
tensor([2.7726, 2.1972, 1.3863])

```
Copy to clipboard 

torch.special.xlogy(_input_ , _other_ , _*_ , _out =None_) → Tensor
    
Computes `input * log(other)` with the following cases.
outi={NaNif otheri=NaN0if inputi=0.0inputi∗log⁡(otheri)otherwise\text{out}_{i} = \begin{cases} \text{NaN} & \text{if } \text{other}_{i} = \text{NaN} \\\ 0 & \text{if } \text{input}_{i} = 0.0 \\\ \text{input}_{i} * \log{(\text{other}_{i})} & \text{otherwise} \end{cases} outi​=⎩⎨⎧​NaN0inputi​∗log(otheri​)​if otheri​=NaNif inputi​=0.0otherwise​
Similar to SciPy’s scipy.special.xlogy. 

Parameters
    
  * **input** (_Number_ _or_ _Tensor_) – Multiplier
  * **other** (_Number_ _or_ _Tensor_) – Argument


Note
At least one of `input` or `other` must be a tensor. 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor.
Example:
```
>>> x = torch.zeros(5,)
>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])
>>> torch.special.xlogy(x, y)
tensor([0., 0., 0., 0., nan])
>>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([3, 2, 1])
>>> torch.special.xlogy(x, y)
tensor([1.0986, 1.3863, 0.0000])
>>> torch.special.xlogy(x, 4)
tensor([1.3863, 2.7726, 4.1589])
>>> torch.special.xlogy(2, y)
tensor([2.1972, 1.3863, 0.0000])

```
Copy to clipboard 

torch.special.zeta(_input_ , _other_ , _*_ , _out =None_) → Tensor
    
Computes the Hurwitz zeta function, elementwise.
ζ(x,q)=∑k=0∞1(k+q)x\zeta(x, q) = \sum_{k=0}^{\infty} \frac{1}{(k + q)^x} ζ(x,q)=k=0∑∞​(k+q)x1​ 

Parameters
    
  * **input** (_Tensor_) – the input tensor corresponding to x.
  * **other** (_Tensor_) – the input tensor corresponding to q.


Note
The Riemann zeta function corresponds to the case when q = 1 

Keyword Arguments
    
**out** (_Tensor_ _,__optional_) – the output tensor. 

Example::
    
```
>>> x = torch.tensor([2., 4.])
>>> torch.special.zeta(x, 1)
tensor([1.6449, 1.0823])
>>> torch.special.zeta(x, torch.tensor([1., 2.]))
tensor([1.6449, 0.0823])
>>> torch.special.zeta(2, torch.tensor([1., 2.]))
tensor([1.6449, 0.6449])

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.special
    * Functions
      * `airy_ai()`
      * `bessel_j0()`
      * `bessel_j1()`
      * `digamma()`
      * `entr()`
      * `erf()`
      * `erfc()`
      * `erfcx()`
      * `erfinv()`
      * `exp2()`
      * `expit()`
      * `expm1()`
      * `gammainc()`
      * `gammaincc()`
      * `gammaln()`
      * `i0()`
      * `i0e()`
      * `i1()`
      * `i1e()`
      * `log1p()`
      * `log_ndtr()`
      * `log_softmax()`
      * `logit()`
      * `logsumexp()`
      * `multigammaln()`
      * `ndtr()`
      * `ndtri()`
      * `polygamma()`
      * `psi()`
      * `round()`
      * `scaled_modified_bessel_k0()`
      * `scaled_modified_bessel_k1()`
      * `sinc()`
      * `softmax()`
      * `spherical_bessel_j0()`
      * `xlog1py()`
      * `xlogy()`
      * `zeta()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.Storage
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.Storage
In PyTorch, a regular tensor is a multi-dimensional array that is defined by the following components:
  * Storage: The actual data of the tensor, stored as a contiguous, one-dimensional array of bytes.
  * `dtype`: The data type of the elements in the tensor, such as torch.float32 or torch.int64.
  * `shape`: A tuple indicating the size of the tensor in each dimension.
  * Stride: The step size needed to move from one element to the next in each dimension.
  * Offset: The starting point in the storage from which the tensor data begins. This will usually be 0 for newly created tensors.


These components together define the structure and data of a tensor, with the storage holding the actual data and the rest serving as metadata.
## Untyped Storage API
A `torch.UntypedStorage` is a contiguous, one-dimensional array of elements. Its length is equal to the number of bytes of the tensor. The storage serves as the underlying data container for tensors. In general, a tensor created in PyTorch using regular constructors such as `zeros()`, `zeros_like()` or `new_zeros()` will produce tensors where there is a one-to-one correspondence between the tensor storage and the tensor itself.
However, a storage is allowed to be shared by multiple tensors. For instance, any view of a tensor (obtained through `view()` or some, but not all, kinds of indexing like integers and slices) will point to the same underlying storage as the original tensor. When serializing and deserializing tensors that share a common storage, the relationship is preserved, and the tensors continue to point to the same storage. Interestingly, deserializing multiple tensors that point to a single storage can be faster than deserializing multiple independent tensors.
A tensor storage can be accessed through the `untyped_storage()` method. This will return an object of type `torch.UntypedStorage`. Fortunately, storages have a unique identifier called accessed through the `torch.UntypedStorage.data_ptr()` method. In regular settings, two tensors with the same data storage will have the same storage `data_ptr`. However, tensors themselves can point to two separate storages, one for its data attribute and another for its grad attribute. Each will require a `data_ptr()` of its own. In general, there is no guarantee that a `torch.Tensor.data_ptr()` and `torch.UntypedStorage.data_ptr()` match and this should not be assumed to be true.
Untyped storages are somewhat independent of the tensors that are built on them. Practically, this means that tensors with different dtypes or shape can point to the same storage. It also implies that a tensor storage can be changed, as the following example shows:
```
>>> t = torch.ones(3)
>>> s0 = t.untyped_storage()
>>> s0
 0
 0
 128
 63
 0
 0
 128
 63
 0
 0
 128
 63
[torch.storage.UntypedStorage(device=cpu) of size 12]
>>> s1 = s0.clone()
>>> s1.fill_(0)
 0
 0
 0
 0
 0
 0
 0
 0
 0
 0
 0
 0
[torch.storage.UntypedStorage(device=cpu) of size 12]
>>> # Fill the tensor with a zeroed storage
>>> t.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size())
tensor([0., 0., 0.])

```
Copy to clipboard
Warning
Please note that directly modifying a tensor’s storage as shown in this example is not a recommended practice. This low-level manipulation is illustrated solely for educational purposes, to demonstrate the relationship between tensors and their underlying storages. In general, it’s more efficient and safer to use standard `torch.Tensor` methods, such as `clone()` and `fill_()`, to achieve the same results.
Other than `data_ptr`, untyped storage also have other attributes such as `filename` (in case the storage points to a file on disk), `device` or `is_cuda` for device checks. A storage can also be manipulated in-place or out-of-place with methods like `copy_`, `fill_` or `pin_memory`. FOr more information, check the API reference below. Keep in mind that modifying storages is a low-level API and comes with risks! Most of these APIs also exist on the tensor level: if present, they should be prioritized over their storage counterparts.
## Special cases
We mentioned that a tensor that has a non-None `grad` attribute has actually two pieces of data within it. In this case, `untyped_storage()` will return the storage of the `data` attribute, whereas the storage of the gradient can be obtained through `tensor.grad.untyped_storage()`.
```
>>> t = torch.zeros(3, requires_grad=True)
>>> t.sum().backward()
>>> assert list(t.untyped_storage()) == [0] * 12 # the storage of the tensor is just 0s
>>> assert list(t.grad.untyped_storage()) != [0] * 12 # the storage of the gradient isn't

```
Copy to clipboard 

There are also special cases where tensors do not have a typical storage, or no storage at all:
    
  * Tensors on `"meta"` device: Tensors on the `"meta"` device are used for shape inference and do not hold actual data.
  * Fake Tensors: Another internal tool used by PyTorch’s compiler is FakeTensor which is based on a similar idea.


Tensor subclasses or tensor-like objects can also display unusual behaviours. In general, we do not expect many use cases to require operating at the Storage level! 

_class_ torch.UntypedStorage(_* args_, _** kwargs_)[source][source]
     

bfloat16()[source]
    
Casts this storage to bfloat16 type. 

bool()[source]
    
Casts this storage to bool type. 

byte()[source]
    
Casts this storage to byte type. 

byteswap(_dtype_)[source]
    
Swap bytes in underlying data. 

char()[source]
    
Casts this storage to char type. 

clone()[source]
    
Return a copy of this storage. 

complex_double()[source]
    
Casts this storage to complex double type. 

complex_float()[source]
    
Casts this storage to complex float type. 

copy_()


cpu()[source]
    
Return a CPU copy of this storage if it’s not already on the CPU. 

cuda(_device =None_, _non_blocking =False_)[source]
    
Returns a copy of this object in CUDA memory.
If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned. 

Parameters
    
  * **device** (_int_) – The destination GPU id. Defaults to the current device.
  * **non_blocking** (_bool_) – If `True` and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.



Return type
    
_Union_[__StorageBase_ , _TypedStorage_] 

data_ptr()


device _: device_


double()[source]
    
Casts this storage to double type. 

element_size()


_property_ filename _: Optional[str]_
    
Returns the file name associated with this storage.
The file name will be a string if the storage is on CPU and was created via `from_file()` with `shared` as `True`. This attribute is `None` otherwise. 

fill_()


float()[source]
    
Casts this storage to float type. 

float8_e4m3fn()[source]
    
Casts this storage to float8_e4m3fn type 

float8_e4m3fnuz()[source]
    
Casts this storage to float8_e4m3fnuz type 

float8_e5m2()[source]
    
Casts this storage to float8_e5m2 type 

float8_e5m2fnuz()[source]
    
Casts this storage to float8_e5m2fnuz type 

_static_ from_buffer()


_static_ from_file(_filename_ , _shared =False_, _size =0_) → Storage
    
Creates a CPU storage backed by a memory-mapped file.
If `shared` is `True`, then memory is shared between all processes. All changes are written to the file. If `shared` is `False`, then the changes on the storage do not affect the file.
`size` is the number of elements in the storage. If `shared` is `False`, then the file must contain at least `size * sizeof(Type)` bytes (`Type` is the type of storage, in the case of an `UnTypedStorage` the file must contain at least `size` bytes). If `shared` is `True` the file will be created if needed. 

Parameters
    
  * **filename** (_str_) – file name to map
  * **shared** (_bool_) – whether to share memory (whether `MAP_SHARED` or `MAP_PRIVATE` is passed to the underlying mmap(2) call)
  * **size** (_int_) – number of elements in the storage



get_device()[source]
     

Return type
    
int 

half()[source]
    
Casts this storage to half type. 

hpu(_device =None_, _non_blocking =False_)[source]
    
Returns a copy of this object in HPU memory.
If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned. 

Parameters
    
  * **device** (_int_) – The destination HPU id. Defaults to the current device.
  * **non_blocking** (_bool_) – If `True` and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.



Return type
    
_Union_[__StorageBase_ , _TypedStorage_] 

int()[source]
    
Casts this storage to int type. 

_property_ is_cuda


_property_ is_hpu


is_pinned(_device ='cuda'_)[source]
    
Determine whether the CPU storage is already pinned on device. 

Parameters
    
**device** (_str_ _or_ _torch.device_) – The device to pin memory on (default: `'cuda'`). This argument is discouraged and subject to deprecated. 

Returns
    
A boolean variable. 

is_shared()


is_sparse _: bool_ _= False_


is_sparse_csr _: bool_ _= False_


long()[source]
    
Casts this storage to long type. 

mps()[source]
    
Return a MPS copy of this storage if it’s not already on the MPS. 

nbytes()


new()


pin_memory(_device ='cuda'_)[source]
    
Copy the CPU storage to pinned memory, if it’s not already pinned. 

Parameters
    
**device** (_str_ _or_ _torch.device_) – The device to pin memory on (default: `'cuda'`). This argument is discouraged and subject to deprecated. 

Returns
    
A pinned CPU storage. 

resizable()


resize_()


share_memory_(_* args_, _** kwargs_)[source][source]
    
Moves the storage to shared memory.
This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.
Note that to mitigate issues like this it is thread safe to call this function from multiple threads on the same object. It is NOT thread safe though to call any other function on self without proper synchronization. Please see Multiprocessing best practices for more details.
Note
When all references to a storage in shared memory are deleted, the associated shared memory object will also be deleted. PyTorch has a special cleanup process to ensure that this happens even if the current process exits unexpectedly.
It is worth noting the difference between `share_memory_()` and `from_file()` with `shared = True`
  1. `share_memory_` uses shm_open(3) to create a POSIX shared memory object while `from_file()` uses open(2) to open the filename passed by the user.
  2. Both use an mmap(2) call with `MAP_SHARED` to map the file/object into the current virtual address space
  3. `share_memory_` will call `shm_unlink(3)` on the object after mapping it to make sure the shared memory object is freed when no process has the object open. `torch.from_file(shared=True)` does not unlink the file. This file is persistent and will remain until it is deleted by the user.



Returns
    
`self` 

short()[source]
    
Casts this storage to short type. 

size()[source]
     

Return type
    
int 

to(_*_ , _device_ , _non_blocking =False_)[source]


tolist()[source]
    
Return a list containing the elements of this storage. 

type(_dtype =None_, _non_blocking =False_)[source]
     

Return type
    
_Union_[__StorageBase_ , _TypedStorage_] 

untyped()[source]

## Legacy Typed Storage
Warning
For historical context, PyTorch previously used typed storage classes, which are now deprecated and should be avoided. The following details this API in case you should encounter it, although its usage is highly discouraged. All storage classes except for `torch.UntypedStorage` will be removed in the future, and `torch.UntypedStorage` will be used in all cases.
`torch.Storage` is an alias for the storage class that corresponds with the default data type (`torch.get_default_dtype()`). For example, if the default data type is `torch.float`, `torch.Storage` resolves to `torch.FloatStorage`.
The `torch.<type>Storage` and `torch.cuda.<type>Storage` classes, like `torch.FloatStorage`, `torch.IntStorage`, etc., are not actually ever instantiated. Calling their constructors creates a `torch.TypedStorage` with the appropriate `torch.dtype` and `torch.device`. `torch.<type>Storage` classes have all of the same class methods that `torch.TypedStorage` has.
A `torch.TypedStorage` is a contiguous, one-dimensional array of elements of a particular `torch.dtype`. It can be given any `torch.dtype`, and the internal data will be interpreted appropriately. `torch.TypedStorage` contains a `torch.UntypedStorage` which holds the data as an untyped array of bytes.
Every strided `torch.Tensor` contains a `torch.TypedStorage`, which stores all of the data that the `torch.Tensor` views. 

_class_ torch.TypedStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

bfloat16()[source][source]
    
Casts this storage to bfloat16 type. 

bool()[source][source]
    
Casts this storage to bool type. 

byte()[source][source]
    
Casts this storage to byte type. 

char()[source][source]
    
Casts this storage to char type. 

clone()[source][source]
    
Return a copy of this storage. 

complex_double()[source][source]
    
Casts this storage to complex double type. 

complex_float()[source][source]
    
Casts this storage to complex float type. 

copy_(_source_ , _non_blocking =None_)[source][source]


cpu()[source][source]
    
Return a CPU copy of this storage if it’s not already on the CPU. 

cuda(_device =None_, _non_blocking =False_)[source][source]
    
Returns a copy of this object in CUDA memory.
If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned. 

Parameters
    
  * **device** (_int_) – The destination GPU id. Defaults to the current device.
  * **non_blocking** (_bool_) – If `True` and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.



Return type
    
_Self_ 

data_ptr()[source][source]


_property_ device


double()[source][source]
    
Casts this storage to double type. 

dtype _: dtype_


element_size()[source][source]


_property_ filename _: Optional[str]_
    
Returns the file name associated with this storage if the storage was memory mapped from a file. or `None` if the storage was not created by memory mapping a file. 

fill_(_value_)[source][source]


float()[source][source]
    
Casts this storage to float type. 

float8_e4m3fn()[source][source]
    
Casts this storage to float8_e4m3fn type 

float8_e4m3fnuz()[source][source]
    
Casts this storage to float8_e4m3fnuz type 

float8_e5m2()[source][source]
    
Casts this storage to float8_e5m2 type 

float8_e5m2fnuz()[source][source]
    
Casts this storage to float8_e5m2fnuz type 

_classmethod_ from_buffer(_* args_, _** kwargs_)[source][source]


_classmethod_ from_file(_filename_ , _shared =False_, _size =0_) → Storage[source][source]
    
Creates a CPU storage backed by a memory-mapped file.
If `shared` is `True`, then memory is shared between all processes. All changes are written to the file. If `shared` is `False`, then the changes on the storage do not affect the file.
`size` is the number of elements in the storage. If `shared` is `False`, then the file must contain at least `size * sizeof(Type)` bytes (`Type` is the type of storage). If `shared` is `True` the file will be created if needed. 

Parameters
    
  * **filename** (_str_) – file name to map
  * **shared** (_bool_) – 
whether to share memory (whether `MAP_SHARED` or `MAP_PRIVATE` is passed to the underlying mmap(2) call)
  * **size** (_int_) – number of elements in the storage



get_device()[source][source]
     

Return type
    
int 

half()[source][source]
    
Casts this storage to half type. 

hpu(_device =None_, _non_blocking =False_)[source][source]
    
Returns a copy of this object in HPU memory.
If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned. 

Parameters
    
  * **device** (_int_) – The destination HPU id. Defaults to the current device.
  * **non_blocking** (_bool_) – If `True` and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.



Return type
    
_Self_ 

int()[source][source]
    
Casts this storage to int type. 

_property_ is_cuda


_property_ is_hpu


is_pinned(_device ='cuda'_)[source][source]
    
Determine whether the CPU TypedStorage is already pinned on device. 

Parameters
    
**device** (_str_ _or_ _torch.device_) – The device to pin memory on (default: `'cuda'`). This argument is discouraged and subject to deprecated. 

Returns
    
A boolean variable. 

is_shared()[source][source]


is_sparse _: bool_ _= False_


long()[source][source]
    
Casts this storage to long type. 

nbytes()[source][source]


pickle_storage_type()[source][source]


pin_memory(_device ='cuda'_)[source][source]
    
Copy the CPU TypedStorage to pinned memory, if it’s not already pinned. 

Parameters
    
**device** (_str_ _or_ _torch.device_) – The device to pin memory on (default: `'cuda'`). This argument is discouraged and subject to deprecated. 

Returns
    
A pinned CPU storage. 

resizable()[source][source]


resize_(_size_)[source][source]


share_memory_()[source][source]
    
See `torch.UntypedStorage.share_memory_()` 

short()[source][source]
    
Casts this storage to short type. 

size()[source][source]


to(_*_ , _device_ , _non_blocking =False_)[source][source]
    
Returns a copy of this object in device memory.
If this object is already on the correct device, then no copy is performed and the original object is returned. 

Parameters
    
  * **device** (_int_) – The destination device.
  * **non_blocking** (_bool_) – If `True` and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.



Return type
    
Self 

tolist()[source][source]
    
Return a list containing the elements of this storage. 

type(_dtype =None_, _non_blocking =False_)[source][source]
    
Returns the type if dtype is not provided, else casts this object to the specified type.
If this is already of the correct type, no copy is performed and the original object is returned. 

Parameters
    
  * **dtype** (_type_ _or_ _string_) – The desired type
  * **non_blocking** (_bool_) – If `True`, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect.
  * ****kwargs** – For compatibility, may contain the key `async` in place of the `non_blocking` argument. The `async` arg is deprecated.



Return type
    
_Union_[__StorageBase_ , _TypedStorage_, str] 

untyped()[source][source]
    
Return the internal `torch.UntypedStorage`. 

_class_ torch.DoubleStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.float64_[source]


_class_ torch.FloatStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.float32_[source]


_class_ torch.HalfStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.float16_[source]


_class_ torch.LongStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.int64_[source]


_class_ torch.IntStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.int32_[source]


_class_ torch.ShortStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.int16_[source]


_class_ torch.CharStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.int8_[source]


_class_ torch.ByteStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.uint8_[source]


_class_ torch.BoolStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.bool_[source]


_class_ torch.BFloat16Storage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.bfloat16_[source]


_class_ torch.ComplexDoubleStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.complex128_[source]


_class_ torch.ComplexFloatStorage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.complex64_[source]


_class_ torch.QUInt8Storage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.quint8_[source]


_class_ torch.QInt8Storage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.qint8_[source]


_class_ torch.QInt32Storage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.qint32_[source]


_class_ torch.QUInt4x2Storage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.quint4x2_[source]


_class_ torch.QUInt2x4Storage(_* args_, _wrap_storage =None_, _dtype =None_, _device =None_, __internal =False_)[source][source]
     

dtype _: torch.dtype_ _= torch.quint2x4_[source]

Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.Storage
    * Untyped Storage API
    * Special cases
      * `UntypedStorage`
        * `UntypedStorage.bfloat16()`
        * `UntypedStorage.bool()`
        * `UntypedStorage.byte()`
        * `UntypedStorage.byteswap()`
        * `UntypedStorage.char()`
        * `UntypedStorage.clone()`
        * `UntypedStorage.complex_double()`
        * `UntypedStorage.complex_float()`
        * `UntypedStorage.copy_()`
        * `UntypedStorage.cpu()`
        * `UntypedStorage.cuda()`
        * `UntypedStorage.data_ptr()`
        * `UntypedStorage.device`
        * `UntypedStorage.double()`
        * `UntypedStorage.element_size()`
        * `UntypedStorage.filename`
        * `UntypedStorage.fill_()`
        * `UntypedStorage.float()`
        * `UntypedStorage.float8_e4m3fn()`
        * `UntypedStorage.float8_e4m3fnuz()`
        * `UntypedStorage.float8_e5m2()`
        * `UntypedStorage.float8_e5m2fnuz()`
        * `UntypedStorage.from_buffer()`
        * `UntypedStorage.from_file()`
        * `UntypedStorage.get_device()`
        * `UntypedStorage.half()`
        * `UntypedStorage.hpu()`
        * `UntypedStorage.int()`
        * `UntypedStorage.is_cuda`
        * `UntypedStorage.is_hpu`
        * `UntypedStorage.is_pinned()`
        * `UntypedStorage.is_shared()`
        * `UntypedStorage.is_sparse`
        * `UntypedStorage.is_sparse_csr`
        * `UntypedStorage.long()`
        * `UntypedStorage.mps()`
        * `UntypedStorage.nbytes()`
        * `UntypedStorage.new()`
        * `UntypedStorage.pin_memory()`
        * `UntypedStorage.resizable()`
        * `UntypedStorage.resize_()`
        * `UntypedStorage.share_memory_()`
        * `UntypedStorage.short()`
        * `UntypedStorage.size()`
        * `UntypedStorage.to()`
        * `UntypedStorage.tolist()`
        * `UntypedStorage.type()`
        * `UntypedStorage.untyped()`
    * Legacy Typed Storage
      * `TypedStorage`
        * `TypedStorage.bfloat16()`
        * `TypedStorage.bool()`
        * `TypedStorage.byte()`
        * `TypedStorage.char()`
        * `TypedStorage.clone()`
        * `TypedStorage.complex_double()`
        * `TypedStorage.complex_float()`
        * `TypedStorage.copy_()`
        * `TypedStorage.cpu()`
        * `TypedStorage.cuda()`
        * `TypedStorage.data_ptr()`
        * `TypedStorage.device`
        * `TypedStorage.double()`
        * `TypedStorage.dtype`
        * `TypedStorage.element_size()`
        * `TypedStorage.filename`
        * `TypedStorage.fill_()`
        * `TypedStorage.float()`
        * `TypedStorage.float8_e4m3fn()`
        * `TypedStorage.float8_e4m3fnuz()`
        * `TypedStorage.float8_e5m2()`
        * `TypedStorage.float8_e5m2fnuz()`
        * `TypedStorage.from_buffer()`
        * `TypedStorage.from_file()`
        * `TypedStorage.get_device()`
        * `TypedStorage.half()`
        * `TypedStorage.hpu()`
        * `TypedStorage.int()`
        * `TypedStorage.is_cuda`
        * `TypedStorage.is_hpu`
        * `TypedStorage.is_pinned()`
        * `TypedStorage.is_shared()`
        * `TypedStorage.is_sparse`
        * `TypedStorage.long()`
        * `TypedStorage.nbytes()`
        * `TypedStorage.pickle_storage_type()`
        * `TypedStorage.pin_memory()`
        * `TypedStorage.resizable()`
        * `TypedStorage.resize_()`
        * `TypedStorage.share_memory_()`
        * `TypedStorage.short()`
        * `TypedStorage.size()`
        * `TypedStorage.to()`
        * `TypedStorage.tolist()`
        * `TypedStorage.type()`
        * `TypedStorage.untyped()`
      * `DoubleStorage`
        * `DoubleStorage.dtype`
      * `FloatStorage`
        * `FloatStorage.dtype`
      * `HalfStorage`
        * `HalfStorage.dtype`
      * `LongStorage`
        * `LongStorage.dtype`
      * `IntStorage`
        * `IntStorage.dtype`
      * `ShortStorage`
        * `ShortStorage.dtype`
      * `CharStorage`
        * `CharStorage.dtype`
      * `ByteStorage`
        * `ByteStorage.dtype`
      * `BoolStorage`
        * `BoolStorage.dtype`
      * `BFloat16Storage`
        * `BFloat16Storage.dtype`
      * `ComplexDoubleStorage`
        * `ComplexDoubleStorage.dtype`
      * `ComplexFloatStorage`
        * `ComplexFloatStorage.dtype`
      * `QUInt8Storage`
        * `QUInt8Storage.dtype`
      * `QInt8Storage`
        * `QInt8Storage.dtype`
      * `QInt32Storage`
        * `QInt32Storage.dtype`
      * `QUInt4x2Storage`
        * `QUInt4x2Storage.dtype`
      * `QUInt2x4Storage`
        * `QUInt2x4Storage.dtype`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Tensor Attributes
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Tensor Attributes
Each `torch.Tensor` has a `torch.dtype`, `torch.device`, and `torch.layout`.
## torch.dtype 

_class_ torch.dtype

A `torch.dtype` is an object that represents the data type of a `torch.Tensor`. PyTorch has twelve different data types:
Data type | dtype | Legacy Constructors  
---|---|---  
32-bit floating point | `torch.float32` or `torch.float` | `torch.*.FloatTensor`  
64-bit floating point | `torch.float64` or `torch.double` | `torch.*.DoubleTensor`  
64-bit complex | `torch.complex64` or `torch.cfloat` |   
128-bit complex | `torch.complex128` or `torch.cdouble` |   
16-bit floating point 1 | `torch.float16` or `torch.half` | `torch.*.HalfTensor`  
16-bit floating point 2 | `torch.bfloat16` | `torch.*.BFloat16Tensor`  
8-bit integer (unsigned) | `torch.uint8` | `torch.*.ByteTensor`  
8-bit integer (signed) | `torch.int8` | `torch.*.CharTensor`  
16-bit integer (signed) | `torch.int16` or `torch.short` | `torch.*.ShortTensor`  
32-bit integer (signed) | `torch.int32` or `torch.int` | `torch.*.IntTensor`  
64-bit integer (signed) | `torch.int64` or `torch.long` | `torch.*.LongTensor`  
Boolean | `torch.bool` | `torch.*.BoolTensor` 

1
      
Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important. 

2
    
Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as `float32`
To find out if a `torch.dtype` is a floating point data type, the property `is_floating_point` can be used, which returns `True` if the data type is a floating point data type.
To find out if a `torch.dtype` is a complex data type, the property `is_complex` can be used, which returns `True` if the data type is a complex data type.
When the dtypes of inputs to an arithmetic operation (add, sub, div, mul) differ, we promote by finding the minimum dtype that satisfies the following rules:
  * If the type of a scalar operand is of a higher category than tensor operands (where complex > floating > integral > boolean), we promote to a type with sufficient size to hold all scalar operands of that category.
  * If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.
  * If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.


A floating point scalar operand has dtype torch.get_default_dtype() and an integral non-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect values when determining the minimum dtypes of an operand. Quantized and complex types are not yet supported.
Promotion Examples:
```
>>> float_tensor = torch.ones(1, dtype=torch.float)
>>> double_tensor = torch.ones(1, dtype=torch.double)
>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)
>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)
>>> int_tensor = torch.ones(1, dtype=torch.int)
>>> long_tensor = torch.ones(1, dtype=torch.long)
>>> uint_tensor = torch.ones(1, dtype=torch.uint8)
>>> bool_tensor = torch.ones(1, dtype=torch.bool)
# zero-dim tensors
>>> long_zerodim = torch.tensor(1, dtype=torch.long)
>>> int_zerodim = torch.tensor(1, dtype=torch.int)
>>> torch.add(5, 5).dtype
torch.int64
# 5 is an int64, but does not have higher category than int_tensor so is not considered.
>>> (int_tensor + 5).dtype
torch.int32
>>> (int_tensor + long_zerodim).dtype
torch.int32
>>> (long_tensor + int_tensor).dtype
torch.int64
>>> (bool_tensor + long_tensor).dtype
torch.int64
>>> (bool_tensor + uint_tensor).dtype
torch.uint8
>>> (float_tensor + double_tensor).dtype
torch.float64
>>> (complex_float_tensor + complex_double_tensor).dtype
torch.complex128
>>> (bool_tensor + int_tensor).dtype
torch.int32
# Since long is a different kind than float, result dtype only needs to be large enough
# to hold the float.
>>> torch.add(long_tensor, float_tensor).dtype
torch.float32

```
Copy to clipboard 

When the output tensor of an arithmetic operation is specified, we allow casting to its dtype except that:
    
  * An integral output tensor cannot accept a floating point tensor.
  * A boolean output tensor cannot accept a non-boolean tensor.
  * A non-complex output tensor cannot accept a complex tensor


Casting Examples:
```
# allowed:
>>> float_tensor *= float_tensor
>>> float_tensor *= int_tensor
>>> float_tensor *= uint_tensor
>>> float_tensor *= bool_tensor
>>> float_tensor *= double_tensor
>>> int_tensor *= long_tensor
>>> int_tensor *= uint_tensor
>>> uint_tensor *= int_tensor
# disallowed (RuntimeError: result type can't be cast to the desired output type):
>>> int_tensor *= float_tensor
>>> bool_tensor *= int_tensor
>>> bool_tensor *= uint_tensor
>>> float_tensor *= complex_float_tensor

```
Copy to clipboard
## torch.device 

_class_ torch.device

A `torch.device` is an object representing the device on which a `torch.Tensor` is or will be allocated.
The `torch.device` contains a device type (most commonly “cpu” or “cuda”, but also potentially “mps”, “xpu”, “xla” or “meta”) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after `torch.cuda.set_device()` is called; e.g., a `torch.Tensor` constructed with device `'cuda'` is equivalent to `'cuda:X'` where X is the result of `torch.cuda.current_device()`.
A `torch.Tensor`’s device can be accessed via the `Tensor.device` property.
A `torch.device` can be constructed via a string or via a string and device ordinal
Via a string:
```
>>> torch.device('cuda:0')
device(type='cuda', index=0)
>>> torch.device('cpu')
device(type='cpu')
>>> torch.device('mps')
device(type='mps')
>>> torch.device('cuda') # current cuda device
device(type='cuda')

```
Copy to clipboard
Via a string and device ordinal:
```
>>> torch.device('cuda', 0)
device(type='cuda', index=0)
>>> torch.device('mps', 0)
device(type='mps', index=0)
>>> torch.device('cpu', 0)
device(type='cpu', index=0)

```
Copy to clipboard
The device object can also be used as a context manager to change the default device tensors are allocated on:
```
>>> with torch.device('cuda:1'):
...   r = torch.randn(2, 3)
>>> r.device
device(type='cuda', index=1)

```
Copy to clipboard
This context manager has no effect if a factory function is passed an explicit, non-None device argument. To globally change the default device, see also `torch.set_default_device()`.
Warning
This function imposes a slight performance cost on every Python call to the torch API (not just factory functions). If this is causing problems for you, please comment on https://github.com/pytorch/pytorch/issues/92701
Note
The `torch.device` argument in functions can generally be substituted with a string. This allows for fast prototyping of code.
```
>>> # Example of a function that takes in a torch.device
>>> cuda1 = torch.device('cuda:1')
>>> torch.randn((2,3), device=cuda1)

```
Copy to clipboard
```
>>> # You can substitute the torch.device with a string
>>> torch.randn((2,3), device='cuda:1')

```
Copy to clipboard
Note
For legacy reasons, a device can be constructed via a single device ordinal, which is treated as the current accelerator type. This matches `Tensor.get_device()`, which returns an ordinal for device tensors and is not supported for cpu tensors.
```
>>> torch.device(1)
device(type='cuda', index=1)

```
Copy to clipboard
Note
Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:
```
>>> torch.randn((2,3), device=torch.device('cuda:1'))
>>> torch.randn((2,3), device='cuda:1')
>>> torch.randn((2,3), device=1) # legacy

```
Copy to clipboard
Note
Tensors are never moved automatically between devices and require an explicit call from the user. Scalar Tensors (with tensor.dim()==0) are the only exception to this rule and they are automatically transferred from CPU to GPU when needed as this operation can be done “for free”. Example:
```
>>> # two scalars
>>> torch.ones(()) + torch.ones(()).cuda() # OK, scalar auto-transferred from CPU to GPU
>>> torch.ones(()).cuda() + torch.ones(()) # OK, scalar auto-transferred from CPU to GPU

```
Copy to clipboard
```
>>> # one scalar (CPU), one vector (GPU)
>>> torch.ones(()) + torch.ones(1).cuda() # OK, scalar auto-transferred from CPU to GPU
>>> torch.ones(1).cuda() + torch.ones(()) # OK, scalar auto-transferred from CPU to GPU

```
Copy to clipboard
```
>>> # one scalar (GPU), one vector (CPU)
>>> torch.ones(()).cuda() + torch.ones(1) # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU
>>> torch.ones(1) + torch.ones(()).cuda() # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU

```
Copy to clipboard
## torch.layout 

_class_ torch.layout

Warning
The `torch.layout` class is in beta and subject to change.
A `torch.layout` is an object that represents the memory layout of a `torch.Tensor`. Currently, we support `torch.strided` (dense Tensors) and have beta support for `torch.sparse_coo` (sparse COO Tensors).
`torch.strided` represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated `torch.Storage`, which holds its data. These tensors provide multi-dimensional, strided view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.
Example:
```
>>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
>>> x.stride()
(5, 1)
>>> x.t().stride()
(1, 5)

```
Copy to clipboard
For more information on `torch.sparse_coo` tensors, see torch.sparse.
## torch.memory_format 

_class_ torch.memory_format

A `torch.memory_format` is an object representing the memory format on which a `torch.Tensor` is or will be allocated.
Possible values are:
  * `torch.contiguous_format`: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.
  * `torch.channels_last`: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in `strides[0] > strides[2] > strides[3] > strides[1] == 1` aka NHWC order.
  * `torch.channels_last_3d`: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in `strides[0] > strides[2] > strides[3] > strides[4] > strides[1] == 1` aka NDHWC order.
  * `torch.preserve_format`: Used in functions like clone to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow `torch.contiguous_format`


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Tensor Attributes
    * torch.dtype
      * `dtype`
    * torch.device
      * `device`
    * torch.layout
      * `layout`
    * torch.memory_format
      * `memory_format`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.Tensor
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.Tensor
A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type.
## Data types
Torch defines tensor types with the following data types:
Data type | dtype  
---|---  
32-bit floating point | `torch.float32` or `torch.float`  
64-bit floating point | `torch.float64` or `torch.double`  
16-bit floating point 1 | `torch.float16` or `torch.half`  
16-bit floating point 2 | `torch.bfloat16`  
32-bit complex | `torch.complex32` or `torch.chalf`  
64-bit complex | `torch.complex64` or `torch.cfloat`  
128-bit complex | `torch.complex128` or `torch.cdouble`  
8-bit integer (unsigned) | `torch.uint8`  
16-bit integer (unsigned) | `torch.uint16` (limited support) 4  
32-bit integer (unsigned) | `torch.uint32` (limited support) 4  
64-bit integer (unsigned) | `torch.uint64` (limited support) 4  
8-bit integer (signed) | `torch.int8`  
16-bit integer (signed) | `torch.int16` or `torch.short`  
32-bit integer (signed) | `torch.int32` or `torch.int`  
64-bit integer (signed) | `torch.int64` or `torch.long`  
Boolean | `torch.bool`  
quantized 8-bit integer (unsigned) | `torch.quint8`  
quantized 8-bit integer (signed) | `torch.qint8`  
quantized 32-bit integer (signed) | `torch.qint32`  
quantized 4-bit integer (unsigned) 3 | `torch.quint4x2`  
8-bit floating point, e4m3 5 | `torch.float8_e4m3fn` (limited support)  
8-bit floating point, e5m2 5 | `torch.float8_e5m2` (limited support) 

1
      
Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range. 

2
    
Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as `float32` 

3
    
quantized 4-bit integer is stored as a 8-bit signed integer. Currently it’s only supported in EmbeddingBag operator. 

4(1,2,3)
    
Unsigned types asides from `uint8` are currently planned to only have limited support in eager mode (they primarily exist to assist usage with torch.compile); if you need eager support and the extra range is not needed, we recommend using their signed variants instead. See https://github.com/pytorch/pytorch/issues/58734 for more details. 

5(1,2)
    
`torch.float8_e4m3fn` and `torch.float8_e5m2` implement the spec for 8-bit floating point types from https://arxiv.org/abs/2209.05433. The op support is very limited.
For backwards compatibility, we support the following alternate class names for these data types:
Data type | CPU tensor | GPU tensor  
---|---|---  
32-bit floating point | `torch.FloatTensor` | `torch.cuda.FloatTensor`  
64-bit floating point | `torch.DoubleTensor` | `torch.cuda.DoubleTensor`  
16-bit floating point | `torch.HalfTensor` | `torch.cuda.HalfTensor`  
16-bit floating point | `torch.BFloat16Tensor` | `torch.cuda.BFloat16Tensor`  
8-bit integer (unsigned) | `torch.ByteTensor` | `torch.cuda.ByteTensor`  
8-bit integer (signed) | `torch.CharTensor` | `torch.cuda.CharTensor`  
16-bit integer (signed) | `torch.ShortTensor` | `torch.cuda.ShortTensor`  
32-bit integer (signed) | `torch.IntTensor` | `torch.cuda.IntTensor`  
64-bit integer (signed) | `torch.LongTensor` | `torch.cuda.LongTensor`  
Boolean | `torch.BoolTensor` | `torch.cuda.BoolTensor`  
However, to construct tensors, we recommend using factory functions such as `torch.empty()` with the `dtype` argument instead. The `torch.Tensor` constructor is an alias for the default tensor type (`torch.FloatTensor`).
## Initializing and basic operations
A tensor can be constructed from a Python `list` or sequence using the `torch.tensor()` constructor:
```
>>> torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
    [ 1.0000, -1.0000]])
>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1, 2, 3],
    [ 4, 5, 6]])

```
Copy to clipboard
Warning
`torch.tensor()` always copies `data`. If you have a Tensor `data` and just want to change its `requires_grad` flag, use `requires_grad_()` or `detach()` to avoid a copy. If you have a numpy array and want to avoid a copy, use `torch.as_tensor()`.
A tensor of specific data type can be constructed by passing a `torch.dtype` and/or a `torch.device` to a constructor or tensor creation op:
```
>>> torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0, 0, 0, 0],
    [ 0, 0, 0, 0]], dtype=torch.int32)
>>> cuda0 = torch.device('cuda:0')
>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000, 1.0000, 1.0000, 1.0000],
    [ 1.0000, 1.0000, 1.0000, 1.0000]], dtype=torch.float64, device='cuda:0')

```
Copy to clipboard
For more information about building Tensors, see Creation Ops
The contents of a tensor can be accessed and modified using Python’s indexing and slicing notation:
```
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
tensor(6)
>>> x[0][1] = 8
>>> print(x)
tensor([[ 1, 8, 3],
    [ 4, 5, 6]])

```
Copy to clipboard
Use `torch.Tensor.item()` to get a Python number from a tensor containing a single value:
```
>>> x = torch.tensor([[1]])
>>> x
tensor([[ 1]])
>>> x.item()
1
>>> x = torch.tensor(2.5)
>>> x
tensor(2.5000)
>>> x.item()
2.5

```
Copy to clipboard
For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops
A tensor can be created with `requires_grad=True` so that `torch.autograd` records operations on them for automatic differentiation.
```
>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
>>> out = x.pow(2).sum()
>>> out.backward()
>>> x.grad
tensor([[ 2.0000, -2.0000],
    [ 2.0000, 2.0000]])

```
Copy to clipboard
Each tensor has an associated `torch.Storage`, which holds its data. The tensor class also provides multi-dimensional, strided view of a storage and defines numeric operations on it.
Note
For more information on tensor views, see Tensor Views.
Note
For more information on the `torch.dtype`, `torch.device`, and `torch.layout` attributes of a `torch.Tensor`, see Tensor Attributes.
Note
Methods which mutate a tensor are marked with an underscore suffix. For example, `torch.FloatTensor.abs_()` computes the absolute value in-place and returns the modified tensor, while `torch.FloatTensor.abs()` computes the result in a new tensor.
Note
To change an existing tensor’s `torch.device` and/or `torch.dtype`, consider using `to()` method on the tensor.
Warning
Current implementation of `torch.Tensor` introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.
## Tensor class reference 

_class_ torch.Tensor
    
There are a few main ways to create a tensor, depending on your use case.
  * To create a tensor with pre-existing data, use `torch.tensor()`.
  * To create a tensor with specific size, use `torch.*` tensor creation ops (see Creation Ops).
  * To create a tensor with the same size (and similar types) as another tensor, use `torch.*_like` tensor creation ops (see Creation Ops).
  * To create a tensor with similar type but different size as another tensor, use `tensor.new_*` creation ops.
  * There is a legacy constructor `torch.Tensor` whose use is discouraged. Use `torch.tensor()` instead.



Tensor.__init__(_self_ , _data_)
    
This constructor is deprecated, we recommend using `torch.tensor()` instead. What this constructor does depends on the type of `data`.
  * If `data` is a Tensor, returns an alias to the original Tensor. Unlike `torch.tensor()`, this tracks autograd and will propagate gradients to the original Tensor. `device` kwarg is not supported for this `data` type.
  * If `data` is a sequence or nested sequence, create a tensor of the default dtype (typically `torch.float32`) whose data is the values in the sequences, performing coercions if necessary. Notably, this differs from `torch.tensor()` in that this constructor will always construct a float tensor, even if the inputs are all integers.
  * If `data` is a `torch.Size`, returns an empty tensor of that size.


This constructor does not support explicitly specifying `dtype` or `device` of the returned tensor. We recommend using `torch.tensor()` which provides this functionality. 

Args:
    
data (array_like): The tensor to construct from. 

Keyword args:
     

device (`torch.device`, optional): the desired device of returned tensor.
    
Default: if None, same `torch.device` as this tensor. 

Tensor.T
    
Returns a view of this tensor with its dimensions reversed.
If `n` is the number of dimensions in `x`, `x.T` is equivalent to `x.permute(n-1, n-2, ..., 0)`.
Warning
The use of `Tensor.T()` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `mT` to transpose batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse the dimensions of a tensor. 

Tensor.H
    
Returns a view of a matrix (2-D tensor) conjugated and transposed.
`x.H` is equivalent to `x.transpose(0, 1).conj()` for complex matrices and `x.transpose(0, 1)` for real matrices.
See also
`mH`: An attribute that also works on batches of matrices. 

Tensor.mT
    
Returns a view of this tensor with the last two dimensions transposed.
`x.mT` is equivalent to `x.transpose(-2, -1)`. 

Tensor.mH
    
Accessing this property is equivalent to calling `adjoint()`.
`Tensor.new_tensor` | Returns a new Tensor with `data` as the tensor data.  
---|---  
`Tensor.new_full` | Returns a Tensor of size `size` filled with `fill_value`.  
`Tensor.new_empty` | Returns a Tensor of size `size` filled with uninitialized data.  
`Tensor.new_ones` | Returns a Tensor of size `size` filled with `1`.  
`Tensor.new_zeros` | Returns a Tensor of size `size` filled with `0`.  
`Tensor.is_cuda` | Is `True` if the Tensor is stored on the GPU, `False` otherwise.  
`Tensor.is_quantized` | Is `True` if the Tensor is quantized, `False` otherwise.  
`Tensor.is_meta` | Is `True` if the Tensor is a meta tensor, `False` otherwise.  
`Tensor.device` | Is the `torch.device` where this Tensor is.  
`Tensor.grad` | This attribute is `None` by default and becomes a Tensor the first time a call to `backward()` computes gradients for `self`.  
`Tensor.ndim` | Alias for `dim()`  
`Tensor.real` | Returns a new tensor containing real values of the `self` tensor for a complex-valued input tensor.  
`Tensor.imag` | Returns a new tensor containing imaginary values of the `self` tensor.  
`Tensor.nbytes` | Returns the number of bytes consumed by the "view" of elements of the Tensor if the Tensor does not use sparse storage layout.  
`Tensor.itemsize` | Alias for `element_size()`  
`Tensor.abs` | See `torch.abs()`  
`Tensor.abs_` | In-place version of `abs()`  
`Tensor.absolute` | Alias for `abs()`  
`Tensor.absolute_` | In-place version of `absolute()` Alias for `abs_()`  
`Tensor.acos` | See `torch.acos()`  
`Tensor.acos_` | In-place version of `acos()`  
`Tensor.arccos` | See `torch.arccos()`  
`Tensor.arccos_` | In-place version of `arccos()`  
`Tensor.add` | Add a scalar or tensor to `self` tensor.  
`Tensor.add_` | In-place version of `add()`  
`Tensor.addbmm` | See `torch.addbmm()`  
`Tensor.addbmm_` | In-place version of `addbmm()`  
`Tensor.addcdiv` | See `torch.addcdiv()`  
`Tensor.addcdiv_` | In-place version of `addcdiv()`  
`Tensor.addcmul` | See `torch.addcmul()`  
`Tensor.addcmul_` | In-place version of `addcmul()`  
`Tensor.addmm` | See `torch.addmm()`  
`Tensor.addmm_` | In-place version of `addmm()`  
`Tensor.sspaddmm` | See `torch.sspaddmm()`  
`Tensor.addmv` | See `torch.addmv()`  
`Tensor.addmv_` | In-place version of `addmv()`  
`Tensor.addr` | See `torch.addr()`  
`Tensor.addr_` | In-place version of `addr()`  
`Tensor.adjoint` | Alias for `adjoint()`  
`Tensor.allclose` | See `torch.allclose()`  
`Tensor.amax` | See `torch.amax()`  
`Tensor.amin` | See `torch.amin()`  
`Tensor.aminmax` | See `torch.aminmax()`  
`Tensor.angle` | See `torch.angle()`  
`Tensor.apply_` | Applies the function `callable` to each element in the tensor, replacing each element with the value returned by `callable`.  
`Tensor.argmax` | See `torch.argmax()`  
`Tensor.argmin` | See `torch.argmin()`  
`Tensor.argsort` | See `torch.argsort()`  
`Tensor.argwhere` | See `torch.argwhere()`  
`Tensor.asin` | See `torch.asin()`  
`Tensor.asin_` | In-place version of `asin()`  
`Tensor.arcsin` | See `torch.arcsin()`  
`Tensor.arcsin_` | In-place version of `arcsin()`  
`Tensor.as_strided` | See `torch.as_strided()`  
`Tensor.atan` | See `torch.atan()`  
`Tensor.atan_` | In-place version of `atan()`  
`Tensor.arctan` | See `torch.arctan()`  
`Tensor.arctan_` | In-place version of `arctan()`  
`Tensor.atan2` | See `torch.atan2()`  
`Tensor.atan2_` | In-place version of `atan2()`  
`Tensor.arctan2` | See `torch.arctan2()`  
`Tensor.arctan2_` | atan2_(other) -> Tensor  
`Tensor.all` | See `torch.all()`  
`Tensor.any` | See `torch.any()`  
`Tensor.backward` | Computes the gradient of current tensor wrt graph leaves.  
`Tensor.baddbmm` | See `torch.baddbmm()`  
`Tensor.baddbmm_` | In-place version of `baddbmm()`  
`Tensor.bernoulli` | Returns a result tensor where each result[i]\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\text{Bernoulli}(\texttt{self[i]})Bernoulli(self[i]).  
`Tensor.bernoulli_` | Fills each location of `self` with an independent sample from Bernoulli(p)\text{Bernoulli}(\texttt{p})Bernoulli(p).  
`Tensor.bfloat16` | `self.bfloat16()` is equivalent to `self.to(torch.bfloat16)`.  
`Tensor.bincount` | See `torch.bincount()`  
`Tensor.bitwise_not` | See `torch.bitwise_not()`  
`Tensor.bitwise_not_` | In-place version of `bitwise_not()`  
`Tensor.bitwise_and` | See `torch.bitwise_and()`  
`Tensor.bitwise_and_` | In-place version of `bitwise_and()`  
`Tensor.bitwise_or` | See `torch.bitwise_or()`  
`Tensor.bitwise_or_` | In-place version of `bitwise_or()`  
`Tensor.bitwise_xor` | See `torch.bitwise_xor()`  
`Tensor.bitwise_xor_` | In-place version of `bitwise_xor()`  
`Tensor.bitwise_left_shift` | See `torch.bitwise_left_shift()`  
`Tensor.bitwise_left_shift_` | In-place version of `bitwise_left_shift()`  
`Tensor.bitwise_right_shift` | See `torch.bitwise_right_shift()`  
`Tensor.bitwise_right_shift_` | In-place version of `bitwise_right_shift()`  
`Tensor.bmm` | See `torch.bmm()`  
`Tensor.bool` | `self.bool()` is equivalent to `self.to(torch.bool)`.  
`Tensor.byte` | `self.byte()` is equivalent to `self.to(torch.uint8)`.  
`Tensor.broadcast_to` | See `torch.broadcast_to()`.  
`Tensor.cauchy_` | Fills the tensor with numbers drawn from the Cauchy distribution:  
`Tensor.ceil` | See `torch.ceil()`  
`Tensor.ceil_` | In-place version of `ceil()`  
`Tensor.char` | `self.char()` is equivalent to `self.to(torch.int8)`.  
`Tensor.cholesky` | See `torch.cholesky()`  
`Tensor.cholesky_inverse` | See `torch.cholesky_inverse()`  
`Tensor.cholesky_solve` | See `torch.cholesky_solve()`  
`Tensor.chunk` | See `torch.chunk()`  
`Tensor.clamp` | See `torch.clamp()`  
`Tensor.clamp_` | In-place version of `clamp()`  
`Tensor.clip` | Alias for `clamp()`.  
`Tensor.clip_` | Alias for `clamp_()`.  
`Tensor.clone` | See `torch.clone()`  
`Tensor.contiguous` | Returns a contiguous in memory tensor containing the same data as `self` tensor.  
`Tensor.copy_` | Copies the elements from `src` into `self` tensor and returns `self`.  
`Tensor.conj` | See `torch.conj()`  
`Tensor.conj_physical` | See `torch.conj_physical()`  
`Tensor.conj_physical_` | In-place version of `conj_physical()`  
`Tensor.resolve_conj` | See `torch.resolve_conj()`  
`Tensor.resolve_neg` | See `torch.resolve_neg()`  
`Tensor.copysign` | See `torch.copysign()`  
`Tensor.copysign_` | In-place version of `copysign()`  
`Tensor.cos` | See `torch.cos()`  
`Tensor.cos_` | In-place version of `cos()`  
`Tensor.cosh` | See `torch.cosh()`  
`Tensor.cosh_` | In-place version of `cosh()`  
`Tensor.corrcoef` | See `torch.corrcoef()`  
`Tensor.count_nonzero` | See `torch.count_nonzero()`  
`Tensor.cov` | See `torch.cov()`  
`Tensor.acosh` | See `torch.acosh()`  
`Tensor.acosh_` | In-place version of `acosh()`  
`Tensor.arccosh` | acosh() -> Tensor  
`Tensor.arccosh_` | acosh_() -> Tensor  
`Tensor.cpu` | Returns a copy of this object in CPU memory.  
`Tensor.cross` | See `torch.cross()`  
`Tensor.cuda` | Returns a copy of this object in CUDA memory.  
`Tensor.logcumsumexp` | See `torch.logcumsumexp()`  
`Tensor.cummax` | See `torch.cummax()`  
`Tensor.cummin` | See `torch.cummin()`  
`Tensor.cumprod` | See `torch.cumprod()`  
`Tensor.cumprod_` | In-place version of `cumprod()`  
`Tensor.cumsum` | See `torch.cumsum()`  
`Tensor.cumsum_` | In-place version of `cumsum()`  
`Tensor.chalf` | `self.chalf()` is equivalent to `self.to(torch.complex32)`.  
`Tensor.cfloat` | `self.cfloat()` is equivalent to `self.to(torch.complex64)`.  
`Tensor.cdouble` | `self.cdouble()` is equivalent to `self.to(torch.complex128)`.  
`Tensor.data_ptr` | Returns the address of the first element of `self` tensor.  
`Tensor.deg2rad` | See `torch.deg2rad()`  
`Tensor.dequantize` | Given a quantized Tensor, dequantize it and return the dequantized float Tensor.  
`Tensor.det` | See `torch.det()`  
`Tensor.dense_dim` | Return the number of dense dimensions in a sparse tensor `self`.  
`Tensor.detach` | Returns a new Tensor, detached from the current graph.  
`Tensor.detach_` | Detaches the Tensor from the graph that created it, making it a leaf.  
`Tensor.diag` | See `torch.diag()`  
`Tensor.diag_embed` | See `torch.diag_embed()`  
`Tensor.diagflat` | See `torch.diagflat()`  
`Tensor.diagonal` | See `torch.diagonal()`  
`Tensor.diagonal_scatter` | See `torch.diagonal_scatter()`  
`Tensor.fill_diagonal_` | Fill the main diagonal of a tensor that has at least 2-dimensions.  
`Tensor.fmax` | See `torch.fmax()`  
`Tensor.fmin` | See `torch.fmin()`  
`Tensor.diff` | See `torch.diff()`  
`Tensor.digamma` | See `torch.digamma()`  
`Tensor.digamma_` | In-place version of `digamma()`  
`Tensor.dim` | Returns the number of dimensions of `self` tensor.  
`Tensor.dim_order` | Returns the uniquely determined tuple of int describing the dim order or physical layout of `self`.  
`Tensor.dist` | See `torch.dist()`  
`Tensor.div` | See `torch.div()`  
`Tensor.div_` | In-place version of `div()`  
`Tensor.divide` | See `torch.divide()`  
`Tensor.divide_` | In-place version of `divide()`  
`Tensor.dot` | See `torch.dot()`  
`Tensor.double` | `self.double()` is equivalent to `self.to(torch.float64)`.  
`Tensor.dsplit` | See `torch.dsplit()`  
`Tensor.element_size` | Returns the size in bytes of an individual element.  
`Tensor.eq` | See `torch.eq()`  
`Tensor.eq_` | In-place version of `eq()`  
`Tensor.equal` | See `torch.equal()`  
`Tensor.erf` | See `torch.erf()`  
`Tensor.erf_` | In-place version of `erf()`  
`Tensor.erfc` | See `torch.erfc()`  
`Tensor.erfc_` | In-place version of `erfc()`  
`Tensor.erfinv` | See `torch.erfinv()`  
`Tensor.erfinv_` | In-place version of `erfinv()`  
`Tensor.exp` | See `torch.exp()`  
`Tensor.exp_` | In-place version of `exp()`  
`Tensor.expm1` | See `torch.expm1()`  
`Tensor.expm1_` | In-place version of `expm1()`  
`Tensor.expand` | Returns a new view of the `self` tensor with singleton dimensions expanded to a larger size.  
`Tensor.expand_as` | Expand this tensor to the same size as `other`.  
`Tensor.exponential_` | Fills `self` tensor with elements drawn from the PDF (probability density function):  
`Tensor.fix` | See `torch.fix()`.  
`Tensor.fix_` | In-place version of `fix()`  
`Tensor.fill_` | Fills `self` tensor with the specified value.  
`Tensor.flatten` | See `torch.flatten()`  
`Tensor.flip` | See `torch.flip()`  
`Tensor.fliplr` | See `torch.fliplr()`  
`Tensor.flipud` | See `torch.flipud()`  
`Tensor.float` | `self.float()` is equivalent to `self.to(torch.float32)`.  
`Tensor.float_power` | See `torch.float_power()`  
`Tensor.float_power_` | In-place version of `float_power()`  
`Tensor.floor` | See `torch.floor()`  
`Tensor.floor_` | In-place version of `floor()`  
`Tensor.floor_divide` | See `torch.floor_divide()`  
`Tensor.floor_divide_` | In-place version of `floor_divide()`  
`Tensor.fmod` | See `torch.fmod()`  
`Tensor.fmod_` | In-place version of `fmod()`  
`Tensor.frac` | See `torch.frac()`  
`Tensor.frac_` | In-place version of `frac()`  
`Tensor.frexp` | See `torch.frexp()`  
`Tensor.gather` | See `torch.gather()`  
`Tensor.gcd` | See `torch.gcd()`  
`Tensor.gcd_` | In-place version of `gcd()`  
`Tensor.ge` | See `torch.ge()`.  
`Tensor.ge_` | In-place version of `ge()`.  
`Tensor.greater_equal` | See `torch.greater_equal()`.  
`Tensor.greater_equal_` | In-place version of `greater_equal()`.  
`Tensor.geometric_` | Fills `self` tensor with elements drawn from the geometric distribution:  
`Tensor.geqrf` | See `torch.geqrf()`  
`Tensor.ger` | See `torch.ger()`  
`Tensor.get_device` | For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.  
`Tensor.gt` | See `torch.gt()`.  
`Tensor.gt_` | In-place version of `gt()`.  
`Tensor.greater` | See `torch.greater()`.  
`Tensor.greater_` | In-place version of `greater()`.  
`Tensor.half` | `self.half()` is equivalent to `self.to(torch.float16)`.  
`Tensor.hardshrink` | See `torch.nn.functional.hardshrink()`  
`Tensor.heaviside` | See `torch.heaviside()`  
`Tensor.histc` | See `torch.histc()`  
`Tensor.histogram` | See `torch.histogram()`  
`Tensor.hsplit` | See `torch.hsplit()`  
`Tensor.hypot` | See `torch.hypot()`  
`Tensor.hypot_` | In-place version of `hypot()`  
`Tensor.i0` | See `torch.i0()`  
`Tensor.i0_` | In-place version of `i0()`  
`Tensor.igamma` | See `torch.igamma()`  
`Tensor.igamma_` | In-place version of `igamma()`  
`Tensor.igammac` | See `torch.igammac()`  
`Tensor.igammac_` | In-place version of `igammac()`  
`Tensor.index_add_` | Accumulate the elements of `alpha` times `source` into the `self` tensor by adding to the indices in the order given in `index`.  
`Tensor.index_add` | Out-of-place version of `torch.Tensor.index_add_()`.  
`Tensor.index_copy_` | Copies the elements of `tensor` into the `self` tensor by selecting the indices in the order given in `index`.  
`Tensor.index_copy` | Out-of-place version of `torch.Tensor.index_copy_()`.  
`Tensor.index_fill_` | Fills the elements of the `self` tensor with value `value` by selecting the indices in the order given in `index`.  
`Tensor.index_fill` | Out-of-place version of `torch.Tensor.index_fill_()`.  
`Tensor.index_put_` | Puts values from the tensor `values` into the tensor `self` using the indices specified in `indices` (which is a tuple of Tensors).  
`Tensor.index_put` | Out-place version of `index_put_()`.  
`Tensor.index_reduce_` | Accumulate the elements of `source` into the `self` tensor by accumulating to the indices in the order given in `index` using the reduction given by the `reduce` argument.  
`Tensor.index_reduce` |   
`Tensor.index_select` | See `torch.index_select()`  
`Tensor.indices` | Return the indices tensor of a sparse COO tensor.  
`Tensor.inner` | See `torch.inner()`.  
`Tensor.int` | `self.int()` is equivalent to `self.to(torch.int32)`.  
`Tensor.int_repr` | Given a quantized Tensor, `self.int_repr()` returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.  
`Tensor.inverse` | See `torch.inverse()`  
`Tensor.isclose` | See `torch.isclose()`  
`Tensor.isfinite` | See `torch.isfinite()`  
`Tensor.isinf` | See `torch.isinf()`  
`Tensor.isposinf` | See `torch.isposinf()`  
`Tensor.isneginf` | See `torch.isneginf()`  
`Tensor.isnan` | See `torch.isnan()`  
`Tensor.is_contiguous` | Returns True if `self` tensor is contiguous in memory in the order specified by memory format.  
`Tensor.is_complex` | Returns True if the data type of `self` is a complex data type.  
`Tensor.is_conj` | Returns True if the conjugate bit of `self` is set to true.  
`Tensor.is_floating_point` | Returns True if the data type of `self` is a floating point data type.  
`Tensor.is_inference` | See `torch.is_inference()`  
`Tensor.is_leaf` | All Tensors that have `requires_grad` which is `False` will be leaf Tensors by convention.  
`Tensor.is_pinned` | Returns true if this tensor resides in pinned memory.  
`Tensor.is_set_to` | Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).  
`Tensor.is_shared` | Checks if tensor is in shared memory.  
`Tensor.is_signed` | Returns True if the data type of `self` is a signed data type.  
`Tensor.is_sparse` | Is `True` if the Tensor uses sparse COO storage layout, `False` otherwise.  
`Tensor.istft` | See `torch.istft()`  
`Tensor.isreal` | See `torch.isreal()`  
`Tensor.item` | Returns the value of this tensor as a standard Python number.  
`Tensor.kthvalue` | See `torch.kthvalue()`  
`Tensor.lcm` | See `torch.lcm()`  
`Tensor.lcm_` | In-place version of `lcm()`  
`Tensor.ldexp` | See `torch.ldexp()`  
`Tensor.ldexp_` | In-place version of `ldexp()`  
`Tensor.le` | See `torch.le()`.  
`Tensor.le_` | In-place version of `le()`.  
`Tensor.less_equal` | See `torch.less_equal()`.  
`Tensor.less_equal_` | In-place version of `less_equal()`.  
`Tensor.lerp` | See `torch.lerp()`  
`Tensor.lerp_` | In-place version of `lerp()`  
`Tensor.lgamma` | See `torch.lgamma()`  
`Tensor.lgamma_` | In-place version of `lgamma()`  
`Tensor.log` | See `torch.log()`  
`Tensor.log_` | In-place version of `log()`  
`Tensor.logdet` | See `torch.logdet()`  
`Tensor.log10` | See `torch.log10()`  
`Tensor.log10_` | In-place version of `log10()`  
`Tensor.log1p` | See `torch.log1p()`  
`Tensor.log1p_` | In-place version of `log1p()`  
`Tensor.log2` | See `torch.log2()`  
`Tensor.log2_` | In-place version of `log2()`  
`Tensor.log_normal_` | Fills `self` tensor with numbers samples from the log-normal distribution parameterized by the given mean μ\muμ and standard deviation σ\sigmaσ.  
`Tensor.logaddexp` | See `torch.logaddexp()`  
`Tensor.logaddexp2` | See `torch.logaddexp2()`  
`Tensor.logsumexp` | See `torch.logsumexp()`  
`Tensor.logical_and` | See `torch.logical_and()`  
`Tensor.logical_and_` | In-place version of `logical_and()`  
`Tensor.logical_not` | See `torch.logical_not()`  
`Tensor.logical_not_` | In-place version of `logical_not()`  
`Tensor.logical_or` | See `torch.logical_or()`  
`Tensor.logical_or_` | In-place version of `logical_or()`  
`Tensor.logical_xor` | See `torch.logical_xor()`  
`Tensor.logical_xor_` | In-place version of `logical_xor()`  
`Tensor.logit` | See `torch.logit()`  
`Tensor.logit_` | In-place version of `logit()`  
`Tensor.long` | `self.long()` is equivalent to `self.to(torch.int64)`.  
`Tensor.lt` | See `torch.lt()`.  
`Tensor.lt_` | In-place version of `lt()`.  
`Tensor.less` | lt(other) -> Tensor  
`Tensor.less_` | In-place version of `less()`.  
`Tensor.lu` | See `torch.lu()`  
`Tensor.lu_solve` | See `torch.lu_solve()`  
`Tensor.as_subclass` | Makes a `cls` instance with the same data pointer as `self`.  
`Tensor.map_` | Applies `callable` for each element in `self` tensor and the given `tensor` and stores the results in `self` tensor.  
`Tensor.masked_scatter_` | Copies elements from `source` into `self` tensor at positions where the `mask` is True.  
`Tensor.masked_scatter` | Out-of-place version of `torch.Tensor.masked_scatter_()`  
`Tensor.masked_fill_` | Fills elements of `self` tensor with `value` where `mask` is True.  
`Tensor.masked_fill` | Out-of-place version of `torch.Tensor.masked_fill_()`  
`Tensor.masked_select` | See `torch.masked_select()`  
`Tensor.matmul` | See `torch.matmul()`  
`Tensor.matrix_power` |  Note `matrix_power()` is deprecated, use `torch.linalg.matrix_power()` instead.  
`Tensor.matrix_exp` | See `torch.matrix_exp()`  
`Tensor.max` | See `torch.max()`  
`Tensor.maximum` | See `torch.maximum()`  
`Tensor.mean` | See `torch.mean()`  
`Tensor.module_load` | Defines how to transform `other` when loading it into `self` in `load_state_dict()`.  
`Tensor.nanmean` | See `torch.nanmean()`  
`Tensor.median` | See `torch.median()`  
`Tensor.nanmedian` | See `torch.nanmedian()`  
`Tensor.min` | See `torch.min()`  
`Tensor.minimum` | See `torch.minimum()`  
`Tensor.mm` | See `torch.mm()`  
`Tensor.smm` | See `torch.smm()`  
`Tensor.mode` | See `torch.mode()`  
`Tensor.movedim` | See `torch.movedim()`  
`Tensor.moveaxis` | See `torch.moveaxis()`  
`Tensor.msort` | See `torch.msort()`  
`Tensor.mul` | See `torch.mul()`.  
`Tensor.mul_` | In-place version of `mul()`.  
`Tensor.multiply` | See `torch.multiply()`.  
`Tensor.multiply_` | In-place version of `multiply()`.  
`Tensor.multinomial` | See `torch.multinomial()`  
`Tensor.mv` | See `torch.mv()`  
`Tensor.mvlgamma` | See `torch.mvlgamma()`  
`Tensor.mvlgamma_` | In-place version of `mvlgamma()`  
`Tensor.nansum` | See `torch.nansum()`  
`Tensor.narrow` | See `torch.narrow()`.  
`Tensor.narrow_copy` | See `torch.narrow_copy()`.  
`Tensor.ndimension` | Alias for `dim()`  
`Tensor.nan_to_num` | See `torch.nan_to_num()`.  
`Tensor.nan_to_num_` | In-place version of `nan_to_num()`.  
`Tensor.ne` | See `torch.ne()`.  
`Tensor.ne_` | In-place version of `ne()`.  
`Tensor.not_equal` | See `torch.not_equal()`.  
`Tensor.not_equal_` | In-place version of `not_equal()`.  
`Tensor.neg` | See `torch.neg()`  
`Tensor.neg_` | In-place version of `neg()`  
`Tensor.negative` | See `torch.negative()`  
`Tensor.negative_` | In-place version of `negative()`  
`Tensor.nelement` | Alias for `numel()`  
`Tensor.nextafter` | See `torch.nextafter()`  
`Tensor.nextafter_` | In-place version of `nextafter()`  
`Tensor.nonzero` | See `torch.nonzero()`  
`Tensor.norm` | See `torch.norm()`  
`Tensor.normal_` | Fills `self` tensor with elements samples from the normal distribution parameterized by `mean` and `std`.  
`Tensor.numel` | See `torch.numel()`  
`Tensor.numpy` | Returns the tensor as a NumPy `ndarray`.  
`Tensor.orgqr` | See `torch.orgqr()`  
`Tensor.ormqr` | See `torch.ormqr()`  
`Tensor.outer` | See `torch.outer()`.  
`Tensor.permute` | See `torch.permute()`  
`Tensor.pin_memory` | Copies the tensor to pinned memory, if it's not already pinned.  
`Tensor.pinverse` | See `torch.pinverse()`  
`Tensor.polygamma` | See `torch.polygamma()`  
`Tensor.polygamma_` | In-place version of `polygamma()`  
`Tensor.positive` | See `torch.positive()`  
`Tensor.pow` | See `torch.pow()`  
`Tensor.pow_` | In-place version of `pow()`  
`Tensor.prod` | See `torch.prod()`  
`Tensor.put_` | Copies the elements from `source` into the positions specified by `index`.  
`Tensor.qr` | See `torch.qr()`  
`Tensor.qscheme` | Returns the quantization scheme of a given QTensor.  
`Tensor.quantile` | See `torch.quantile()`  
`Tensor.nanquantile` | See `torch.nanquantile()`  
`Tensor.q_scale` | Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().  
`Tensor.q_zero_point` | Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().  
`Tensor.q_per_channel_scales` | Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.  
`Tensor.q_per_channel_zero_points` | Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.  
`Tensor.q_per_channel_axis` | Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.  
`Tensor.rad2deg` | See `torch.rad2deg()`  
`Tensor.random_` | Fills `self` tensor with numbers sampled from the discrete uniform distribution over `[from, to - 1]`.  
`Tensor.ravel` | see `torch.ravel()`  
`Tensor.reciprocal` | See `torch.reciprocal()`  
`Tensor.reciprocal_` | In-place version of `reciprocal()`  
`Tensor.record_stream` | Marks the tensor as having been used by this stream.  
`Tensor.register_hook` | Registers a backward hook.  
`Tensor.register_post_accumulate_grad_hook` | Registers a backward hook that runs after grad accumulation.  
`Tensor.remainder` | See `torch.remainder()`  
`Tensor.remainder_` | In-place version of `remainder()`  
`Tensor.renorm` | See `torch.renorm()`  
`Tensor.renorm_` | In-place version of `renorm()`  
`Tensor.repeat` | Repeats this tensor along the specified dimensions.  
`Tensor.repeat_interleave` | See `torch.repeat_interleave()`.  
`Tensor.requires_grad` | Is `True` if gradients need to be computed for this Tensor, `False` otherwise.  
`Tensor.requires_grad_` | Change if autograd should record operations on this tensor: sets this tensor's `requires_grad` attribute in-place.  
`Tensor.reshape` | Returns a tensor with the same data and number of elements as `self` but with the specified shape.  
`Tensor.reshape_as` | Returns this tensor as the same shape as `other`.  
`Tensor.resize_` | Resizes `self` tensor to the specified size.  
`Tensor.resize_as_` | Resizes the `self` tensor to be the same size as the specified `tensor`.  
`Tensor.retain_grad` | Enables this Tensor to have their `grad` populated during `backward()`.  
`Tensor.retains_grad` | Is `True` if this Tensor is non-leaf and its `grad` is enabled to be populated during `backward()`, `False` otherwise.  
`Tensor.roll` | See `torch.roll()`  
`Tensor.rot90` | See `torch.rot90()`  
`Tensor.round` | See `torch.round()`  
`Tensor.round_` | In-place version of `round()`  
`Tensor.rsqrt` | See `torch.rsqrt()`  
`Tensor.rsqrt_` | In-place version of `rsqrt()`  
`Tensor.scatter` | Out-of-place version of `torch.Tensor.scatter_()`  
`Tensor.scatter_` | Writes all values from the tensor `src` into `self` at the indices specified in the `index` tensor.  
`Tensor.scatter_add_` | Adds all values from the tensor `src` into `self` at the indices specified in the `index` tensor in a similar fashion as `scatter_()`.  
`Tensor.scatter_add` | Out-of-place version of `torch.Tensor.scatter_add_()`  
`Tensor.scatter_reduce_` | Reduces all values from the `src` tensor to the indices specified in the `index` tensor in the `self` tensor using the applied reduction defined via the `reduce` argument (`"sum"`, `"prod"`, `"mean"`, `"amax"`, `"amin"`).  
`Tensor.scatter_reduce` | Out-of-place version of `torch.Tensor.scatter_reduce_()`  
`Tensor.select` | See `torch.select()`  
`Tensor.select_scatter` | See `torch.select_scatter()`  
`Tensor.set_` | Sets the underlying storage, size, and strides.  
`Tensor.share_memory_` | Moves the underlying storage to shared memory.  
`Tensor.short` | `self.short()` is equivalent to `self.to(torch.int16)`.  
`Tensor.sigmoid` | See `torch.sigmoid()`  
`Tensor.sigmoid_` | In-place version of `sigmoid()`  
`Tensor.sign` | See `torch.sign()`  
`Tensor.sign_` | In-place version of `sign()`  
`Tensor.signbit` | See `torch.signbit()`  
`Tensor.sgn` | See `torch.sgn()`  
`Tensor.sgn_` | In-place version of `sgn()`  
`Tensor.sin` | See `torch.sin()`  
`Tensor.sin_` | In-place version of `sin()`  
`Tensor.sinc` | See `torch.sinc()`  
`Tensor.sinc_` | In-place version of `sinc()`  
`Tensor.sinh` | See `torch.sinh()`  
`Tensor.sinh_` | In-place version of `sinh()`  
`Tensor.asinh` | See `torch.asinh()`  
`Tensor.asinh_` | In-place version of `asinh()`  
`Tensor.arcsinh` | See `torch.arcsinh()`  
`Tensor.arcsinh_` | In-place version of `arcsinh()`  
`Tensor.shape` | Returns the size of the `self` tensor.  
`Tensor.size` | Returns the size of the `self` tensor.  
`Tensor.slogdet` | See `torch.slogdet()`  
`Tensor.slice_scatter` | See `torch.slice_scatter()`  
`Tensor.softmax` | Alias for `torch.nn.functional.softmax()`.  
`Tensor.sort` | See `torch.sort()`  
`Tensor.split` | See `torch.split()`  
`Tensor.sparse_mask` | Returns a new sparse tensor with values from a strided tensor `self` filtered by the indices of the sparse tensor `mask`.  
`Tensor.sparse_dim` | Return the number of sparse dimensions in a sparse tensor `self`.  
`Tensor.sqrt` | See `torch.sqrt()`  
`Tensor.sqrt_` | In-place version of `sqrt()`  
`Tensor.square` | See `torch.square()`  
`Tensor.square_` | In-place version of `square()`  
`Tensor.squeeze` | See `torch.squeeze()`  
`Tensor.squeeze_` | In-place version of `squeeze()`  
`Tensor.std` | See `torch.std()`  
`Tensor.stft` | See `torch.stft()`  
`Tensor.storage` | Returns the underlying `TypedStorage`.  
`Tensor.untyped_storage` | Returns the underlying `UntypedStorage`.  
`Tensor.storage_offset` | Returns `self` tensor's offset in the underlying storage in terms of number of storage elements (not bytes).  
`Tensor.storage_type` | Returns the type of the underlying storage.  
`Tensor.stride` | Returns the stride of `self` tensor.  
`Tensor.sub` | See `torch.sub()`.  
`Tensor.sub_` | In-place version of `sub()`  
`Tensor.subtract` | See `torch.subtract()`.  
`Tensor.subtract_` | In-place version of `subtract()`.  
`Tensor.sum` | See `torch.sum()`  
`Tensor.sum_to_size` | Sum `this` tensor to `size`.  
`Tensor.svd` | See `torch.svd()`  
`Tensor.swapaxes` | See `torch.swapaxes()`  
`Tensor.swapdims` | See `torch.swapdims()`  
`Tensor.t` | See `torch.t()`  
`Tensor.t_` | In-place version of `t()`  
`Tensor.tensor_split` | See `torch.tensor_split()`  
`Tensor.tile` | See `torch.tile()`  
`Tensor.to` | Performs Tensor dtype and/or device conversion.  
`Tensor.to_mkldnn` | Returns a copy of the tensor in `torch.mkldnn` layout.  
`Tensor.take` | See `torch.take()`  
`Tensor.take_along_dim` | See `torch.take_along_dim()`  
`Tensor.tan` | See `torch.tan()`  
`Tensor.tan_` | In-place version of `tan()`  
`Tensor.tanh` | See `torch.tanh()`  
`Tensor.tanh_` | In-place version of `tanh()`  
`Tensor.atanh` | See `torch.atanh()`  
`Tensor.atanh_` | In-place version of `atanh()`  
`Tensor.arctanh` | See `torch.arctanh()`  
`Tensor.arctanh_` | In-place version of `arctanh()`  
`Tensor.tolist` | Returns the tensor as a (nested) list.  
`Tensor.topk` | See `torch.topk()`  
`Tensor.to_dense` | Creates a strided copy of `self` if `self` is not a strided tensor, otherwise returns `self`.  
`Tensor.to_sparse` | Returns a sparse copy of the tensor.  
`Tensor.to_sparse_csr` | Convert a tensor to compressed row storage format (CSR).  
`Tensor.to_sparse_csc` | Convert a tensor to compressed column storage (CSC) format.  
`Tensor.to_sparse_bsr` | Convert a tensor to a block sparse row (BSR) storage format of given blocksize.  
`Tensor.to_sparse_bsc` | Convert a tensor to a block sparse column (BSC) storage format of given blocksize.  
`Tensor.trace` | See `torch.trace()`  
`Tensor.transpose` | See `torch.transpose()`  
`Tensor.transpose_` | In-place version of `transpose()`  
`Tensor.triangular_solve` | See `torch.triangular_solve()`  
`Tensor.tril` | See `torch.tril()`  
`Tensor.tril_` | In-place version of `tril()`  
`Tensor.triu` | See `torch.triu()`  
`Tensor.triu_` | In-place version of `triu()`  
`Tensor.true_divide` | See `torch.true_divide()`  
`Tensor.true_divide_` | In-place version of `true_divide_()`  
`Tensor.trunc` | See `torch.trunc()`  
`Tensor.trunc_` | In-place version of `trunc()`  
`Tensor.type` | Returns the type if dtype is not provided, else casts this object to the specified type.  
`Tensor.type_as` | Returns this tensor cast to the type of the given tensor.  
`Tensor.unbind` | See `torch.unbind()`  
`Tensor.unflatten` | See `torch.unflatten()`.  
`Tensor.unfold` | Returns a view of the original tensor which contains all slices of size `size` from `self` tensor in the dimension `dimension`.  
`Tensor.uniform_` | Fills `self` tensor with numbers sampled from the continuous uniform distribution:  
`Tensor.unique` | Returns the unique elements of the input tensor.  
`Tensor.unique_consecutive` | Eliminates all but the first element from every consecutive group of equivalent elements.  
`Tensor.unsqueeze` | See `torch.unsqueeze()`  
`Tensor.unsqueeze_` | In-place version of `unsqueeze()`  
`Tensor.values` | Return the values tensor of a sparse COO tensor.  
`Tensor.var` | See `torch.var()`  
`Tensor.vdot` | See `torch.vdot()`  
`Tensor.view` | Returns a new tensor with the same data as the `self` tensor but of a different `shape`.  
`Tensor.view_as` | View this tensor as the same size as `other`.  
`Tensor.vsplit` | See `torch.vsplit()`  
`Tensor.where` | `self.where(condition, y)` is equivalent to `torch.where(condition, self, y)`.  
`Tensor.xlogy` | See `torch.xlogy()`  
`Tensor.xlogy_` | In-place version of `xlogy()`  
`Tensor.xpu` | Returns a copy of this object in XPU memory.  
`Tensor.zero_` | Fills `self` tensor with zeros.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.Tensor
    * Data types
    * Initializing and basic operations
    * Tensor class reference
      * `Tensor`
        * `Tensor.__init__()`
        * `Tensor.T`
        * `Tensor.H`
        * `Tensor.mT`
        * `Tensor.mH`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.compiler
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.compiler
`torch.compiler` is a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is `torch.compile`.
`torch.compile` is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. `torch.compile` is written in Python and it marks the transition of PyTorch from C++ to Python.
`torch.compile` leverages the following underlying technologies:
  * **TorchDynamo (torch._dynamo)** is an internal API that uses a CPython feature called the Frame Evaluation API to safely capture PyTorch graphs. Methods that are available externally for PyTorch users are surfaced through the `torch.compiler` namespace.
  * **TorchInductor** is the default `torch.compile` deep learning compiler that generates fast code for multiple accelerators and backends. You need to use a backend compiler to make speedups through `torch.compile` possible. For NVIDIA, AMD and Intel GPUs, it leverages OpenAI Triton as the key building block.
  * **AOT Autograd** captures not only the user-level code, but also backpropagation, which results in capturing the backwards pass “ahead-of-time”. This enables acceleration of both forwards and backwards pass using TorchInductor.


Note
In some cases, the terms `torch.compile`, TorchDynamo, `torch.compiler` might be used interchangeably in this documentation.
As mentioned above, to run your workflows faster, `torch.compile` through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as _inductor_ , TorchDynamo has a list of supported backends developed by our partners, which can be see by running `torch.compiler.list_backends()` each of which with its optional dependencies.
Some of the most commonly used backends include:
**Training & inference backends**
Backend | Description  
---|---  
`torch.compile(m, backend="inductor")` | Uses the TorchInductor backend. Read more  
`torch.compile(m, backend="cudagraphs")` | CUDA graphs with AOT Autograd. Read more  
`torch.compile(m, backend="ipex")` | Uses IPEX on CPU. Read more  
`torch.compile(m, backend="onnxrt")` | Uses ONNX Runtime for training on CPU/GPU. Read more  
**Inference-only backends**
Backend | Description  
---|---  
`torch.compile(m, backend="tensorrt")` | Uses Torch-TensorRT for inference optimizations. Requires `import torch_tensorrt` in the calling script to register backend. Read more  
`torch.compile(m, backend="ipex")` | Uses IPEX for inference on CPU. Read more  
`torch.compile(m, backend="tvm")` | Uses Apache TVM for inference optimizations. Read more  
`torch.compile(m, backend="openvino")` | Uses OpenVINO for inference optimizations. Read more  
## Read More
Getting Started for PyTorch Users
  * Getting Started
  * torch.compiler API reference
  * torch.compiler.config
  * TorchDynamo APIs for fine-grained tracing
  * AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models
  * TorchInductor GPU Profiling
  * Profiling to understand torch.compile performance
  * Frequently Asked Questions
  * torch.compile Troubleshooting
  * PyTorch 2.0 Performance Dashboard


Deep Dive for PyTorch Developers
  * Dynamo Overview
  * Dynamo Deep-Dive
  * Dynamic shapes
  * PyTorch 2.0 NNModule Support
  * Best Practices for Backends
  * CUDAGraph Trees
  * Fake tensor


HowTo for PyTorch Backend Vendors
  * Custom Backends
  * Writing Graph Transformations on ATen IR
  * IRs


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.compiler
    * Read More


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Tensor Views
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Tensor Views
PyTorch allows a tensor to be a `View` of an existing tensor. View tensor shares the same underlying data with its base tensor. Supporting `View` avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.
For example, to get a view of an existing tensor `t`, you can call `t.view(...)`.
```
>>> t = torch.rand(4, 4)
>>> b = t.view(2, 8)
>>> t.storage().data_ptr() == b.storage().data_ptr() # `t` and `b` share the same underlying data.
True
# Modifying view tensor changes base tensor as well.
>>> b[0][0] = 3.14
>>> t[0][0]
tensor(3.14)

```
Copy to clipboard
Since views share underlying data with its base tensor, if you edit the data in the view, it will be reflected in the base tensor as well.
Typically a PyTorch op returns a new tensor as output, e.g. `add()`. But in case of view ops, outputs are views of input tensors to avoid unnecessary data copy. No data movement occurs when creating a view, view tensor just changes the way it interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor. Users should pay additional attention as contiguity might have implicit performance impact. `transpose()` is a common example.
```
>>> base = torch.tensor([[0, 1],[2, 3]])
>>> base.is_contiguous()
True
>>> t = base.transpose(0, 1) # `t` is a view of `base`. No data movement happened here.
# View tensors might be non-contiguous.
>>> t.is_contiguous()
False
# To get a contiguous tensor, call `.contiguous()` to enforce
# copying data when `t` is not contiguous.
>>> c = t.contiguous()

```
Copy to clipboard
For reference, here’s a full list of view ops in PyTorch:
  * Basic slicing and indexing op, e.g. `tensor[0, 2:, 1:7:2]` returns a view of base `tensor`, see note below.
  * `adjoint()`
  * `as_strided()`
  * `detach()`
  * `diagonal()`
  * `expand()`
  * `expand_as()`
  * `movedim()`
  * `narrow()`
  * `permute()`
  * `select()`
  * `squeeze()`
  * `transpose()`
  * `t()`
  * `T`
  * `H`
  * `mT`
  * `mH`
  * `real`
  * `imag`
  * `view_as_real()`
  * `unflatten()`
  * `unfold()`
  * `unsqueeze()`
  * `view()`
  * `view_as()`
  * `unbind()`
  * `split()`
  * `hsplit()`
  * `vsplit()`
  * `tensor_split()`
  * `split_with_sizes()`
  * `swapaxes()`
  * `swapdims()`
  * `chunk()`
  * `indices()` (sparse tensor only)
  * `values()` (sparse tensor only)


Note
When accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors that basic indexing returns views, while advanced indexing returns a copy. Assignment via either basic or advanced indexing is in-place. See more examples in Numpy indexing documentation.
It’s also worth mentioning a few ops with special behaviors:
  * `reshape()`, `reshape_as()` and `flatten()` can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not.
  * `contiguous()` returns **itself** if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.


For a more detailed walk-through of PyTorch internal implementation, please refer to ezyang’s blogpost about PyTorch Internals.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Tensor Views


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.testing
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.testing 

torch.testing.assert_close(_actual_ , _expected_ , _*_ , _allow_subclasses =True_, _rtol =None_, _atol =None_, _equal_nan =False_, _check_device =True_, _check_dtype =True_, _check_layout =True_, _check_stride =False_, _msg =None_)[source][source]
    
Asserts that `actual` and `expected` are close.
If `actual` and `expected` are strided, non-quantized, real-valued, and finite, they are considered close if
∣actual−expected∣≤atol+rtol⋅∣expected∣\lvert \text{actual} - \text{expected} \rvert \le \texttt{atol} + \texttt{rtol} \cdot \lvert \text{expected} \rvert∣actual−expected∣≤atol+rtol⋅∣expected∣
Non-finite values (`-inf` and `inf`) are only considered close if and only if they are equal. `NaN`’s are only considered equal to each other if `equal_nan` is `True`.
In addition, they are only considered close if they have the same
  * `device` (if `check_device` is `True`),
  * `dtype` (if `check_dtype` is `True`),
  * `layout` (if `check_layout` is `True`), and
  * stride (if `check_stride` is `True`).


If either `actual` or `expected` is a meta tensor, only the attribute checks will be performed.
If `actual` and `expected` are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are checked individually. Indices, namely `indices` for COO, `crow_indices` and `col_indices` for CSR and BSR, or `ccol_indices` and `row_indices` for CSC and BSC layouts, respectively, are always checked for equality whereas the values are checked for closeness according to the definition above.
If `actual` and `expected` are quantized, they are considered close if they have the same `qscheme()` and the result of `dequantize()` is close according to the definition above.
`actual` and `expected` can be `Tensor`’s or any tensor-or-scalar-likes from which `torch.Tensor`’s can be constructed with `torch.as_tensor()`. Except for Python scalars the input types have to be directly related. In addition, `actual` and `expected` can be `Sequence`’s or `Mapping`’s in which case they are considered close if their structure matches and all their elements are considered close according to the above definition.
Note
Python scalars are an exception to the type relation requirement, because their `type()`, i.e. `int`, `float`, and `complex`, is equivalent to the `dtype` of a tensor-like. Thus, Python scalars of different types can be checked, but require `check_dtype=False`. 

Parameters
    
  * **actual** (_Any_) – Actual input.
  * **expected** (_Any_) – Expected input.
  * **allow_subclasses** (_bool_) – If `True` (default) and except for Python scalars, inputs of directly related types are allowed. Otherwise type equality is required.
  * **rtol** (_Optional_ _[__float_ _]_) – Relative tolerance. If specified `atol` must also be specified. If omitted, default values based on the `dtype` are selected with the below table.
  * **atol** (_Optional_ _[__float_ _]_) – Absolute tolerance. If specified `rtol` must also be specified. If omitted, default values based on the `dtype` are selected with the below table.
  * **equal_nan** (_Union_ _[__bool_ _,__str_ _]_) – If `True`, two `NaN` values will be considered equal.
  * **check_device** (_bool_) – If `True` (default), asserts that corresponding tensors are on the same `device`. If this check is disabled, tensors on different `device`’s are moved to the CPU before being compared.
  * **check_dtype** (_bool_) – If `True` (default), asserts that corresponding tensors have the same `dtype`. If this check is disabled, tensors with different `dtype`’s are promoted to a common `dtype` (according to `torch.promote_types()`) before being compared.
  * **check_layout** (_bool_) – If `True` (default), asserts that corresponding tensors have the same `layout`. If this check is disabled, tensors with different `layout`’s are converted to strided tensors before being compared.
  * **check_stride** (_bool_) – If `True` and corresponding tensors are strided, asserts that they have the same stride.
  * **msg** (_Optional_ _[__Union_ _[__str_ _,__Callable_ _[__[__str_ _]__,__str_ _]__]__]_) – Optional error message to use in case a failure occurs during the comparison. Can also passed as callable in which case it will be called with the generated message and should return the new message.



Raises
    
  * **ValueError** – If no `torch.Tensor` can be constructed from an input.
  * **ValueError** – If only `rtol` or `atol` is specified.
  * **AssertionError** – If corresponding inputs are not Python scalars and are not directly related.
  * **AssertionError** – If `allow_subclasses` is `False`, but corresponding inputs are not Python scalars and have different types.
  * **AssertionError** – If the inputs are `Sequence`’s, but their length does not match.
  * **AssertionError** – If the inputs are `Mapping`’s, but their set of keys do not match.
  * **AssertionError** – If corresponding tensors do not have the same `shape`.
  * **AssertionError** – If `check_layout` is `True`, but corresponding tensors do not have the same `layout`.
  * **AssertionError** – If only one of corresponding tensors is quantized.
  * **AssertionError** – If corresponding tensors are quantized, but have different `qscheme()`’s.
  * **AssertionError** – If `check_device` is `True`, but corresponding tensors are not on the same `device`.
  * **AssertionError** – If `check_dtype` is `True`, but corresponding tensors do not have the same `dtype`.
  * **AssertionError** – If `check_stride` is `True`, but corresponding strided tensors do not have the same stride.
  * **AssertionError** – If the values of corresponding tensors are not close according to the definition above.


The following table displays the default `rtol` and `atol` for different `dtype`’s. In case of mismatching `dtype`’s, the maximum of both tolerances is used.
`dtype` | `rtol` | `atol`  
---|---|---  
`float16` | `1e-3` | `1e-5`  
`bfloat16` | `1.6e-2` | `1e-5`  
`float32` | `1.3e-6` | `1e-5`  
`float64` | `1e-7` | `1e-7`  
`complex32` | `1e-3` | `1e-5`  
`complex64` | `1.3e-6` | `1e-5`  
`complex128` | `1e-7` | `1e-7`  
`quint8` | `1.3e-6` | `1e-5`  
`quint2x4` | `1.3e-6` | `1e-5`  
`quint4x2` | `1.3e-6` | `1e-5`  
`qint8` | `1.3e-6` | `1e-5`  
`qint32` | `1.3e-6` | `1e-5`  
other | `0.0` | `0.0`  
Note
`assert_close()` is highly configurable with strict default settings. Users are encouraged to `partial()` it to fit their use case. For example, if an equality check is needed, one might define an `assert_equal` that uses zero tolerances for every `dtype` by default:
```
>>> import functools
>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)
>>> assert_equal(1e-9, 1e-10)
Traceback (most recent call last):
...
AssertionError: Scalars are not equal!
Expected 1e-10 but got 1e-09.
Absolute difference: 9.000000000000001e-10
Relative difference: 9.0

```
Copy to clipboard
Examples
```
>>> # tensor to tensor comparison
>>> expected = torch.tensor([1e0, 1e-1, 1e-2])
>>> actual = torch.acos(torch.cos(expected))
>>> torch.testing.assert_close(actual, expected)

```
Copy to clipboard
```
>>> # scalar to scalar comparison
>>> import math
>>> expected = math.sqrt(2.0)
>>> actual = 2.0 / math.sqrt(2.0)
>>> torch.testing.assert_close(actual, expected)

```
Copy to clipboard
```
>>> # numpy array to numpy array comparison
>>> import numpy as np
>>> expected = np.array([1e0, 1e-1, 1e-2])
>>> actual = np.arccos(np.cos(expected))
>>> torch.testing.assert_close(actual, expected)

```
Copy to clipboard
```
>>> # sequence to sequence comparison
>>> import numpy as np
>>> # The types of the sequences do not have to match. They only have to have the same
>>> # length and their elements have to match.
>>> expected = [torch.tensor([1.0]), 2.0, np.array(3.0)]
>>> actual = tuple(expected)
>>> torch.testing.assert_close(actual, expected)

```
Copy to clipboard
```
>>> # mapping to mapping comparison
>>> from collections import OrderedDict
>>> import numpy as np
>>> foo = torch.tensor(1.0)
>>> bar = 2.0
>>> baz = np.array(3.0)
>>> # The types and a possible ordering of mappings do not have to match. They only
>>> # have to have the same set of keys and their elements have to match.
>>> expected = OrderedDict([("foo", foo), ("bar", bar), ("baz", baz)])
>>> actual = {"baz": baz, "bar": bar, "foo": foo}
>>> torch.testing.assert_close(actual, expected)

```
Copy to clipboard
```
>>> expected = torch.tensor([1.0, 2.0, 3.0])
>>> actual = expected.clone()
>>> # By default, directly related instances can be compared
>>> torch.testing.assert_close(torch.nn.Parameter(actual), expected)
>>> # This check can be made more strict with allow_subclasses=False
>>> torch.testing.assert_close(
...   torch.nn.Parameter(actual), expected, allow_subclasses=False
... )
Traceback (most recent call last):
...
TypeError: No comparison pair was able to handle inputs of type
<class 'torch.nn.parameter.Parameter'> and <class 'torch.Tensor'>.
>>> # If the inputs are not directly related, they are never considered close
>>> torch.testing.assert_close(actual.numpy(), expected)
Traceback (most recent call last):
...
TypeError: No comparison pair was able to handle inputs of type <class 'numpy.ndarray'>
and <class 'torch.Tensor'>.
>>> # Exceptions to these rules are Python scalars. They can be checked regardless of
>>> # their type if check_dtype=False.
>>> torch.testing.assert_close(1.0, 1, check_dtype=False)

```
Copy to clipboard
```
>>> # NaN != NaN by default.
>>> expected = torch.tensor(float("Nan"))
>>> actual = expected.clone()
>>> torch.testing.assert_close(actual, expected)
Traceback (most recent call last):
...
AssertionError: Scalars are not close!
Expected nan but got nan.
Absolute difference: nan (up to 1e-05 allowed)
Relative difference: nan (up to 1.3e-06 allowed)
>>> torch.testing.assert_close(actual, expected, equal_nan=True)

```
Copy to clipboard
```
>>> expected = torch.tensor([1.0, 2.0, 3.0])
>>> actual = torch.tensor([1.0, 4.0, 5.0])
>>> # The default error message can be overwritten.
>>> torch.testing.assert_close(actual, expected, msg="Argh, the tensors are not close!")
Traceback (most recent call last):
...
AssertionError: Argh, the tensors are not close!
>>> # If msg is a callable, it can be used to augment the generated message with
>>> # extra information
>>> torch.testing.assert_close(
...   actual, expected, msg=lambda msg: f"Header\n\n{msg}\n\nFooter"
... )
Traceback (most recent call last):
...
AssertionError: Header
Tensor-likes are not close!
Mismatched elements: 2 / 3 (66.7%)
Greatest absolute difference: 2.0 at index (1,) (up to 1e-05 allowed)
Greatest relative difference: 1.0 at index (1,) (up to 1.3e-06 allowed)
Footer

```
Copy to clipboard 

torch.testing.make_tensor(_* shape_, _dtype_ , _device_ , _low =None_, _high =None_, _requires_grad =False_, _noncontiguous =False_, _exclude_zero =False_, _memory_format =None_)[source][source]
    
Creates a tensor with the given `shape`, `device`, and `dtype`, and filled with values uniformly drawn from `[low, high)`.
If `low` or `high` are specified and are outside the range of the `dtype`’s representable finite values then they are clamped to the lowest or highest representable finite value, respectively. If `None`, then the following table describes the default values for `low` and `high`, which depend on `dtype`.
`dtype` | `low` | `high`  
---|---|---  
boolean type | `0` | `2`  
unsigned integral type | `0` | `10`  
signed integral types | `-9` | `10`  
floating types | `-9` | `9`  
complex types | `-9` | `9` 

Parameters
      
  * **shape** (_Tuple_ _[__int_ _,__...__]_) – Single integer or a sequence of integers defining the shape of the output tensor.
  * **dtype** (`torch.dtype`) – The data type of the returned tensor.
  * **device** (_Union_ _[__str_ _,__torch.device_ _]_) – The device of the returned tensor.
  * **low** (_Optional_ _[__Number_ _]_) – Sets the lower limit (inclusive) of the given range. If a number is provided it is clamped to the least representable finite value of the given dtype. When `None` (default), this value is determined based on the `dtype` (see the table above). Default: `None`.
  * **high** (_Optional_ _[__Number_ _]_) – 
Sets the upper limit (exclusive) of the given range. If a number is provided it is clamped to the greatest representable finite value of the given dtype. When `None` (default) this value is determined based on the `dtype` (see the table above). Default: `None`.
Deprecated since version 2.1: Passing `low==high` to `make_tensor()` for floating or complex types is deprecated since 2.1 and will be removed in 2.3. Use `torch.full()` instead.
  * **requires_grad** (_Optional_ _[__bool_ _]_) – If autograd should record operations on the returned tensor. Default: `False`.
  * **noncontiguous** (_Optional_ _[__bool_ _]_) – If True, the returned tensor will be noncontiguous. This argument is ignored if the constructed tensor has fewer than two elements. Mutually exclusive with `memory_format`.
  * **exclude_zero** (_Optional_ _[__bool_ _]_) – If `True` then zeros are replaced with the dtype’s small positive value depending on the `dtype`. For bool and integer types zero is replaced with one. For floating point types it is replaced with the dtype’s smallest positive normal number (the “tiny” value of the `dtype`’s `finfo()` object), and for complex types it is replaced with a complex number whose real and imaginary parts are both the smallest positive normal number representable by the complex type. Default `False`.
  * **memory_format** (_Optional_ _[__torch.memory_format_ _]_) – The memory format of the returned tensor. Mutually exclusive with `noncontiguous`.



Raises
    
  * **ValueError** – If `requires_grad=True` is passed for integral dtype
  * **ValueError** – If `low >= high`.
  * **ValueError** – If either `low` or `high` is `nan`.
  * **ValueError** – If both `noncontiguous` and `memory_format` are passed.
  * **TypeError** – If `dtype` isn’t supported by this function.



Return type
    
_Tensor_
Examples
```
>>> from torch.testing import make_tensor
>>> # Creates a float tensor with values in [-1, 1)
>>> make_tensor((3,), device='cpu', dtype=torch.float32, low=-1, high=1)
tensor([ 0.1205, 0.2282, -0.6380])
>>> # Creates a bool tensor on CUDA
>>> make_tensor((2, 2), device='cuda', dtype=torch.bool)
tensor([[False, False],
    [False, True]], device='cuda:0')

```
Copy to clipboard 

torch.testing.assert_allclose(_actual_ , _expected_ , _rtol =None_, _atol =None_, _equal_nan =True_, _msg =''_)[source][source]
    
Warning
`torch.testing.assert_allclose()` is deprecated since `1.12` and will be removed in a future release. Please use `torch.testing.assert_close()` instead. You can find detailed upgrade instructions here.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.testing
    * `assert_close()`
    * `make_tensor()`
    * `assert_allclose()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils.tensorboard
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils.tensorboard
Before going further, more details on TensorBoard can be found at https://www.tensorflow.org/tensorboard/
Once you’ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs.
The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:
```
import torch
import torchvision
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms
# Writer will output to ./runs/ directory by default
writer = SummaryWriter()
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
model = torchvision.models.resnet50(False)
# Have ResNet model take in grayscale rather than RGB
model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
images, labels = next(iter(trainloader))
grid = torchvision.utils.make_grid(images)
writer.add_image('images', grid, 0)
writer.add_graph(model, images)
writer.close()

```
Copy to clipboard
This can then be visualized with TensorBoard, which should be installable and runnable with:
```
pip install tensorboard
tensorboard --logdir=runs

```
Copy to clipboard
Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, “Loss/train” and “Loss/test” will be grouped together, while “Accuracy/train” and “Accuracy/test” will be grouped separately in the TensorBoard interface.
```
from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter()
for n_iter in range(100):
  writer.add_scalar('Loss/train', np.random.random(), n_iter)
  writer.add_scalar('Loss/test', np.random.random(), n_iter)
  writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
  writer.add_scalar('Accuracy/test', np.random.random(), n_iter)

```
Copy to clipboard
Expected result:
![_images/hier_tags.png](https://pytorch.org/docs/stable/_images/hier_tags.png) 

_class_ torch.utils.tensorboard.writer.SummaryWriter(_log_dir =None_, _comment =''_, _purge_step =None_, _max_queue =10_, _flush_secs =120_, _filename_suffix =''_)[source][source]
    
Writes entries directly to event files in the log_dir to be consumed by TensorBoard.
The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training. 

__init__(_log_dir =None_, _comment =''_, _purge_step =None_, _max_queue =10_, _flush_secs =120_, _filename_suffix =''_)[source][source]
    
Create a SummaryWriter that will write out events and summaries to the event file. 

Parameters
    
  * **log_dir** (_str_) – Save directory location. Default is runs/**CURRENT_DATETIME_HOSTNAME** , which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in ‘runs/exp1’, ‘runs/exp2’, etc. for each new experiment to compare across them.
  * **comment** (_str_) – Comment log_dir suffix appended to the default `log_dir`. If `log_dir` is assigned, this argument has no effect.
  * **purge_step** (_int_) – When logging crashes at step T+XT+XT+X and restarts at step TTT, any events whose global_step larger or equal to TTT will be purged and hidden from TensorBoard. Note that crashed and resumed experiments should have the same `log_dir`.
  * **max_queue** (_int_) – Size of the queue for pending events and summaries before one of the ‘add’ calls forces a flush to disk. Default is ten items.
  * **flush_secs** (_int_) – How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes.
  * **filename_suffix** (_str_) – Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.


Examples:
```
from torch.utils.tensorboard import SummaryWriter
# create a summary writer with automatically generated folder name.
writer = SummaryWriter()
# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/
# create a summary writer using the specified folder name.
writer = SummaryWriter("my_experiment")
# folder location: my_experiment
# create a summary writer with comment appended.
writer = SummaryWriter(comment="LR_0.1_BATCH_16")
# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/

```
Copy to clipboard 

add_scalar(_tag_ , _scalar_value_ , _global_step =None_, _walltime =None_, _new_style =False_, _double_precision =False_)[source][source]
    
Add scalar data to summary. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **scalar_value** (_float_ _or_ _string/blobname_) – Value to save
  * **global_step** (_int_) – Global step value to record
  * **walltime** (_float_) – Optional override default walltime (time.time()) with seconds after epoch of event
  * **new_style** (_boolean_) – Whether to use new style (tensor field) or old style (simple_value field). New style could lead to faster data loading.


Examples:
```
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
x = range(100)
for i in x:
  writer.add_scalar('y=2x', i * 2, i)
writer.close()

```
Copy to clipboard
Expected result:
![_images/add_scalar.png](https://pytorch.org/docs/stable/_images/add_scalar.png) 

add_scalars(_main_tag_ , _tag_scalar_dict_ , _global_step =None_, _walltime =None_)[source][source]
    
Add many scalar data to summary. 

Parameters
    
  * **main_tag** (_str_) – The parent name for the tags
  * **tag_scalar_dict** (_dict_) – Key-value pair storing the tag and corresponding values
  * **global_step** (_int_) – Global step value to record
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event


Examples:
```
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
r = 5
for i in range(100):
  writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),
                  'xcosx':i*np.cos(i/r),
                  'tanx': np.tan(i/r)}, i)
writer.close()
# This call adds three values to the same scalar plot with the tag
# 'run_14h' in TensorBoard's scalar section.

```
Copy to clipboard
Expected result:
![_images/add_scalars.png](https://pytorch.org/docs/stable/_images/add_scalars.png) 

add_histogram(_tag_ , _values_ , _global_step =None_, _bins ='tensorflow'_, _walltime =None_, _max_bins =None_)[source][source]
    
Add histogram to summary. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **values** (_torch.Tensor_ _,__numpy.ndarray_ _, or_ _string/blobname_) – Values to build histogram
  * **global_step** (_int_) – Global step value to record
  * **bins** (_str_) – One of {‘tensorflow’,’auto’, ‘fd’, …}. This determines how the bins are made. You can find other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event


Examples:
```
from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter()
for i in range(10):
  x = np.random.random(1000)
  writer.add_histogram('distribution centers', x + i, i)
writer.close()

```
Copy to clipboard
Expected result:
![_images/add_histogram.png](https://pytorch.org/docs/stable/_images/add_histogram.png) 

add_image(_tag_ , _img_tensor_ , _global_step =None_, _walltime =None_, _dataformats ='CHW'_)[source][source]
    
Add image data to summary.
Note that this requires the `pillow` package. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **img_tensor** (_torch.Tensor_ _,__numpy.ndarray_ _, or_ _string/blobname_) – Image data
  * **global_step** (_int_) – Global step value to record
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event
  * **dataformats** (_str_) – Image data format specification of the form CHW, HWC, HW, WH, etc.



Shape:
    
img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use `torchvision.utils.make_grid()` to convert a batch of tensor into 3xHxW format or call `add_images` and let us do the job. Tensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as corresponding `dataformats` argument is passed, e.g. `CHW`, `HWC`, `HW`.
Examples:
```
from torch.utils.tensorboard import SummaryWriter
import numpy as np
img = np.zeros((3, 100, 100))
img[0] = np.arange(0, 10000).reshape(100, 100) / 10000
img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000
img_HWC = np.zeros((100, 100, 3))
img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000
img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000
writer = SummaryWriter()
writer.add_image('my_image', img, 0)
# If you have non-default dimension setting, set the dataformats argument.
writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')
writer.close()

```
Copy to clipboard
Expected result:
![_images/add_image.png](https://pytorch.org/docs/stable/_images/add_image.png) 

add_images(_tag_ , _img_tensor_ , _global_step =None_, _walltime =None_, _dataformats ='NCHW'_)[source][source]
    
Add batched image data to summary.
Note that this requires the `pillow` package. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **img_tensor** (_torch.Tensor_ _,__numpy.ndarray_ _, or_ _string/blobname_) – Image data
  * **global_step** (_int_) – Global step value to record
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event
  * **dataformats** (_str_) – Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.



Shape:
    
img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If `dataformats` is specified, other shape will be accepted. e.g. NCHW or NHWC.
Examples:
```
from torch.utils.tensorboard import SummaryWriter
import numpy as np
img_batch = np.zeros((16, 3, 100, 100))
for i in range(16):
  img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i
  img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i
writer = SummaryWriter()
writer.add_images('my_image_batch', img_batch, 0)
writer.close()

```
Copy to clipboard
Expected result:
![_images/add_images.png](https://pytorch.org/docs/stable/_images/add_images.png) 

add_figure(_tag_ , _figure_ , _global_step =None_, _close =True_, _walltime =None_)[source][source]
    
Render matplotlib figure into an image and add it to summary.
Note that this requires the `matplotlib` package. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **figure** (_Union_ _[__Figure_ _,__list_ _[__'Figure'__]__]_) – Figure or a list of figures
  * **global_step** (_Optional_ _[__int_ _]_) – Global step value to record
  * **close** (_bool_) – Flag to automatically close the figure
  * **walltime** (_Optional_ _[__float_ _]_) – Optional override default walltime (time.time()) seconds after epoch of event



add_video(_tag_ , _vid_tensor_ , _global_step =None_, _fps =4_, _walltime =None_)[source][source]
    
Add video data to summary.
Note that this requires the `moviepy` package. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **vid_tensor** (_torch.Tensor_) – Video data
  * **global_step** (_int_) – Global step value to record
  * **fps** (_float_ _or_ _int_) – Frames per second
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event



Shape:
    
vid_tensor: (N,T,C,H,W)(N, T, C, H, W)(N,T,C,H,W). The values should lie in [0, 255] for type uint8 or [0, 1] for type float. 

add_audio(_tag_ , _snd_tensor_ , _global_step =None_, _sample_rate =44100_, _walltime =None_)[source][source]
    
Add audio data to summary. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **snd_tensor** (_torch.Tensor_) – Sound data
  * **global_step** (_int_) – Global step value to record
  * **sample_rate** (_int_) – sample rate in Hz
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event



Shape:
    
snd_tensor: (1,L)(1, L)(1,L). The values should lie between [-1, 1]. 

add_text(_tag_ , _text_string_ , _global_step =None_, _walltime =None_)[source][source]
    
Add text data to summary. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **text_string** (_str_) – String to save
  * **global_step** (_int_) – Global step value to record
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event


Examples:
```
writer.add_text('lstm', 'This is an lstm', 0)
writer.add_text('rnn', 'This is an rnn', 10)

```
Copy to clipboard 

add_graph(_model_ , _input_to_model =None_, _verbose =False_, _use_strict_trace =True_)[source][source]
    
Add graph data to summary. 

Parameters
    
  * **model** (_torch.nn.Module_) – Model to draw.
  * **input_to_model** (_torch.Tensor_ _or_ _list_ _of_ _torch.Tensor_) – A variable or a tuple of variables to be fed.
  * **verbose** (_bool_) – Whether to print graph structure in console.
  * **use_strict_trace** (_bool_) – Whether to pass keyword argument strict to torch.jit.trace. Pass False when you want the tracer to record your mutable container types (list, dict)



add_embedding(_mat_ , _metadata =None_, _label_img =None_, _global_step =None_, _tag ='default'_, _metadata_header =None_)[source][source]
    
Add embedding projector data to summary. 

Parameters
    
  * **mat** (_torch.Tensor_ _or_ _numpy.ndarray_) – A matrix which each row is the feature vector of the data point
  * **metadata** (_list_) – A list of labels, each element will be converted to string
  * **label_img** (_torch.Tensor_) – Images correspond to each data point
  * **global_step** (_int_) – Global step value to record
  * **tag** (_str_) – Name for the embedding
  * **metadata_header** (_list_) – A list of headers for multi-column metadata. If given, each metadata must be a list with values corresponding to headers.



Shape:
    
mat: (N,D)(N, D)(N,D), where N is number of data and D is feature dimension
label_img: (N,C,H,W)(N, C, H, W)(N,C,H,W)
Examples:
```
import keyword
import torch
meta = []
while len(meta)<100:
  meta = meta+keyword.kwlist # get some strings
meta = meta[:100]
for i, v in enumerate(meta):
  meta[i] = v+str(i)
label_img = torch.rand(100, 3, 10, 32)
for i in range(100):
  label_img[i]*=i/100.0
writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)
writer.add_embedding(torch.randn(100, 5), label_img=label_img)
writer.add_embedding(torch.randn(100, 5), metadata=meta)

```
Copy to clipboard
Note
Categorical (i.e. non-numeric) metadata cannot have more than 50 unique values if they are to be used for coloring in the embedding projector. 

add_pr_curve(_tag_ , _labels_ , _predictions_ , _global_step =None_, _num_thresholds =127_, _weights =None_, _walltime =None_)[source][source]
    
Add precision recall curve.
Plotting a precision-recall curve lets you understand your model’s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **labels** (_torch.Tensor_ _,__numpy.ndarray_ _, or_ _string/blobname_) – Ground truth data. Binary label for each element.
  * **predictions** (_torch.Tensor_ _,__numpy.ndarray_ _, or_ _string/blobname_) – The probability that an element be classified as true. Value should be in [0, 1]
  * **global_step** (_int_) – Global step value to record
  * **num_thresholds** (_int_) – Number of thresholds used to draw the curve.
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event


Examples:
```
from torch.utils.tensorboard import SummaryWriter
import numpy as np
labels = np.random.randint(2, size=100) # binary label
predictions = np.random.rand(100)
writer = SummaryWriter()
writer.add_pr_curve('pr_curve', labels, predictions, 0)
writer.close()

```
Copy to clipboard 

add_custom_scalars(_layout_)[source][source]
    
Create special chart by collecting charts tags in ‘scalars’.
NOTE: This function can only be called once for each SummaryWriter() object.
Because it only provides metadata to tensorboard, the function can be called before or after the training loop. 

Parameters
    
**layout** (_dict_) – {categoryName: _charts_}, where _charts_ is also a dictionary {chartName: _ListOfProperties_}. The first element in _ListOfProperties_ is the chart’s type (one of **Multiline** or **Margin**) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.
Examples:
```
layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},
       'USA':{ 'dow':['Margin',  ['dow/aaa', 'dow/bbb', 'dow/ccc']],
         'nasdaq':['Margin',  ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}
writer.add_custom_scalars(layout)

```
Copy to clipboard 

add_mesh(_tag_ , _vertices_ , _colors =None_, _faces =None_, _config_dict =None_, _global_step =None_, _walltime =None_)[source][source]
    
Add meshes or 3D point clouds to TensorBoard.
The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage. 

Parameters
    
  * **tag** (_str_) – Data identifier
  * **vertices** (_torch.Tensor_) – List of the 3D coordinates of vertices.
  * **colors** (_torch.Tensor_) – Colors for each vertex
  * **faces** (_torch.Tensor_) – Indices of vertices within each triangle. (Optional)
  * **config_dict** – Dictionary with ThreeJS classes names and configuration.
  * **global_step** (_int_) – Global step value to record
  * **walltime** (_float_) – Optional override default walltime (time.time()) seconds after epoch of event



Shape:
    
vertices: (B,N,3)(B, N, 3)(B,N,3). (batch, number_of_vertices, channels)
colors: (B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.
faces: (B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, number_of_vertices] for type uint8.
Examples:
```
from torch.utils.tensorboard import SummaryWriter
vertices_tensor = torch.as_tensor([
  [1, 1, 1],
  [-1, -1, 1],
  [1, -1, -1],
  [-1, 1, -1],
], dtype=torch.float).unsqueeze(0)
colors_tensor = torch.as_tensor([
  [255, 0, 0],
  [0, 255, 0],
  [0, 0, 255],
  [255, 0, 255],
], dtype=torch.int).unsqueeze(0)
faces_tensor = torch.as_tensor([
  [0, 2, 3],
  [0, 3, 1],
  [0, 1, 2],
  [1, 3, 2],
], dtype=torch.int).unsqueeze(0)
writer = SummaryWriter()
writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)
writer.close()

```
Copy to clipboard 

add_hparams(_hparam_dict_ , _metric_dict_ , _hparam_domain_discrete =None_, _run_name =None_, _global_step =None_)[source][source]
    
Add a set of hyperparameters to be compared in TensorBoard. 

Parameters
    
  * **hparam_dict** (_dict_) – Each key-value pair in the dictionary is the name of the hyper parameter and it’s corresponding value. The type of the value can be one of bool, string, float, int, or None.
  * **metric_dict** (_dict_) – Each key-value pair in the dictionary is the name of the metric and it’s corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by `add_scalar` will be displayed in hparam plugin. In most cases, this is unwanted.
  * **hparam_domain_discrete** – (Optional[Dict[str, List[Any]]]) A dictionary that contains names of the hyperparameters and all discrete values they can hold
  * **run_name** (_str_) – Name of the run, to be included as part of the logdir. If unspecified, will use current timestamp.
  * **global_step** (_int_) – Global step value to record


Examples:
```
from torch.utils.tensorboard import SummaryWriter
with SummaryWriter() as w:
  for i in range(5):
    w.add_hparams({'lr': 0.1*i, 'bsize': i},
           {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})

```
Copy to clipboard
Expected result:
![_images/add_hparam.png](https://pytorch.org/docs/stable/_images/add_hparam.png) 

flush()[source][source]
    
Flushes the event file to disk.
Call this method to make sure that all pending events have been written to disk. 

close()[source][source]

Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils.tensorboard
    * `SummaryWriter`
      * `SummaryWriter.__init__()`
      * `SummaryWriter.add_scalar()`
      * `SummaryWriter.add_scalars()`
      * `SummaryWriter.add_histogram()`
      * `SummaryWriter.add_image()`
      * `SummaryWriter.add_images()`
      * `SummaryWriter.add_figure()`
      * `SummaryWriter.add_video()`
      * `SummaryWriter.add_audio()`
      * `SummaryWriter.add_text()`
      * `SummaryWriter.add_graph()`
      * `SummaryWriter.add_embedding()`
      * `SummaryWriter.add_pr_curve()`
      * `SummaryWriter.add_custom_scalars()`
      * `SummaryWriter.add_mesh()`
      * `SummaryWriter.add_hparams()`
      * `SummaryWriter.flush()`
      * `SummaryWriter.close()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch
The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.
It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0.
## Tensors
`is_tensor` | Returns True if obj is a PyTorch tensor.  
---|---  
`is_storage` | Returns True if obj is a PyTorch storage object.  
`is_complex` | Returns True if the data type of `input` is a complex data type i.e., one of `torch.complex64`, and `torch.complex128`.  
`is_conj` | Returns True if the `input` is a conjugated tensor, i.e. its conjugate bit is set to True.  
`is_floating_point` | Returns True if the data type of `input` is a floating point data type i.e., one of `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`.  
`is_nonzero` | Returns True if the `input` is a single element tensor which is not equal to zero after type conversions.  
`set_default_dtype` | Sets the default floating point dtype to `d`.  
`get_default_dtype` | Get the current default floating point `torch.dtype`.  
`set_default_device` | Sets the default `torch.Tensor` to be allocated on `device`.  
`get_default_device` | Gets the default `torch.Tensor` to be allocated on `device`  
`set_default_tensor_type` |   
`numel` | Returns the total number of elements in the `input` tensor.  
`set_printoptions` | Set options for printing.  
`set_flush_denormal` | Disables denormal floating numbers on CPU.  
### Creation Ops
Note
Random sampling creation ops are listed under Random sampling and include: `torch.rand()` `torch.rand_like()` `torch.randn()` `torch.randn_like()` `torch.randint()` `torch.randint_like()` `torch.randperm()` You may also use `torch.empty()` with the In-place random sampling methods to create `torch.Tensor` s with values sampled from a broader range of distributions.
`tensor` | Constructs a tensor with no autograd history (also known as a "leaf tensor", see Autograd mechanics) by copying `data`.  
---|---  
`sparse_coo_tensor` | Constructs a sparse tensor in COO(rdinate) format with specified values at the given `indices`.  
`sparse_csr_tensor` | Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given `crow_indices` and `col_indices`.  
`sparse_csc_tensor` | Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given `ccol_indices` and `row_indices`.  
`sparse_bsr_tensor` | Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given `crow_indices` and `col_indices`.  
`sparse_bsc_tensor` | Constructs a sparse tensor in BSC (Block Compressed Sparse Column)) with specified 2-dimensional blocks at the given `ccol_indices` and `row_indices`.  
`asarray` | Converts `obj` to a tensor.  
`as_tensor` | Converts `data` into a tensor, sharing data and preserving autograd history if possible.  
`as_strided` | Create a view of an existing torch.Tensor `input` with specified `size`, `stride` and `storage_offset`.  
`from_file` | Creates a CPU tensor with a storage backed by a memory-mapped file.  
`from_numpy` | Creates a `Tensor` from a `numpy.ndarray`.  
`from_dlpack` | Converts a tensor from an external library into a `torch.Tensor`.  
`frombuffer` | Creates a 1-dimensional `Tensor` from an object that implements the Python buffer protocol.  
`zeros` | Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument `size`.  
`zeros_like` | Returns a tensor filled with the scalar value 0, with the same size as `input`.  
`ones` | Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument `size`.  
`ones_like` | Returns a tensor filled with the scalar value 1, with the same size as `input`.  
`arange` | Returns a 1-D tensor of size ⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉ with values from the interval `[start, end)` taken with common difference `step` beginning from start.  
`range` | Returns a 1-D tensor of size ⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1 with values from `start` to `end` with step `step`.  
`linspace` | Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end`, inclusive.  
`logspace` | Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart{{\text{{base}}}}^{{\text{{start}}}}basestart to baseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with base `base`.  
`eye` | Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.  
`empty` | Returns a tensor filled with uninitialized data.  
`empty_like` | Returns an uninitialized tensor with the same size as `input`.  
`empty_strided` | Creates a tensor with the specified `size` and `stride` and filled with undefined data.  
`full` | Creates a tensor of size `size` filled with `fill_value`.  
`full_like` | Returns a tensor with the same size as `input` filled with `fill_value`.  
`quantize_per_tensor` | Converts a float tensor to a quantized tensor with given scale and zero point.  
`quantize_per_channel` | Converts a float tensor to a per-channel quantized tensor with given scales and zero points.  
`dequantize` | Returns an fp32 Tensor by dequantizing a quantized Tensor  
`complex` | Constructs a complex tensor with its real part equal to `real` and its imaginary part equal to `imag`.  
`polar` | Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value `abs` and angle `angle`.  
`heaviside` | Computes the Heaviside step function for each element in `input`.  
### Indexing, Slicing, Joining, Mutating Ops
`adjoint` | Returns a view of the tensor conjugated and with the last two dimensions transposed.  
---|---  
`argwhere` | Returns a tensor containing the indices of all non-zero elements of `input`.  
`cat` | Concatenates the given sequence of tensors in `tensors` in the given dimension.  
`concat` | Alias of `torch.cat()`.  
`concatenate` | Alias of `torch.cat()`.  
`conj` | Returns a view of `input` with a flipped conjugate bit.  
`chunk` | Attempts to split a tensor into the specified number of chunks.  
`dsplit` | Splits `input`, a tensor with three or more dimensions, into multiple tensors depthwise according to `indices_or_sections`.  
`column_stack` | Creates a new tensor by horizontally stacking the tensors in `tensors`.  
`dstack` | Stack tensors in sequence depthwise (along third axis).  
`gather` | Gathers values along an axis specified by dim.  
`hsplit` | Splits `input`, a tensor with one or more dimensions, into multiple tensors horizontally according to `indices_or_sections`.  
`hstack` | Stack tensors in sequence horizontally (column wise).  
`index_add` | See `index_add_()` for function description.  
`index_copy` | See `index_add_()` for function description.  
`index_reduce` | See `index_reduce_()` for function description.  
`index_select` | Returns a new tensor which indexes the `input` tensor along dimension `dim` using the entries in `index` which is a LongTensor.  
`masked_select` | Returns a new 1-D tensor which indexes the `input` tensor according to the boolean mask `mask` which is a BoolTensor.  
`movedim` | Moves the dimension(s) of `input` at the position(s) in `source` to the position(s) in `destination`.  
`moveaxis` | Alias for `torch.movedim()`.  
`narrow` | Returns a new tensor that is a narrowed version of `input` tensor.  
`narrow_copy` | Same as `Tensor.narrow()` except this returns a copy rather than shared storage.  
`nonzero` |   
`permute` | Returns a view of the original tensor `input` with its dimensions permuted.  
`reshape` | Returns a tensor with the same data and number of elements as `input`, but with the specified shape.  
`row_stack` | Alias of `torch.vstack()`.  
`select` | Slices the `input` tensor along the selected dimension at the given index.  
`scatter` | Out-of-place version of `torch.Tensor.scatter_()`  
`diagonal_scatter` | Embeds the values of the `src` tensor into `input` along the diagonal elements of `input`, with respect to `dim1` and `dim2`.  
`select_scatter` | Embeds the values of the `src` tensor into `input` at the given index.  
`slice_scatter` | Embeds the values of the `src` tensor into `input` at the given dimension.  
`scatter_add` | Out-of-place version of `torch.Tensor.scatter_add_()`  
`scatter_reduce` | Out-of-place version of `torch.Tensor.scatter_reduce_()`  
`split` | Splits the tensor into chunks.  
`squeeze` | Returns a tensor with all specified dimensions of `input` of size 1 removed.  
`stack` | Concatenates a sequence of tensors along a new dimension.  
`swapaxes` | Alias for `torch.transpose()`.  
`swapdims` | Alias for `torch.transpose()`.  
`t` | Expects `input` to be <= 2-D tensor and transposes dimensions 0 and 1.  
`take` | Returns a new tensor with the elements of `input` at the given indices.  
`take_along_dim` | Selects values from `input` at the 1-dimensional indices from `indices` along the given `dim`.  
`tensor_split` | Splits a tensor into multiple sub-tensors, all of which are views of `input`, along dimension `dim` according to the indices or number of sections specified by `indices_or_sections`.  
`tile` | Constructs a tensor by repeating the elements of `input`.  
`transpose` | Returns a tensor that is a transposed version of `input`.  
`unbind` | Removes a tensor dimension.  
`unravel_index` | Converts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape.  
`unsqueeze` | Returns a new tensor with a dimension of size one inserted at the specified position.  
`vsplit` | Splits `input`, a tensor with two or more dimensions, into multiple tensors vertically according to `indices_or_sections`.  
`vstack` | Stack tensors in sequence vertically (row wise).  
`where` | Return a tensor of elements selected from either `input` or `other`, depending on `condition`.  
## Accelerators
Within the PyTorch repo, we define an “Accelerator” as a `torch.device` that is being used alongside a CPU to speed up computation. These device use an asynchronous execution scheme, using `torch.Stream` and `torch.Event` as their main way to perform synchronization. We also assume that only one such accelerator can be available at once on a given host. This allows us to use the current accelerator as the default device for relevant concepts such as pinned memory, Stream device_type, FSDP, etc.
As of today, accelerator devices are (in no particular order) “CUDA”, “MTIA”, “XPU”, “MPS”, “HPU”, and PrivateUse1 (many device not in the PyTorch repo itself).
Many tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading or intra-op parallelism), it is thus important to delay as much as possible any operation that would prevent further forks. This is especially important here as most accelerator’s initialization has such effect. In practice, you should keep in mind that checking `torch.accelerator.current_accelerator()` is a compile-time check by default, it is thus always fork-safe. On the contrary, passing the `check_available=True` flag to this function or calling `torch.accelerator.is_available()` will usually prevent later fork.
Some backends provide an experimental opt-in option to make the runtime availability check fork-safe. When using the CUDA device `PYTORCH_NVML_BASED_CUDA_CHECK=1` can be used for example.
`Stream` | An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.  
---|---  
`Event` | Query and record Stream status to identify or control dependencies across Stream and measure timing.  
## Generators
`Generator` | Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.  
---|---  
## Random sampling
`seed` | Sets the seed for generating random numbers to a non-deterministic random number on all devices.  
---|---  
`manual_seed` | Sets the seed for generating random numbers on all devices.  
`initial_seed` | Returns the initial seed for generating random numbers as a Python long.  
`get_rng_state` | Returns the random number generator state as a torch.ByteTensor.  
`set_rng_state` | Sets the random number generator state. 

torch.default_generator _Returns the default CPU torch.Generator_
  
`bernoulli` | Draws binary random numbers (0 or 1) from a Bernoulli distribution.  
---|---  
`multinomial` | Returns a tensor where each row contains `num_samples` indices sampled from the multinomial (a stricter definition would be multivariate, refer to `torch.distributions.multinomial.Multinomial` for more details) probability distribution located in the corresponding row of tensor `input`.  
`normal` | Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.  
`poisson` | Returns a tensor of the same size as `input` with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in `input` i.e.,  
`rand` | Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)  
`rand_like` | Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).  
`randint` | Returns a tensor filled with random integers generated uniformly between `low` (inclusive) and `high` (exclusive).  
`randint_like` | Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` (inclusive) and `high` (exclusive).  
`randn` | Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).  
`randn_like` | Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1.  
`randperm` | Returns a random permutation of integers from `0` to `n - 1`.  
### In-place random sampling
There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:
  * `torch.Tensor.bernoulli_()` - in-place version of `torch.bernoulli()`
  * `torch.Tensor.cauchy_()` - numbers drawn from the Cauchy distribution
  * `torch.Tensor.exponential_()` - numbers drawn from the exponential distribution
  * `torch.Tensor.geometric_()` - elements drawn from the geometric distribution
  * `torch.Tensor.log_normal_()` - samples from the log-normal distribution
  * `torch.Tensor.normal_()` - in-place version of `torch.normal()`
  * `torch.Tensor.random_()` - numbers sampled from the discrete uniform distribution
  * `torch.Tensor.uniform_()` - numbers sampled from the continuous uniform distribution


### Quasi-random sampling
`quasirandom.SobolEngine` | The `torch.quasirandom.SobolEngine` is an engine for generating (scrambled) Sobol sequences.  
---|---  
## Serialization
`save` | Saves an object to a disk file.  
---|---  
`load` | Loads an object saved with `torch.save()` from a file.  
## Parallelism
`get_num_threads` | Returns the number of threads used for parallelizing CPU operations  
---|---  
`set_num_threads` | Sets the number of threads used for intraop parallelism on CPU.  
`get_num_interop_threads` | Returns the number of threads used for inter-op parallelism on CPU (e.g.  
`set_num_interop_threads` | Sets the number of threads used for interop parallelism (e.g.  
## Locally disabling gradient computation
The context managers `torch.no_grad()`, `torch.enable_grad()`, and `torch.set_grad_enabled()` are helpful for locally disabling and enabling gradient computation. See Locally disabling gradient computation for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the `threading` module, etc.
Examples:
```
>>> x = torch.zeros(1, requires_grad=True)
>>> with torch.no_grad():
...   y = x * 2
>>> y.requires_grad
False
>>> is_train = False
>>> with torch.set_grad_enabled(is_train):
...   y = x * 2
>>> y.requires_grad
False
>>> torch.set_grad_enabled(True) # this can also be used as a function
>>> y = x * 2
>>> y.requires_grad
True
>>> torch.set_grad_enabled(False)
>>> y = x * 2
>>> y.requires_grad
False

```
Copy to clipboard
`no_grad` | Context-manager that disables gradient calculation.  
---|---  
`enable_grad` | Context-manager that enables gradient calculation.  
`autograd.grad_mode.set_grad_enabled` | Context-manager that sets gradient calculation on or off.  
`is_grad_enabled` | Returns True if grad mode is currently enabled.  
`autograd.grad_mode.inference_mode` | Context-manager that enables or disables inference mode.  
`is_inference_mode_enabled` | Returns True if inference mode is currently enabled.  
## Math operations
### Constants
`inf` | A floating-point positive infinity. Alias for `math.inf`.  
---|---  
`nan` | A floating-point “not a number” value. This value is not a legal number. Alias for `math.nan`.  
### Pointwise Ops
`abs` | Computes the absolute value of each element in `input`.  
---|---  
`absolute` | Alias for `torch.abs()`  
`acos` | Computes the inverse cosine of each element in `input`.  
`arccos` | Alias for `torch.acos()`.  
`acosh` | Returns a new tensor with the inverse hyperbolic cosine of the elements of `input`.  
`arccosh` | Alias for `torch.acosh()`.  
`add` | Adds `other`, scaled by `alpha`, to `input`.  
`addcdiv` | Performs the element-wise division of `tensor1` by `tensor2`, multiplies the result by the scalar `value` and adds it to `input`.  
`addcmul` | Performs the element-wise multiplication of `tensor1` by `tensor2`, multiplies the result by the scalar `value` and adds it to `input`.  
`angle` | Computes the element-wise angle (in radians) of the given `input` tensor.  
`asin` | Returns a new tensor with the arcsine of the elements of `input`.  
`arcsin` | Alias for `torch.asin()`.  
`asinh` | Returns a new tensor with the inverse hyperbolic sine of the elements of `input`.  
`arcsinh` | Alias for `torch.asinh()`.  
`atan` | Returns a new tensor with the arctangent of the elements of `input`.  
`arctan` | Alias for `torch.atan()`.  
`atanh` | Returns a new tensor with the inverse hyperbolic tangent of the elements of `input`.  
`arctanh` | Alias for `torch.atanh()`.  
`atan2` | Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​ with consideration of the quadrant.  
`arctan2` | Alias for `torch.atan2()`.  
`bitwise_not` | Computes the bitwise NOT of the given input tensor.  
`bitwise_and` | Computes the bitwise AND of `input` and `other`.  
`bitwise_or` | Computes the bitwise OR of `input` and `other`.  
`bitwise_xor` | Computes the bitwise XOR of `input` and `other`.  
`bitwise_left_shift` | Computes the left arithmetic shift of `input` by `other` bits.  
`bitwise_right_shift` | Computes the right arithmetic shift of `input` by `other` bits.  
`ceil` | Returns a new tensor with the ceil of the elements of `input`, the smallest integer greater than or equal to each element.  
`clamp` | Clamps all elements in `input` into the range [ `min`, `max` ].  
`clip` | Alias for `torch.clamp()`.  
`conj_physical` | Computes the element-wise conjugate of the given `input` tensor.  
`copysign` | Create a new floating-point tensor with the magnitude of `input` and the sign of `other`, elementwise.  
`cos` | Returns a new tensor with the cosine of the elements of `input`.  
`cosh` | Returns a new tensor with the hyperbolic cosine of the elements of `input`.  
`deg2rad` | Returns a new tensor with each of the elements of `input` converted from angles in degrees to radians.  
`div` | Divides each element of the input `input` by the corresponding element of `other`.  
`divide` | Alias for `torch.div()`.  
`digamma` | Alias for `torch.special.digamma()`.  
`erf` | Alias for `torch.special.erf()`.  
`erfc` | Alias for `torch.special.erfc()`.  
`erfinv` | Alias for `torch.special.erfinv()`.  
`exp` | Returns a new tensor with the exponential of the elements of the input tensor `input`.  
`exp2` | Alias for `torch.special.exp2()`.  
`expm1` | Alias for `torch.special.expm1()`.  
`fake_quantize_per_channel_affine` | Returns a new tensor with the data in `input` fake quantized per channel using `scale`, `zero_point`, `quant_min` and `quant_max`, across the channel specified by `axis`.  
`fake_quantize_per_tensor_affine` | Returns a new tensor with the data in `input` fake quantized using `scale`, `zero_point`, `quant_min` and `quant_max`.  
`fix` | Alias for `torch.trunc()`  
`float_power` | Raises `input` to the power of `exponent`, elementwise, in double precision.  
`floor` | Returns a new tensor with the floor of the elements of `input`, the largest integer less than or equal to each element.  
`floor_divide` |   
`fmod` | Applies C++'s std::fmod entrywise.  
`frac` | Computes the fractional portion of each element in `input`.  
`frexp` | Decomposes `input` into mantissa and exponent tensors such that input=mantissa×2exponent\text{input} = \text{mantissa} \times 2^{\text{exponent}}input=mantissa×2exponent.  
`gradient` | Estimates the gradient of a function g:Rn→Rg : \mathbb{R}^n \rightarrow \mathbb{R}g:Rn→R in one or more dimensions using the second-order accurate central differences method and either first or second order estimates at the boundaries.  
`imag` | Returns a new tensor containing imaginary values of the `self` tensor.  
`ldexp` | Multiplies `input` by 2 ** `other`.  
`lerp` | Does a linear interpolation of two tensors `start` (given by `input`) and `end` based on a scalar or tensor `weight` and returns the resulting `out` tensor.  
`lgamma` | Computes the natural logarithm of the absolute value of the gamma function on `input`.  
`log` | Returns a new tensor with the natural logarithm of the elements of `input`.  
`log10` | Returns a new tensor with the logarithm to the base 10 of the elements of `input`.  
`log1p` | Returns a new tensor with the natural logarithm of (1 + `input`).  
`log2` | Returns a new tensor with the logarithm to the base 2 of the elements of `input`.  
`logaddexp` | Logarithm of the sum of exponentiations of the inputs.  
`logaddexp2` | Logarithm of the sum of exponentiations of the inputs in base-2.  
`logical_and` | Computes the element-wise logical AND of the given input tensors.  
`logical_not` | Computes the element-wise logical NOT of the given input tensor.  
`logical_or` | Computes the element-wise logical OR of the given input tensors.  
`logical_xor` | Computes the element-wise logical XOR of the given input tensors.  
`logit` | Alias for `torch.special.logit()`.  
`hypot` | Given the legs of a right triangle, return its hypotenuse.  
`i0` | Alias for `torch.special.i0()`.  
`igamma` | Alias for `torch.special.gammainc()`.  
`igammac` | Alias for `torch.special.gammaincc()`.  
`mul` | Multiplies `input` by `other`.  
`multiply` | Alias for `torch.mul()`.  
`mvlgamma` | Alias for `torch.special.multigammaln()`.  
`nan_to_num` | Replaces `NaN`, positive infinity, and negative infinity values in `input` with the values specified by `nan`, `posinf`, and `neginf`, respectively.  
`neg` | Returns a new tensor with the negative of the elements of `input`.  
`negative` | Alias for `torch.neg()`  
`nextafter` | Return the next floating-point value after `input` towards `other`, elementwise.  
`polygamma` | Alias for `torch.special.polygamma()`.  
`positive` | Returns `input`.  
`pow` | Takes the power of each element in `input` with `exponent` and returns a tensor with the result.  
`quantized_batch_norm` | Applies batch normalization on a 4D (NCHW) quantized tensor.  
`quantized_max_pool1d` | Applies a 1D max pooling over an input quantized tensor composed of several input planes.  
`quantized_max_pool2d` | Applies a 2D max pooling over an input quantized tensor composed of several input planes.  
`rad2deg` | Returns a new tensor with each of the elements of `input` converted from angles in radians to degrees.  
`real` | Returns a new tensor containing real values of the `self` tensor.  
`reciprocal` | Returns a new tensor with the reciprocal of the elements of `input`  
`remainder` | Computes Python's modulus operation entrywise.  
`round` | Rounds elements of `input` to the nearest integer.  
`rsqrt` | Returns a new tensor with the reciprocal of the square-root of each of the elements of `input`.  
`sigmoid` | Alias for `torch.special.expit()`.  
`sign` | Returns a new tensor with the signs of the elements of `input`.  
`sgn` | This function is an extension of torch.sign() to complex tensors.  
`signbit` | Tests if each element of `input` has its sign bit set or not.  
`sin` | Returns a new tensor with the sine of the elements of `input`.  
`sinc` | Alias for `torch.special.sinc()`.  
`sinh` | Returns a new tensor with the hyperbolic sine of the elements of `input`.  
`softmax` | Alias for `torch.nn.functional.softmax()`.  
`sqrt` | Returns a new tensor with the square-root of the elements of `input`.  
`square` | Returns a new tensor with the square of the elements of `input`.  
`sub` | Subtracts `other`, scaled by `alpha`, from `input`.  
`subtract` | Alias for `torch.sub()`.  
`tan` | Returns a new tensor with the tangent of the elements of `input`.  
`tanh` | Returns a new tensor with the hyperbolic tangent of the elements of `input`.  
`true_divide` | Alias for `torch.div()` with `rounding_mode=None`.  
`trunc` | Returns a new tensor with the truncated integer values of the elements of `input`.  
`xlogy` | Alias for `torch.special.xlogy()`.  
### Reduction Ops
`argmax` | Returns the indices of the maximum value of all elements in the `input` tensor.  
---|---  
`argmin` | Returns the indices of the minimum value(s) of the flattened tensor or along a dimension  
`amax` | Returns the maximum value of each slice of the `input` tensor in the given dimension(s) `dim`.  
`amin` | Returns the minimum value of each slice of the `input` tensor in the given dimension(s) `dim`.  
`aminmax` | Computes the minimum and maximum values of the `input` tensor.  
`all` | Tests if all elements in `input` evaluate to True.  
`any` | Tests if any element in `input` evaluates to True.  
`max` | Returns the maximum value of all elements in the `input` tensor.  
`min` | Returns the minimum value of all elements in the `input` tensor.  
`dist` | Returns the p-norm of (`input` - `other`)  
`logsumexp` | Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim`.  
`mean` |   
`nanmean` | Computes the mean of all non-NaN elements along the specified dimensions.  
`median` | Returns the median of the values in `input`.  
`nanmedian` | Returns the median of the values in `input`, ignoring `NaN` values.  
`mode` | Returns a namedtuple `(values, indices)` where `values` is the mode value of each row of the `input` tensor in the given dimension `dim`, i.e. a value which appears most often in that row, and `indices` is the index location of each mode value found.  
`norm` | Returns the matrix norm or vector norm of a given tensor.  
`nansum` | Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.  
`prod` | Returns the product of all elements in the `input` tensor.  
`quantile` | Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim`.  
`nanquantile` | This is a variant of `torch.quantile()` that "ignores" `NaN` values, computing the quantiles `q` as if `NaN` values in `input` did not exist.  
`std` | Calculates the standard deviation over the dimensions specified by `dim`.  
`std_mean` | Calculates the standard deviation and mean over the dimensions specified by `dim`.  
`sum` | Returns the sum of all elements in the `input` tensor.  
`unique` | Returns the unique elements of the input tensor.  
`unique_consecutive` | Eliminates all but the first element from every consecutive group of equivalent elements.  
`var` | Calculates the variance over the dimensions specified by `dim`.  
`var_mean` | Calculates the variance and mean over the dimensions specified by `dim`.  
`count_nonzero` | Counts the number of non-zero values in the tensor `input` along the given `dim`.  
### Comparison Ops
`allclose` | This function checks if `input` and `other` satisfy the condition:  
---|---  
`argsort` | Returns the indices that sort a tensor along a given dimension in ascending order by value.  
`eq` | Computes element-wise equality  
`equal` | `True` if two tensors have the same size and elements, `False` otherwise.  
`ge` | Computes input≥other\text{input} \geq \text{other}input≥other element-wise.  
`greater_equal` | Alias for `torch.ge()`.  
`gt` | Computes input>other\text{input} > \text{other}input>other element-wise.  
`greater` | Alias for `torch.gt()`.  
`isclose` | Returns a new tensor with boolean elements representing if each element of `input` is "close" to the corresponding element of `other`.  
`isfinite` | Returns a new tensor with boolean elements representing if each element is finite or not.  
`isin` | Tests if each element of `elements` is in `test_elements`.  
`isinf` | Tests if each element of `input` is infinite (positive or negative infinity) or not.  
`isposinf` | Tests if each element of `input` is positive infinity or not.  
`isneginf` | Tests if each element of `input` is negative infinity or not.  
`isnan` | Returns a new tensor with boolean elements representing if each element of `input` is NaN or not.  
`isreal` | Returns a new tensor with boolean elements representing if each element of `input` is real-valued or not.  
`kthvalue` | Returns a namedtuple `(values, indices)` where `values` is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim`.  
`le` | Computes input≤other\text{input} \leq \text{other}input≤other element-wise.  
`less_equal` | Alias for `torch.le()`.  
`lt` | Computes input<other\text{input} < \text{other}input<other element-wise.  
`less` | Alias for `torch.lt()`.  
`maximum` | Computes the element-wise maximum of `input` and `other`.  
`minimum` | Computes the element-wise minimum of `input` and `other`.  
`fmax` | Computes the element-wise maximum of `input` and `other`.  
`fmin` | Computes the element-wise minimum of `input` and `other`.  
`ne` | Computes input≠other\text{input} \neq \text{other}input=other element-wise.  
`not_equal` | Alias for `torch.ne()`.  
`sort` | Sorts the elements of the `input` tensor along a given dimension in ascending order by value.  
`topk` | Returns the `k` largest elements of the given `input` tensor along a given dimension.  
`msort` | Sorts the elements of the `input` tensor along its first dimension in ascending order by value.  
### Spectral Ops
`stft` | Short-time Fourier transform (STFT).  
---|---  
`istft` | Inverse short time Fourier Transform.  
`bartlett_window` | Bartlett window function.  
`blackman_window` | Blackman window function.  
`hamming_window` | Hamming window function.  
`hann_window` | Hann window function.  
`kaiser_window` | Computes the Kaiser window with window length `window_length` and shape parameter `beta`.  
### Other Operations
`atleast_1d` | Returns a 1-dimensional view of each input tensor with zero dimensions.  
---|---  
`atleast_2d` | Returns a 2-dimensional view of each input tensor with zero dimensions.  
`atleast_3d` | Returns a 3-dimensional view of each input tensor with zero dimensions.  
`bincount` | Count the frequency of each value in an array of non-negative ints.  
`block_diag` | Create a block diagonal matrix from provided tensors.  
`broadcast_tensors` | Broadcasts the given tensors according to Broadcasting semantics.  
`broadcast_to` | Broadcasts `input` to the shape `shape`.  
`broadcast_shapes` | Similar to `broadcast_tensors()` but for shapes.  
`bucketize` | Returns the indices of the buckets to which each value in the `input` belongs, where the boundaries of the buckets are set by `boundaries`.  
`cartesian_prod` | Do cartesian product of the given sequence of tensors.  
`cdist` | Computes batched the p-norm distance between each pair of the two collections of row vectors.  
`clone` | Returns a copy of `input`.  
`combinations` | Compute combinations of length rrr of the given tensor.  
`corrcoef` | Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the `input` matrix, where rows are the variables and columns are the observations.  
`cov` | Estimates the covariance matrix of the variables given by the `input` matrix, where rows are the variables and columns are the observations.  
`cross` | Returns the cross product of vectors in dimension `dim` of `input` and `other`.  
`cummax` | Returns a namedtuple `(values, indices)` where `values` is the cumulative maximum of elements of `input` in the dimension `dim`.  
`cummin` | Returns a namedtuple `(values, indices)` where `values` is the cumulative minimum of elements of `input` in the dimension `dim`.  
`cumprod` | Returns the cumulative product of elements of `input` in the dimension `dim`.  
`cumsum` | Returns the cumulative sum of elements of `input` in the dimension `dim`.  
`diag` | 
  * If `input` is a vector (1-D tensor), then returns a 2-D square tensor

  
`diag_embed` | Creates a tensor whose diagonals of certain 2D planes (specified by `dim1` and `dim2`) are filled by `input`.  
`diagflat` | 
  * If `input` is a vector (1-D tensor), then returns a 2-D square tensor

  
`diagonal` | Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape.  
`diff` | Computes the n-th forward difference along the given dimension.  
`einsum` | Sums the product of the elements of the input `operands` along dimensions specified using a notation based on the Einstein summation convention.  
`flatten` | Flattens `input` by reshaping it into a one-dimensional tensor.  
`flip` | Reverse the order of an n-D tensor along given axis in dims.  
`fliplr` | Flip tensor in the left/right direction, returning a new tensor.  
`flipud` | Flip tensor in the up/down direction, returning a new tensor.  
`kron` | Computes the Kronecker product, denoted by ⊗\otimes⊗, of `input` and `other`.  
`rot90` | Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.  
`gcd` | Computes the element-wise greatest common divisor (GCD) of `input` and `other`.  
`histc` | Computes the histogram of a tensor.  
`histogram` | Computes a histogram of the values in a tensor.  
`histogramdd` | Computes a multi-dimensional histogram of the values in a tensor.  
`meshgrid` | Creates grids of coordinates specified by the 1D inputs in attr:tensors.  
`lcm` | Computes the element-wise least common multiple (LCM) of `input` and `other`.  
`logcumsumexp` | Returns the logarithm of the cumulative summation of the exponentiation of elements of `input` in the dimension `dim`.  
`ravel` | Return a contiguous flattened tensor.  
`renorm` | Returns a tensor where each sub-tensor of `input` along dimension `dim` is normalized such that the p-norm of the sub-tensor is lower than the value `maxnorm`  
`repeat_interleave` | Repeat elements of a tensor.  
`roll` | Roll the tensor `input` along the given dimension(s).  
`searchsorted` | Find the indices from the _innermost_ dimension of `sorted_sequence` such that, if the corresponding values in `values` were inserted before the indices, when sorted, the order of the corresponding _innermost_ dimension within `sorted_sequence` would be preserved.  
`tensordot` | Returns a contraction of a and b over multiple dimensions.  
`trace` | Returns the sum of the elements of the diagonal of the input 2-D matrix.  
`tril` | Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices `input`, the other elements of the result tensor `out` are set to 0.  
`tril_indices` | Returns the indices of the lower triangular part of a `row`-by- `col` matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  
`triu` | Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices `input`, the other elements of the result tensor `out` are set to 0.  
`triu_indices` | Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  
`unflatten` | Expands a dimension of the input tensor over multiple dimensions.  
`vander` | Generates a Vandermonde matrix.  
`view_as_real` | Returns a view of `input` as a real tensor.  
`view_as_complex` | Returns a view of `input` as a complex tensor.  
`resolve_conj` | Returns a new tensor with materialized conjugation if `input`'s conjugate bit is set to True, else returns `input`.  
`resolve_neg` | Returns a new tensor with materialized negation if `input`'s negative bit is set to True, else returns `input`.  
### BLAS and LAPACK Operations
`addbmm` | Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2`, with a reduced add step (all matrix multiplications get accumulated along the first dimension).  
---|---  
`addmm` | Performs a matrix multiplication of the matrices `mat1` and `mat2`.  
`addmv` | Performs a matrix-vector product of the matrix `mat` and the vector `vec`.  
`addr` | Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input`.  
`baddbmm` | Performs a batch matrix-matrix product of matrices in `batch1` and `batch2`.  
`bmm` | Performs a batch matrix-matrix product of matrices stored in `input` and `mat2`.  
`chain_matmul` | Returns the matrix product of the NNN 2-D tensors.  
`cholesky` | Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.  
`cholesky_inverse` | Computes the inverse of a complex Hermitian or real symmetric positive-definite matrix given its Cholesky decomposition.  
`cholesky_solve` | Computes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition.  
`dot` | Computes the dot product of two 1D tensors.  
`geqrf` | This is a low-level function for calling LAPACK's geqrf directly.  
`ger` | Alias of `torch.outer()`.  
`inner` | Computes the dot product for 1D tensors.  
`inverse` | Alias for `torch.linalg.inv()`  
`det` | Alias for `torch.linalg.det()`  
`logdet` | Calculates log determinant of a square matrix or batches of square matrices.  
`slogdet` | Alias for `torch.linalg.slogdet()`  
`lu` | Computes the LU factorization of a matrix or batches of matrices `A`.  
`lu_solve` | Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from `lu_factor()`.  
`lu_unpack` | Unpacks the LU decomposition returned by `lu_factor()` into the P, L, U matrices.  
`matmul` | Matrix product of two tensors.  
`matrix_power` | Alias for `torch.linalg.matrix_power()`  
`matrix_exp` | Alias for `torch.linalg.matrix_exp()`.  
`mm` | Performs a matrix multiplication of the matrices `input` and `mat2`.  
`mv` | Performs a matrix-vector product of the matrix `input` and the vector `vec`.  
`orgqr` | Alias for `torch.linalg.householder_product()`.  
`ormqr` | Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.  
`outer` | Outer product of `input` and `vec2`.  
`pinverse` | Alias for `torch.linalg.pinv()`  
`qr` | Computes the QR decomposition of a matrix or a batch of matrices `input`, and returns a namedtuple (Q, R) of tensors such that input=QR\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.  
`svd` | Computes the singular value decomposition of either a matrix or batch of matrices `input`.  
`svd_lowrank` | Return the singular value decomposition `(U, S, V)` of a matrix, batches of matrices, or a sparse matrix AAA such that A≈Udiag⁡(S)VHA \approx U \operatorname{diag}(S) V^{\text{H}}A≈Udiag(S)VH.  
`pca_lowrank` | Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.  
`lobpcg` | Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.  
`trapz` | Alias for `torch.trapezoid()`.  
`trapezoid` | Computes the trapezoidal rule along `dim`.  
`cumulative_trapezoid` | Cumulatively computes the trapezoidal rule along `dim`.  
`triangular_solve` | Solves a system of equations with a square upper or lower triangular invertible matrix AAA and multiple right-hand sides bbb.  
`vdot` | Computes the dot product of two 1D vectors along a dimension.  
### Foreach Operations
Warning
This API is in beta and subject to future changes. Forward-mode AD is not supported.
`_foreach_abs` | Apply `torch.abs()` to each Tensor of the input list.  
---|---  
`_foreach_abs_` | Apply `torch.abs()` to each Tensor of the input list.  
`_foreach_acos` | Apply `torch.acos()` to each Tensor of the input list.  
`_foreach_acos_` | Apply `torch.acos()` to each Tensor of the input list.  
`_foreach_asin` | Apply `torch.asin()` to each Tensor of the input list.  
`_foreach_asin_` | Apply `torch.asin()` to each Tensor of the input list.  
`_foreach_atan` | Apply `torch.atan()` to each Tensor of the input list.  
`_foreach_atan_` | Apply `torch.atan()` to each Tensor of the input list.  
`_foreach_ceil` | Apply `torch.ceil()` to each Tensor of the input list.  
`_foreach_ceil_` | Apply `torch.ceil()` to each Tensor of the input list.  
`_foreach_cos` | Apply `torch.cos()` to each Tensor of the input list.  
`_foreach_cos_` | Apply `torch.cos()` to each Tensor of the input list.  
`_foreach_cosh` | Apply `torch.cosh()` to each Tensor of the input list.  
`_foreach_cosh_` | Apply `torch.cosh()` to each Tensor of the input list.  
`_foreach_erf` | Apply `torch.erf()` to each Tensor of the input list.  
`_foreach_erf_` | Apply `torch.erf()` to each Tensor of the input list.  
`_foreach_erfc` | Apply `torch.erfc()` to each Tensor of the input list.  
`_foreach_erfc_` | Apply `torch.erfc()` to each Tensor of the input list.  
`_foreach_exp` | Apply `torch.exp()` to each Tensor of the input list.  
`_foreach_exp_` | Apply `torch.exp()` to each Tensor of the input list.  
`_foreach_expm1` | Apply `torch.expm1()` to each Tensor of the input list.  
`_foreach_expm1_` | Apply `torch.expm1()` to each Tensor of the input list.  
`_foreach_floor` | Apply `torch.floor()` to each Tensor of the input list.  
`_foreach_floor_` | Apply `torch.floor()` to each Tensor of the input list.  
`_foreach_log` | Apply `torch.log()` to each Tensor of the input list.  
`_foreach_log_` | Apply `torch.log()` to each Tensor of the input list.  
`_foreach_log10` | Apply `torch.log10()` to each Tensor of the input list.  
`_foreach_log10_` | Apply `torch.log10()` to each Tensor of the input list.  
`_foreach_log1p` | Apply `torch.log1p()` to each Tensor of the input list.  
`_foreach_log1p_` | Apply `torch.log1p()` to each Tensor of the input list.  
`_foreach_log2` | Apply `torch.log2()` to each Tensor of the input list.  
`_foreach_log2_` | Apply `torch.log2()` to each Tensor of the input list.  
`_foreach_neg` | Apply `torch.neg()` to each Tensor of the input list.  
`_foreach_neg_` | Apply `torch.neg()` to each Tensor of the input list.  
`_foreach_tan` | Apply `torch.tan()` to each Tensor of the input list.  
`_foreach_tan_` | Apply `torch.tan()` to each Tensor of the input list.  
`_foreach_sin` | Apply `torch.sin()` to each Tensor of the input list.  
`_foreach_sin_` | Apply `torch.sin()` to each Tensor of the input list.  
`_foreach_sinh` | Apply `torch.sinh()` to each Tensor of the input list.  
`_foreach_sinh_` | Apply `torch.sinh()` to each Tensor of the input list.  
`_foreach_round` | Apply `torch.round()` to each Tensor of the input list.  
`_foreach_round_` | Apply `torch.round()` to each Tensor of the input list.  
`_foreach_sqrt` | Apply `torch.sqrt()` to each Tensor of the input list.  
`_foreach_sqrt_` | Apply `torch.sqrt()` to each Tensor of the input list.  
`_foreach_lgamma` | Apply `torch.lgamma()` to each Tensor of the input list.  
`_foreach_lgamma_` | Apply `torch.lgamma()` to each Tensor of the input list.  
`_foreach_frac` | Apply `torch.frac()` to each Tensor of the input list.  
`_foreach_frac_` | Apply `torch.frac()` to each Tensor of the input list.  
`_foreach_reciprocal` | Apply `torch.reciprocal()` to each Tensor of the input list.  
`_foreach_reciprocal_` | Apply `torch.reciprocal()` to each Tensor of the input list.  
`_foreach_sigmoid` | Apply `torch.sigmoid()` to each Tensor of the input list.  
`_foreach_sigmoid_` | Apply `torch.sigmoid()` to each Tensor of the input list.  
`_foreach_trunc` | Apply `torch.trunc()` to each Tensor of the input list.  
`_foreach_trunc_` | Apply `torch.trunc()` to each Tensor of the input list.  
`_foreach_zero_` | Apply `torch.zero()` to each Tensor of the input list.  
## Utilities
`compiled_with_cxx11_abi` | Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1  
---|---  
`result_type` | Returns the `torch.dtype` that would result from performing an arithmetic operation on the provided input tensors.  
`can_cast` | Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.  
`promote_types` | Returns the `torch.dtype` with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.  
`use_deterministic_algorithms` | Sets whether PyTorch operations must use "deterministic" algorithms.  
`are_deterministic_algorithms_enabled` | Returns True if the global deterministic flag is turned on.  
`is_deterministic_algorithms_warn_only_enabled` | Returns True if the global deterministic flag is set to warn only.  
`set_deterministic_debug_mode` | Sets the debug mode for deterministic operations.  
`get_deterministic_debug_mode` | Returns the current value of the debug mode for deterministic operations.  
`set_float32_matmul_precision` | Sets the internal precision of float32 matrix multiplications.  
`get_float32_matmul_precision` | Returns the current value of float32 matrix multiplication precision.  
`set_warn_always` | When this flag is False (default) then some PyTorch warnings may only appear once per process.  
`get_device_module` | Returns the module associated with a given device(e.g., torch.device('cuda'), "mtia:0", "xpu", ...).  
`is_warn_always_enabled` | Returns True if the global warn_always flag is turned on.  
`vmap` | vmap is the vectorizing map; `vmap(func)` returns a new function that maps `func` over some dimension of the inputs.  
`_assert` | A wrapper around Python's assert which is symbolically traceable.  
## Symbolic Numbers 

_class_ torch.SymInt(_node_)[source][source]
    
Like an int (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow. 

as_integer_ratio()[source][source]
    
Represent this int as an exact integer ratio 

Return type
    
tuple[‘SymInt’, int] 

_class_ torch.SymFloat(_node_)[source][source]
    
Like an float (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow. 

as_integer_ratio()[source][source]
    
Represent this float as an exact integer ratio 

Return type
    
tuple[int, int] 

conjugate()[source][source]
    
Returns the complex conjugate of the float. 

Return type
    
_SymFloat_ 

hex()[source][source]
    
Returns the hexadecimal representation of the float. 

Return type
    
str 

is_integer()[source][source]
    
Return True if the float is an integer. 

_class_ torch.SymBool(_node_)[source][source]
    
Like an bool (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.
Unlike regular bools, regular boolean operators will force extra guards instead of symbolically evaluate. Use the bitwise operators instead to handle this.
`sym_float` | SymInt-aware utility for float casting.  
---|---  
`sym_fresh_size` |   
`sym_int` | SymInt-aware utility for int casting.  
`sym_max` | SymInt-aware utility for max which avoids branching on a < b.  
`sym_min` | SymInt-aware utility for min().  
`sym_not` | SymInt-aware utility for logical negation.  
`sym_ite` |   
`sym_sum` | N-ary add which is faster to compute for long lists than iterated binary addition.  
## Export Path
Warning
This feature is a prototype and may have compatibility breaking changes in the future.
export generated/exportdb/index
## Control Flow
Warning
This feature is a prototype and may have compatibility breaking changes in the future.
`cond` | Conditionally applies true_fn or false_fn.  
---|---  
## Optimizations
`compile` | Optimizes given model/function using TorchDynamo and specified backend.  
---|---  
torch.compile documentation
## Operator Tags 

_class_ torch.Tag
    
Members:
core
data_dependent_output
dynamic_output_shape
flexible_layout
generated
inplace_view
maybe_aliasing_or_mutating
needs_fixed_stride_order
nondeterministic_bitwise
nondeterministic_seeded
pointwise
pt2_compliant_tag
view_copy 

_property_ name

Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch
    * Tensors
      * Creation Ops
      * Indexing, Slicing, Joining, Mutating Ops
    * Accelerators
    * Generators
    * Random sampling
      * `torch.default_generator`
      * In-place random sampling
      * Quasi-random sampling
    * Serialization
    * Parallelism
    * Locally disabling gradient computation
    * Math operations
      * Constants
      * Pointwise Ops
      * Reduction Ops
      * Comparison Ops
      * Spectral Ops
      * Other Operations
      * BLAS and LAPACK Operations
      * Foreach Operations
    * Utilities
    * Symbolic Numbers
      * `SymInt`
        * `SymInt.as_integer_ratio()`
      * `SymFloat`
        * `SymFloat.as_integer_ratio()`
        * `SymFloat.conjugate()`
        * `SymFloat.hex()`
        * `SymFloat.is_integer()`
      * `SymBool`
    * Export Path
    * Control Flow
    * Optimizations
    * Operator Tags
      * `Tag`
        * `Tag.name`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.overrides
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.overrides
This module exposes various helper functions for the `__torch_function__` protocol. See Extending torch Python API for more details on the `__torch_function__` protocol.
## Functions 

torch.overrides.get_ignored_functions()[source][source]
    
Return public functions that cannot be overridden by `__torch_function__`. 

Returns
    
A tuple of functions that are publicly available in the torch API but cannot be overridden with `__torch_function__`. Mostly this is because none of the arguments of these functions are tensors or tensor-likes. 

Return type
    
set[Callable]
Examples
```
>>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()
True
>>> torch.add in torch.overrides.get_ignored_functions()
False

```
Copy to clipboard 

torch.overrides.get_overridable_functions()[source][source]
    
List functions that are overridable via __torch_function__ 

Returns
    
A dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden. 

Return type
    
Dict[Any, List[Callable]] 

torch.overrides.resolve_name(_f_)[source][source]
    
Get a human readable string name for a function passed to __torch_function__ 

Parameters
    
**f** (_Callable_) – Function to resolve the name of. 

Returns
    
Name of the function; if eval’ed it should give back the input function. 

Return type
    
str 

torch.overrides.get_testing_overrides()[source][source]
    
Return a dict containing dummy overrides for all overridable functions 

Returns
    
A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines `__torch_function__`. 

Return type
    
Dict[Callable, Callable]
Examples
```
>>> import inspect
>>> my_add = torch.overrides.get_testing_overrides()[torch.add]
>>> inspect.signature(my_add)
<Signature (input, other, out=None)>

```
Copy to clipboard 

torch.overrides.handle_torch_function(_public_api_ , _relevant_args_ , _* args_, _** kwargs_)[source][source]
    
Implement a function with checks for `__torch_function__` overrides.
See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation. 

Parameters
    
  * **public_api** (_function_) – Function exposed by the public torch API originally called like `public_api(*args, **kwargs)` on which arguments are now being checked.
  * **relevant_args** (_iterable_) – Iterable of arguments to check for __torch_function__ methods.
  * **args** (_tuple_) – Arbitrary positional arguments originally passed into `public_api`.
  * **kwargs** (_tuple_) – Arbitrary keyword arguments originally passed into `public_api`.



Returns
    
Result from calling `implementation` or an `__torch_function__` method, as appropriate. 

Return type
    
object
:raises TypeError : if no implementation is found.:
Example
```
>>> def func(a):
...   if has_torch_function_unary(a):
...     return handle_torch_function(func, (a,), a)
...   return a + 0

```
Copy to clipboard 

torch.overrides.has_torch_function()
    
Check for __torch_function__ implementations in the elements of an iterable or if a __torch_function__ mode is enabled. Considers exact `Tensor` s and `Parameter` s non-dispatchable. Use this to guard a call to `handle_torch_function()`; don’t use it to test if something is Tensor-like, use `is_tensor_like()` instead. :param relevant_args: Iterable or arguments to check for __torch_function__ methods. :type relevant_args: iterable 

Returns
    
True if any of the elements of relevant_args have __torch_function__ implementations, False otherwise. 

Return type
    
bool
See also 

`torch.is_tensor_like`
    
Checks if something is a Tensor-like, including an exact `Tensor`. 

torch.overrides.is_tensor_like(_inp_)[source][source]
    
Returns `True` if the passed-in input is a Tensor-like.
Currently, this occurs whenever there’s a `__torch_function__` attribute on the type of the input.
Examples
A subclass of tensor is generally a Tensor-like.
```
>>> class SubTensor(torch.Tensor): ...
>>> is_tensor_like(SubTensor([0]))
True

```
Copy to clipboard
Built-in or user types aren’t usually Tensor-like.
```
>>> is_tensor_like(6)
False
>>> is_tensor_like(None)
False
>>> class NotATensor: ...
>>> is_tensor_like(NotATensor())
False

```
Copy to clipboard
But, they can be made Tensor-like by implementing __torch_function__.
```
>>> class TensorLike:
...   @classmethod
...   def __torch_function__(cls, func, types, args, kwargs):
...     return -1
>>> is_tensor_like(TensorLike())
True

```
Copy to clipboard 

torch.overrides.is_tensor_method_or_property(_func_)[source][source]
    
Returns True if the function passed in is a handler for a method or property belonging to `torch.Tensor`, as passed into `__torch_function__`.
Note
For properties, their `__get__` method must be passed in.
This may be needed, in particular, for the following reasons:
  1. Methods/properties sometimes don’t contain a __module__ slot.
  2. They require that the first passed-in argument is an instance of `torch.Tensor`.


Examples
```
>>> is_tensor_method_or_property(torch.Tensor.add)
True
>>> is_tensor_method_or_property(torch.add)
False

```
Copy to clipboard 

Return type
    
bool 

torch.overrides.wrap_torch_function(_dispatcher_)[source][source]
    
Wraps a given function with `__torch_function__` -related functionality. 

Parameters
    
**dispatcher** (_Callable_) – A callable that returns an iterable of Tensor-likes passed into the function.
Note
This decorator may reduce the performance of your code. Generally, it’s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you’re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available.
Examples
```
>>> def dispatcher(a): # Must have the same signature as func
...   return (a,)
>>> @torch.overrides.wrap_torch_function(dispatcher)
>>> def func(a): # This will make func dispatchable by __torch_function__
...   return a + 0

```
Copy to clipboard
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.overrides
    * Functions
      * `get_ignored_functions()`
      * `get_overridable_functions()`
      * `resolve_name()`
      * `get_testing_overrides()`
      * `handle_torch_function()`
      * `has_torch_function()`
      * `is_tensor_like()`
      * `is_tensor_method_or_property()`
      * `wrap_torch_function()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Torch Environment Variables
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Torch Environment Variables
PyTorch leverages environment variables for adjusting various settings that influence its runtime behavior. These variables offer control over key functionalities, such as displaying the C++ stack trace upon encountering errors, synchronizing the execution of CUDA kernels, specifying the number of threads for parallel processing tasks and many more.
Moreover, PyTorch leverages several high-performance libraries, such as MKL and cuDNN, which also utilize environment variables to modify their functionality. This interplay of settings allows for a highly customizable development environment that can be optimized for efficiency, debugging, and computational resource management.
Please note that while this documentation covers a broad spectrum of environment variables relevant to PyTorch and its associated libraries, it is not exhaustive. If you find anything in this documentation that is missing, incorrect, or could be improved, please let us know by filing an issue or opening a pull request.
  * Threading Environment Variables
  * CUDA Environment Variables
  * MPS Environment Variables
  * Debugging Environment Variables
  * Miscellaneous Environment Variables
  * torch._logging
  * PYTORCH ProcessGroupNCCL Environment Variables


Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Torch Environment Variables


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Understanding CUDA Memory Usage
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Understanding CUDA Memory Usage
To debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory at any point in time, and optionally record the history of allocation events that led up to that snapshot.
The generated snapshots can then be drag and dropped onto the interactiver viewer hosted at pytorch.org/memory_viz which can be used to explore the snapshot.
# Generating a Snapshot
The common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot:
```
# enable memory history, which will
# add tracebacks and event history to snapshots
torch.cuda.memory._record_memory_history()
run_your_code()
torch.cuda.memory._dump_snapshot("my_snapshot.pickle")

```
Copy to clipboard
# Using the visualizer
Open pytorch.org/memory_viz and drag/drop the pickled snapshot file into the visualizer. The visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.
## Active Memory Timeline
The Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations. Mouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to render fewer allocations and improve performance when there is a lot of data.
![_images/active_memory_timeline.png](https://pytorch.org/docs/stable/_images/active_memory_timeline.png)
## Allocator State History
The Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the allocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations or free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occurred, such as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why an allocation failed even though reserved memory still exists.
![_images/allocator_state_history.png](https://pytorch.org/docs/stable/_images/allocator_state_history.png)
The stack trace information also reports the address at which an allocation occurred. The address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the “_0”th time this address was allocated. This unique string can be looked up in the Active Memory Timeline and searched in the Active State History to examine the memory state when a tensor was allocated or freed.
# Snapshot API Reference 

torch.cuda.memory._record_memory_history(_enabled ='all'_, _context ='all'_, _stacks ='all'_, _max_entries =9223372036854775807_, _device =None_)[source][source]
    
Enable recording of stack traces associated with memory allocations, so you can tell what allocated any piece of memory in `torch.cuda.memory._snapshot()`.
In addition too keeping stack traces with each current allocation and free, this will also enable recording of a history of all alloc/free events.
Use `torch.cuda.memory._snapshot()` to retrieve this information, and the tools in _memory_viz.py to visualize snapshots.
The Python trace collection is fast (2us per trace), so you may consider enabling this on production jobs if you anticipate ever having to debug memory issues.
C++ trace collection is also fast (~50ns/frame), which for many typical programs works out to ~2us per trace, but can vary depending on stack depth. 

Parameters
    
  * **enabled** (_Literal_ _[__None_ _,__"state"__,__"all"__]__,__optional_) – None, disable recording memory history. “state”, keep information for currenly allocated memory. “all”, additionally keep a history of all alloc/free calls. Defaults to “all”.
  * **context** (_Literal_ _[__None_ _,__"state"__,__"alloc"__,__"all"__]__,__optional_) – None, Do not record any tracebacks. “state”, Record tracebacks for currently allocated memory. “alloc”, additionally keep tracebacks for alloc calls. “all”, additionally keep tracebacks for free calls. Defaults to “all”.
  * **stacks** (_Literal_ _[__"python"__,__"all"__]__,__optional_) – “python”, include Python, TorchScript, and inductor frames in tracebacks “all”, additionally include C++ frames Defaults to “all”.
  * **max_entries** (_int_ _,__optional_) – Keep a maximum of max_entries alloc/free events in the recorded history recorded.



torch.cuda.memory._snapshot(_device =None_)[source][source]
    
Save a snapshot of CUDA memory state at the time it was called.
The state is represented as a dictionary with the following structure.
```
class Snapshot(TypedDict):
  segments : List[Segment]
  device_traces: List[List[TraceEntry]]
class Segment(TypedDict):
  # Segments are memory returned from a cudaMalloc call.
  # The size of reserved memory is the sum of all Segments.
  # Segments are cached and reused for future allocations.
  # If the reuse is smaller than the segment, the segment
  # is split into more then one Block.
  # empty_cache() frees Segments that are entirely inactive.
  address: int
  total_size: int # cudaMalloc'd size of segment
  stream: int
  segment_type: Literal['small', 'large'] # 'large' (>1MB)
  allocated_size: int # size of memory in use
  active_size: int # size of memory in use or in active_awaiting_free state
  blocks : List[Block]
class Block(TypedDict):
  # A piece of memory returned from the allocator, or
  # current cached but inactive.
  size: int
  requested_size: int # size requested during malloc, may be smaller than
            # size due to rounding
  address: int
  state: Literal['active_allocated', # used by a tensor
        'active_awaiting_free', # waiting for another stream to finish using
                    # this, then it will become free
        'inactive',] # free for reuse
  frames: List[Frame] # stack trace from where the allocation occurred
class Frame(TypedDict):
    filename: str
    line: int
    name: str
class TraceEntry(TypedDict):
  # When `torch.cuda.memory._record_memory_history()` is enabled,
  # the snapshot will contain TraceEntry objects that record each
  # action the allocator took.
  action: Literal[
  'alloc' # memory allocated
  'free_requested', # the allocated received a call to free memory
  'free_completed', # the memory that was requested to be freed is now
          # able to be used in future allocation calls
  'segment_alloc', # the caching allocator ask cudaMalloc for more memory
          # and added it as a segment in its cache
  'segment_free', # the caching allocator called cudaFree to return memory
          # to cuda possibly trying free up memory to
          # allocate more segments or because empty_caches was called
  'oom',     # the allocator threw an OOM exception. 'size' is
          # the requested number of bytes that did not succeed
  'snapshot'   # the allocator generated a memory snapshot
          # useful to coorelate a previously taken
          # snapshot with this trace
  ]
  addr: int # not present for OOM
  frames: List[Frame]
  size: int
  stream: int
  device_free: int # only present for OOM, the amount of
          # memory cuda still reports to be free

```
Copy to clipboard 

Returns
    
The Snapshot dictionary object 

torch.cuda.memory._dump_snapshot(_filename ='dump_snapshot.pickle'_)[source][source]
    
Save a pickled version of the torch.memory._snapshot() dictionary to a file.
This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz 

Parameters
    
**filename** (_str_ _,__optional_) – Name of the file to create. Defaults to “dump_snapshot.pickle”.
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
    * Active Memory Timeline
    * Allocator State History
  * Snapshot API Reference
    * `_record_memory_history()`
    * `_snapshot()`
    * `_dump_snapshot()`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us




---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.utils
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.utils
`rename_privateuse1_backend` | Rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.  
---|---  
`generate_methods_for_privateuse1_backend` | Automatically generate attributes and methods for the custom backend after rename privateuse1 backend.  
`get_cpp_backtrace` | Return a string containing the C++ stack trace of the current thread.  
`set_module` | Set the module attribute on a python object for a given object for nicer printing  
`swap_tensors` | This function swaps the content of the two Tensor objects.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.utils


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * torch.xpu
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# torch.xpu
This package introduces support for the XPU backend, specifically tailored for Intel GPU optimization.
This package is lazily initialized, so you can always import it, and use `is_available()` to determine if your system supports XPU.
`StreamContext` | Context-manager that selects a given stream.  
---|---  
`current_device` | Return the index of a currently selected device.  
`current_stream` | Return the currently selected `Stream` for a given device.  
`device` | Context-manager that changes the selected device.  
`device_count` | Return the number of XPU device available.  
`device_of` | Context-manager that changes the current device to that of given object.  
`get_arch_list` | Return list XPU architectures this library was compiled for.  
`get_device_capability` | Get the xpu capability of a device.  
`get_device_name` | Get the name of a device.  
`get_device_properties` | Get the properties of a device.  
`get_gencode_flags` | Return XPU AOT(ahead-of-time) build flags this library was compiled with.  
`get_stream_from_external` | Return a `Stream` from an external SYCL queue.  
`init` | Initialize PyTorch's XPU state.  
`is_available` | Return a bool indicating if XPU is currently available.  
`is_initialized` | Return whether PyTorch's XPU state has been initialized.  
`set_device` | Set the current device.  
`set_stream` | Set the current stream.This is a wrapper API to set the stream.  
`stream` | Wrap around the Context-manager StreamContext that selects a given stream.  
`synchronize` | Wait for all kernels in all streams on a XPU device to complete.  
## Random Number Generator
`get_rng_state` | Return the random number generator state of the specified GPU as a ByteTensor.  
---|---  
`get_rng_state_all` | Return a list of ByteTensor representing the random number states of all devices.  
`initial_seed` | Return the current random seed of the current GPU.  
`manual_seed` | Set the seed for generating random numbers for the current GPU.  
`manual_seed_all` | Set the seed for generating random numbers on all GPUs.  
`seed` | Set the seed for generating random numbers to a random number for the current GPU.  
`seed_all` | Set the seed for generating random numbers to a random number on all GPUs.  
`set_rng_state` | Set the random number generator state of the specified GPU.  
`set_rng_state_all` | Set the random number generator state of all devices.  
## Streams and events
`Event` | Wrapper around a XPU event.  
---|---  
`Stream` | Wrapper around a XPU stream.  
## Memory management
`empty_cache` | Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other XPU application.  
---|---  
`max_memory_allocated` | Return the maximum GPU memory occupied by tensors in bytes for a given device.  
`max_memory_reserved` | Return the maximum GPU memory managed by the caching allocator in bytes for a given device.  
`mem_get_info` | Return the global free and total GPU memory for a given device.  
`memory_allocated` | Return the current GPU memory occupied by tensors in bytes for a given device.  
`memory_reserved` | Return the current GPU memory managed by the caching allocator in bytes for a given device.  
`memory_stats` | Return a dictionary of XPU memory allocator statistics for a given device.  
`memory_stats_as_nested_dict` | Return the result of `memory_stats()` as a nested dictionary.  
`reset_accumulated_memory_stats` | Reset the "accumulated" (historical) stats tracked by the XPU memory allocator.  
`reset_peak_memory_stats` | Reset the "peak" stats tracked by the XPU memory allocator.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * torch.xpu
    * Random Number Generator
    * Streams and events
    * Memory management


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

  * Learn 
Get Started
Run PyTorch locally or get started quickly with one of the supported cloud platforms
Tutorials
Whats new in PyTorch tutorials
Learn the Basics
Familiarize yourself with PyTorch concepts and modules
PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples
Intro to PyTorch - YouTube Series
Master PyTorch basics with our engaging YouTube tutorial series
  * Ecosystem 
Tools
Learn about the tools and frameworks in the PyTorch Ecosystem
Community
Join the PyTorch developer community to contribute, learn, and get your questions answered
Forums
A place to discuss PyTorch code, issues, install, research
Developer Resources
Find resources and get questions answered
Contributor Awards - 2024
Award winners announced at this year's PyTorch Conference
  * Edge 
About PyTorch Edge
Build innovative and privacy-aware AI experiences for edge devices
ExecuTorch
End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
ExecuTorch Docs
  * Docs 
PyTorch
Explore the documentation for comprehensive guidance on how to use PyTorch
PyTorch Domains
Read the PyTorch Domains documentation to learn more about domain-specific libraries
  * Blogs & News 
PyTorch Blog
Catch up on the latest technical news and happenings
Community Blog
Stories from the PyTorch ecosystem
Videos
Learn about the latest PyTorch tutorials, new, and more 
Community Stories
Learn how our community solves real, everyday machine learning problems with PyTorch
Events
Find events, webinars, and podcasts
Newsletter
Stay up-to-date with the latest updates
  * About 
PyTorch Foundation
Learn more about the PyTorch Foundation
Governing Board Cloud Credit Program Technical Advisory Council Staff Contact Us
  * Become a Member 
  * 

Table of Contents
2.7 ▼
| | ×  
---|---  
search|   
Custom Search
| Sort by:RelevanceRelevanceDate  
---|---  
Google Search  Classic Search 
Community[ - ][ + ]
  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers


Developer Notes[ - ][ + ]
  * Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * PyTorch Custom Operators Landing Page
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * FSDP Notes
  * Getting Started on Intel GPU
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * LibTorch Stable ABI
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ


Language Bindings[ - ][ + ]
  * C++
  * Javadoc
  * torch::deploy


Python API[ - ][ + ]
  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.accelerator
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.xpu
  * torch.mtia
  * torch.mtia.memory
  * Meta device
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.tensor
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.fsdp.fully_shard
  * torch.distributed.tensor.parallel
  * torch.distributed.optim
  * torch.distributed.pipelining
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.fx.experimental
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.nn.attention
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.Size
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.deterministic
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * torch.utils.module_tracker
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch.__future__
  * torch._logging
  * Torch Environment Variables


Libraries[ - ][ + ]
  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices
  * torchao


  * Docs  >
  * Type Info
  * ![](https://pytorch.org/docs/stable/_static/images/view-page-source-icon.svg)


Shortcuts 
# Type Info
The numerical properties of a `torch.dtype` can be accessed through either the `torch.finfo` or the `torch.iinfo`.
## torch.finfo 

_class_ torch.finfo

A `torch.finfo` is an object that represents the numerical properties of a floating point `torch.dtype`, (i.e. `torch.float32`, `torch.float64`, `torch.float16`, and `torch.bfloat16`). This is similar to numpy.finfo.
A `torch.finfo` provides the following attributes:
Name | Type | Description  
---|---|---  
bits | int | The number of bits occupied by the type.  
eps | float | The smallest representable number such that `1.0 + eps != 1.0`.  
max | float | The largest representable number.  
min | float | The smallest representable number (typically `-max`).  
tiny | float | The smallest positive normal number. Equivalent to `smallest_normal`.  
smallest_normal | float | The smallest positive normal number. See notes.  
resolution | float | The approximate decimal resolution of this type, i.e., `10**-precision`.  
Note
The constructor of `torch.finfo` can be called without argument, in which case the class is created for the pytorch default dtype (as returned by `torch.get_default_dtype()`).
Note
smallest_normal returns the smallest _normal_ number, but there are smaller subnormal numbers. See https://en.wikipedia.org/wiki/Denormal_number for more information.
## torch.iinfo 

_class_ torch.iinfo

A `torch.iinfo` is an object that represents the numerical properties of a integer `torch.dtype` (i.e. `torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, and `torch.int64`). This is similar to numpy.iinfo.
A `torch.iinfo` provides the following attributes:
Name | Type | Description  
---|---|---  
bits | int | The number of bits occupied by the type.  
max | int | The largest representable number.  
min | int | The smallest representable number.  
Next ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) ![](https://pytorch.org/docs/stable/_static/images/chevron-right-orange.svg) Previous
© Copyright PyTorch Contributors. 
Built with Sphinx using a theme provided by Read the Docs. 
  * Type Info
    * torch.finfo
      * `torch.finfo`
    * torch.iinfo
      * `torch.iinfo`


![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)
## Docs
Access comprehensive developer documentation for PyTorch
View Docs
## Tutorials
Get in-depth tutorials for beginners and advanced developers
View Tutorials
## Resources
Find development resources and get your questions answered
View Resources
  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing


  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines


  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn


  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon


  * Terms
  * |
  * Privacy


© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy.
![](https://pytorch.org/docs/stable/_static/images/pytorch-x.svg)
  * Learn
    * Get Started
    * Tutorials
    * Learn the Basics
    * PyTorch Recipes
    * Introduction to PyTorch - YouTube Series
  * Ecosystem
    * Tools
    * Community
    * Forums
    * Developer Resources
    * Contributor Awards - 2024
  * Edge
    * About PyTorch Edge
    * ExecuTorch
    * ExecuTorch Documentation
  * Docs
    * PyTorch
    * PyTorch Domains
  * Blog & News
    * PyTorch Blog
    * Community Blog
    * Videos
    * Community Stories
    * Events
    * Newsletter
  * About
    * PyTorch Foundation
    * Governing Board
    * Cloud Credit Program
    * Technical Advisory Council
    * Staff
    * Contact Us


|   
---|---


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/_sources/accelerator.rst.txt) ![](https://pytorch.org/docs/stable/accelerator.html/_sources/accelerator.rst.txt)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/amp.html) ![](https://pytorch.org/docs/stable/accelerator.html/amp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/checkpoint.html) ![](https://pytorch.org/docs/stable/accelerator.html/checkpoint.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/backends.html) ![](https://pytorch.org/docs/stable/accelerator.html/backends.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/autograd.html) ![](https://pytorch.org/docs/stable/accelerator.html/autograd.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/bottleneck.html) ![](https://pytorch.org/docs/stable/accelerator.html/bottleneck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/community/contribution_guide.html) ![](https://pytorch.org/docs/stable/accelerator.html/community/contribution_guide.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/community/build_ci_governance.html) ![](https://pytorch.org/docs/stable/accelerator.html/community/build_ci_governance.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/benchmark_utils.html) ![](https://pytorch.org/docs/stable/accelerator.html/benchmark_utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/community/design.html) ![](https://pytorch.org/docs/stable/accelerator.html/community/design.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/community/governance.html) ![](https://pytorch.org/docs/stable/accelerator.html/community/governance.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/community/persons_of_interest.html) ![](https://pytorch.org/docs/stable/accelerator.html/community/persons_of_interest.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/complex_numbers.html) ![](https://pytorch.org/docs/stable/accelerator.html/complex_numbers.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/cpp_extension.html) ![](https://pytorch.org/docs/stable/accelerator.html/cpp_extension.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/cpu.html) ![](https://pytorch.org/docs/stable/accelerator.html/cpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/config_mod.html) ![](https://pytorch.org/docs/stable/accelerator.html/config_mod.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/cpp_index.html) ![](https://pytorch.org/docs/stable/accelerator.html/cpp_index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/cuda.html) ![](https://pytorch.org/docs/stable/accelerator.html/cuda.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/ddp_comm_hooks.html) ![](https://pytorch.org/docs/stable/accelerator.html/ddp_comm_hooks.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/data.html) ![](https://pytorch.org/docs/stable/accelerator.html/data.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/deploy.html) ![](https://pytorch.org/docs/stable/accelerator.html/deploy.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.algorithms.join.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.algorithms.join.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.elastic.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.elastic.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.fsdp.fully_shard.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.fsdp.fully_shard.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.checkpoint.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.checkpoint.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/deterministic.html) ![](https://pytorch.org/docs/stable/accelerator.html/deterministic.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.optim.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.optim.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.tensor.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.tensor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.pipelining.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.pipelining.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributed.tensor.parallel.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributed.tensor.parallel.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/distributions.html) ![](https://pytorch.org/docs/stable/accelerator.html/distributions.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/export.html) ![](https://pytorch.org/docs/stable/accelerator.html/export.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/dlpack.html) ![](https://pytorch.org/docs/stable/accelerator.html/dlpack.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/fft.html) ![](https://pytorch.org/docs/stable/accelerator.html/fft.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/func.html) ![](https://pytorch.org/docs/stable/accelerator.html/func.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/fsdp.html) ![](https://pytorch.org/docs/stable/accelerator.html/fsdp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/futures.html) ![](https://pytorch.org/docs/stable/accelerator.html/futures.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/future_mod.html) ![](https://pytorch.org/docs/stable/accelerator.html/future_mod.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/fx.experimental.html) ![](https://pytorch.org/docs/stable/accelerator.html/fx.experimental.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/fx.html) ![](https://pytorch.org/docs/stable/accelerator.html/fx.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_accelerator.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_accelerator.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_device_index.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_device_index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_device_idx.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_device_idx.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.device_count.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.device_count.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_stream.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.current_stream.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.is_available.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.is_available.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.set_device_index.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.set_device_index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.set_device_idx.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.set_device_idx.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.set_stream.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.set_stream.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/logging.html) ![](https://pytorch.org/docs/stable/accelerator.html/logging.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/masked.html) ![](https://pytorch.org/docs/stable/accelerator.html/masked.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.synchronize.html) ![](https://pytorch.org/docs/stable/accelerator.html/generated/torch.accelerator.synchronize.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/jit.html) ![](https://pytorch.org/docs/stable/accelerator.html/jit.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/hub.html) ![](https://pytorch.org/docs/stable/accelerator.html/hub.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/library.html) ![](https://pytorch.org/docs/stable/accelerator.html/library.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/jit_utils.html) ![](https://pytorch.org/docs/stable/accelerator.html/jit_utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/linalg.html) ![](https://pytorch.org/docs/stable/accelerator.html/linalg.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/index.html) ![](https://pytorch.org/docs/stable/accelerator.html/index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/meta.html) ![](https://pytorch.org/docs/stable/accelerator.html/meta.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/mtia.memory.html) ![](https://pytorch.org/docs/stable/accelerator.html/mtia.memory.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/name_inference.html) ![](https://pytorch.org/docs/stable/accelerator.html/name_inference.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/named_tensor.html) ![](https://pytorch.org/docs/stable/accelerator.html/named_tensor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/model_zoo.html) ![](https://pytorch.org/docs/stable/accelerator.html/model_zoo.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/monitor.html) ![](https://pytorch.org/docs/stable/accelerator.html/monitor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/mtia.html) ![](https://pytorch.org/docs/stable/accelerator.html/mtia.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/mps.html) ![](https://pytorch.org/docs/stable/accelerator.html/mps.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/module_tracker.html) ![](https://pytorch.org/docs/stable/accelerator.html/module_tracker.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/mobile_optimizer.html) ![](https://pytorch.org/docs/stable/accelerator.html/mobile_optimizer.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/nested.html) ![](https://pytorch.org/docs/stable/accelerator.html/nested.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/nn.functional.html) ![](https://pytorch.org/docs/stable/accelerator.html/nn.functional.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/amp_examples.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/amp_examples.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/autograd.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/autograd.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/nn.init.html) ![](https://pytorch.org/docs/stable/accelerator.html/nn.init.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/nn.attention.html) ![](https://pytorch.org/docs/stable/accelerator.html/nn.attention.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/nn.html) ![](https://pytorch.org/docs/stable/accelerator.html/nn.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/cpu_threading_torchscript_inference.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/cpu_threading_torchscript_inference.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/broadcasting.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/broadcasting.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/cuda.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/cuda.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/custom_operators.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/custom_operators.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/ddp.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/ddp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/extending.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/extending.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/extending.func.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/extending.func.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/fsdp.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/fsdp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/get_start_xpu.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/get_start_xpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/large_scale_deployments.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/large_scale_deployments.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/gradcheck.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/gradcheck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/faq.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/faq.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/hip.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/hip.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/libtorch_stable_abi.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/libtorch_stable_abi.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/optim.html) ![](https://pytorch.org/docs/stable/accelerator.html/optim.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/modules.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/modules.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/onnx.html) ![](https://pytorch.org/docs/stable/accelerator.html/onnx.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/windows.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/windows.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/multiprocessing.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/multiprocessing.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/mps.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/mps.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/serialization.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/serialization.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/numerical_accuracy.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/numerical_accuracy.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/notes/randomness.html) ![](https://pytorch.org/docs/stable/accelerator.html/notes/randomness.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/package.html) ![](https://pytorch.org/docs/stable/accelerator.html/package.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/quantization.html) ![](https://pytorch.org/docs/stable/accelerator.html/quantization.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/signal.html) ![](https://pytorch.org/docs/stable/accelerator.html/signal.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/size.html) ![](https://pytorch.org/docs/stable/accelerator.html/size.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/rpc.html) ![](https://pytorch.org/docs/stable/accelerator.html/rpc.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/profiler.html) ![](https://pytorch.org/docs/stable/accelerator.html/profiler.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/random.html) ![](https://pytorch.org/docs/stable/accelerator.html/random.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/sparse.html) ![](https://pytorch.org/docs/stable/accelerator.html/sparse.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/storage.html) ![](https://pytorch.org/docs/stable/accelerator.html/storage.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/special.html) ![](https://pytorch.org/docs/stable/accelerator.html/special.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/tensor_attributes.html) ![](https://pytorch.org/docs/stable/accelerator.html/tensor_attributes.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/tensor_view.html) ![](https://pytorch.org/docs/stable/accelerator.html/tensor_view.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/tensorboard.html) ![](https://pytorch.org/docs/stable/accelerator.html/tensorboard.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/tensors.html) ![](https://pytorch.org/docs/stable/accelerator.html/tensors.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/torch.compiler.html) ![](https://pytorch.org/docs/stable/accelerator.html/torch.compiler.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/testing.html) ![](https://pytorch.org/docs/stable/accelerator.html/testing.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/torch.html) ![](https://pytorch.org/docs/stable/accelerator.html/torch.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/torch_cuda_memory.html) ![](https://pytorch.org/docs/stable/accelerator.html/torch_cuda_memory.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/torch.overrides.html) ![](https://pytorch.org/docs/stable/accelerator.html/torch.overrides.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/torch_environment_variables.html) ![](https://pytorch.org/docs/stable/accelerator.html/torch_environment_variables.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/utils.html) ![](https://pytorch.org/docs/stable/accelerator.html/utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/_modules/torch/amp/autocast_mode.html) ![](https://pytorch.org/docs/stable/amp.html/_modules/torch/amp/autocast_mode.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/type_info.html) ![](https://pytorch.org/docs/stable/accelerator.html/type_info.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/accelerator.html/xpu.html) ![](https://pytorch.org/docs/stable/accelerator.html/xpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cpu/amp/autocast_mode.html) ![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cpu/amp/autocast_mode.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cpu/amp/grad_scaler.html) ![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cpu/amp/grad_scaler.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cuda/amp/autocast_mode.html) ![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cuda/amp/autocast_mode.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/autograd.html) ![](https://pytorch.org/docs/stable/amp.html/autograd.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/accelerator.html) ![](https://pytorch.org/docs/stable/amp.html/accelerator.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cuda/amp/grad_scaler.html) ![](https://pytorch.org/docs/stable/amp.html/_modules/torch/cuda/amp/grad_scaler.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/_sources/amp.rst.txt) ![](https://pytorch.org/docs/stable/amp.html/_sources/amp.rst.txt)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/backends.html) ![](https://pytorch.org/docs/stable/amp.html/backends.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/community/contribution_guide.html) ![](https://pytorch.org/docs/stable/amp.html/community/contribution_guide.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/community/persons_of_interest.html) ![](https://pytorch.org/docs/stable/amp.html/community/persons_of_interest.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/community/design.html) ![](https://pytorch.org/docs/stable/amp.html/community/design.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/community/governance.html) ![](https://pytorch.org/docs/stable/amp.html/community/governance.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/benchmark_utils.html) ![](https://pytorch.org/docs/stable/amp.html/benchmark_utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/bottleneck.html) ![](https://pytorch.org/docs/stable/amp.html/bottleneck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/checkpoint.html) ![](https://pytorch.org/docs/stable/amp.html/checkpoint.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/community/build_ci_governance.html) ![](https://pytorch.org/docs/stable/amp.html/community/build_ci_governance.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/complex_numbers.html) ![](https://pytorch.org/docs/stable/amp.html/complex_numbers.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/ddp_comm_hooks.html) ![](https://pytorch.org/docs/stable/amp.html/ddp_comm_hooks.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/deploy.html) ![](https://pytorch.org/docs/stable/amp.html/deploy.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/deterministic.html) ![](https://pytorch.org/docs/stable/amp.html/deterministic.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/config_mod.html) ![](https://pytorch.org/docs/stable/amp.html/config_mod.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/cpp_extension.html) ![](https://pytorch.org/docs/stable/amp.html/cpp_extension.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/cpp_index.html) ![](https://pytorch.org/docs/stable/amp.html/cpp_index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/data.html) ![](https://pytorch.org/docs/stable/amp.html/data.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/cuda.html) ![](https://pytorch.org/docs/stable/amp.html/cuda.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/cpu.html) ![](https://pytorch.org/docs/stable/amp.html/cpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.algorithms.join.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.algorithms.join.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributions.html) ![](https://pytorch.org/docs/stable/amp.html/distributions.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.tensor.parallel.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.tensor.parallel.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.tensor.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.tensor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.fsdp.fully_shard.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.fsdp.fully_shard.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.checkpoint.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.checkpoint.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.elastic.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.elastic.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.optim.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.optim.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/distributed.pipelining.html) ![](https://pytorch.org/docs/stable/amp.html/distributed.pipelining.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/dlpack.html) ![](https://pytorch.org/docs/stable/amp.html/dlpack.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/export.html) ![](https://pytorch.org/docs/stable/amp.html/export.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.BCELoss.html) ![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.BCELoss.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/fx.experimental.html) ![](https://pytorch.org/docs/stable/amp.html/fx.experimental.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/fx.html) ![](https://pytorch.org/docs/stable/amp.html/fx.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/fsdp.html) ![](https://pytorch.org/docs/stable/amp.html/fsdp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/fft.html) ![](https://pytorch.org/docs/stable/amp.html/fft.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/future_mod.html) ![](https://pytorch.org/docs/stable/amp.html/future_mod.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/func.html) ![](https://pytorch.org/docs/stable/amp.html/func.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/futures.html) ![](https://pytorch.org/docs/stable/amp.html/futures.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.BCEWithLogitsLoss.html) ![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.BCEWithLogitsLoss.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/index.html) ![](https://pytorch.org/docs/stable/amp.html/index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/jit.html) ![](https://pytorch.org/docs/stable/amp.html/jit.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/jit_utils.html) ![](https://pytorch.org/docs/stable/amp.html/jit_utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.DataParallel.html) ![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.DataParallel.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/hub.html) ![](https://pytorch.org/docs/stable/amp.html/hub.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.Module.html) ![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.Module.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.functional.binary_cross_entropy.html) ![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.functional.binary_cross_entropy.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.functional.binary_cross_entropy_with_logits.html) ![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.functional.binary_cross_entropy_with_logits.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.parallel.DistributedDataParallel.html) ![](https://pytorch.org/docs/stable/amp.html/generated/torch.nn.parallel.DistributedDataParallel.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/library.html) ![](https://pytorch.org/docs/stable/amp.html/library.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/module_tracker.html) ![](https://pytorch.org/docs/stable/amp.html/module_tracker.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/mps.html) ![](https://pytorch.org/docs/stable/amp.html/mps.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/monitor.html) ![](https://pytorch.org/docs/stable/amp.html/monitor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/masked.html) ![](https://pytorch.org/docs/stable/amp.html/masked.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/model_zoo.html) ![](https://pytorch.org/docs/stable/amp.html/model_zoo.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/mobile_optimizer.html) ![](https://pytorch.org/docs/stable/amp.html/mobile_optimizer.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/linalg.html) ![](https://pytorch.org/docs/stable/amp.html/linalg.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/meta.html) ![](https://pytorch.org/docs/stable/amp.html/meta.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/logging.html) ![](https://pytorch.org/docs/stable/amp.html/logging.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/mtia.html) ![](https://pytorch.org/docs/stable/amp.html/mtia.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/mtia.memory.html) ![](https://pytorch.org/docs/stable/amp.html/mtia.memory.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/name_inference.html) ![](https://pytorch.org/docs/stable/amp.html/name_inference.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/nn.attention.html) ![](https://pytorch.org/docs/stable/amp.html/nn.attention.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/nested.html) ![](https://pytorch.org/docs/stable/amp.html/nested.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/named_tensor.html) ![](https://pytorch.org/docs/stable/amp.html/named_tensor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/nn.functional.html) ![](https://pytorch.org/docs/stable/amp.html/nn.functional.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/nn.html) ![](https://pytorch.org/docs/stable/amp.html/nn.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/amp_examples.html) ![](https://pytorch.org/docs/stable/amp.html/notes/amp_examples.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/nn.init.html) ![](https://pytorch.org/docs/stable/amp.html/nn.init.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/autograd.html) ![](https://pytorch.org/docs/stable/amp.html/notes/autograd.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/broadcasting.html) ![](https://pytorch.org/docs/stable/amp.html/notes/broadcasting.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/cpu_threading_torchscript_inference.html) ![](https://pytorch.org/docs/stable/amp.html/notes/cpu_threading_torchscript_inference.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/cuda.html) ![](https://pytorch.org/docs/stable/amp.html/notes/cuda.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/extending.func.html) ![](https://pytorch.org/docs/stable/amp.html/notes/extending.func.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/ddp.html) ![](https://pytorch.org/docs/stable/amp.html/notes/ddp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/custom_operators.html) ![](https://pytorch.org/docs/stable/amp.html/notes/custom_operators.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/extending.html) ![](https://pytorch.org/docs/stable/amp.html/notes/extending.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/hip.html) ![](https://pytorch.org/docs/stable/amp.html/notes/hip.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/libtorch_stable_abi.html) ![](https://pytorch.org/docs/stable/amp.html/notes/libtorch_stable_abi.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/faq.html) ![](https://pytorch.org/docs/stable/amp.html/notes/faq.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/large_scale_deployments.html) ![](https://pytorch.org/docs/stable/amp.html/notes/large_scale_deployments.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/modules.html) ![](https://pytorch.org/docs/stable/amp.html/notes/modules.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/mps.html) ![](https://pytorch.org/docs/stable/amp.html/notes/mps.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/fsdp.html) ![](https://pytorch.org/docs/stable/amp.html/notes/fsdp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/get_start_xpu.html) ![](https://pytorch.org/docs/stable/amp.html/notes/get_start_xpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/gradcheck.html) ![](https://pytorch.org/docs/stable/amp.html/notes/gradcheck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/multiprocessing.html) ![](https://pytorch.org/docs/stable/amp.html/notes/multiprocessing.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/numerical_accuracy.html) ![](https://pytorch.org/docs/stable/amp.html/notes/numerical_accuracy.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/onnx.html) ![](https://pytorch.org/docs/stable/amp.html/onnx.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/serialization.html) ![](https://pytorch.org/docs/stable/amp.html/notes/serialization.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/windows.html) ![](https://pytorch.org/docs/stable/amp.html/notes/windows.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/notes/randomness.html) ![](https://pytorch.org/docs/stable/amp.html/notes/randomness.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/optim.html) ![](https://pytorch.org/docs/stable/amp.html/optim.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/package.html) ![](https://pytorch.org/docs/stable/amp.html/package.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/quantization.html) ![](https://pytorch.org/docs/stable/amp.html/quantization.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/profiler.html) ![](https://pytorch.org/docs/stable/amp.html/profiler.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/random.html) ![](https://pytorch.org/docs/stable/amp.html/random.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/rpc.html) ![](https://pytorch.org/docs/stable/amp.html/rpc.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/size.html) ![](https://pytorch.org/docs/stable/amp.html/size.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/tensor_attributes.html) ![](https://pytorch.org/docs/stable/amp.html/tensor_attributes.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/sparse.html) ![](https://pytorch.org/docs/stable/amp.html/sparse.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/special.html) ![](https://pytorch.org/docs/stable/amp.html/special.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/storage.html) ![](https://pytorch.org/docs/stable/amp.html/storage.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/signal.html) ![](https://pytorch.org/docs/stable/amp.html/signal.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/tensor_view.html) ![](https://pytorch.org/docs/stable/amp.html/tensor_view.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/testing.html) ![](https://pytorch.org/docs/stable/amp.html/testing.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/tensorboard.html) ![](https://pytorch.org/docs/stable/amp.html/tensorboard.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/tensors.html) ![](https://pytorch.org/docs/stable/amp.html/tensors.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/torch.html) ![](https://pytorch.org/docs/stable/amp.html/torch.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/torch.compiler.html) ![](https://pytorch.org/docs/stable/amp.html/torch.compiler.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/torch.overrides.html) ![](https://pytorch.org/docs/stable/amp.html/torch.overrides.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/torch_cuda_memory.html) ![](https://pytorch.org/docs/stable/amp.html/torch_cuda_memory.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/torch_environment_variables.html) ![](https://pytorch.org/docs/stable/amp.html/torch_environment_variables.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/xpu.html) ![](https://pytorch.org/docs/stable/amp.html/xpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/utils.html) ![](https://pytorch.org/docs/stable/amp.html/utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/amp.html/type_info.html) ![](https://pytorch.org/docs/stable/amp.html/type_info.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/function.html) ![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/function.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/graph.html) ![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/graph.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/anomaly_mode.html) ![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/anomaly_mode.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/profiler.html) ![](https://pytorch.org/docs/stable/autograd.html/_modules/torch/autograd/profiler.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/_sources/autograd.rst.txt) ![](https://pytorch.org/docs/stable/autograd.html/_sources/autograd.rst.txt)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/accelerator.html) ![](https://pytorch.org/docs/stable/autograd.html/accelerator.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/backends.html) ![](https://pytorch.org/docs/stable/autograd.html/backends.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/bottleneck.html) ![](https://pytorch.org/docs/stable/autograd.html/bottleneck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/checkpoint.html) ![](https://pytorch.org/docs/stable/autograd.html/checkpoint.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/amp.html) ![](https://pytorch.org/docs/stable/autograd.html/amp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/benchmark_utils.html) ![](https://pytorch.org/docs/stable/autograd.html/benchmark_utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/community/design.html) ![](https://pytorch.org/docs/stable/autograd.html/community/design.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/community/build_ci_governance.html) ![](https://pytorch.org/docs/stable/autograd.html/community/build_ci_governance.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/community/contribution_guide.html) ![](https://pytorch.org/docs/stable/autograd.html/community/contribution_guide.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/community/governance.html) ![](https://pytorch.org/docs/stable/autograd.html/community/governance.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/community/persons_of_interest.html) ![](https://pytorch.org/docs/stable/autograd.html/community/persons_of_interest.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/complex_numbers.html) ![](https://pytorch.org/docs/stable/autograd.html/complex_numbers.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/cpp_index.html) ![](https://pytorch.org/docs/stable/autograd.html/cpp_index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/cpp_extension.html) ![](https://pytorch.org/docs/stable/autograd.html/cpp_extension.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/config_mod.html) ![](https://pytorch.org/docs/stable/autograd.html/config_mod.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/data.html) ![](https://pytorch.org/docs/stable/autograd.html/data.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/cuda.html) ![](https://pytorch.org/docs/stable/autograd.html/cuda.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/deploy.html) ![](https://pytorch.org/docs/stable/autograd.html/deploy.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/cpu.html) ![](https://pytorch.org/docs/stable/autograd.html/cpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/ddp_comm_hooks.html) ![](https://pytorch.org/docs/stable/autograd.html/ddp_comm_hooks.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/deterministic.html) ![](https://pytorch.org/docs/stable/autograd.html/deterministic.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.optim.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.optim.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.pipelining.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.pipelining.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.tensor.parallel.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.tensor.parallel.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.tensor.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.tensor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.algorithms.join.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.algorithms.join.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.checkpoint.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.checkpoint.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.elastic.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.elastic.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.fsdp.fully_shard.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.fsdp.fully_shard.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributed.html) ![](https://pytorch.org/docs/stable/autograd.html/distributed.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/distributions.html) ![](https://pytorch.org/docs/stable/autograd.html/distributions.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/future_mod.html) ![](https://pytorch.org/docs/stable/autograd.html/future_mod.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/export.html) ![](https://pytorch.org/docs/stable/autograd.html/export.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/fft.html) ![](https://pytorch.org/docs/stable/autograd.html/fft.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/func.html) ![](https://pytorch.org/docs/stable/autograd.html/func.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/futures.html) ![](https://pytorch.org/docs/stable/autograd.html/futures.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/dlpack.html) ![](https://pytorch.org/docs/stable/autograd.html/dlpack.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/fx.html) ![](https://pytorch.org/docs/stable/autograd.html/fx.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/fx.experimental.html) ![](https://pytorch.org/docs/stable/autograd.html/fx.experimental.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/fsdp.html) ![](https://pytorch.org/docs/stable/autograd.html/fsdp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.Tensor.backward.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.Tensor.backward.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.backward.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.backward.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.UnpackedDualTensor.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.UnpackedDualTensor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.backward.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.backward.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.enter_dual_level.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.enter_dual_level.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.dual_level.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.dual_level.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.forward.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.forward.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.jvp.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.jvp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.vmap.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.Function.vmap.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.exit_dual_level.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.exit_dual_level.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.NestedIOFunction.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.NestedIOFunction.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.unpack_dual.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.unpack_dual.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.InplaceFunction.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.InplaceFunction.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.save_for_backward.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.save_for_backward.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.make_dual.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.forward_ad.make_dual.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.BackwardCFunction.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.BackwardCFunction.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.mark_dirty.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.FunctionCtx.mark_dirty.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.once_differentiable.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.function.once_differentiable.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.grad_mode.set_multithreading_enabled.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.grad_mode.set_multithreading_enabled.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.gradcheck.GradcheckError.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.gradcheck.GradcheckError.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.grad.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.grad.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.jacobian.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.jacobian.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.hvp.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.hvp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.hessian.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.hessian.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.jvp.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.jvp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.vhp.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.vhp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.vjp.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.functional.vjp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.gradcheck.gradcheck.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.gradcheck.gradcheck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.metadata.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.metadata.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.name.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.name.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.gradcheck.gradgradcheck.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.gradcheck.gradgradcheck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.register_hook.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.register_hook.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.register_prehook.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.register_prehook.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.KinetoStepTracker.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.KinetoStepTracker.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.next_functions.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.Node.next_functions.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.increment_version.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.graph.increment_version.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.EnforceUnique.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.EnforceUnique.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.load_nvprof.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.load_nvprof.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.parse_nvprof_trace.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.parse_nvprof_trace.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.key_averages.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.key_averages.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.export_chrome_trace.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.export_chrome_trace.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.record_function.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.record_function.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.total_average.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.total_average.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.Kernel.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.Kernel.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.self_cpu_time_total.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler.profile.self_cpu_time_total.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.Interval.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.Interval.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.MemRecordsAcc.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.MemRecordsAcc.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.StringTable.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.autograd.profiler_util.StringTable.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/library.html) ![](https://pytorch.org/docs/stable/autograd.html/library.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.ones.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.ones.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.vmap.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.vmap.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/jit.html) ![](https://pytorch.org/docs/stable/autograd.html/jit.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.randn.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.randn.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/jit_utils.html) ![](https://pytorch.org/docs/stable/autograd.html/jit_utils.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/index.html) ![](https://pytorch.org/docs/stable/autograd.html/index.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/generated/torch.zeros.html) ![](https://pytorch.org/docs/stable/autograd.html/generated/torch.zeros.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/hub.html) ![](https://pytorch.org/docs/stable/autograd.html/hub.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/linalg.html) ![](https://pytorch.org/docs/stable/autograd.html/linalg.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/logging.html) ![](https://pytorch.org/docs/stable/autograd.html/logging.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/meta.html) ![](https://pytorch.org/docs/stable/autograd.html/meta.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/mobile_optimizer.html) ![](https://pytorch.org/docs/stable/autograd.html/mobile_optimizer.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/masked.html) ![](https://pytorch.org/docs/stable/autograd.html/masked.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/model_zoo.html) ![](https://pytorch.org/docs/stable/autograd.html/model_zoo.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/mtia.html) ![](https://pytorch.org/docs/stable/autograd.html/mtia.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/mps.html) ![](https://pytorch.org/docs/stable/autograd.html/mps.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/monitor.html) ![](https://pytorch.org/docs/stable/autograd.html/monitor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/module_tracker.html) ![](https://pytorch.org/docs/stable/autograd.html/module_tracker.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/mtia.memory.html) ![](https://pytorch.org/docs/stable/autograd.html/mtia.memory.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/nn.html) ![](https://pytorch.org/docs/stable/autograd.html/nn.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/amp_examples.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/amp_examples.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/nn.attention.html) ![](https://pytorch.org/docs/stable/autograd.html/nn.attention.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/nn.init.html) ![](https://pytorch.org/docs/stable/autograd.html/nn.init.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/named_tensor.html) ![](https://pytorch.org/docs/stable/autograd.html/named_tensor.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/nested.html) ![](https://pytorch.org/docs/stable/autograd.html/nested.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/nn.functional.html) ![](https://pytorch.org/docs/stable/autograd.html/nn.functional.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/autograd.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/autograd.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/name_inference.html) ![](https://pytorch.org/docs/stable/autograd.html/name_inference.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/broadcasting.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/broadcasting.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/cuda.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/cuda.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/extending.func.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/extending.func.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/cpu_threading_torchscript_inference.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/cpu_threading_torchscript_inference.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/ddp.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/ddp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/custom_operators.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/custom_operators.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/extending.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/extending.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/fsdp.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/fsdp.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/large_scale_deployments.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/large_scale_deployments.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/faq.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/faq.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/get_start_xpu.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/get_start_xpu.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/hip.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/hip.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/gradcheck.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/gradcheck.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/libtorch_stable_abi.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/libtorch_stable_abi.html)


---

# 404
**File not found**
The site configured at this address does not contain the requested file. 
If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like `http://example.com/`) you must provide an `index.html` file. 
Read the full documentation for more information about using **GitHub Pages**. 
GitHub Status — @githubstatus
![](https://pytorch.org/docs/stable/autograd.html/notes/modules.html) ![](https://pytorch.org/docs/stable/autograd.html/notes/modules.html)
